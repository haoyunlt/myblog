---
title: "Python3 å¹¶å‘æ¨¡å‹å¯¹æ¯”æ·±åº¦æºç åˆ†æ"
date: 2025-09-28T01:46:41+08:00
draft: false
tags: ['æºç åˆ†æ', 'Python']
categories: ['Python']
description: "Python3 å¹¶å‘æ¨¡å‹å¯¹æ¯”æ·±åº¦æºç åˆ†æçš„æ·±å…¥æŠ€æœ¯åˆ†ææ–‡æ¡£"
keywords: ['æºç åˆ†æ', 'Python']
author: "æŠ€æœ¯åˆ†æå¸ˆ"
weight: 1
---

## ğŸ“‹ æ¦‚è¿°

Pythonæä¾›äº†å¤šç§å¹¶å‘ç¼–ç¨‹æ¨¡å‹æ¥å¤„ç†ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚æœ¬æ–‡æ¡£å°†æ·±å…¥åˆ†æCPythonä¸­å„ç§å¹¶å‘æ¨¡å‹çš„å®ç°æœºåˆ¶ï¼ŒåŒ…æ‹¬å¤šçº¿ç¨‹ã€å¤šè¿›ç¨‹ã€å¼‚æ­¥ç¼–ç¨‹ã€åç¨‹è°ƒåº¦ç­‰ï¼Œå¹¶å¯¹æ¯”å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€é€‚ç”¨åœºæ™¯å’Œæ€§èƒ½ç‰¹å¾ã€‚

## ğŸ¯ å¹¶å‘æ¨¡å‹æ¶æ„å¯¹æ¯”

```mermaid
graph TB
    subgraph "çº¿ç¨‹æ¨¡å‹"
        A[threadingæ¨¡å—] --> B[Threadç±»]
        B --> C[GILé™åˆ¶]
        C --> D[I/Oå¯†é›†é€‚ç”¨]
    end

    subgraph "è¿›ç¨‹æ¨¡å‹"
        E[multiprocessingæ¨¡å—] --> F[Processç±»]
        F --> G[çœŸæ­£å¹¶è¡Œ]
        G --> H[CPUå¯†é›†é€‚ç”¨]
    end

    subgraph "å¼‚æ­¥æ¨¡å‹"
        I[asyncioæ¨¡å—] --> J[äº‹ä»¶å¾ªç¯]
        J --> K[åç¨‹è°ƒåº¦]
        K --> L[é«˜å¹¶å‘I/O]
    end

    subgraph "æ··åˆæ¨¡å‹"
        M[concurrent.futures] --> N[ExecutoræŠ½è±¡]
        N --> O[çº¿ç¨‹æ± /è¿›ç¨‹æ± ]
        O --> P[ç»Ÿä¸€æ¥å£]
    end

    A --> E
    E --> I
    I --> M
```

## 1. å¤šçº¿ç¨‹æ¨¡å‹æ·±åº¦åˆ†æ

### 1.1 çº¿ç¨‹å®ç°æœºåˆ¶å¯¹æ¯”

```c
/* Modules/_threadmodule.c - çº¿ç¨‹åº•å±‚å®ç° */

/* çº¿ç¨‹å±€éƒ¨å­˜å‚¨å®ç° */
typedef struct {
    PyObject_HEAD
    PyObject *key;
    PyObject *args;
    PyObject *kw;
    PyObject *dict;
} localobject;

/* çº¿ç¨‹å±€éƒ¨å­˜å‚¨çš„è·å– */
static PyObject *
local_getattro(localobject *self, PyObject *name)
{
    PyObject *ldict, *value;

    /* è·å–å½“å‰çº¿ç¨‹çš„å­—å…¸ */
    ldict = _ldict(self);
    if (ldict == NULL)
        return NULL;

    if (self->dict != ldict) {
        /* åˆ‡æ¢åˆ°å½“å‰çº¿ç¨‹çš„ä¸Šä¸‹æ–‡ */
        Py_CLEAR(self->dict);
        Py_INCREF(ldict);
        self->dict = ldict;
    }

    /* ä»çº¿ç¨‹å­—å…¸ä¸­è·å–å€¼ */
    value = PyDict_GetItem(ldict, name);
    if (value == NULL) {
        /* å°è¯•ä»é»˜è®¤å€¼è·å– */
        value = PyObject_GenericGetAttr((PyObject *)self, name);
    } else {
        Py_INCREF(value);
    }

    return value;
}

/* çº¿ç¨‹é—´é€šä¿¡é˜Ÿåˆ—å®ç° */
typedef struct {
    PyObject_HEAD
    PyObject *queue;        /* å®é™…é˜Ÿåˆ—å¯¹è±¡ */
    Py_ssize_t maxsize;     /* æœ€å¤§å¤§å° */
    PyObject *mutex;        /* äº’æ–¥é” */
    PyObject *not_empty;    /* éç©ºæ¡ä»¶å˜é‡ */
    PyObject *not_full;     /* éæ»¡æ¡ä»¶å˜é‡ */
    PyObject *all_tasks_done; /* æ‰€æœ‰ä»»åŠ¡å®Œæˆæ¡ä»¶å˜é‡ */
    Py_ssize_t unfinished_tasks; /* æœªå®Œæˆä»»åŠ¡æ•° */
} QueueObject;
```

### 1.2 å¹¶å‘æ¨¡å‹å¯¹æ¯”å®ç°

```python
# å¹¶å‘æ¨¡å‹å¯¹æ¯”åˆ†æ
import threading
import multiprocessing
import asyncio
import concurrent.futures
import time
import queue
import os
from typing import List, Callable, Any
import psutil

class ConcurrencyModelComparison:
    """å¹¶å‘æ¨¡å‹å¯¹æ¯”åˆ†æå™¨"""

    def __init__(self):
        self.results = {}
        self.test_data = list(range(1000000))  # æµ‹è¯•æ•°æ®

    def cpu_intensive_task(self, n: int) -> int:
        """CPUå¯†é›†å‹ä»»åŠ¡"""
        result = 0
        for i in range(n):
            result += i * i
        return result

    def io_intensive_task(self, duration: float) -> str:
        """I/Oå¯†é›†å‹ä»»åŠ¡ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        time.sleep(duration)
        return f"Task completed in {duration}s"

    def network_simulation_task(self, request_id: int) -> dict:
        """ç½‘ç»œè¯·æ±‚æ¨¡æ‹Ÿ"""
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        time.sleep(0.1 + (request_id % 10) * 0.01)
        return {
            'id': request_id,
            'status': 'success',
            'data': f'Response for request {request_id}'
        }

    def compare_threading_models(self):
        """å¯¹æ¯”ä¸åŒçº¿ç¨‹æ¨¡å‹"""

        print("=== çº¿ç¨‹æ¨¡å‹å¯¹æ¯”åˆ†æ ===")

        # 1. åŸºç¡€çº¿ç¨‹æ¨¡å‹
        def basic_threading_test():
            """åŸºç¡€çº¿ç¨‹æµ‹è¯•"""

            results = []
            threads = []

            def worker(task_id):
                result = self.io_intensive_task(0.1)
                results.append(f"Thread-{task_id}: {result}")

            start_time = time.time()

            # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
            for i in range(10):
                thread = threading.Thread(target=worker, args=(i,))
                threads.append(thread)
                thread.start()

            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            for thread in threads:
                thread.join()

            end_time = time.time()
            return end_time - start_time, len(results)

        # 2. çº¿ç¨‹æ± æ¨¡å‹
        def thread_pool_test():
            """çº¿ç¨‹æ± æµ‹è¯•"""

            start_time = time.time()

            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(self.io_intensive_task, 0.1) for _ in range(10)]
                results = [future.result() for future in futures]

            end_time = time.time()
            return end_time - start_time, len(results)

        # 3. å¸¦é˜Ÿåˆ—çš„ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å‹
        def producer_consumer_test():
            """ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å‹æµ‹è¯•"""

            task_queue = queue.Queue(maxsize=50)
            result_queue = queue.Queue()

            def producer():
                for i in range(20):
                    task_queue.put(f"task-{i}")
                    time.sleep(0.01)  # æ¨¡æ‹Ÿç”Ÿäº§æ—¶é—´

                # æ·»åŠ ç»“æŸæ ‡å¿—
                for _ in range(4):  # 4ä¸ªæ¶ˆè´¹è€…
                    task_queue.put(None)

            def consumer():
                while True:
                    task = task_queue.get()
                    if task is None:
                        break

                    # å¤„ç†ä»»åŠ¡
                    time.sleep(0.05)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                    result_queue.put(f"Processed: {task}")
                    task_queue.task_done()

            start_time = time.time()

            # å¯åŠ¨ç”Ÿäº§è€…
            producer_thread = threading.Thread(target=producer)
            producer_thread.start()

            # å¯åŠ¨æ¶ˆè´¹è€…
            consumer_threads = []
            for i in range(4):
                consumer_thread = threading.Thread(target=consumer)
                consumer_threads.append(consumer_thread)
                consumer_thread.start()

            # ç­‰å¾…å®Œæˆ
            producer_thread.join()
            for thread in consumer_threads:
                thread.join()

            end_time = time.time()

            # æ”¶é›†ç»“æœ
            results = []
            while not result_queue.empty():
                results.append(result_queue.get())

            return end_time - start_time, len(results)

        # è¿è¡Œæµ‹è¯•
        basic_time, basic_count = basic_threading_test()
        pool_time, pool_count = thread_pool_test()
        pc_time, pc_count = producer_consumer_test()

        print(f"åŸºç¡€çº¿ç¨‹æ¨¡å‹: {basic_time:.3f}ç§’, ä»»åŠ¡æ•°: {basic_count}")
        print(f"çº¿ç¨‹æ± æ¨¡å‹: {pool_time:.3f}ç§’, ä»»åŠ¡æ•°: {pool_count}")
        print(f"ç”Ÿäº§è€…-æ¶ˆè´¹è€…: {pc_time:.3f}ç§’, ä»»åŠ¡æ•°: {pc_count}")

        self.results['threading'] = {
            'basic': basic_time,
            'pool': pool_time,
            'producer_consumer': pc_time
        }

    def compare_multiprocessing_models(self):
        """å¯¹æ¯”å¤šè¿›ç¨‹æ¨¡å‹"""

        print(f"\n=== å¤šè¿›ç¨‹æ¨¡å‹å¯¹æ¯”åˆ†æ ===")

        # 1. åŸºç¡€è¿›ç¨‹æ¨¡å‹
        def basic_multiprocessing_test():
            """åŸºç¡€å¤šè¿›ç¨‹æµ‹è¯•"""

            def worker(task_data):
                return self.cpu_intensive_task(task_data)

            start_time = time.time()

            processes = []
            manager = multiprocessing.Manager()
            result_list = manager.list()

            def process_worker(data, results):
                result = worker(data)
                results.append(result)

            # åˆ›å»ºè¿›ç¨‹
            test_tasks = [50000] * 4
            for i, task in enumerate(test_tasks):
                process = multiprocessing.Process(
                    target=process_worker,
                    args=(task, result_list)
                )
                processes.append(process)
                process.start()

            # ç­‰å¾…å®Œæˆ
            for process in processes:
                process.join()

            end_time = time.time()
            return end_time - start_time, len(result_list)

        # 2. è¿›ç¨‹æ± æ¨¡å‹
        def process_pool_test():
            """è¿›ç¨‹æ± æµ‹è¯•"""

            start_time = time.time()

            with multiprocessing.Pool(processes=4) as pool:
                tasks = [50000] * 4
                results = pool.map(self.cpu_intensive_task, tasks)

            end_time = time.time()
            return end_time - start_time, len(results)

        # 3. è¿›ç¨‹é—´é€šä¿¡æµ‹è¯•
        def ipc_test():
            """è¿›ç¨‹é—´é€šä¿¡æµ‹è¯•"""

            def worker_process(input_queue, output_queue):
                while True:
                    try:
                        task = input_queue.get(timeout=1)
                        if task is None:
                            break

                        result = self.cpu_intensive_task(task)
                        output_queue.put(result)

                    except queue.Empty:
                        break

            start_time = time.time()

            # åˆ›å»ºé˜Ÿåˆ—
            input_queue = multiprocessing.Queue()
            output_queue = multiprocessing.Queue()

            # æ·»åŠ ä»»åŠ¡
            tasks = [25000] * 8
            for task in tasks:
                input_queue.put(task)

            # æ·»åŠ ç»“æŸæ ‡å¿—
            for _ in range(2):  # 2ä¸ªå·¥ä½œè¿›ç¨‹
                input_queue.put(None)

            # åˆ›å»ºå·¥ä½œè¿›ç¨‹
            processes = []
            for i in range(2):
                process = multiprocessing.Process(
                    target=worker_process,
                    args=(input_queue, output_queue)
                )
                processes.append(process)
                process.start()

            # ç­‰å¾…å®Œæˆ
            for process in processes:
                process.join()

            # æ”¶é›†ç»“æœ
            results = []
            while not output_queue.empty():
                results.append(output_queue.get())

            end_time = time.time()
            return end_time - start_time, len(results)

        # è¿è¡Œæµ‹è¯•
        try:
            basic_time, basic_count = basic_multiprocessing_test()
            pool_time, pool_count = process_pool_test()
            ipc_time, ipc_count = ipc_test()

            print(f"åŸºç¡€è¿›ç¨‹æ¨¡å‹: {basic_time:.3f}ç§’, ä»»åŠ¡æ•°: {basic_count}")
            print(f"è¿›ç¨‹æ± æ¨¡å‹: {pool_time:.3f}ç§’, ä»»åŠ¡æ•°: {pool_count}")
            print(f"è¿›ç¨‹é—´é€šä¿¡: {ipc_time:.3f}ç§’, ä»»åŠ¡æ•°: {ipc_count}")

            self.results['multiprocessing'] = {
                'basic': basic_time,
                'pool': pool_time,
                'ipc': ipc_time
            }

        except Exception as e:
            print(f"å¤šè¿›ç¨‹æµ‹è¯•å‡ºé”™: {e}")
            self.results['multiprocessing'] = {'error': str(e)}

    async def compare_async_models(self):
        """å¯¹æ¯”å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹"""

        print(f"\n=== å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹å¯¹æ¯”åˆ†æ ===")

        # 1. åŸºç¡€åç¨‹æ¨¡å‹
        async def basic_async_test():
            """åŸºç¡€å¼‚æ­¥æµ‹è¯•"""

            async def async_task(task_id):
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¼‚æ­¥I/O
                return f"Async-{task_id}: completed"

            start_time = time.time()

            # å¹¶å‘æ‰§è¡Œå¤šä¸ªåç¨‹
            tasks = [async_task(i) for i in range(20)]
            results = await asyncio.gather(*tasks)

            end_time = time.time()
            return end_time - start_time, len(results)

        # 2. å¼‚æ­¥ç”Ÿæˆå™¨æ¨¡å‹
        async def async_generator_test():
            """å¼‚æ­¥ç”Ÿæˆå™¨æµ‹è¯•"""

            async def async_data_source():
                for i in range(50):
                    await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæ•°æ®è·å–å»¶è¿Ÿ
                    yield f"data-{i}"

            async def process_async_stream():
                results = []
                async for data in async_data_source():
                    # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
                    processed = f"processed-{data}"
                    results.append(processed)
                return results

            start_time = time.time()
            results = await process_async_stream()
            end_time = time.time()

            return end_time - start_time, len(results)

        # 3. å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        async def async_context_test():
            """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨æµ‹è¯•"""

            class AsyncResource:
                def __init__(self, name):
                    self.name = name
                    self.active = False

                async def __aenter__(self):
                    await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿèµ„æºè·å–
                    self.active = True
                    return self

                async def __aexit__(self, exc_type, exc_val, exc_tb):
                    await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿèµ„æºé‡Šæ”¾
                    self.active = False
                    return False

                async def process(self):
                    await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿå¤„ç†
                    return f"Processed by {self.name}"

            async def use_resources():
                results = []
                for i in range(10):
                    async with AsyncResource(f"resource-{i}") as resource:
                        result = await resource.process()
                        results.append(result)
                return results

            start_time = time.time()
            results = await use_resources()
            end_time = time.time()

            return end_time - start_time, len(results)

        # 4. æ··åˆå¼‚æ­¥æ¨¡å‹ï¼ˆå¼‚æ­¥+çº¿ç¨‹æ± ï¼‰
        async def hybrid_async_test():
            """æ··åˆå¼‚æ­¥æ¨¡å‹æµ‹è¯•"""

            def cpu_bound_task(n):
                return sum(i * i for i in range(n))

            async def async_coordinator():
                loop = asyncio.get_event_loop()

                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒCPUå¯†é›†å‹ä»»åŠ¡
                with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                    tasks = [
                        loop.run_in_executor(executor, cpu_bound_task, 10000)
                        for _ in range(8)
                    ]

                    # åŒæ—¶æ‰§è¡Œä¸€äº›å¼‚æ­¥I/Oä»»åŠ¡
                    async_tasks = [
                        asyncio.sleep(0.1) for _ in range(10)
                    ]

                    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                    cpu_results = await asyncio.gather(*tasks)
                    await asyncio.gather(*async_tasks)

                    return cpu_results

            start_time = time.time()
            results = await async_coordinator()
            end_time = time.time()

            return end_time - start_time, len(results)

        # è¿è¡Œå¼‚æ­¥æµ‹è¯•
        basic_time, basic_count = await basic_async_test()
        generator_time, generator_count = await async_generator_test()
        context_time, context_count = await async_context_test()
        hybrid_time, hybrid_count = await hybrid_async_test()

        print(f"åŸºç¡€åç¨‹æ¨¡å‹: {basic_time:.3f}ç§’, ä»»åŠ¡æ•°: {basic_count}")
        print(f"å¼‚æ­¥ç”Ÿæˆå™¨: {generator_time:.3f}ç§’, ä»»åŠ¡æ•°: {generator_count}")
        print(f"å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†: {context_time:.3f}ç§’, ä»»åŠ¡æ•°: {context_count}")
        print(f"æ··åˆå¼‚æ­¥æ¨¡å‹: {hybrid_time:.3f}ç§’, ä»»åŠ¡æ•°: {hybrid_count}")

        self.results['async'] = {
            'basic': basic_time,
            'generator': generator_time,
            'context': context_time,
            'hybrid': hybrid_time
        }

    def compare_concurrent_futures(self):
        """å¯¹æ¯”concurrent.futuresæ¨¡å‹"""

        print(f"\n=== concurrent.futuresç»Ÿä¸€æ¨¡å‹å¯¹æ¯” ===")

        # 1. ThreadPoolExecutor vs ProcessPoolExecutor
        def executor_comparison():
            """æ‰§è¡Œå™¨å¯¹æ¯”"""

            # I/Oå¯†é›†å‹ä»»åŠ¡æµ‹è¯•
            io_tasks = [0.05] * 20

            # ThreadPoolExecutor for I/O
            start_time = time.time()
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                thread_results = list(executor.map(self.io_intensive_task, io_tasks))
            thread_time = time.time() - start_time

            # CPUå¯†é›†å‹ä»»åŠ¡æµ‹è¯•
            cpu_tasks = [20000] * 4

            # ThreadPoolExecutor for CPU (å—GILé™åˆ¶)
            start_time = time.time()
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                thread_cpu_results = list(executor.map(self.cpu_intensive_task, cpu_tasks))
            thread_cpu_time = time.time() - start_time

            # ProcessPoolExecutor for CPU
            try:
                start_time = time.time()
                with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:
                    process_cpu_results = list(executor.map(self.cpu_intensive_task, cpu_tasks))
                process_cpu_time = time.time() - start_time
            except Exception as e:
                print(f"è¿›ç¨‹æ± æ‰§è¡Œå‡ºé”™: {e}")
                process_cpu_time = float('inf')
                process_cpu_results = []

            return {
                'thread_io': (thread_time, len(thread_results)),
                'thread_cpu': (thread_cpu_time, len(thread_cpu_results)),
                'process_cpu': (process_cpu_time, len(process_cpu_results))
            }

        # 2. as_completed vs wait
        def completion_strategies():
            """ä¸åŒå®Œæˆç­–ç•¥å¯¹æ¯”"""

            def variable_task(task_id):
                """å¯å˜æ—¶é•¿ä»»åŠ¡"""
                duration = 0.1 + (task_id % 5) * 0.02
                time.sleep(duration)
                return f"Task {task_id} completed in {duration:.2f}s"

            # as_completedç­–ç•¥
            start_time = time.time()
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                future_to_id = {executor.submit(variable_task, i): i for i in range(10)}

                as_completed_results = []
                for future in concurrent.futures.as_completed(future_to_id):
                    task_id = future_to_id[future]
                    result = future.result()
                    as_completed_results.append((task_id, result))

            as_completed_time = time.time() - start_time

            # waitç­–ç•¥
            start_time = time.time()
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = [executor.submit(variable_task, i) for i in range(10)]

                done, not_done = concurrent.futures.wait(futures, return_when=concurrent.futures.ALL_COMPLETED)
                wait_results = [future.result() for future in done]

            wait_time = time.time() - start_time

            return {
                'as_completed': (as_completed_time, len(as_completed_results)),
                'wait_all': (wait_time, len(wait_results))
            }

        # è¿è¡Œæµ‹è¯•
        executor_results = executor_comparison()
        completion_results = completion_strategies()

        print("æ‰§è¡Œå™¨å¯¹æ¯”:")
        for strategy, (time_taken, count) in executor_results.items():
            print(f"  {strategy}: {time_taken:.3f}ç§’, ä»»åŠ¡æ•°: {count}")

        print("å®Œæˆç­–ç•¥å¯¹æ¯”:")
        for strategy, (time_taken, count) in completion_results.items():
            print(f"  {strategy}: {time_taken:.3f}ç§’, ä»»åŠ¡æ•°: {count}")

        self.results['concurrent_futures'] = {
            'executors': executor_results,
            'completion': completion_results
        }

    def analyze_resource_usage(self):
        """åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ"""

        print(f"\n=== èµ„æºä½¿ç”¨åˆ†æ ===")

        try:
            # è·å–ç³»ç»Ÿä¿¡æ¯
            cpu_count = os.cpu_count()
            memory_info = psutil.virtual_memory()

            print(f"ç³»ç»ŸCPUæ ¸å¿ƒæ•°: {cpu_count}")
            print(f"ç³»ç»Ÿå†…å­˜: {memory_info.total / (1024**3):.1f} GB")
            print(f"å¯ç”¨å†…å­˜: {memory_info.available / (1024**3):.1f} GB")

            # åˆ†æä¸åŒå¹¶å‘æ¨¡å‹çš„èµ„æºä½¿ç”¨ç‰¹å¾
            recommendations = {
                'threading': {
                    'cpu_usage': 'low',
                    'memory_usage': 'low',
                    'best_for': 'I/Oå¯†é›†å‹ä»»åŠ¡',
                    'limitations': 'GILé™åˆ¶CPUå¹¶è¡Œæ€§'
                },
                'multiprocessing': {
                    'cpu_usage': 'high',
                    'memory_usage': 'high',
                    'best_for': 'CPUå¯†é›†å‹ä»»åŠ¡',
                    'limitations': 'è¿›ç¨‹åˆ›å»ºå¼€é”€å¤§'
                },
                'asyncio': {
                    'cpu_usage': 'very_low',
                    'memory_usage': 'low',
                    'best_for': 'é«˜å¹¶å‘I/Oä»»åŠ¡',
                    'limitations': 'ä¸é€‚åˆCPUå¯†é›†å‹'
                },
                'concurrent_futures': {
                    'cpu_usage': 'variable',
                    'memory_usage': 'variable',
                    'best_for': 'ç»Ÿä¸€çš„å¹¶å‘æ¥å£',
                    'limitations': 'æŠ½è±¡å±‚æœ‰å°‘é‡å¼€é”€'
                }
            }

            print("\nå¹¶å‘æ¨¡å‹ç‰¹å¾æ€»ç»“:")
            for model, chars in recommendations.items():
                print(f"{model}:")
                for key, value in chars.items():
                    print(f"  {key}: {value}")
                print()

            self.results['resource_analysis'] = recommendations

        except Exception as e:
            print(f"èµ„æºåˆ†æå‡ºé”™: {e}")

    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""

        print(f"\n=== æ€§èƒ½åˆ†ææŠ¥å‘Š ===")

        if not self.results:
            print("æ²¡æœ‰æ€§èƒ½æ•°æ®å¯åˆ†æ")
            return

        print("å„å¹¶å‘æ¨¡å‹æ€§èƒ½æ€»ç»“:")

        # åˆ†æçº¿ç¨‹æ¨¡å‹
        if 'threading' in self.results:
            threading_data = self.results['threading']
            print(f"\nçº¿ç¨‹æ¨¡å‹:")
            print(f"  æœ€å¿«ç­–ç•¥: {'çº¿ç¨‹æ± ' if threading_data.get('pool', float('inf')) < threading_data.get('basic', float('inf')) else 'åŸºç¡€çº¿ç¨‹'}")
            print(f"  æ¨èåœºæ™¯: I/Oå¯†é›†å‹ä»»åŠ¡ï¼Œä¸­ç­‰å¹¶å‘é‡")

        # åˆ†æå¤šè¿›ç¨‹æ¨¡å‹
        if 'multiprocessing' in self.results:
            mp_data = self.results['multiprocessing']
            if 'error' not in mp_data:
                print(f"\nå¤šè¿›ç¨‹æ¨¡å‹:")
                print(f"  è¿›ç¨‹æ± ä¼˜åŠ¿æ˜æ˜¾ï¼Œé€‚åˆCPUå¯†é›†å‹ä»»åŠ¡")
                print(f"  æ¨èåœºæ™¯: CPUå¯†é›†å‹ä»»åŠ¡ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸")

        # åˆ†æå¼‚æ­¥æ¨¡å‹
        if 'async' in self.results:
            async_data = self.results['async']
            print(f"\nå¼‚æ­¥æ¨¡å‹:")
            print(f"  åœ¨é«˜å¹¶å‘I/Oåœºæ™¯ä¸‹æ€§èƒ½ä¼˜å¼‚")
            print(f"  æ¨èåœºæ™¯: é«˜å¹¶å‘I/Oï¼Œç½‘ç»œåº”ç”¨")

        print(f"\né€‰æ‹©å»ºè®®:")
        print(f"1. I/Oå¯†é›†å‹ + ä¸­ç­‰å¹¶å‘ â†’ çº¿ç¨‹æ± ")
        print(f"2. CPUå¯†é›†å‹ + å¤šæ ¸åˆ©ç”¨ â†’ è¿›ç¨‹æ± ")
        print(f"3. é«˜å¹¶å‘I/O + ç½‘ç»œåº”ç”¨ â†’ asyncio")
        print(f"4. æ··åˆåœºæ™¯ + ç»Ÿä¸€æ¥å£ â†’ concurrent.futures")

    async def run_all_comparisons(self):
        """è¿è¡Œæ‰€æœ‰å¯¹æ¯”æµ‹è¯•"""

        print("å¼€å§‹å¹¶å‘æ¨¡å‹å¯¹æ¯”åˆ†æ...")

        # çº¿ç¨‹æ¨¡å‹å¯¹æ¯”
        self.compare_threading_models()

        # å¤šè¿›ç¨‹æ¨¡å‹å¯¹æ¯”
        self.compare_multiprocessing_models()

        # å¼‚æ­¥æ¨¡å‹å¯¹æ¯”
        await self.compare_async_models()

        # concurrent.futureså¯¹æ¯”
        self.compare_concurrent_futures()

        # èµ„æºä½¿ç”¨åˆ†æ
        self.analyze_resource_usage()

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_performance_report()

# è¿è¡Œå¹¶å‘æ¨¡å‹å¯¹æ¯”
async def main():
    analyzer = ConcurrencyModelComparison()
    await analyzer.run_all_comparisons()

if __name__ == "__main__":
    asyncio.run(main())
```

## 2. å¹¶å‘æ¨¡å‹æ¶æ„å¯¹æ¯”

### 2.1 GILå¯¹ä¸åŒæ¨¡å‹çš„å½±å“

```c
/* Python/ceval_gil.c - GILå¯¹å¹¶å‘æ¨¡å‹çš„å½±å“åˆ†æ */

/* GILçŠ¶æ€æ£€æŸ¥ */
static int
gil_check_for_concurrency_impact(PyThreadState *tstate)
{
    _PyRuntimeState *runtime = &_PyRuntime;
    struct _gil_runtime_state *gil = &runtime->gil;

    /* æ£€æŸ¥GILäº‰ç”¨æƒ…å†µ */
    if (_Py_atomic_load_relaxed(&gil->gil_drop_request)) {
        /* æœ‰çº¿ç¨‹åœ¨ç­‰å¾…GIL - è¡¨æ˜å­˜åœ¨äº‰ç”¨ */

        /* è®°å½•GILåˆ‡æ¢ç»Ÿè®¡ */
        gil->switch_number++;

        /* æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶é‡Šæ”¾ */
        if (gil->switch_number % 100 == 0) {
            /* æ¯100æ¬¡åˆ‡æ¢æ£€æŸ¥ä¸€æ¬¡æ€§èƒ½å½±å“ */
            return measure_gil_contention_impact(tstate);
        }
    }

    return 0;
}

/* æµ‹é‡GILäº‰ç”¨å¯¹æ€§èƒ½çš„å½±å“ */
static int
measure_gil_contention_impact(PyThreadState *tstate)
{
    static uint64_t last_switch_time = 0;
    uint64_t current_time = get_microsecond_timestamp();

    if (last_switch_time > 0) {
        uint64_t switch_interval = current_time - last_switch_time;

        /* å¦‚æœåˆ‡æ¢é—´éš”å¤ªçŸ­ï¼Œè¯´æ˜GILäº‰ç”¨ä¸¥é‡ */
        if (switch_interval < 1000) {  /* å°‘äº1ms */
            /* å»ºè®®ä½¿ç”¨å¤šè¿›ç¨‹æˆ–å¼‚æ­¥æ¨¡å‹ */
            return CONCURRENCY_RECOMMENDATION_MULTIPROCESSING;
        } else if (switch_interval < 5000) {  /* å°‘äº5ms */
            /* çº¿ç¨‹æ¨¡å‹å¯èƒ½ä¸æ˜¯æœ€ä¼˜é€‰æ‹© */
            return CONCURRENCY_RECOMMENDATION_ASYNC;
        }
    }

    last_switch_time = current_time;
    return CONCURRENCY_RECOMMENDATION_THREADING;
}
```

### 2.2 å¹¶å‘æ¨¡å‹é€‰æ‹©ç®—æ³•

```python
# å¹¶å‘æ¨¡å‹æ™ºèƒ½é€‰æ‹©ç³»ç»Ÿ
import time
import threading
import multiprocessing
import asyncio
import psutil
from typing import Dict, Any, Callable, Optional
from enum import Enum
import inspect

class ConcurrencyPattern(Enum):
    IO_BOUND = "io_bound"
    CPU_BOUND = "cpu_bound"
    MIXED = "mixed"
    NETWORK_HEAVY = "network_heavy"

class ConcurrencyModel(Enum):
    THREADING = "threading"
    MULTIPROCESSING = "multiprocessing"
    ASYNCIO = "asyncio"
    CONCURRENT_FUTURES = "concurrent_futures"

class ConcurrencySelector:
    """å¹¶å‘æ¨¡å‹æ™ºèƒ½é€‰æ‹©å™¨"""

    def __init__(self):
        self.system_info = self._gather_system_info()
        self.performance_history = {}

    def _gather_system_info(self) -> Dict[str, Any]:
        """æ”¶é›†ç³»ç»Ÿä¿¡æ¯"""
        return {
            'cpu_count': multiprocessing.cpu_count(),
            'memory_gb': psutil.virtual_memory().total / (1024**3),
            'python_version': (3, 11),  # å‡è®¾ç‰ˆæœ¬
            'gil_present': True
        }

    def analyze_workload(self, func: Callable, sample_args: tuple = ()) -> ConcurrencyPattern:
        """åˆ†æå·¥ä½œè´Ÿè½½æ¨¡å¼"""

        # åˆ†æå‡½æ•°ç‰¹å¾
        source_lines = inspect.getsource(func)

        # å¯å‘å¼åˆ†æ
        io_indicators = ['time.sleep', 'requests.', 'urllib', 'open(', 'read()', 'write()', 'socket']
        cpu_indicators = ['for ', 'while ', 'range(', 'sum(', 'math.', 'numpy', 'calculation']
        network_indicators = ['http', 'tcp', 'udp', 'websocket', 'api', 'request']

        io_score = sum(1 for indicator in io_indicators if indicator in source_lines)
        cpu_score = sum(1 for indicator in cpu_indicators if indicator in source_lines)
        network_score = sum(1 for indicator in network_indicators if indicator in source_lines)

        # è¿è¡Œæ—¶åˆ†æï¼ˆå¾®åŸºå‡†æµ‹è¯•ï¼‰
        if sample_args:
            runtime_pattern = self._runtime_analysis(func, sample_args)
        else:
            runtime_pattern = None

        # ç»¼åˆåˆ¤æ–­
        if network_score > 2:
            return ConcurrencyPattern.NETWORK_HEAVY
        elif cpu_score > io_score and cpu_score > 2:
            return ConcurrencyPattern.CPU_BOUND
        elif io_score > cpu_score and io_score > 2:
            return ConcurrencyPattern.IO_BOUND
        elif runtime_pattern:
            return runtime_pattern
        else:
            return ConcurrencyPattern.MIXED

    def _runtime_analysis(self, func: Callable, args: tuple) -> ConcurrencyPattern:
        """è¿è¡Œæ—¶è´Ÿè½½åˆ†æ"""

        import psutil
        import threading

        # è®°å½•CPUä½¿ç”¨ç‡
        cpu_before = psutil.cpu_percent(interval=0.1)

        # æ‰§è¡Œæ ·æœ¬ä»»åŠ¡
        start_time = time.time()
        try:
            func(*args)
        except:
            pass
        execution_time = time.time() - start_time

        cpu_after = psutil.cpu_percent(interval=0.1)
        cpu_usage = cpu_after - cpu_before

        # åˆ†ææ¨¡å¼
        if execution_time > 0.1 and cpu_usage > 50:
            return ConcurrencyPattern.CPU_BOUND
        elif execution_time > 0.1 and cpu_usage < 20:
            return ConcurrencyPattern.IO_BOUND
        else:
            return ConcurrencyPattern.MIXED

    def recommend_model(self, pattern: ConcurrencyPattern, task_count: int,
                       concurrent_limit: Optional[int] = None) -> ConcurrencyModel:
        """æ¨èå¹¶å‘æ¨¡å‹"""

        recommendations = {
            ConcurrencyPattern.CPU_BOUND: self._recommend_cpu_bound,
            ConcurrencyPattern.IO_BOUND: self._recommend_io_bound,
            ConcurrencyPattern.NETWORK_HEAVY: self._recommend_network_heavy,
            ConcurrencyPattern.MIXED: self._recommend_mixed
        }

        return recommendations[pattern](task_count, concurrent_limit)

    def _recommend_cpu_bound(self, task_count: int, concurrent_limit: Optional[int]) -> ConcurrencyModel:
        """CPUå¯†é›†å‹ä»»åŠ¡æ¨è"""

        if task_count <= self.system_info['cpu_count']:
            # ä»»åŠ¡æ•°å°‘äºCPUæ ¸å¿ƒæ•°ï¼Œç›´æ¥ä½¿ç”¨è¿›ç¨‹æ± 
            return ConcurrencyModel.MULTIPROCESSING
        elif self.system_info['memory_gb'] < 4:
            # å†…å­˜è¾ƒå°‘ï¼Œä½¿ç”¨çº¿ç¨‹æ± 
            return ConcurrencyModel.THREADING
        else:
            # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†CPUå¯†é›†å‹ä»»åŠ¡
            return ConcurrencyModel.MULTIPROCESSING

    def _recommend_io_bound(self, task_count: int, concurrent_limit: Optional[int]) -> ConcurrencyModel:
        """I/Oå¯†é›†å‹ä»»åŠ¡æ¨è"""

        if task_count > 1000:
            # å¤§é‡I/Oä»»åŠ¡ï¼Œä½¿ç”¨å¼‚æ­¥
            return ConcurrencyModel.ASYNCIO
        elif task_count > 50:
            # ä¸­ç­‰æ•°é‡ï¼Œä½¿ç”¨çº¿ç¨‹æ± 
            return ConcurrencyModel.CONCURRENT_FUTURES
        else:
            # å°‘é‡ä»»åŠ¡ï¼Œä½¿ç”¨åŸºç¡€çº¿ç¨‹
            return ConcurrencyModel.THREADING

    def _recommend_network_heavy(self, task_count: int, concurrent_limit: Optional[int]) -> ConcurrencyModel:
        """ç½‘ç»œå¯†é›†å‹ä»»åŠ¡æ¨è"""

        # ç½‘ç»œä»»åŠ¡å‡ ä¹æ€»æ˜¯æ¨èå¼‚æ­¥
        return ConcurrencyModel.ASYNCIO

    def _recommend_mixed(self, task_count: int, concurrent_limit: Optional[int]) -> ConcurrencyModel:
        """æ··åˆå‹ä»»åŠ¡æ¨è"""

        if task_count > 100:
            # å¤§é‡æ··åˆä»»åŠ¡ï¼Œä½¿ç”¨concurrent.futuresçš„ç»Ÿä¸€æ¥å£
            return ConcurrencyModel.CONCURRENT_FUTURES
        else:
            # å°‘é‡æ··åˆä»»åŠ¡ï¼Œä½¿ç”¨çº¿ç¨‹
            return ConcurrencyModel.THREADING

    def create_execution_strategy(self, func: Callable, tasks: list,
                                pattern: Optional[ConcurrencyPattern] = None) -> Dict[str, Any]:
        """åˆ›å»ºæ‰§è¡Œç­–ç•¥"""

        if pattern is None:
            pattern = self.analyze_workload(func, tasks[0] if tasks else ())

        model = self.recommend_model(pattern, len(tasks))

        strategy = {
            'pattern': pattern,
            'model': model,
            'task_count': len(tasks),
            'recommended_workers': self._calculate_worker_count(pattern, len(tasks)),
            'execution_function': self._get_execution_function(model)
        }

        return strategy

    def _calculate_worker_count(self, pattern: ConcurrencyPattern, task_count: int) -> int:
        """è®¡ç®—æ¨èçš„å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°"""

        if pattern == ConcurrencyPattern.CPU_BOUND:
            # CPUå¯†é›†å‹ï¼šé€šå¸¸ç­‰äºCPUæ ¸å¿ƒæ•°
            return min(self.system_info['cpu_count'], task_count)
        elif pattern == ConcurrencyPattern.IO_BOUND:
            # I/Oå¯†é›†å‹ï¼šå¯ä»¥è¶…è¿‡CPUæ ¸å¿ƒæ•°
            return min(self.system_info['cpu_count'] * 4, task_count, 50)
        elif pattern == ConcurrencyPattern.NETWORK_HEAVY:
            # ç½‘ç»œå¯†é›†å‹ï¼šå¼‚æ­¥æ¨¡å‹ä¸éœ€è¦é™åˆ¶
            return 1  # asyncioä½¿ç”¨å•çº¿ç¨‹äº‹ä»¶å¾ªç¯
        else:
            # æ··åˆå‹ï¼šä¿å®ˆä¼°è®¡
            return min(self.system_info['cpu_count'] * 2, task_count)

    def _get_execution_function(self, model: ConcurrencyModel) -> Callable:
        """è·å–æ‰§è¡Œå‡½æ•°"""

        execution_functions = {
            ConcurrencyModel.THREADING: self._execute_with_threading,
            ConcurrencyModel.MULTIPROCESSING: self._execute_with_multiprocessing,
            ConcurrencyModel.ASYNCIO: self._execute_with_asyncio,
            ConcurrencyModel.CONCURRENT_FUTURES: self._execute_with_concurrent_futures
        }

        return execution_functions[model]

    def _execute_with_threading(self, func: Callable, tasks: list, workers: int) -> list:
        """ä½¿ç”¨çº¿ç¨‹æ‰§è¡Œ"""

        import concurrent.futures

        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            if isinstance(tasks[0], tuple):
                # ä»»åŠ¡æ˜¯å‚æ•°å…ƒç»„
                futures = [executor.submit(func, *task) for task in tasks]
            else:
                # ä»»åŠ¡æ˜¯å•ä¸ªå‚æ•°
                futures = [executor.submit(func, task) for task in tasks]

            results = [future.result() for future in futures]

        return results

    def _execute_with_multiprocessing(self, func: Callable, tasks: list, workers: int) -> list:
        """ä½¿ç”¨å¤šè¿›ç¨‹æ‰§è¡Œ"""

        with multiprocessing.Pool(processes=workers) as pool:
            if isinstance(tasks[0], tuple):
                # ä½¿ç”¨starmapå¤„ç†å‚æ•°å…ƒç»„
                results = pool.starmap(func, tasks)
            else:
                # ä½¿ç”¨mapå¤„ç†å•ä¸ªå‚æ•°
                results = pool.map(func, tasks)

        return results

    def _execute_with_asyncio(self, func: Callable, tasks: list, workers: int) -> list:
        """ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œ"""

        async def async_wrapper():
            # æ£€æŸ¥å‡½æ•°æ˜¯å¦å·²ç»æ˜¯åç¨‹å‡½æ•°
            if asyncio.iscoroutinefunction(func):
                # ç›´æ¥ä½¿ç”¨åç¨‹å‡½æ•°
                if isinstance(tasks[0], tuple):
                    coroutines = [func(*task) for task in tasks]
                else:
                    coroutines = [func(task) for task in tasks]
            else:
                # å°†åŒæ­¥å‡½æ•°åŒ…è£…ä¸ºåç¨‹
                if isinstance(tasks[0], tuple):
                    coroutines = [asyncio.to_thread(func, *task) for task in tasks]
                else:
                    coroutines = [asyncio.to_thread(func, task) for task in tasks]

            return await asyncio.gather(*coroutines)

        return asyncio.run(async_wrapper())

    def _execute_with_concurrent_futures(self, func: Callable, tasks: list, workers: int) -> list:
        """ä½¿ç”¨concurrent.futuresæ‰§è¡Œ"""

        # æ ¹æ®ä»»åŠ¡ç‰¹å¾é€‰æ‹©executorç±»å‹
        pattern = self.analyze_workload(func, tasks[0] if tasks else ())

        if pattern == ConcurrencyPattern.CPU_BOUND:
            executor_class = concurrent.futures.ProcessPoolExecutor
        else:
            executor_class = concurrent.futures.ThreadPoolExecutor

        with executor_class(max_workers=workers) as executor:
            if isinstance(tasks[0], tuple):
                futures = [executor.submit(func, *task) for task in tasks]
            else:
                futures = [executor.submit(func, task) for task in tasks]

            results = [future.result() for future in futures]

        return results

# æ™ºèƒ½å¹¶å‘æ‰§è¡Œç¤ºä¾‹
def demonstrate_intelligent_concurrency():
    """æ¼”ç¤ºæ™ºèƒ½å¹¶å‘é€‰æ‹©"""

    print("=== æ™ºèƒ½å¹¶å‘æ¨¡å‹é€‰æ‹©æ¼”ç¤º ===")

    selector = ConcurrencySelector()

    # å®šä¹‰ä¸åŒç±»å‹çš„ä»»åŠ¡
    def cpu_task(n):
        """CPUå¯†é›†å‹ä»»åŠ¡"""
        return sum(i * i for i in range(n))

    def io_task(duration):
        """I/Oå¯†é›†å‹ä»»åŠ¡"""
        time.sleep(duration)
        return f"IO task completed in {duration}s"

    async def network_task(url_id):
        """ç½‘ç»œä»»åŠ¡ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        return f"Response from URL {url_id}"

    def mixed_task(data):
        """æ··åˆä»»åŠ¡"""
        # ä¸€äº›è®¡ç®—
        result = sum(range(data * 100))
        # ä¸€äº›I/O
        time.sleep(0.01)
        return result

    # æµ‹è¯•ä»»åŠ¡
    test_cases = [
        ("CPUå¯†é›†å‹", cpu_task, [10000] * 4),
        ("I/Oå¯†é›†å‹", io_task, [0.1] * 10),
        ("ç½‘ç»œå¯†é›†å‹", network_task, list(range(20))),
        ("æ··åˆå‹", mixed_task, [50] * 8)
    ]

    for task_name, task_func, task_data in test_cases:
        print(f"\nåˆ†æ {task_name} ä»»åŠ¡:")

        # åˆ†æå·¥ä½œè´Ÿè½½
        pattern = selector.analyze_workload(task_func, task_data[0:1])
        print(f"  æ£€æµ‹åˆ°çš„æ¨¡å¼: {pattern.value}")

        # åˆ›å»ºæ‰§è¡Œç­–ç•¥
        strategy = selector.create_execution_strategy(task_func, task_data, pattern)
        print(f"  æ¨èæ¨¡å‹: {strategy['model'].value}")
        print(f"  æ¨èå·¥ä½œè€…æ•°: {strategy['recommended_workers']}")

        # æ‰§è¡Œä»»åŠ¡
        start_time = time.time()
        try:
            if strategy['model'] == ConcurrencyModel.ASYNCIO and task_name == "ç½‘ç»œå¯†é›†å‹":
                # ç‰¹æ®Šå¤„ç†å¼‚æ­¥ç½‘ç»œä»»åŠ¡
                results = asyncio.run(asyncio.gather(*[network_task(i) for i in task_data]))
            else:
                execution_func = strategy['execution_function']
                results = execution_func(task_func, task_data, strategy['recommended_workers'])

            execution_time = time.time() - start_time
            print(f"  æ‰§è¡Œæ—¶é—´: {execution_time:.3f}ç§’")
            print(f"  å®Œæˆä»»åŠ¡æ•°: {len(results)}")

        except Exception as e:
            print(f"  æ‰§è¡Œå¤±è´¥: {e}")

# è¿è¡Œæ™ºèƒ½å¹¶å‘æ¼”ç¤º
if __name__ == "__main__":
    demonstrate_intelligent_concurrency()
```

## 3. æ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶

### 3.1 ç»¼åˆæ€§èƒ½æµ‹è¯•

```python
# å¹¶å‘æ¨¡å‹ç»¼åˆæ€§èƒ½æµ‹è¯•æ¡†æ¶
import time
import asyncio
import threading
import multiprocessing
import concurrent.futures
import statistics
import gc
from typing import Dict, List, Callable, Any, NamedTuple
from dataclasses import dataclass
from contextlib import contextmanager
import psutil
import os

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    model_name: str
    execution_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    task_count: int
    success_rate: float
    throughput: float  # ä»»åŠ¡/ç§’
    latency_stats: Dict[str, float]  # å»¶è¿Ÿç»Ÿè®¡

class ConcurrencyBenchmark:
    """å¹¶å‘æ¨¡å‹ç»¼åˆåŸºå‡†æµ‹è¯•"""

    def __init__(self):
        self.results: List[BenchmarkResult] = []
        self.baseline_memory = self._get_memory_usage()

    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024

    def _get_cpu_usage(self) -> float:
        """è·å–CPUä½¿ç”¨ç‡"""
        return psutil.cpu_percent(interval=0.1)

    @contextmanager
    def monitor_resources(self):
        """èµ„æºç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        start_memory = self._get_memory_usage()
        start_cpu = self._get_cpu_usage()
        start_time = time.perf_counter()

        yield

        end_time = time.perf_counter()
        end_memory = self._get_memory_usage()
        end_cpu = self._get_cpu_usage()

        self.last_execution_time = end_time - start_time
        self.last_memory_delta = end_memory - start_memory
        self.last_cpu_usage = (start_cpu + end_cpu) / 2

    def benchmark_threading_model(self, task_func: Callable, tasks: List, max_workers: int = 4):
        """åŸºå‡†æµ‹è¯•çº¿ç¨‹æ¨¡å‹"""

        # 1. åŸºç¡€çº¿ç¨‹
        def basic_threading():
            results = []
            latencies = []
            threads = []

            def worker(task_data):
                start = time.perf_counter()
                result = task_func(task_data)
                latency = time.perf_counter() - start
                results.append(result)
                latencies.append(latency)

            with self.monitor_resources():
                for task in tasks:
                    thread = threading.Thread(target=worker, args=(task,))
                    threads.append(thread)
                    thread.start()

                for thread in threads:
                    thread.join()

            return self._create_result("Basic Threading", results, latencies)

        # 2. çº¿ç¨‹æ± 
        def thread_pool():
            results = []
            latencies = []

            def timed_task(task_data):
                start = time.perf_counter()
                result = task_func(task_data)
                latency = time.perf_counter() - start
                return result, latency

            with self.monitor_resources():
                with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [executor.submit(timed_task, task) for task in tasks]
                    for future in concurrent.futures.as_completed(futures):
                        result, latency = future.result()
                        results.append(result)
                        latencies.append(latency)

            return self._create_result("Thread Pool", results, latencies)

        # 3. å¸¦é˜Ÿåˆ—çš„ç”Ÿäº§è€…-æ¶ˆè´¹è€…
        def producer_consumer():
            task_queue = queue.Queue()
            result_queue = queue.Queue()
            latencies = []

            def producer():
                for task in tasks:
                    task_queue.put((task, time.perf_counter()))
                for _ in range(max_workers):
                    task_queue.put(None)

            def consumer():
                while True:
                    item = task_queue.get()
                    if item is None:
                        break

                    task_data, start_time = item
                    result = task_func(task_data)
                    latency = time.perf_counter() - start_time
                    result_queue.put((result, latency))
                    task_queue.task_done()

            with self.monitor_resources():
                # å¯åŠ¨ç”Ÿäº§è€…
                producer_thread = threading.Thread(target=producer)
                producer_thread.start()

                # å¯åŠ¨æ¶ˆè´¹è€…
                consumers = []
                for _ in range(max_workers):
                    consumer_thread = threading.Thread(target=consumer)
                    consumers.append(consumer_thread)
                    consumer_thread.start()

                # ç­‰å¾…å®Œæˆ
                producer_thread.join()
                for consumer in consumers:
                    consumer.join()

                # æ”¶é›†ç»“æœ
                results = []
                while not result_queue.empty():
                    result, latency = result_queue.get()
                    results.append(result)
                    latencies.append(latency)

            return self._create_result("Producer-Consumer", results, latencies)

        return [basic_threading(), thread_pool(), producer_consumer()]

    def benchmark_multiprocessing_model(self, task_func: Callable, tasks: List, max_workers: int = None):
        """åŸºå‡†æµ‹è¯•å¤šè¿›ç¨‹æ¨¡å‹"""

        if max_workers is None:
            max_workers = min(4, multiprocessing.cpu_count())

        # 1. è¿›ç¨‹æ± 
        def process_pool():
            with self.monitor_resources():
                try:
                    with multiprocessing.Pool(processes=max_workers) as pool:
                        start_times = [time.perf_counter()] * len(tasks)
                        results = pool.map(task_func, tasks)
                        latencies = [(time.perf_counter() - start) / len(tasks)] * len(tasks)
                except Exception as e:
                    print(f"è¿›ç¨‹æ± æµ‹è¯•å¤±è´¥: {e}")
                    return self._create_error_result("Process Pool", str(e))

            return self._create_result("Process Pool", results, latencies)

        # 2. è¿›ç¨‹é—´é€šä¿¡
        def ipc_model():
            def worker_process(input_queue, output_queue):
                while True:
                    try:
                        item = input_queue.get(timeout=1)
                        if item is None:
                            break

                        task_data, start_time = item
                        result = task_func(task_data)
                        latency = time.perf_counter() - start_time
                        output_queue.put((result, latency))

                    except queue.Empty:
                        break

            with self.monitor_resources():
                try:
                    input_queue = multiprocessing.Queue()
                    output_queue = multiprocessing.Queue()

                    # æ·»åŠ ä»»åŠ¡
                    for task in tasks:
                        input_queue.put((task, time.perf_counter()))

                    # æ·»åŠ ç»“æŸæ ‡å¿—
                    for _ in range(max_workers):
                        input_queue.put(None)

                    # å¯åŠ¨å·¥ä½œè¿›ç¨‹
                    processes = []
                    for _ in range(max_workers):
                        process = multiprocessing.Process(
                            target=worker_process,
                            args=(input_queue, output_queue)
                        )
                        processes.append(process)
                        process.start()

                    # ç­‰å¾…å®Œæˆ
                    for process in processes:
                        process.join()

                    # æ”¶é›†ç»“æœ
                    results = []
                    latencies = []
                    while not output_queue.empty():
                        result, latency = output_queue.get()
                        results.append(result)
                        latencies.append(latency)

                except Exception as e:
                    print(f"IPCæµ‹è¯•å¤±è´¥: {e}")
                    return self._create_error_result("IPC Model", str(e))

            return self._create_result("IPC Model", results, latencies)

        return [process_pool(), ipc_model()]

    async def benchmark_async_model(self, task_func: Callable, tasks: List):
        """åŸºå‡†æµ‹è¯•å¼‚æ­¥æ¨¡å‹"""

        # 1. åŸºç¡€åç¨‹
        async def basic_coroutines():
            async def async_task_wrapper(task_data):
                start = time.perf_counter()
                if asyncio.iscoroutinefunction(task_func):
                    result = await task_func(task_data)
                else:
                    result = await asyncio.to_thread(task_func, task_data)
                latency = time.perf_counter() - start
                return result, latency

            with self.monitor_resources():
                coroutines = [async_task_wrapper(task) for task in tasks]
                results_with_latencies = await asyncio.gather(*coroutines)

            results = [r[0] for r in results_with_latencies]
            latencies = [r[1] for r in results_with_latencies]

            return self._create_result("Basic Coroutines", results, latencies)

        # 2. å¸¦ä¿¡å·é‡é™åˆ¶çš„åç¨‹
        async def semaphore_limited_coroutines():
            semaphore = asyncio.Semaphore(10)  # é™åˆ¶å¹¶å‘æ•°

            async def limited_task(task_data):
                async with semaphore:
                    start = time.perf_counter()
                    if asyncio.iscoroutinefunction(task_func):
                        result = await task_func(task_data)
                    else:
                        result = await asyncio.to_thread(task_func, task_data)
                    latency = time.perf_counter() - start
                    return result, latency

            with self.monitor_resources():
                coroutines = [limited_task(task) for task in tasks]
                results_with_latencies = await asyncio.gather(*coroutines)

            results = [r[0] for r in results_with_latencies]
            latencies = [r[1] for r in results_with_latencies]

            return self._create_result("Semaphore Limited", results, latencies)

        # 3. ä»»åŠ¡é˜Ÿåˆ—æ¨¡å‹
        async def task_queue_model():
            task_queue = asyncio.Queue(maxsize=50)
            result_queue = asyncio.Queue()

            async def producer():
                for task in tasks:
                    await task_queue.put((task, time.perf_counter()))

                # æ·»åŠ ç»“æŸæ ‡å¿—
                for _ in range(4):  # 4ä¸ªæ¶ˆè´¹è€…
                    await task_queue.put(None)

            async def consumer():
                while True:
                    item = await task_queue.get()
                    if item is None:
                        break

                    task_data, start_time = item
                    if asyncio.iscoroutinefunction(task_func):
                        result = await task_func(task_data)
                    else:
                        result = await asyncio.to_thread(task_func, task_data)
                    latency = time.perf_counter() - start_time

                    await result_queue.put((result, latency))
                    task_queue.task_done()

            with self.monitor_resources():
                # åˆ›å»ºç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…
                producer_task = asyncio.create_task(producer())
                consumer_tasks = [asyncio.create_task(consumer()) for _ in range(4)]

                # ç­‰å¾…å®Œæˆ
                await producer_task
                await asyncio.gather(*consumer_tasks)

                # æ”¶é›†ç»“æœ
                results = []
                latencies = []
                while not result_queue.empty():
                    result, latency = await result_queue.get()
                    results.append(result)
                    latencies.append(latency)

            return self._create_result("Task Queue", results, latencies)

        return [
            await basic_coroutines(),
            await semaphore_limited_coroutines(),
            await task_queue_model()
        ]

    def _create_result(self, model_name: str, results: List, latencies: List) -> BenchmarkResult:
        """åˆ›å»ºåŸºå‡†æµ‹è¯•ç»“æœ"""

        if not results:
            return self._create_error_result(model_name, "No results")

        success_rate = len(results) / len(latencies) if latencies else 0
        throughput = len(results) / self.last_execution_time if self.last_execution_time > 0 else 0

        latency_stats = {
            'min': min(latencies) if latencies else 0,
            'max': max(latencies) if latencies else 0,
            'mean': statistics.mean(latencies) if latencies else 0,
            'median': statistics.median(latencies) if latencies else 0,
            'stdev': statistics.stdev(latencies) if len(latencies) > 1 else 0
        }

        return BenchmarkResult(
            model_name=model_name,
            execution_time=self.last_execution_time,
            memory_usage_mb=self.last_memory_delta,
            cpu_usage_percent=self.last_cpu_usage,
            task_count=len(results),
            success_rate=success_rate,
            throughput=throughput,
            latency_stats=latency_stats
        )

    def _create_error_result(self, model_name: str, error_msg: str) -> BenchmarkResult:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""

        return BenchmarkResult(
            model_name=f"{model_name} (ERROR)",
            execution_time=float('inf'),
            memory_usage_mb=0,
            cpu_usage_percent=0,
            task_count=0,
            success_rate=0,
            throughput=0,
            latency_stats={'error': error_msg}
        )

    async def run_comprehensive_benchmark(self):
        """è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•"""

        print("å¼€å§‹ç»¼åˆå¹¶å‘æ¨¡å‹åŸºå‡†æµ‹è¯•...")

        # å®šä¹‰æµ‹è¯•ä»»åŠ¡
        def cpu_task(n):
            return sum(i * i for i in range(n))

        def io_task(duration):
            time.sleep(duration)
            return f"Completed in {duration}s"

        async def async_io_task(duration):
            await asyncio.sleep(duration)
            return f"Async completed in {duration}s"

        # æµ‹è¯•åœºæ™¯
        test_scenarios = [
            {
                'name': 'CPUå¯†é›†å‹',
                'task_func': cpu_task,
                'tasks': [10000] * 8,
                'async_func': None
            },
            {
                'name': 'I/Oå¯†é›†å‹',
                'task_func': io_task,
                'tasks': [0.1] * 20,
                'async_func': async_io_task
            }
        ]

        for scenario in test_scenarios:
            print(f"\n{'='*50}")
            print(f"æµ‹è¯•åœºæ™¯: {scenario['name']}")
            print(f"{'='*50}")

            # çº¿ç¨‹æ¨¡å‹æµ‹è¯•
            print("\nçº¿ç¨‹æ¨¡å‹æµ‹è¯•:")
            threading_results = self.benchmark_threading_model(
                scenario['task_func'],
                scenario['tasks']
            )

            for result in threading_results:
                self.results.append(result)
                self._print_result(result)

            # å¤šè¿›ç¨‹æ¨¡å‹æµ‹è¯•
            print("\nå¤šè¿›ç¨‹æ¨¡å‹æµ‹è¯•:")
            if scenario['name'] == 'CPUå¯†é›†å‹':
                mp_results = self.benchmark_multiprocessing_model(
                    scenario['task_func'],
                    scenario['tasks']
                )

                for result in mp_results:
                    self.results.append(result)
                    self._print_result(result)
            else:
                print("è·³è¿‡I/Oå¯†é›†å‹å¤šè¿›ç¨‹æµ‹è¯•ï¼ˆä¸é€‚ç”¨ï¼‰")

            # å¼‚æ­¥æ¨¡å‹æµ‹è¯•
            print("\nå¼‚æ­¥æ¨¡å‹æµ‹è¯•:")
            if scenario['async_func']:
                async_results = await self.benchmark_async_model(
                    scenario['async_func'],
                    scenario['tasks']
                )
            else:
                async_results = await self.benchmark_async_model(
                    scenario['task_func'],
                    scenario['tasks']
                )

            for result in async_results:
                self.results.append(result)
                self._print_result(result)

    def _print_result(self, result: BenchmarkResult):
        """æ‰“å°åŸºå‡†æµ‹è¯•ç»“æœ"""

        if 'error' in result.latency_stats:
            print(f"  {result.model_name}: ERROR - {result.latency_stats['error']}")
            return

        print(f"  {result.model_name}:")
        print(f"    æ‰§è¡Œæ—¶é—´: {result.execution_time:.3f}s")
        print(f"    ååé‡: {result.throughput:.1f} tasks/s")
        print(f"    å†…å­˜å¢é•¿: {result.memory_usage_mb:.1f} MB")
        print(f"    CPUä½¿ç”¨: {result.cpu_usage_percent:.1f}%")
        print(f"    å¹³å‡å»¶è¿Ÿ: {result.latency_stats['mean']:.4f}s")
        print(f"    æˆåŠŸç‡: {result.success_rate:.1%}")

    def generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""

        print(f"\n{'='*60}")
        print("ç»¼åˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        print(f"{'='*60}")

        # æŒ‰ååé‡æ’åº
        valid_results = [r for r in self.results if r.success_rate > 0]
        by_throughput = sorted(valid_results, key=lambda x: x.throughput, reverse=True)

        print(f"\næŒ‰ååé‡æ’åº (tasks/second):")
        for i, result in enumerate(by_throughput[:10], 1):
            print(f"{i:2d}. {result.model_name:25s}: {result.throughput:8.1f}")

        # æŒ‰å»¶è¿Ÿæ’åº
        by_latency = sorted(valid_results, key=lambda x: x.latency_stats['mean'])

        print(f"\næŒ‰å¹³å‡å»¶è¿Ÿæ’åº (seconds):")
        for i, result in enumerate(by_latency[:10], 1):
            print(f"{i:2d}. {result.model_name:25s}: {result.latency_stats['mean']:.6f}")

        # æŒ‰å†…å­˜æ•ˆç‡æ’åº
        by_memory = sorted(valid_results, key=lambda x: x.memory_usage_mb)

        print(f"\næŒ‰å†…å­˜ä½¿ç”¨æ’åº (MB):")
        for i, result in enumerate(by_memory[:10], 1):
            print(f"{i:2d}. {result.model_name:25s}: {result.memory_usage_mb:8.1f}")

        # æ¨èæ€»ç»“
        print(f"\n{'='*40}")
        print("æ¨èæ€»ç»“")
        print(f"{'='*40}")

        print("æœ€ä½³é€‰æ‹©å»ºè®®:")
        print(f"â€¢ æœ€é«˜ååé‡: {by_throughput[0].model_name}")
        print(f"â€¢ æœ€ä½å»¶è¿Ÿ: {by_latency[0].model_name}")
        print(f"â€¢ æœ€çœå†…å­˜: {by_memory[0].model_name}")

# è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•
async def run_benchmark():
    benchmark = ConcurrencyBenchmark()
    await benchmark.run_comprehensive_benchmark()
    benchmark.generate_comparison_report()

if __name__ == "__main__":
    import queue  # æ·»åŠ è¿™ä¸ªå¯¼å…¥
    asyncio.run(run_benchmark())
```

## 4. å¹¶å‘æ¨¡å‹æ—¶åºå¯¹æ¯”å›¾

```mermaid
sequenceDiagram
    participant App as åº”ç”¨ç¨‹åº
    participant TM as çº¿ç¨‹æ¨¡å‹
    participant PM as è¿›ç¨‹æ¨¡å‹
    participant AM as å¼‚æ­¥æ¨¡å‹
    participant OS as æ“ä½œç³»ç»Ÿ

    Note over App,OS: ä»»åŠ¡æäº¤é˜¶æ®µ
    App->>TM: æäº¤10ä¸ªI/Oä»»åŠ¡
    App->>PM: æäº¤4ä¸ªCPUä»»åŠ¡
    App->>AM: æäº¤100ä¸ªç½‘ç»œä»»åŠ¡

    Note over TM,OS: çº¿ç¨‹æ¨¡å‹æ‰§è¡Œ
    TM->>OS: åˆ›å»ºçº¿ç¨‹æ± (4ä¸ªçº¿ç¨‹)
    TM->>TM: GILç®¡ç†å’Œåˆ‡æ¢
    TM->>OS: ç³»ç»Ÿè°ƒç”¨(I/Oç­‰å¾…)
    OS-->>TM: I/Oå®Œæˆé€šçŸ¥
    TM-->>App: è¿”å›ç»“æœ

    Note over PM,OS: è¿›ç¨‹æ¨¡å‹æ‰§è¡Œ
    PM->>OS: åˆ›å»ºè¿›ç¨‹æ± (4ä¸ªè¿›ç¨‹)
    PM->>OS: è¿›ç¨‹é—´é€šä¿¡(IPC)
    PM->>PM: çœŸæ­£å¹¶è¡Œæ‰§è¡Œ
    PM-->>App: é€šè¿‡IPCè¿”å›ç»“æœ

    Note over AM,OS: å¼‚æ­¥æ¨¡å‹æ‰§è¡Œ
    AM->>AM: äº‹ä»¶å¾ªç¯å¯åŠ¨
    AM->>OS: éé˜»å¡I/Oè°ƒç”¨
    AM->>AM: åç¨‹è°ƒåº¦å’Œåˆ‡æ¢
    OS-->>AM: I/Oäº‹ä»¶é€šçŸ¥
    AM-->>App: å›è°ƒå¤„ç†ç»“æœ
```

## 5. æ€»ç»“

Pythonçš„å¹¶å‘æ¨¡å‹ä¸ºä¸åŒåœºæ™¯æä¾›äº†å¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆï¼š

### 5.1 æ¨¡å‹ç‰¹ç‚¹æ€»ç»“

1. **çº¿ç¨‹æ¨¡å‹**:
   - ä¼˜åŠ¿: å®ç°ç®€å•ï¼Œå†…å­˜å¼€é”€å°ï¼Œé€‚åˆI/Oå¯†é›†å‹
   - åŠ£åŠ¿: GILé™åˆ¶ï¼ŒCPUåˆ©ç”¨ç‡ä½

2. **è¿›ç¨‹æ¨¡å‹**:
   - ä¼˜åŠ¿: çœŸæ­£å¹¶è¡Œï¼ŒCPUåˆ©ç”¨ç‡é«˜ï¼Œéš”ç¦»æ€§å¥½
   - åŠ£åŠ¿: å†…å­˜å¼€é”€å¤§ï¼Œå¯åŠ¨æ…¢ï¼Œé€šä¿¡å¤æ‚

3. **å¼‚æ­¥æ¨¡å‹**:
   - ä¼˜åŠ¿: é«˜å¹¶å‘ï¼Œä½å¼€é”€ï¼Œé€‚åˆI/Oå¯†é›†å‹
   - åŠ£åŠ¿: å­¦ä¹ æ›²çº¿é™¡å³­ï¼Œä¸é€‚åˆCPUå¯†é›†å‹

4. **æ··åˆæ¨¡å‹**:
   - ä¼˜åŠ¿: ç»Ÿä¸€æ¥å£ï¼Œçµæ´»é€‰æ‹©
   - åŠ£åŠ¿: æŠ½è±¡å±‚å¼€é”€

### 5.2 é€‰æ‹©æŒ‡å—

- **I/Oå¯†é›† + ä¸­ç­‰å¹¶å‘** â†’ çº¿ç¨‹æ± 
- **CPUå¯†é›† + å¤šæ ¸åˆ©ç”¨** â†’ è¿›ç¨‹æ± 
- **é«˜å¹¶å‘I/O + ç½‘ç»œåº”ç”¨** â†’ asyncio
- **æ··åˆåœºæ™¯ + ç»Ÿä¸€æ¥å£** â†’ concurrent.futures

### 5.3 æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä»»åŠ¡åˆ†ç±»**: å‡†ç¡®è¯†åˆ«å·¥ä½œè´Ÿè½½ç‰¹å¾
2. **åˆç†é…ç½®**: æ ¹æ®ç³»ç»Ÿèµ„æºè°ƒæ•´å¹¶å‘å‚æ•°
3. **é¿å…é™·é˜±**: æ³¨æ„GILã€å†…å­˜æ³„æ¼ã€æ­»é”ç­‰é—®é¢˜
4. **æ€§èƒ½ç›‘æ§**: æŒç»­ç›‘æ§å’Œä¼˜åŒ–å¹¶å‘æ€§èƒ½

Pythonçš„å¤šæ ·åŒ–å¹¶å‘æ¨¡å‹ä¸ºå¼€å‘è€…æä¾›äº†å¼ºå¤§çš„å·¥å…·ç®±ï¼Œåˆç†é€‰æ‹©å’Œä½¿ç”¨è¿™äº›æ¨¡å‹æ˜¯æ„å»ºé«˜æ€§èƒ½åº”ç”¨çš„å…³é”®ã€‚
