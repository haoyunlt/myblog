---
title: "TensorRT-LLM æ¡†æ¶ä½¿ç”¨æ‰‹å†Œ"
date: 2024-12-19T10:00:00+08:00
draft: false
tags: ['TensorRT-LLM', 'NVIDIA', 'æ¨ç†ä¼˜åŒ–', 'æ·±åº¦å­¦ä¹ ']
categories: ["tensorrtllm", "æŠ€æœ¯åˆ†æ"]
description: "æ·±å…¥åˆ†æ TensorRT-LLM æ¡†æ¶ä½¿ç”¨æ‰‹å†Œ çš„æŠ€æœ¯å®ç°å’Œæ¶æ„è®¾è®¡"
weight: 580
slug: "TensorRTLLM-æºç å‰–æ_æ¡†æ¶ä½¿ç”¨æ‰‹å†Œ"
---

# TensorRT-LLM æ¡†æ¶ä½¿ç”¨æ‰‹å†Œ

## æ¦‚è¿°

TensorRT-LLM æ˜¯ NVIDIA å¼€æºçš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¼˜åŒ–åº“ï¼ŒåŸºäº PyTorch æ¶æ„ï¼Œæä¾›é«˜æ€§èƒ½çš„ LLM æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ”¯æŒä»å• GPU åˆ°å¤š GPUã€å¤šèŠ‚ç‚¹çš„éƒ¨ç½²ï¼Œå†…ç½®å¤šç§å¹¶è¡Œç­–ç•¥å’Œé«˜çº§ä¼˜åŒ–ç‰¹æ€§ã€‚

## æ ¸å¿ƒç‰¹æ€§

### ğŸ”¥ åŸºäº PyTorch æ¶æ„
- é«˜çº§ Python LLM APIï¼Œæ”¯æŒå¹¿æ³›çš„æ¨ç†è®¾ç½®
- å†…ç½®å¤šç§å¹¶è¡Œç­–ç•¥æ”¯æŒ
- ä¸ NVIDIA Dynamo å’Œ Triton Inference Server æ— ç¼é›†æˆ

### âš¡ é¡¶çº§æ€§èƒ½
- åœ¨æœ€æ–° NVIDIA GPU ä¸Šæä¾›çªç ´æ€§æ€§èƒ½
- DeepSeek R1ï¼šåœ¨ Blackwell GPU ä¸Šåˆ›ä¸–ç•Œçºªå½•æ¨ç†æ€§èƒ½
- Llama 4 Maverickï¼šåœ¨ B200 GPU ä¸Šçªç ´ 1,000 TPS/ç”¨æˆ·å±éšœ

### ğŸ¯ å…¨é¢æ¨¡å‹æ”¯æŒ
- æ”¯æŒæœ€æ–°å’Œæœ€æµè¡Œçš„ LLM æ¶æ„
- FP4 æ ¼å¼æ”¯æŒï¼ˆNVIDIA B200 GPUï¼‰
- è‡ªåŠ¨åˆ©ç”¨ä¼˜åŒ–çš„ FP4 å†…æ ¸

## å®‰è£…æŒ‡å—

### ç³»ç»Ÿè¦æ±‚
- Python 3.10-3.12
- CUDA 13.0.0+
- TensorRT 10.13.2+
- PyTorch 2.0+

### å®‰è£…æ–¹å¼

#### 1. é€šè¿‡ pip å®‰è£…ï¼ˆæ¨èï¼‰
```bash
pip install tensorrt-llm --extra-index-url https://pypi.nvidia.com
```

#### 2. ä»æºç æ„å»º
```bash
git clone https://github.com/NVIDIA/TensorRT-LLM.git
cd TensorRT-LLM
pip install -e .
```

## å¿«é€Ÿå¼€å§‹

### åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

```python
from tensorrt_llm import LLM

# åˆå§‹åŒ– LLM
llm = LLM(model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")

# ç”Ÿæˆæ–‡æœ¬
output = llm.generate("Hello, my name is")
print(output)
```

### é«˜çº§é…ç½®ç¤ºä¾‹

```python
from tensorrt_llm import LLM
from tensorrt_llm.llmapi import BuildConfig, KvCacheConfig, SamplingParams

# æ„å»ºé…ç½®
build_config = BuildConfig(
    max_batch_size=8,
    max_input_len=256,
    max_seq_len=512,
    max_beam_width=4
)

# KV ç¼“å­˜é…ç½®
kv_cache_config = KvCacheConfig(
    free_gpu_memory_fraction=0.9,
    enable_block_reuse=True
)

# åˆå§‹åŒ– LLM
llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    tensor_parallel_size=2,
    build_config=build_config,
    kv_cache_config=kv_cache_config
)

# é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    max_tokens=100,
    temperature=0.8,
    top_p=0.9
)

# ç”Ÿæˆ
outputs = llm.generate(
    ["Tell me about artificial intelligence"],
    sampling_params=sampling_params
)
```

### å®Œæ•´ç”Ÿäº§ç¯å¢ƒç¤ºä¾‹

```python
import torch
from tensorrt_llm import LLM
from tensorrt_llm.llmapi import (
    BuildConfig, KvCacheConfig, SamplingParams,
    QuantConfig, QuantAlgo, LoRARequest
)

class ProductionLLMService:
    """ç”Ÿäº§ç¯å¢ƒ LLM æœåŠ¡å°è£…"""

    def __init__(self, model_path: str, config: dict):
        """
        åˆå§‹åŒ–ç”Ÿäº§çº§ LLM æœåŠ¡

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            config: é…ç½®å­—å…¸
        """
        self.model_path = model_path
        self.config = config
        self.llm = None
        self._init_llm()

    def _init_llm(self):
        """åˆå§‹åŒ– LLM å®ä¾‹"""

        # æ„å»ºé…ç½®
        build_config = BuildConfig(
            max_batch_size=self.config.get('max_batch_size', 16),
            max_input_len=self.config.get('max_input_len', 2048),
            max_seq_len=self.config.get('max_seq_len', 4096),
            max_beam_width=self.config.get('max_beam_width', 1),
            strongly_typed=True,
            use_refit=True,  # æ”¯æŒæƒé‡æ›´æ–°
            weight_streaming=self.config.get('weight_streaming', False)
        )

        # KV ç¼“å­˜é…ç½®
        kv_cache_config = KvCacheConfig(
            free_gpu_memory_fraction=self.config.get('kv_cache_fraction', 0.85),
            enable_block_reuse=True,
            max_tokens_in_paged_kv_cache=None  # è‡ªåŠ¨è®¡ç®—
        )

        # é‡åŒ–é…ç½®
        quant_config = None
        if self.config.get('enable_quantization', False):
            quant_config = QuantConfig(
                quant_algo=QuantAlgo.FP8,
                kv_cache_quant_algo=QuantAlgo.INT8,
                group_size=128
            )

        # åˆå§‹åŒ– LLM
        self.llm = LLM(
            model=self.model_path,
            tensor_parallel_size=self.config.get('tensor_parallel_size', 1),
            pipeline_parallel_size=self.config.get('pipeline_parallel_size', 1),
            build_config=build_config,
            kv_cache_config=kv_cache_config,
            quant_config=quant_config,
            trust_remote_code=True
        )

    def generate_text(self,
                     prompts: list,
                     max_tokens: int = 100,
                     temperature: float = 0.8,
                     top_p: float = 0.9,
                     lora_request: LoRARequest = None) -> list:
        """
        ç”Ÿæˆæ–‡æœ¬

        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-p é‡‡æ ·å‚æ•°
            lora_request: LoRA è¯·æ±‚ï¼ˆå¯é€‰ï¼‰

        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨
        """

        # é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=1.1,
            length_penalty=1.0
        )

        try:
            # æ‰§è¡Œç”Ÿæˆ
            outputs = self.llm.generate(
                prompts,
                sampling_params=sampling_params,
                lora_request=lora_request
            )

            # æå–ç”Ÿæˆæ–‡æœ¬
            results = []
            for output in outputs:
                generated_text = output.outputs[0].text
                results.append({
                    'prompt': output.prompt,
                    'generated_text': generated_text,
                    'finish_reason': output.outputs[0].finish_reason,
                    'token_ids': output.outputs[0].token_ids
                })

            return results

        except Exception as e:
            print(f"ç”Ÿæˆå¤±è´¥: {e}")
            return []

    def generate_streaming(self, prompt: str, **kwargs):
        """æµå¼ç”Ÿæˆ"""
        sampling_params = SamplingParams(**kwargs)

        for output in self.llm.generate(
            prompt,
            sampling_params=sampling_params,
            streaming=True
        ):
            yield output.outputs[0].text

    def shutdown(self):
        """ä¼˜é›…å…³é—­"""
        if self.llm:
            self.llm.shutdown()

# ä½¿ç”¨ç¤ºä¾‹
config = {
    'max_batch_size': 8,
    'max_input_len': 1024,
    'max_seq_len': 2048,
    'tensor_parallel_size': 2,
    'enable_quantization': True,
    'kv_cache_fraction': 0.8
}

service = ProductionLLMService("meta-llama/Llama-2-7b-hf", config)

# æ‰¹é‡ç”Ÿæˆ
prompts = [
    "è§£é‡Šäººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µ",
    "æè¿°æ·±åº¦å­¦ä¹ çš„å·¥ä½œåŸç†",
    "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ"
]

results = service.generate_text(
    prompts,
    max_tokens=150,
    temperature=0.7
)

for result in results:
    print(f"è¾“å…¥: {result['prompt']}")
    print(f"è¾“å‡º: {result['generated_text']}")
    print("-" * 50)
```

### å¤šæ¨¡æ€ä½¿ç”¨ç¤ºä¾‹

```python
from tensorrt_llm import LLM
from tensorrt_llm.llmapi import MultimodalEncoder

# åˆå§‹åŒ–å¤šæ¨¡æ€ç¼–ç å™¨
mm_encoder = MultimodalEncoder(
    vision_encoder_path="/path/to/vision/encoder"
)

# å¤šæ¨¡æ€ LLM
llm = LLM(
    model="llava-v1.5-7b",
    multimodal_encoder=mm_encoder
)

# å›¾åƒ+æ–‡æœ¬è¾“å…¥
from PIL import Image
image = Image.open("example.jpg")
text_prompt = "æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹"

output = llm.generate(
    inputs={"text": text_prompt, "image": image},
    sampling_params=SamplingParams(max_tokens=200)
)
```

## å‘½ä»¤è¡Œå·¥å…·

TensorRT-LLM æä¾›äº†å¤šä¸ªå‘½ä»¤è¡Œå·¥å…·ï¼š

### 1. trtllm-build - æ„å»ºå¼•æ“
```bash
trtllm-build --model_dir /path/to/model \
             --output_dir /path/to/engine \
             --max_batch_size 8 \
             --max_input_len 1024 \
             --max_seq_len 2048
```

### 2. trtllm-serve - å¯åŠ¨æœåŠ¡
```bash
trtllm-serve --model /path/to/model \
             --host 0.0.0.0 \
             --port 8000 \
             --tp_size 2
```

### 3. trtllm-bench - æ€§èƒ½æµ‹è¯•
```bash
trtllm-bench --model /path/to/model \
             --batch_size 8 \
             --input_len 128 \
             --output_len 128
```

### 4. trtllm-eval - æ¨¡å‹è¯„ä¼°
```bash
trtllm-eval --model /path/to/model \
            --backend tensorrt \
            --tp_size 2
```

## é…ç½®å‚æ•°è¯¦è§£

### BuildConfig å‚æ•°
- `max_input_len`: æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆé»˜è®¤ 1024ï¼‰
- `max_seq_len`: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ Noneï¼Œè‡ªåŠ¨æ¨å¯¼ï¼‰
- `max_batch_size`: æœ€å¤§æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ 2048ï¼‰
- `max_beam_width`: æœ€å¤§æŸæœç´¢å®½åº¦ï¼ˆé»˜è®¤ 1ï¼‰
- `max_num_tokens`: æœ€å¤§æ‰¹æ¬¡ token æ•°ï¼ˆé»˜è®¤ 8192ï¼‰

### KvCacheConfig å‚æ•°
- `free_gpu_memory_fraction`: KV ç¼“å­˜ä½¿ç”¨çš„ GPU å†…å­˜æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.9ï¼‰
- `enable_block_reuse`: å¯ç”¨å—é‡ç”¨ä¼˜åŒ–ï¼ˆé»˜è®¤ Trueï¼‰

### SamplingParams å‚æ•°
- `max_tokens`: æœ€å¤§ç”Ÿæˆ token æ•°
- `temperature`: æ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤ 1.0ï¼‰
- `top_p`: Top-p é‡‡æ ·ï¼ˆé»˜è®¤ 1.0ï¼‰
- `top_k`: Top-k é‡‡æ ·ï¼ˆé»˜è®¤ 0ï¼‰
- `beam_width`: æŸæœç´¢å®½åº¦ï¼ˆé»˜è®¤ 1ï¼‰

## å¹¶è¡Œç­–ç•¥

### å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼‰
```python
llm = LLM(
    model="meta-llama/Llama-2-70b-hf",
    tensor_parallel_size=8  # 8 GPU å¼ é‡å¹¶è¡Œ
)
```

### æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰
```python
llm = LLM(
    model="meta-llama/Llama-2-70b-hf",
    tensor_parallel_size=4,
    pipeline_parallel_size=2  # 2 é˜¶æ®µæµæ°´çº¿
)
```

### ä¸“å®¶å¹¶è¡Œï¼ˆExpert Parallelismï¼‰
```python
llm = LLM(
    model="mixtral-8x7b",
    tensor_parallel_size=2,
    moe_expert_parallel_size=4  # MoE ä¸“å®¶å¹¶è¡Œ
)
```

## é‡åŒ–æ”¯æŒ

### FP8 é‡åŒ–
```python
from tensorrt_llm.quantization import QuantConfig, QuantAlgo

quant_config = QuantConfig(quant_algo=QuantAlgo.FP8)
llm = LLM(model="meta-llama/Llama-2-7b-hf", quant_config=quant_config)
```

### INT4 AWQ é‡åŒ–
```python
quant_config = QuantConfig(quant_algo=QuantAlgo.W4A16_AWQ)
llm = LLM(model="meta-llama/Llama-2-7b-hf", quant_config=quant_config)
```

### FP4 é‡åŒ–ï¼ˆB200 GPUï¼‰
```python
quant_config = QuantConfig(quant_algo=QuantAlgo.NVFP4)
llm = LLM(model="meta-llama/Llama-2-7b-hf", quant_config=quant_config)
```

## é«˜çº§ç‰¹æ€§

### æŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰
```python
from tensorrt_llm.llmapi import SpeculativeDecodingConfig

spec_config = SpeculativeDecodingConfig(
    draft_model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    draft_tokens=4
)

llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    speculative_decoding_config=spec_config
)
```

### LoRA é€‚é…å™¨
```python
from tensorrt_llm.executor import LoRARequest

lora_request = LoRARequest(
    lora_name="math_lora",
    lora_path="/path/to/lora/adapter"
)

outputs = llm.generate(
    ["Solve this equation: 2x + 3 = 7"],
    lora_request=lora_request
)
```

### å¤šæ¨¡æ€æ”¯æŒ
```python
from tensorrt_llm.llmapi import MultimodalEncoder

# åˆå§‹åŒ–å¤šæ¨¡æ€ç¼–ç å™¨
mm_encoder = MultimodalEncoder(
    vision_encoder_path="/path/to/vision/encoder"
)

# å¤šæ¨¡æ€ LLM
llm = LLM(
    model="llava-v1.5-7b",
    multimodal_encoder=mm_encoder
)
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨ KV ç¼“å­˜å—é‡ç”¨ï¼š`enable_block_reuse=True`
- è°ƒæ•´ GPU å†…å­˜åˆ†é…ï¼š`free_gpu_memory_fraction=0.9`
- å¯ç”¨æƒé‡æµå¼ä¼ è¾“ï¼š`weight_streaming=True`

### 2. è®¡ç®—ä¼˜åŒ–
- ä½¿ç”¨ CUDA å›¾ï¼š`enable_cuda_graph=True`
- å¯ç”¨èåˆæ³¨æ„åŠ›ï¼š`use_fused_attention=True`
- ä¼˜åŒ–æ‰¹æ¬¡å¤§å°ï¼šæ ¹æ® GPU å†…å­˜è°ƒæ•´ `max_batch_size`

### 3. å¹¶è¡Œä¼˜åŒ–
- åˆç†é€‰æ‹©å¼ é‡å¹¶è¡Œåº¦ï¼šé€šå¸¸ä¸º GPU æ•°é‡çš„å› å­
- æµæ°´çº¿å¹¶è¡Œé€‚ç”¨äºå¤§æ¨¡å‹ï¼šå‡å°‘å•å¡å†…å­˜å‹åŠ›
- MoE æ¨¡å‹ä½¿ç”¨ä¸“å®¶å¹¶è¡Œï¼šæé«˜ä¸“å®¶åˆ©ç”¨ç‡

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. å†…å­˜ä¸è¶³
```bash
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘æ‰¹æ¬¡å¤§å°æˆ–åºåˆ—é•¿åº¦
trtllm-build --max_batch_size 4 --max_seq_len 1024
```

#### 2. ç¼–è¯‘å¤±è´¥
```bash
# æ£€æŸ¥ CUDA å’Œ TensorRT ç‰ˆæœ¬
nvidia-smi
python -c "import tensorrt; print(tensorrt.__version__)"
```

#### 3. æ€§èƒ½ä¸ä½³
```bash
# ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·
trtllm-bench --model /path/to/model --profile
```

### è°ƒè¯•æ¨¡å¼
```python
import os
os.environ["TRTLLM_LOG_LEVEL"] = "DEBUG"

from tensorrt_llm import LLM
llm = LLM(model="meta-llama/Llama-2-7b-hf")
```

## æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©
- æ ¹æ®ç¡¬ä»¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹å¤§å°
- è€ƒè™‘é‡åŒ–ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
- ä½¿ç”¨é¢„ä¼˜åŒ–çš„æ¨¡å‹æ£€æŸ¥ç‚¹

### 2. é…ç½®ä¼˜åŒ–
- æ ¹æ®å®é™…éœ€æ±‚è®¾ç½®åºåˆ—é•¿åº¦
- åˆç†é…ç½®å¹¶è¡Œç­–ç•¥
- å¯ç”¨é€‚å½“çš„ä¼˜åŒ–ç‰¹æ€§

### 3. éƒ¨ç½²å»ºè®®
- ä½¿ç”¨å®¹å™¨åŒ–éƒ¨ç½²
- é…ç½®é€‚å½“çš„ç›‘æ§å’Œæ—¥å¿—
- å®æ–½è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»

## ç¤¾åŒºå’Œæ”¯æŒ

- **GitHub**: https://github.com/NVIDIA/TensorRT-LLM
- **æ–‡æ¡£**: https://nvidia.github.io/TensorRT-LLM/
- **è®¨è®ºç¾¤**: å¾®ä¿¡è®¨è®ºç¾¤ï¼ˆè§ GitHub Issues #5359ï¼‰
- **æŠ€æœ¯åšå®¢**: NVIDIA å¼€å‘è€…åšå®¢

## ç‰ˆæœ¬ä¿¡æ¯

å½“å‰ç‰ˆæœ¬ï¼š1.1.0rc6
- æ”¯æŒ Python 3.10-3.12
- å…¼å®¹ CUDA 13.0.0+
- éœ€è¦ TensorRT 10.13.2+

---

*æœ¬æ‰‹å†ŒåŸºäº TensorRT-LLM æœ€æ–°ç‰ˆæœ¬ç¼–å†™ï¼Œå¦‚æœ‰æ›´æ–°è¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ã€‚*
