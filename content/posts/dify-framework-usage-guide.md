---
title: "Difyæ¡†æ¶ä½¿ç”¨æŒ‡å—ï¼šä»å…¥é—¨åˆ°ç²¾é€šçš„å®Œæ•´å¼€å‘æ‰‹å†Œ"
date: 2025-01-27T14:00:00+08:00
draft: false
featured: true
series: "dify-architecture"
tags: ["Dify", "æ¡†æ¶ä½¿ç”¨", "APIå¼€å‘", "æœ€ä½³å®è·µ", "å¼€å‘æŒ‡å—"]
categories: ["dify", "å¼€å‘æŒ‡å—"]
description: "Difyæ¡†æ¶çš„å®Œæ•´ä½¿ç”¨æŒ‡å—ï¼ŒåŒ…å«APIæ¥å£è¯¦è§£ã€å¼€å‘ç¤ºä¾‹ã€æœ€ä½³å®è·µå’Œå®æˆ˜æ¡ˆä¾‹"
toc: true
tocOpen: true
showReadingTime: true
showWordCount: true
weight: 10
slug: "dify-framework-usage-guide"
---

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›Difyæ¡†æ¶çš„å®Œæ•´ä½¿ç”¨æŒ‡å—ï¼Œä»åŸºç¡€æ¦‚å¿µåˆ°é«˜çº§åº”ç”¨ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€ŸæŒæ¡Difyå¹³å°çš„å¼€å‘æŠ€èƒ½ã€‚

<!--more-->

## 1. Difyæ¡†æ¶æ¦‚è¿°

### 1.1 æ¡†æ¶ç‰¹ç‚¹

Difyæ˜¯ä¸€ä¸ªä¼ä¸šçº§AIåº”ç”¨å¼€å‘å¹³å°ï¼Œå…·æœ‰ä»¥ä¸‹æ ¸å¿ƒç‰¹ç‚¹ï¼š

**æ¶æ„ç‰¹ç‚¹**ï¼š
- **åˆ†å±‚è®¾è®¡**ï¼šæ¸…æ™°çš„APIå±‚ã€ä¸šåŠ¡å±‚ã€æœåŠ¡å±‚å’Œæ•°æ®å±‚åˆ†ç¦»
- **æ¨¡å—åŒ–**ï¼šå„åŠŸèƒ½æ¨¡å—ç‹¬ç«‹ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
- **äº‹ä»¶é©±åŠ¨**ï¼šåŸºäºé˜Ÿåˆ—çš„å¼‚æ­¥äº‹ä»¶å¤„ç†æœºåˆ¶
- **å¤šç§Ÿæˆ·**ï¼šåŸç”Ÿæ”¯æŒå¤šç§Ÿæˆ·æ¶æ„å’Œæ•°æ®éš”ç¦»

**æŠ€æœ¯ç‰¹ç‚¹**ï¼š
- **å¤šæ¨¡å‹æ”¯æŒ**ï¼šç»Ÿä¸€æ¥å£æ”¯æŒOpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹ç­‰
- **å·¥ä½œæµå¼•æ“**ï¼šå¯è§†åŒ–çš„å·¥ä½œæµç¼–æ’å’Œæ‰§è¡Œ
- **RAGèƒ½åŠ›**ï¼šå®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“
- **æ™ºèƒ½ä½“ç³»ç»Ÿ**ï¼šæ”¯æŒå‡½æ•°è°ƒç”¨å’Œæ€ç»´é“¾æ¨ç†

### 1.2 æ ¸å¿ƒç»„ä»¶æ¶æ„

```mermaid
graph TB
    subgraph "Difyæ¡†æ¶æ ¸å¿ƒç»„ä»¶"
        subgraph "APIæ¥å£å±‚"
            ConsoleAPI[Console API<br/>ç®¡ç†åå°æ¥å£]
            ServiceAPI[Service API<br/>åº”ç”¨æœåŠ¡æ¥å£]
            WebAPI[Web API<br/>å‰ç«¯ä¸“ç”¨æ¥å£]
        end
        
        subgraph "åº”ç”¨å¼•æ“å±‚"
            AppGenerator[åº”ç”¨ç”Ÿæˆå™¨<br/>AppGenerator]
            TaskPipeline[ä»»åŠ¡ç®¡é“<br/>TaskPipeline]
            QueueManager[é˜Ÿåˆ—ç®¡ç†å™¨<br/>QueueManager]
        end
        
        subgraph "æ ¸å¿ƒä¸šåŠ¡å±‚"
            AppCore[åº”ç”¨æ ¸å¿ƒ<br/>core/app]
            WorkflowEngine[å·¥ä½œæµå¼•æ“<br/>core/workflow]
            RAGEngine[RAGå¼•æ“<br/>core/rag]
            ModelRuntime[æ¨¡å‹è¿è¡Œæ—¶<br/>core/model_runtime]
            AgentCore[æ™ºèƒ½ä½“æ ¸å¿ƒ<br/>core/agent]
        end
        
        subgraph "æœåŠ¡æ”¯æ’‘å±‚"
            AppService[åº”ç”¨æœåŠ¡]
            DatasetService[æ•°æ®é›†æœåŠ¡]
            ModelService[æ¨¡å‹æœåŠ¡]
            FileService[æ–‡ä»¶æœåŠ¡]
        end
    end
    
    ConsoleAPI --> AppGenerator
    ServiceAPI --> TaskPipeline
    WebAPI --> QueueManager
    
    AppGenerator --> AppCore
    TaskPipeline --> WorkflowEngine
    QueueManager --> RAGEngine
    
    AppCore --> AppService
    WorkflowEngine --> DatasetService
    RAGEngine --> ModelService
    
    style ConsoleAPI fill:#e3f2fd
    style ServiceAPI fill:#e8f5e8
    style WebAPI fill:#fff3e0
    style AppCore fill:#fce4ec
```

## 2. APIæ¥å£è¯¦è§£

### 2.1 APIæ¶æ„è®¾è®¡

Difyé‡‡ç”¨ä¸‰å±‚APIæ¶æ„ï¼Œæ¯å±‚é¢å‘ä¸åŒçš„ç”¨æˆ·ç¾¤ä½“ï¼š

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant Gateway as APIç½‘å…³
    participant ServiceAPI as Service API
    participant AppService as åº”ç”¨æœåŠ¡
    participant TaskPipeline as ä»»åŠ¡ç®¡é“
    participant ModelRuntime as æ¨¡å‹è¿è¡Œæ—¶
    
    Note over Client,ModelRuntime: Dify APIå®Œæ•´è°ƒç”¨é“¾è·¯
    
    Client->>Gateway: HTTPè¯·æ±‚ + API Key
    Gateway->>Gateway: è®¤è¯å’Œé™æµ
    Gateway->>ServiceAPI: è½¬å‘è¯·æ±‚
    
    ServiceAPI->>ServiceAPI: å‚æ•°éªŒè¯
    ServiceAPI->>AppService: AppGenerateService.generate()
    
    AppService->>TaskPipeline: åˆ›å»ºä»»åŠ¡ç®¡é“
    TaskPipeline->>TaskPipeline: ä¸‰é˜¶æ®µå¤„ç†
    
    TaskPipeline->>ModelRuntime: è°ƒç”¨æ¨¡å‹æ¨ç†
    ModelRuntime-->>TaskPipeline: æµå¼å“åº”
    
    TaskPipeline-->>ServiceAPI: äº‹ä»¶æµ
    ServiceAPI-->>Client: SSEå“åº”æµ
```

### 2.2 Service APIæ ¸å¿ƒæ¥å£

#### 2.2.1 Chat Messagesæ¥å£

**æ¥å£å®šä¹‰**ï¼š
```python
@service_api_ns.route("/chat-messages")
class ChatApi(Resource):
    """
    å‘é€èŠå¤©æ¶ˆæ¯æ¥å£
    
    åŠŸèƒ½ï¼šå¤„ç†èŠå¤©ã€æ™ºèƒ½ä½“èŠå¤©å’Œé«˜çº§èŠå¤©åº”ç”¨çš„æ¶ˆæ¯
    æ”¯æŒï¼šå¯¹è¯ç®¡ç†ã€æµå¼å’Œé˜»å¡å“åº”æ¨¡å¼
    """
    
    @service_api_ns.expect(chat_parser)
    @service_api_ns.doc("create_chat_message")
    @validate_app_token(fetch_user_arg=FetchUserArg(fetch_from=WhereisUserArg.JSON, required=True))
    def post(self, app_model: App, end_user: EndUser):
        """
        å‘é€èŠå¤©æ¶ˆæ¯
        
        Args:
            app_model (App): åº”ç”¨æ¨¡å‹å®ä¾‹ï¼Œé€šè¿‡è£…é¥°å™¨æ³¨å…¥
            end_user (EndUser): ç»ˆç«¯ç”¨æˆ·å®ä¾‹ï¼Œé€šè¿‡è£…é¥°å™¨æ³¨å…¥
            
        Returns:
            Generator: æµå¼å“åº”ç”Ÿæˆå™¨æˆ–é˜»å¡å“åº”
            
        Raises:
            NotChatAppError: åº”ç”¨ç±»å‹ä¸æ”¯æŒèŠå¤©
            ConversationNotExistsError: å¯¹è¯ä¸å­˜åœ¨
            AppUnavailableError: åº”ç”¨ä¸å¯ç”¨
            ProviderNotInitializeError: æ¨¡å‹æä¾›å•†æœªåˆå§‹åŒ–
        """
        # 1. éªŒè¯åº”ç”¨ç±»å‹
        app_mode = AppMode.value_of(app_model.mode)
        if app_mode not in {AppMode.CHAT, AppMode.AGENT_CHAT, AppMode.ADVANCED_CHAT}:
            raise NotChatAppError()

        # 2. è§£æè¯·æ±‚å‚æ•°
        args = chat_parser.parse_args()
        
        # 3. å¤„ç†å¤–éƒ¨è¿½è¸ªID
        external_trace_id = get_external_trace_id(request)
        if external_trace_id:
            args["external_trace_id"] = external_trace_id

        # 4. ç¡®å®šå“åº”æ¨¡å¼
        streaming = args["response_mode"] == "streaming"

        try:
            # 5. è°ƒç”¨åº”ç”¨ç”ŸæˆæœåŠ¡
            response = AppGenerateService.generate(
                app_model=app_model, 
                user=end_user, 
                args=args, 
                invoke_from=InvokeFrom.SERVICE_API, 
                streaming=streaming
            )

            # 6. æ ¼å¼åŒ–å“åº”
            return helper.compact_generate_response(response)
            
        except Exception as e:
            # 7. å¼‚å¸¸å¤„ç†
            logger.exception("èŠå¤©æ¶ˆæ¯å¤„ç†å¤±è´¥")
            raise InternalServerError()
```

**è¯·æ±‚å‚æ•°**ï¼š
```python
# èŠå¤©æ¶ˆæ¯è¯·æ±‚å‚æ•°å®šä¹‰
chat_parser = reqparse.RequestParser()
chat_parser.add_argument("inputs", type=dict, required=True, location="json", 
                        help="è¾“å…¥å˜é‡å­—å…¸")
chat_parser.add_argument("query", type=str, required=True, location="json", 
                        help="ç”¨æˆ·æŸ¥è¯¢å†…å®¹")
chat_parser.add_argument("response_mode", type=str, choices=["blocking", "streaming"], 
                        default="streaming", location="json", help="å“åº”æ¨¡å¼")
chat_parser.add_argument("conversation_id", type=str, location="json", 
                        help="å¯¹è¯IDï¼Œæ–°å¯¹è¯æ—¶ä¸ºç©º")
chat_parser.add_argument("user", type=str, required=True, location="json", 
                        help="ç”¨æˆ·æ ‡è¯†")
chat_parser.add_argument("files", type=list, default=[], location="json", 
                        help="æ–‡ä»¶åˆ—è¡¨")
```

**è°ƒç”¨ç¤ºä¾‹**ï¼š
```python
import requests
import json

def send_chat_message():
    """å‘é€èŠå¤©æ¶ˆæ¯ç¤ºä¾‹"""
    
    # APIé…ç½®
    api_base_url = "https://api.dify.ai/v1"
    api_key = "app-your-api-key"
    app_id = "your-app-id"
    
    # è¯·æ±‚å¤´
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    # è¯·æ±‚æ•°æ®
    data = {
        "inputs": {
            "topic": "äººå·¥æ™ºèƒ½",
            "context": "æŠ€æœ¯è®¨è®º"
        },
        "query": "è¯·ä»‹ç»ä¸€ä¸‹å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•å†ç¨‹",
        "response_mode": "streaming",
        "conversation_id": "",  # æ–°å¯¹è¯
        "user": "user_123",
        "files": []
    }
    
    # å‘é€è¯·æ±‚
    response = requests.post(
        f"{api_base_url}/chat-messages",
        headers=headers,
        json=data,
        stream=True
    )
    
    # å¤„ç†æµå¼å“åº”
    if response.status_code == 200:
        for line in response.iter_lines():
            if line and line.startswith(b'data: '):
                try:
                    data = json.loads(line[6:].decode('utf-8'))
                    event = data.get('event')
                    
                    if event == 'message':
                        print(f"æ¶ˆæ¯: {data.get('answer', '')}")
                    elif event == 'message_end':
                        print(f"å¯¹è¯ID: {data.get('conversation_id')}")
                        print(f"æ¶ˆæ¯ID: {data.get('id')}")
                        break
                    elif event == 'error':
                        print(f"é”™è¯¯: {data.get('message')}")
                        break
                        
                except json.JSONDecodeError:
                    continue
    else:
        print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
        print(response.text)

# ä½¿ç”¨ç¤ºä¾‹
send_chat_message()
```

#### 2.2.2 Completion Messagesæ¥å£

**æ¥å£å®šä¹‰**ï¼š
```python
@service_api_ns.route("/completion-messages")
class CompletionApi(Resource):
    """
    æ–‡æœ¬å®Œæˆæ¥å£
    
    åŠŸèƒ½ï¼šåŸºäºæç¤ºç”Ÿæˆæ–‡æœ¬å®Œæˆ
    æ”¯æŒï¼šæµå¼å’Œé˜»å¡å“åº”æ¨¡å¼
    """
    
    @service_api_ns.expect(completion_parser)
    @validate_app_token(fetch_user_arg=FetchUserArg(fetch_from=WhereisUserArg.JSON, required=True))
    def post(self, app_model: App, end_user: EndUser):
        """
        åˆ›å»ºæ–‡æœ¬å®Œæˆ
        
        Args:
            app_model (App): åº”ç”¨æ¨¡å‹å®ä¾‹
            end_user (EndUser): ç»ˆç«¯ç”¨æˆ·å®ä¾‹
            
        Returns:
            Generator: å®Œæˆç»“æœç”Ÿæˆå™¨
            
        Raises:
            AppUnavailableError: åº”ç”¨ä¸å¯ç”¨ï¼ˆécompletionæ¨¡å¼ï¼‰
            ProviderNotInitializeError: æ¨¡å‹æä¾›å•†æœªåˆå§‹åŒ–
            CompletionRequestError: å®Œæˆè¯·æ±‚é”™è¯¯
        """
        # 1. éªŒè¯åº”ç”¨æ¨¡å¼
        if app_model.mode != "completion":
            raise AppUnavailableError()

        # 2. è§£æå‚æ•°
        args = completion_parser.parse_args()
        
        # 3. å¤„ç†å¤–éƒ¨è¿½è¸ª
        external_trace_id = get_external_trace_id(request)
        if external_trace_id:
            args["external_trace_id"] = external_trace_id

        # 4. è®¾ç½®å“åº”æ¨¡å¼
        streaming = args["response_mode"] == "streaming"
        args["auto_generate_name"] = False

        try:
            # 5. ç”Ÿæˆå®Œæˆ
            response = AppGenerateService.generate(
                app_model=app_model,
                user=end_user,
                args=args,
                invoke_from=InvokeFrom.SERVICE_API,
                streaming=streaming,
            )

            return helper.compact_generate_response(response)
            
        except Exception as e:
            logger.exception("æ–‡æœ¬å®Œæˆç”Ÿæˆå¤±è´¥")
            raise InternalServerError()
```

### 2.3 AppGenerateServiceæ ¸å¿ƒå®ç°

**æœåŠ¡å…¥å£**ï¼š
```python
class AppGenerateService:
    """
    åº”ç”¨ç”ŸæˆæœåŠ¡
    
    åŠŸèƒ½ï¼šç»Ÿä¸€çš„åº”ç”¨å†…å®¹ç”Ÿæˆå…¥å£
    æ”¯æŒï¼šChatã€Completionã€Agentã€Workflowç­‰å¤šç§åº”ç”¨ç±»å‹
    """
    
    # ç³»ç»Ÿçº§é™æµå™¨
    system_rate_limiter = RateLimiter(
        prefix="system_generate_rate_limit",
        max_attempts=dify_config.APP_DAILY_RATE_LIMIT,
        time_window=86400  # 24å°æ—¶
    )

    @classmethod
    def generate(
        cls,
        app_model: App,
        user: Union[Account, EndUser],
        args: Mapping[str, Any],
        invoke_from: InvokeFrom,
        streaming: bool = True,
    ):
        """
        åº”ç”¨å†…å®¹ç”Ÿæˆä¸»å…¥å£
        
        Args:
            app_model (App): åº”ç”¨æ¨¡å‹å®ä¾‹
            user (Union[Account, EndUser]): ç”¨æˆ·å®ä¾‹
            args (Mapping[str, Any]): ç”Ÿæˆå‚æ•°
            invoke_from (InvokeFrom): è°ƒç”¨æ¥æº
            streaming (bool): æ˜¯å¦æµå¼è¾“å‡º
            
        Returns:
            Generator: ç”Ÿæˆç»“æœæµ
            
        Raises:
            InvokeRateLimitError: è°ƒç”¨é¢‘ç‡é™åˆ¶
            WorkflowNotFoundError: å·¥ä½œæµæœªæ‰¾åˆ°
            AppUnavailableError: åº”ç”¨ä¸å¯ç”¨
        """
        
        # 1. ç³»ç»Ÿçº§é™æµæ£€æŸ¥
        if dify_config.BILLING_ENABLED:
            limit_info = BillingService.get_info(app_model.tenant_id)
            if limit_info["subscription"]["plan"] == "sandbox":
                if cls.system_rate_limiter.is_rate_limited(app_model.tenant_id):
                    raise InvokeRateLimitError(
                        f"Rate limit exceeded, please upgrade your plan "
                        f"or your RPD was {dify_config.APP_DAILY_RATE_LIMIT} requests/day"
                    )
                cls.system_rate_limiter.increment_rate_limit(app_model.tenant_id)

        # 2. åº”ç”¨çº§é™æµè®¾ç½®
        max_active_request = cls._get_max_active_requests(app_model)
        rate_limit = RateLimit(
            tenant_id=app_model.tenant_id,
            app_id=app_model.id,
            max_active_requests=max_active_request
        )

        # 3. ç”Ÿæˆè¯·æ±‚IDå¹¶è¿›å…¥é™æµ
        request_id = RateLimit.gen_request_key()
        try:
            request_id = rate_limit.enter(request_id)
            
            # 4. æ ¹æ®åº”ç”¨æ¨¡å¼é€‰æ‹©ç”Ÿæˆå™¨
            if app_model.mode == AppMode.COMPLETION:
                return rate_limit.generate(
                    CompletionAppGenerator.convert_to_event_stream(
                        CompletionAppGenerator().generate(
                            app_model=app_model, 
                            user=user, 
                            args=args, 
                            invoke_from=invoke_from, 
                            streaming=streaming
                        ),
                    ),
                    request_id=request_id,
                )
                
            elif app_model.mode == AppMode.AGENT_CHAT or app_model.is_agent:
                return rate_limit.generate(
                    AgentChatAppGenerator.convert_to_event_stream(
                        AgentChatAppGenerator().generate(
                            app_model=app_model, 
                            user=user, 
                            args=args, 
                            invoke_from=invoke_from, 
                            streaming=streaming
                        ),
                    ),
                    request_id,
                )
                
            elif app_model.mode == AppMode.CHAT:
                return rate_limit.generate(
                    ChatAppGenerator.convert_to_event_stream(
                        ChatAppGenerator().generate(
                            app_model=app_model, 
                            user=user, 
                            args=args, 
                            invoke_from=invoke_from, 
                            streaming=streaming
                        ),
                    ),
                    request_id=request_id,
                )
                
            elif app_model.mode == AppMode.ADVANCED_CHAT:
                # é«˜çº§èŠå¤©éœ€è¦å·¥ä½œæµID
                workflow_id = args.get("workflow_id")
                if not workflow_id:
                    raise WorkflowNotFoundError("workflow_id is required for advanced chat")
                
                # éªŒè¯å·¥ä½œæµæ ¼å¼
                try:
                    uuid.UUID(workflow_id)
                except ValueError:
                    raise WorkflowIdFormatError("Invalid workflow ID format")
                
                # è·å–å·¥ä½œæµ
                workflow = WorkflowService.get_published_workflow(
                    app_model=app_model, 
                    workflow_id=workflow_id
                )
                if not workflow:
                    raise WorkflowNotFoundError("Workflow not found or not published")
                
                return rate_limit.generate(
                    AdvancedChatAppGenerator.convert_to_event_stream(
                        AdvancedChatAppGenerator().generate(
                            app_model=app_model,
                            workflow=workflow,
                            user=user,
                            args=args,
                            invoke_from=invoke_from,
                            streaming=streaming,
                        ),
                    ),
                    request_id=request_id,
                )
                
            elif app_model.mode == AppMode.WORKFLOW:
                # å·¥ä½œæµåº”ç”¨
                workflow = WorkflowService.get_published_workflow(app_model=app_model)
                if not workflow:
                    raise WorkflowNotFoundError("Workflow not found")
                
                return rate_limit.generate(
                    WorkflowAppGenerator.convert_to_event_stream(
                        WorkflowAppGenerator().generate(
                            app_model=app_model,
                            workflow=workflow,
                            user=user,
                            args=args,
                            invoke_from=invoke_from,
                            streaming=streaming,
                        ),
                    ),
                    request_id=request_id,
                )
            else:
                raise ValueError(f"Invalid app mode: {app_model.mode}")
                
        except RateLimitError:
            raise InvokeRateLimitError("Rate limit exceeded")
        finally:
            # 5. é€€å‡ºé™æµ
            rate_limit.exit(request_id)

    @classmethod
    def _get_max_active_requests(cls, app_model: App) -> int:
        """
        è·å–åº”ç”¨æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
        
        Args:
            app_model (App): åº”ç”¨æ¨¡å‹
            
        Returns:
            int: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
        """
        max_active_requests = app_model.max_active_requests
        if max_active_requests is None:
            max_active_requests = int(dify_config.APP_MAX_ACTIVE_REQUESTS)
        return max_active_requests
```

## 3. åº”ç”¨å¼€å‘ç¤ºä¾‹

### 3.1 åˆ›å»ºChatåº”ç”¨

```python
def create_chat_app_example():
    """
    åˆ›å»ºChatåº”ç”¨ç¤ºä¾‹
    
    åŠŸèƒ½ï¼šåˆ›å»ºä¸€ä¸ªåŸºç¡€çš„èŠå¤©åº”ç”¨
    é…ç½®ï¼šæ¨¡å‹ã€æç¤ºæ¨¡æ¿ã€ç”¨æˆ·è¾“å…¥è¡¨å•
    """
    from services.app_service import AppService
    from services.app_model_config_service import AppModelConfigService
    
    # 1. åº”ç”¨åŸºæœ¬ä¿¡æ¯
    app_args = {
        'name': 'æ™ºèƒ½å®¢æœåŠ©æ‰‹',
        'mode': 'chat',
        'icon': 'ğŸ¤–',
        'description': 'åŸºäºGPT-4çš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”ç”¨æˆ·é—®é¢˜å¹¶æä¾›å¸®åŠ©'
    }
    
    # 2. æ¨¡å‹é…ç½®
    model_config = {
        'provider': 'openai',
        'model': 'gpt-4',
        'parameters': {
            'temperature': 0.7,
            'max_tokens': 2048,
            'top_p': 1.0,
            'frequency_penalty': 0.0,
            'presence_penalty': 0.0
        },
        'stop': []
    }
    
    # 3. æç¤ºæ¨¡æ¿é…ç½®
    prompt_template = {
        'prompt_type': 'simple',
        'simple_prompt_template': '''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€å‹å¥½çš„å›ç­”ã€‚

ç”¨æˆ·ä¿¡æ¯ï¼š
- å§“åï¼š{{name}}
- é—®é¢˜ç±»å‹ï¼š{{question_type}}

ç”¨æˆ·é—®é¢˜ï¼š{{query}}

è¯·æä¾›è¯¦ç»†çš„å›ç­”ï¼Œå¦‚æœéœ€è¦è¿›ä¸€æ­¥ä¿¡æ¯ï¼Œè¯·ä¸»åŠ¨è¯¢é—®ã€‚'''
    }
    
    # 4. ç”¨æˆ·è¾“å…¥è¡¨å•é…ç½®
    user_input_form = [
        {
            'variable': 'name',
            'label': 'æ‚¨çš„å§“å',
            'type': 'text-input',
            'required': True,
            'max_length': 50
        },
        {
            'variable': 'question_type',
            'label': 'é—®é¢˜ç±»å‹',
            'type': 'select',
            'required': True,
            'options': ['æŠ€æœ¯æ”¯æŒ', 'äº§å“å’¨è¯¢', 'è´¦æˆ·é—®é¢˜', 'å…¶ä»–']
        }
    ]
    
    # 5. åˆ›å»ºåº”ç”¨
    app = AppService.create_app(
        tenant_id='tenant_123',
        args=app_args,
        account=account  # å½“å‰ç”¨æˆ·è´¦æˆ·
    )
    
    # 6. æ›´æ–°åº”ç”¨é…ç½®
    config_data = {
        'model_config': model_config,
        'prompt_template': prompt_template,
        'user_input_form': user_input_form,
        'opening_statement': 'æ‚¨å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›å¸®åŠ©æ‚¨ã€‚',
        'suggested_questions': [
            'å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ',
            'äº§å“æœ‰å“ªäº›åŠŸèƒ½ï¼Ÿ',
            'å¦‚ä½•è”ç³»æŠ€æœ¯æ”¯æŒï¼Ÿ'
        ]
    }
    
    AppModelConfigService.update_app_config(
        app_id=app.id,
        config=config_data,
        account=account
    )
    
    return app
```

### 3.2 åˆ›å»ºAgentåº”ç”¨

```python
def create_agent_app_example():
    """
    åˆ›å»ºAgentåº”ç”¨ç¤ºä¾‹
    
    åŠŸèƒ½ï¼šåˆ›å»ºå…·å¤‡å·¥å…·è°ƒç”¨èƒ½åŠ›çš„æ™ºèƒ½ä½“åº”ç”¨
    å·¥å…·ï¼šæœç´¢ã€è®¡ç®—å™¨ã€å¤©æ°”æŸ¥è¯¢
    """
    from services.app_service import AppService
    from services.app_model_config_service import AppModelConfigService
    
    # 1. åº”ç”¨åŸºæœ¬ä¿¡æ¯
    app_args = {
        'name': 'æ™ºèƒ½åŠ©æ‰‹Agent',
        'mode': 'agent-chat',
        'icon': 'ğŸ¤–',
        'description': 'å…·å¤‡æœç´¢ã€è®¡ç®—å’Œå¤©æ°”æŸ¥è¯¢èƒ½åŠ›çš„æ™ºèƒ½åŠ©æ‰‹'
    }
    
    # 2. Agenté…ç½®
    agent_config = {
        'strategy': 'function-calling',  # ä½¿ç”¨å‡½æ•°è°ƒç”¨ç­–ç•¥
        'max_iteration': 10,
        'tools': [
            {
                'type': 'builtin',
                'provider': 'duckduckgo',
                'tool_name': 'duckduckgo_search',
                'tool_configuration': {
                    'result_type': 'text'
                }
            },
            {
                'type': 'builtin',
                'provider': 'calculator',
                'tool_name': 'calculator',
                'tool_configuration': {}
            },
            {
                'type': 'builtin',
                'provider': 'weather',
                'tool_name': 'weather_query',
                'tool_configuration': {
                    'api_key': 'your-weather-api-key'
                }
            }
        ]
    }
    
    # 3. æ¨¡å‹é…ç½®ï¼ˆéœ€è¦æ”¯æŒå‡½æ•°è°ƒç”¨çš„æ¨¡å‹ï¼‰
    model_config = {
        'provider': 'openai',
        'model': 'gpt-4',
        'parameters': {
            'temperature': 0.2,  # è¾ƒä½æ¸©åº¦ä¿è¯å·¥å…·è°ƒç”¨å‡†ç¡®æ€§
            'max_tokens': 4096,
            'top_p': 1.0
        }
    }
    
    # 4. æç¤ºæ¨¡æ¿
    prompt_template = {
        'prompt_type': 'simple',
        'simple_prompt_template': '''ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æœç´¢ä¿¡æ¯ã€è¿›è¡Œè®¡ç®—å’ŒæŸ¥è¯¢å¤©æ°”ã€‚

ä½ æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š
1. æœç´¢äº’è”ç½‘ä¿¡æ¯
2. è¿›è¡Œæ•°å­¦è®¡ç®—
3. æŸ¥è¯¢å¤©æ°”ä¿¡æ¯

è¯·æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ã€‚å¦‚æœéœ€è¦ä½¿ç”¨å·¥å…·ï¼Œè¯·å…ˆè¯´æ˜ä½ è¦åšä»€ä¹ˆï¼Œç„¶åè°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{{query}}'''
    }
    
    # 5. åˆ›å»ºåº”ç”¨
    app = AppService.create_app(
        tenant_id='tenant_123',
        args=app_args,
        account=account
    )
    
    # 6. æ›´æ–°é…ç½®
    config_data = {
        'model_config': model_config,
        'prompt_template': prompt_template,
        'agent': agent_config,
        'opening_statement': 'ä½ å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®æ‚¨æœç´¢ä¿¡æ¯ã€è¿›è¡Œè®¡ç®—å’ŒæŸ¥è¯¢å¤©æ°”ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚',
        'suggested_questions': [
            'ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ',
            'å¸®æˆ‘è®¡ç®— 123 * 456',
            'æœç´¢æœ€æ–°çš„AIæŠ€æœ¯å‘å±•'
        ]
    }
    
    AppModelConfigService.update_app_config(
        app_id=app.id,
        config=config_data,
        account=account
    )
    
    return app
```

### 3.3 åˆ›å»ºWorkflowåº”ç”¨

```python
def create_workflow_app_example():
    """
    åˆ›å»ºWorkflowåº”ç”¨ç¤ºä¾‹
    
    åŠŸèƒ½ï¼šåˆ›å»ºä¸€ä¸ªæ–‡æ¡£åˆ†æå·¥ä½œæµåº”ç”¨
    æµç¨‹ï¼šæ–‡æ¡£ä¸Šä¼  -> å†…å®¹æå– -> æ‘˜è¦ç”Ÿæˆ -> å…³é”®è¯æå–
    """
    from services.app_service import AppService
    from services.workflow_service import WorkflowService
    
    # 1. åº”ç”¨åŸºæœ¬ä¿¡æ¯
    app_args = {
        'name': 'æ–‡æ¡£åˆ†æå·¥ä½œæµ',
        'mode': 'workflow',
        'icon': 'ğŸ“„',
        'description': 'è‡ªåŠ¨åˆ†ææ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆæ‘˜è¦å’Œå…³é”®è¯'
    }
    
    # 2. å·¥ä½œæµå›¾å®šä¹‰
    workflow_graph = {
        'nodes': [
            {
                'id': 'start',
                'type': 'start',
                'data': {
                    'title': 'å¼€å§‹',
                    'variables': [
                        {
                            'variable': 'document',
                            'type': 'file',
                            'label': 'æ–‡æ¡£æ–‡ä»¶',
                            'required': True,
                            'allowed_file_types': ['pdf', 'docx', 'txt']
                        }
                    ]
                }
            },
            {
                'id': 'extract_content',
                'type': 'code',
                'data': {
                    'title': 'æå–æ–‡æ¡£å†…å®¹',
                    'code': '''
def main(document):
    """æå–æ–‡æ¡£å†…å®¹"""
    import PyPDF2
    import docx
    
    content = ""
    file_type = document.get('type', '').lower()
    
    if file_type == 'pdf':
        # å¤„ç†PDFæ–‡ä»¶
        content = extract_pdf_content(document['content'])
    elif file_type == 'docx':
        # å¤„ç†Wordæ–‡æ¡£
        content = extract_docx_content(document['content'])
    elif file_type == 'txt':
        # å¤„ç†æ–‡æœ¬æ–‡ä»¶
        content = document['content'].decode('utf-8')
    
    return {
        'content': content,
        'word_count': len(content.split())
    }

def extract_pdf_content(pdf_content):
    # PDFå†…å®¹æå–é€»è¾‘
    pass

def extract_docx_content(docx_content):
    # Wordæ–‡æ¡£å†…å®¹æå–é€»è¾‘
    pass
                    ''',
                    'outputs': [
                        {'variable': 'content', 'type': 'string'},
                        {'variable': 'word_count', 'type': 'number'}
                    ]
                }
            },
            {
                'id': 'generate_summary',
                'type': 'llm',
                'data': {
                    'title': 'ç”Ÿæˆæ‘˜è¦',
                    'model': {
                        'provider': 'openai',
                        'name': 'gpt-4',
                        'parameters': {
                            'temperature': 0.3,
                            'max_tokens': 500
                        }
                    },
                    'prompt': '''è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{{#extract_content.content#}}

è¦æ±‚ï¼š
1. æ‘˜è¦é•¿åº¦æ§åˆ¶åœ¨200å­—ä»¥å†…
2. çªå‡ºæ–‡æ¡£çš„æ ¸å¿ƒè§‚ç‚¹å’Œä¸»è¦å†…å®¹
3. ä½¿ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€

æ‘˜è¦ï¼š''',
                    'outputs': [
                        {'variable': 'summary', 'type': 'string'}
                    ]
                }
            },
            {
                'id': 'extract_keywords',
                'type': 'llm',
                'data': {
                    'title': 'æå–å…³é”®è¯',
                    'model': {
                        'provider': 'openai',
                        'name': 'gpt-4',
                        'parameters': {
                            'temperature': 0.1,
                            'max_tokens': 200
                        }
                    },
                    'prompt': '''è¯·ä»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ä¸­æå–5-10ä¸ªå…³é”®è¯ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{{#extract_content.content#}}

è¦æ±‚ï¼š
1. å…³é”®è¯åº”è¯¥æ˜¯æ–‡æ¡£ä¸­æœ€é‡è¦çš„æ¦‚å¿µæˆ–ä¸»é¢˜
2. æ¯ä¸ªå…³é”®è¯ç”¨é€—å·åˆ†éš”
3. æŒ‰é‡è¦æ€§æ’åº

å…³é”®è¯ï¼š''',
                    'outputs': [
                        {'variable': 'keywords', 'type': 'string'}
                    ]
                }
            },
            {
                'id': 'end',
                'type': 'end',
                'data': {
                    'title': 'ç»“æŸ',
                    'outputs': [
                        {
                            'variable': 'analysis_result',
                            'type': 'object',
                            'value': {
                                'summary': '{{#generate_summary.summary#}}',
                                'keywords': '{{#extract_keywords.keywords#}}',
                                'word_count': '{{#extract_content.word_count#}}'
                            }
                        }
                    ]
                }
            }
        ],
        'edges': [
            {'source': 'start', 'target': 'extract_content'},
            {'source': 'extract_content', 'target': 'generate_summary'},
            {'source': 'extract_content', 'target': 'extract_keywords'},
            {'source': 'generate_summary', 'target': 'end'},
            {'source': 'extract_keywords', 'target': 'end'}
        ]
    }
    
    # 3. åˆ›å»ºåº”ç”¨
    app = AppService.create_app(
        tenant_id='tenant_123',
        args=app_args,
        account=account
    )
    
    # 4. åˆ›å»ºå·¥ä½œæµ
    workflow = WorkflowService.create_workflow(
        app_id=app.id,
        graph=workflow_graph,
        account=account
    )
    
    # 5. å‘å¸ƒå·¥ä½œæµ
    WorkflowService.publish_workflow(
        workflow_id=workflow.id,
        account=account
    )
    
    return app, workflow
```

## 4. æœ€ä½³å®è·µ

### 4.1 APIè°ƒç”¨æœ€ä½³å®è·µ

#### 4.1.1 é”™è¯¯å¤„ç†

```python
import requests
import json
import time
from typing import Generator, Dict, Any

class DifyAPIClient:
    """
    Dify APIå®¢æˆ·ç«¯
    
    åŠŸèƒ½ï¼šå°è£…Dify APIè°ƒç”¨ï¼Œæä¾›é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
    """
    
    def __init__(self, api_key: str, base_url: str = "https://api.dify.ai/v1"):
        """
        åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        
        Args:
            api_key (str): APIå¯†é’¥
            base_url (str): APIåŸºç¡€URL
        """
        self.api_key = api_key
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        })
    
    def send_message(
        self, 
        inputs: Dict[str, Any], 
        query: str, 
        user: str,
        conversation_id: str = "",
        stream: bool = True,
        max_retries: int = 3
    ) -> Generator[Dict[str, Any], None, None]:
        """
        å‘é€èŠå¤©æ¶ˆæ¯
        
        Args:
            inputs (Dict[str, Any]): è¾“å…¥å˜é‡
            query (str): ç”¨æˆ·æŸ¥è¯¢
            user (str): ç”¨æˆ·æ ‡è¯†
            conversation_id (str): å¯¹è¯ID
            stream (bool): æ˜¯å¦æµå¼è¾“å‡º
            max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Yields:
            Dict[str, Any]: å“åº”äº‹ä»¶
            
        Raises:
            APIError: APIè°ƒç”¨é”™è¯¯
            RateLimitError: é¢‘ç‡é™åˆ¶é”™è¯¯
        """
        data = {
            "inputs": inputs,
            "query": query,
            "response_mode": "streaming" if stream else "blocking",
            "conversation_id": conversation_id,
            "user": user
        }
        
        for attempt in range(max_retries + 1):
            try:
                response = self.session.post(
                    f"{self.base_url}/chat-messages",
                    json=data,
                    stream=stream,
                    timeout=60
                )
                
                if response.status_code == 200:
                    if stream:
                        yield from self._parse_stream_response(response)
                    else:
                        yield response.json()
                    return
                    
                elif response.status_code == 429:
                    # é¢‘ç‡é™åˆ¶ï¼ŒæŒ‡æ•°é€€é¿é‡è¯•
                    if attempt < max_retries:
                        wait_time = 2 ** attempt
                        time.sleep(wait_time)
                        continue
                    else:
                        raise RateLimitError("Rate limit exceeded")
                        
                elif response.status_code == 401:
                    raise AuthenticationError("Invalid API key")
                    
                elif response.status_code == 400:
                    error_data = response.json()
                    raise ValidationError(f"Validation error: {error_data.get('message')}")
                    
                else:
                    response.raise_for_status()
                    
            except requests.RequestException as e:
                if attempt < max_retries:
                    time.sleep(2 ** attempt)
                    continue
                else:
                    raise APIError(f"Request failed: {str(e)}")
    
    def _parse_stream_response(self, response: requests.Response) -> Generator[Dict[str, Any], None, None]:
        """
        è§£ææµå¼å“åº”
        
        Args:
            response (requests.Response): HTTPå“åº”å¯¹è±¡
            
        Yields:
            Dict[str, Any]: è§£æåçš„äº‹ä»¶æ•°æ®
        """
        for line in response.iter_lines():
            if line and line.startswith(b'data: '):
                try:
                    data = json.loads(line[6:].decode('utf-8'))
                    yield data
                except json.JSONDecodeError:
                    continue

# è‡ªå®šä¹‰å¼‚å¸¸ç±»
class APIError(Exception):
    """APIè°ƒç”¨é”™è¯¯"""
    pass

class RateLimitError(APIError):
    """é¢‘ç‡é™åˆ¶é”™è¯¯"""
    pass

class AuthenticationError(APIError):
    """è®¤è¯é”™è¯¯"""
    pass

class ValidationError(APIError):
    """éªŒè¯é”™è¯¯"""
    pass
```

#### 4.1.2 å¹¶å‘è°ƒç”¨ç®¡ç†

```python
import asyncio
import aiohttp
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor

class AsyncDifyClient:
    """
    å¼‚æ­¥Dify APIå®¢æˆ·ç«¯
    
    åŠŸèƒ½ï¼šæ”¯æŒå¹¶å‘APIè°ƒç”¨ï¼Œæé«˜å¤„ç†æ•ˆç‡
    """
    
    def __init__(self, api_key: str, base_url: str = "https://api.dify.ai/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
    
    async def send_multiple_messages(
        self, 
        messages: List[Dict[str, Any]], 
        max_concurrent: int = 5
    ) -> List[Dict[str, Any]]:
        """
        å¹¶å‘å‘é€å¤šä¸ªæ¶ˆæ¯
        
        Args:
            messages (List[Dict[str, Any]]): æ¶ˆæ¯åˆ—è¡¨
            max_concurrent (int): æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            List[Dict[str, Any]]: å“åº”ç»“æœåˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def send_single_message(message_data: Dict[str, Any]) -> Dict[str, Any]:
            async with semaphore:
                async with aiohttp.ClientSession() as session:
                    try:
                        async with session.post(
                            f"{self.base_url}/chat-messages",
                            json=message_data,
                            headers=self.headers,
                            timeout=aiohttp.ClientTimeout(total=60)
                        ) as response:
                            if response.status == 200:
                                return await response.json()
                            else:
                                return {
                                    "error": f"HTTP {response.status}",
                                    "message": await response.text()
                                }
                    except Exception as e:
                        return {
                            "error": "Request failed",
                            "message": str(e)
                        }
        
        tasks = [send_single_message(msg) for msg in messages]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
async def batch_process_example():
    """æ‰¹é‡å¤„ç†ç¤ºä¾‹"""
    client = AsyncDifyClient("your-api-key")
    
    messages = [
        {
            "inputs": {"topic": "AI"},
            "query": "What is artificial intelligence?",
            "user": f"user_{i}",
            "response_mode": "blocking"
        }
        for i in range(10)
    ]
    
    results = await client.send_multiple_messages(messages, max_concurrent=3)
    
    for i, result in enumerate(results):
        if "error" not in result:
            print(f"Message {i}: Success")
        else:
            print(f"Message {i}: Error - {result['error']}")

# è¿è¡Œç¤ºä¾‹
# asyncio.run(batch_process_example())
```

### 4.2 åº”ç”¨é…ç½®æœ€ä½³å®è·µ

#### 4.2.1 é…ç½®ç®¡ç†

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
import yaml
import json

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    provider: str
    model: str
    parameters: Dict[str, Any]
    stop: List[str] = None
    
    def __post_init__(self):
        if self.stop is None:
            self.stop = []

@dataclass
class PromptConfig:
    """æç¤ºé…ç½®"""
    prompt_type: str
    simple_prompt_template: Optional[str] = None
    chat_prompt_config: Optional[Dict[str, Any]] = None

@dataclass
class ToolConfig:
    """å·¥å…·é…ç½®"""
    tool_type: str
    provider: str
    tool_name: str
    tool_configuration: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.tool_configuration is None:
            self.tool_configuration = {}

class AppConfigManager:
    """
    åº”ç”¨é…ç½®ç®¡ç†å™¨
    
    åŠŸèƒ½ï¼šç»Ÿä¸€ç®¡ç†åº”ç”¨é…ç½®ï¼Œæ”¯æŒé…ç½®æ–‡ä»¶å’Œç¯å¢ƒå˜é‡
    """
    
    def __init__(self, config_file: Optional[str] = None):
        """
        åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        
        Args:
            config_file (Optional[str]): é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = config_file
        self.config_data = {}
        
        if config_file:
            self.load_from_file(config_file)
    
    def load_from_file(self, file_path: str):
        """
        ä»æ–‡ä»¶åŠ è½½é…ç½®
        
        Args:
            file_path (str): é…ç½®æ–‡ä»¶è·¯å¾„
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                if file_path.endswith('.yaml') or file_path.endswith('.yml'):
                    self.config_data = yaml.safe_load(f)
                elif file_path.endswith('.json'):
                    self.config_data = json.load(f)
                else:
                    raise ValueError("Unsupported config file format")
        except Exception as e:
            raise ConfigError(f"Failed to load config file: {e}")
    
    def get_model_config(self, app_type: str) -> ModelConfig:
        """
        è·å–æ¨¡å‹é…ç½®
        
        Args:
            app_type (str): åº”ç”¨ç±»å‹
            
        Returns:
            ModelConfig: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        model_configs = self.config_data.get('model_configs', {})
        config = model_configs.get(app_type, model_configs.get('default', {}))
        
        return ModelConfig(
            provider=config.get('provider', 'openai'),
            model=config.get('model', 'gpt-3.5-turbo'),
            parameters=config.get('parameters', {}),
            stop=config.get('stop', [])
        )
    
    def get_prompt_config(self, app_type: str) -> PromptConfig:
        """
        è·å–æç¤ºé…ç½®
        
        Args:
            app_type (str): åº”ç”¨ç±»å‹
            
        Returns:
            PromptConfig: æç¤ºé…ç½®å¯¹è±¡
        """
        prompt_configs = self.config_data.get('prompt_configs', {})
        config = prompt_configs.get(app_type, {})
        
        return PromptConfig(
            prompt_type=config.get('prompt_type', 'simple'),
            simple_prompt_template=config.get('simple_prompt_template'),
            chat_prompt_config=config.get('chat_prompt_config')
        )
    
    def get_tool_configs(self, app_type: str) -> List[ToolConfig]:
        """
        è·å–å·¥å…·é…ç½®åˆ—è¡¨
        
        Args:
            app_type (str): åº”ç”¨ç±»å‹
            
        Returns:
            List[ToolConfig]: å·¥å…·é…ç½®åˆ—è¡¨
        """
        tool_configs = self.config_data.get('tool_configs', {})
        configs = tool_configs.get(app_type, [])
        
        return [
            ToolConfig(
                tool_type=config.get('tool_type'),
                provider=config.get('provider'),
                tool_name=config.get('tool_name'),
                tool_configuration=config.get('tool_configuration', {})
            )
            for config in configs
        ]

# é…ç½®æ–‡ä»¶ç¤ºä¾‹ (config.yaml)
CONFIG_EXAMPLE = """
model_configs:
  default:
    provider: openai
    model: gpt-3.5-turbo
    parameters:
      temperature: 0.7
      max_tokens: 2048
    stop: []
  
  chat:
    provider: openai
    model: gpt-4
    parameters:
      temperature: 0.8
      max_tokens: 4096
  
  agent:
    provider: openai
    model: gpt-4
    parameters:
      temperature: 0.2
      max_tokens: 4096

prompt_configs:
  chat:
    prompt_type: simple
    simple_prompt_template: |
      ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¾“å…¥æä¾›å‡†ç¡®å’Œæœ‰å¸®åŠ©çš„å›ç­”ã€‚
      
      ç”¨æˆ·è¾“å…¥ï¼š{{query}}
  
  agent:
    prompt_type: simple
    simple_prompt_template: |
      ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å„ç§å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ã€‚
      è¯·æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚
      
      ç”¨æˆ·éœ€æ±‚ï¼š{{query}}

tool_configs:
  agent:
    - tool_type: builtin
      provider: duckduckgo
      tool_name: duckduckgo_search
      tool_configuration:
        result_type: text
    
    - tool_type: builtin
      provider: calculator
      tool_name: calculator
      tool_configuration: {}
"""

class ConfigError(Exception):
    """é…ç½®é”™è¯¯"""
    pass
```

### 4.3 æ€§èƒ½ä¼˜åŒ–å®è·µ

#### 4.3.1 ç¼“å­˜ç­–ç•¥

```python
import redis
import json
import hashlib
from typing import Any, Optional, Callable
from functools import wraps

class CacheManager:
    """
    ç¼“å­˜ç®¡ç†å™¨
    
    åŠŸèƒ½ï¼šæä¾›å¤šçº§ç¼“å­˜æ”¯æŒï¼Œä¼˜åŒ–APIå“åº”æ€§èƒ½
    """
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            redis_url (str): Redisè¿æ¥URL
        """
        self.redis_client = redis.from_url(redis_url)
        self.local_cache = {}
        self.max_local_cache_size = 1000
    
    def get_cache_key(self, prefix: str, **kwargs) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            prefix (str): ç¼“å­˜å‰ç¼€
            **kwargs: ç¼“å­˜å‚æ•°
            
        Returns:
            str: ç¼“å­˜é”®
        """
        # å°†å‚æ•°æ’åºå¹¶åºåˆ—åŒ–
        sorted_params = sorted(kwargs.items())
        param_str = json.dumps(sorted_params, sort_keys=True)
        
        # ç”Ÿæˆå“ˆå¸Œ
        hash_obj = hashlib.md5(param_str.encode('utf-8'))
        return f"{prefix}:{hash_obj.hexdigest()}"
    
    def get(self, key: str) -> Optional[Any]:
        """
        è·å–ç¼“å­˜å€¼
        
        Args:
            key (str): ç¼“å­˜é”®
            
        Returns:
            Optional[Any]: ç¼“å­˜å€¼
        """
        # å…ˆæŸ¥æœ¬åœ°ç¼“å­˜
        if key in self.local_cache:
            return self.local_cache[key]
        
        # å†æŸ¥Redisç¼“å­˜
        try:
            value = self.redis_client.get(key)
            if value:
                decoded_value = json.loads(value)
                # æ›´æ–°æœ¬åœ°ç¼“å­˜
                self._update_local_cache(key, decoded_value)
                return decoded_value
        except Exception as e:
            print(f"Redis get error: {e}")
        
        return None
    
    def set(self, key: str, value: Any, ttl: int = 3600):
        """
        è®¾ç½®ç¼“å­˜å€¼
        
        Args:
            key (str): ç¼“å­˜é”®
            value (Any): ç¼“å­˜å€¼
            ttl (int): è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        # æ›´æ–°æœ¬åœ°ç¼“å­˜
        self._update_local_cache(key, value)
        
        # æ›´æ–°Redisç¼“å­˜
        try:
            self.redis_client.setex(
                key, 
                ttl, 
                json.dumps(value, ensure_ascii=False)
            )
        except Exception as e:
            print(f"Redis set error: {e}")
    
    def _update_local_cache(self, key: str, value: Any):
        """
        æ›´æ–°æœ¬åœ°ç¼“å­˜
        
        Args:
            key (str): ç¼“å­˜é”®
            value (Any): ç¼“å­˜å€¼
        """
        # å¦‚æœæœ¬åœ°ç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€è€çš„æ¡ç›®
        if len(self.local_cache) >= self.max_local_cache_size:
            oldest_key = next(iter(self.local_cache))
            del self.local_cache[oldest_key]
        
        self.local_cache[key] = value

def cached_response(cache_manager: CacheManager, ttl: int = 3600, prefix: str = "api"):
    """
    ç¼“å­˜è£…é¥°å™¨
    
    Args:
        cache_manager (CacheManager): ç¼“å­˜ç®¡ç†å™¨
        ttl (int): ç¼“å­˜è¿‡æœŸæ—¶é—´
        prefix (str): ç¼“å­˜å‰ç¼€
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = cache_manager.get_cache_key(
                prefix=f"{prefix}:{func.__name__}",
                args=args,
                kwargs=kwargs
            )
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = cache_manager.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # æ‰§è¡Œå‡½æ•°
            result = func(*args, **kwargs)
            
            # ç¼“å­˜ç»“æœ
            cache_manager.set(cache_key, result, ttl)
            
            return result
        
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
cache_manager = CacheManager()

@cached_response(cache_manager, ttl=1800, prefix="chat")
def get_chat_response(query: str, model: str = "gpt-3.5-turbo"):
    """
    è·å–èŠå¤©å“åº”ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    Args:
        query (str): ç”¨æˆ·æŸ¥è¯¢
        model (str): æ¨¡å‹åç§°
        
    Returns:
        str: èŠå¤©å“åº”
    """
    # è¿™é‡Œæ˜¯å®é™…çš„APIè°ƒç”¨é€»è¾‘
    # ä¸ºäº†æ¼”ç¤ºï¼Œè¿”å›æ¨¡æ‹Ÿå“åº”
    return f"Response for '{query}' using {model}"
```

#### 4.3.2 è¿æ¥æ± ç®¡ç†

```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from typing import Dict, Any
import threading

class ConnectionPoolManager:
    """
    è¿æ¥æ± ç®¡ç†å™¨
    
    åŠŸèƒ½ï¼šç®¡ç†HTTPè¿æ¥æ± ï¼Œæé«˜APIè°ƒç”¨æ•ˆç‡
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        if hasattr(self, '_initialized'):
            return
        
        self._sessions = {}
        self._initialized = True
    
    def get_session(self, base_url: str, **kwargs) -> requests.Session:
        """
        è·å–ä¼šè¯å¯¹è±¡
        
        Args:
            base_url (str): åŸºç¡€URL
            **kwargs: é¢å¤–é…ç½®å‚æ•°
            
        Returns:
            requests.Session: ä¼šè¯å¯¹è±¡
        """
        if base_url not in self._sessions:
            session = self._create_session(**kwargs)
            self._sessions[base_url] = session
        
        return self._sessions[base_url]
    
    def _create_session(self, **kwargs) -> requests.Session:
        """
        åˆ›å»ºä¼šè¯å¯¹è±¡
        
        Args:
            **kwargs: é…ç½®å‚æ•°
            
        Returns:
            requests.Session: é…ç½®å¥½çš„ä¼šè¯å¯¹è±¡
        """
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=kwargs.get('max_retries', 3),
            status_forcelist=[429, 500, 502, 503, 504],
            method_whitelist=["HEAD", "GET", "PUT", "DELETE", "OPTIONS", "TRACE", "POST"],
            backoff_factor=kwargs.get('backoff_factor', 1)
        )
        
        # é…ç½®HTTPé€‚é…å™¨
        adapter = HTTPAdapter(
            pool_connections=kwargs.get('pool_connections', 10),
            pool_maxsize=kwargs.get('pool_maxsize', 20),
            max_retries=retry_strategy
        )
        
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # è®¾ç½®è¶…æ—¶
        session.timeout = kwargs.get('timeout', 30)
        
        return session

class OptimizedDifyClient:
    """
    ä¼˜åŒ–çš„Difyå®¢æˆ·ç«¯
    
    åŠŸèƒ½ï¼šä½¿ç”¨è¿æ¥æ± å’Œç¼“å­˜ä¼˜åŒ–çš„APIå®¢æˆ·ç«¯
    """
    
    def __init__(self, api_key: str, base_url: str = "https://api.dify.ai/v1"):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            api_key (str): APIå¯†é’¥
            base_url (str): APIåŸºç¡€URL
        """
        self.api_key = api_key
        self.base_url = base_url
        
        # è·å–è¿æ¥æ± ç®¡ç†å™¨
        self.pool_manager = ConnectionPoolManager()
        
        # è·å–ä¼šè¯
        self.session = self.pool_manager.get_session(
            base_url,
            max_retries=3,
            pool_connections=20,
            pool_maxsize=50,
            timeout=60
        )
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json',
            'User-Agent': 'DifyClient/1.0'
        })
        
        # åˆå§‹åŒ–ç¼“å­˜
        self.cache_manager = CacheManager()
    
    @cached_response(CacheManager(), ttl=300, prefix="dify_chat")
    def send_message_cached(
        self, 
        inputs: Dict[str, Any], 
        query: str, 
        user: str,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å‘é€æ¶ˆæ¯ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            inputs (Dict[str, Any]): è¾“å…¥å˜é‡
            query (str): ç”¨æˆ·æŸ¥è¯¢
            user (str): ç”¨æˆ·æ ‡è¯†
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            Dict[str, Any]: å“åº”ç»“æœ
        """
        data = {
            "inputs": inputs,
            "query": query,
            "user": user,
            "response_mode": "blocking",
            **kwargs
        }
        
        response = self.session.post(
            f"{self.base_url}/chat-messages",
            json=data
        )
        
        response.raise_for_status()
        return response.json()
    
    def send_message_stream(
        self, 
        inputs: Dict[str, Any], 
        query: str, 
        user: str,
        **kwargs
    ):
        """
        å‘é€æµå¼æ¶ˆæ¯
        
        Args:
            inputs (Dict[str, Any]): è¾“å…¥å˜é‡
            query (str): ç”¨æˆ·æŸ¥è¯¢
            user (str): ç”¨æˆ·æ ‡è¯†
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            Dict[str, Any]: æµå¼å“åº”äº‹ä»¶
        """
        data = {
            "inputs": inputs,
            "query": query,
            "user": user,
            "response_mode": "streaming",
            **kwargs
        }
        
        response = self.session.post(
            f"{self.base_url}/chat-messages",
            json=data,
            stream=True
        )
        
        response.raise_for_status()
        
        for line in response.iter_lines():
            if line and line.startswith(b'data: '):
                try:
                    data = json.loads(line[6:].decode('utf-8'))
                    yield data
                except json.JSONDecodeError:
                    continue
```

## 5. å®æˆ˜æ¡ˆä¾‹

### 5.1 æ™ºèƒ½å®¢æœç³»ç»Ÿ

```python
class IntelligentCustomerService:
    """
    æ™ºèƒ½å®¢æœç³»ç»Ÿ
    
    åŠŸèƒ½ï¼šåŸºäºDifyæ„å»ºçš„å®Œæ•´å®¢æœè§£å†³æ–¹æ¡ˆ
    ç‰¹æ€§ï¼šæ„å›¾è¯†åˆ«ã€çŸ¥è¯†åº“æ£€ç´¢ã€å·¥å•åˆ›å»ºã€æ»¡æ„åº¦è°ƒæŸ¥
    """
    
    def __init__(self, dify_client: OptimizedDifyClient):
        """
        åˆå§‹åŒ–å®¢æœç³»ç»Ÿ
        
        Args:
            dify_client (OptimizedDifyClient): Difyå®¢æˆ·ç«¯
        """
        self.dify_client = dify_client
        self.conversation_manager = ConversationManager()
        self.knowledge_base = KnowledgeBaseManager()
        self.ticket_system = TicketSystem()
    
    async def handle_customer_inquiry(
        self, 
        customer_id: str, 
        message: str, 
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†å®¢æˆ·å’¨è¯¢
        
        Args:
            customer_id (str): å®¢æˆ·ID
            message (str): å®¢æˆ·æ¶ˆæ¯
            context (Dict[str, Any]): ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        try:
            # 1. è·å–æˆ–åˆ›å»ºå¯¹è¯
            conversation = await self.conversation_manager.get_or_create_conversation(
                customer_id
            )
            
            # 2. æ„å›¾è¯†åˆ«
            intent = await self._identify_intent(message)
            
            # 3. æ ¹æ®æ„å›¾å¤„ç†
            if intent['category'] == 'faq':
                # å¸¸è§é—®é¢˜ï¼Œç›´æ¥ä»çŸ¥è¯†åº“æ£€ç´¢
                response = await self._handle_faq(message, context)
            elif intent['category'] == 'technical_support':
                # æŠ€æœ¯æ”¯æŒï¼Œä½¿ç”¨Agentå¤„ç†
                response = await self._handle_technical_support(
                    message, conversation['id'], context
                )
            elif intent['category'] == 'complaint':
                # æŠ•è¯‰ï¼Œåˆ›å»ºå·¥å•å¹¶è½¬äººå·¥
                response = await self._handle_complaint(
                    customer_id, message, context
                )
            else:
                # é€šç”¨å¯¹è¯
                response = await self._handle_general_chat(
                    message, conversation['id'], context
                )
            
            # 4. è®°å½•å¯¹è¯å†å²
            await self.conversation_manager.add_message(
                conversation['id'], 
                'user', 
                message
            )
            await self.conversation_manager.add_message(
                conversation['id'], 
                'assistant', 
                response['answer']
            )
            
            # 5. è¿”å›ç»“æœ
            return {
                'success': True,
                'response': response,
                'conversation_id': conversation['id'],
                'intent': intent
            }
            
        except Exception as e:
            logger.exception(f"å¤„ç†å®¢æˆ·å’¨è¯¢å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'fallback_response': 'æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»äººå·¥å®¢æœã€‚'
            }
    
    async def _identify_intent(self, message: str) -> Dict[str, Any]:
        """
        è¯†åˆ«ç”¨æˆ·æ„å›¾
        
        Args:
            message (str): ç”¨æˆ·æ¶ˆæ¯
            
        Returns:
            Dict[str, Any]: æ„å›¾è¯†åˆ«ç»“æœ
        """
        # ä½¿ç”¨ä¸“é—¨çš„æ„å›¾è¯†åˆ«åº”ç”¨
        response = await self.dify_client.send_message_cached(
            inputs={
                'message': message
            },
            query='è¯·è¯†åˆ«ç”¨æˆ·æ„å›¾',
            user='intent_classifier'
        )
        
        # è§£ææ„å›¾
        intent_text = response.get('answer', '')
        
        # ç®€å•çš„æ„å›¾åˆ†ç±»é€»è¾‘ï¼ˆå®é™…åº”ç”¨ä¸­å¯èƒ½æ›´å¤æ‚ï¼‰
        if any(keyword in message.lower() for keyword in ['å¯†ç ', 'ç™»å½•', 'è´¦æˆ·']):
            category = 'technical_support'
        elif any(keyword in message.lower() for keyword in ['æŠ•è¯‰', 'ä¸æ»¡', 'é—®é¢˜']):
            category = 'complaint'
        elif any(keyword in message.lower() for keyword in ['å¦‚ä½•', 'æ€ä¹ˆ', 'ä»€ä¹ˆæ˜¯']):
            category = 'faq'
        else:
            category = 'general'
        
        return {
            'category': category,
            'confidence': 0.8,
            'details': intent_text
        }
    
    async def _handle_faq(self, message: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†å¸¸è§é—®é¢˜
        
        Args:
            message (str): ç”¨æˆ·æ¶ˆæ¯
            context (Dict[str, Any]): ä¸Šä¸‹æ–‡
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        # ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ç­”æ¡ˆ
        kb_results = await self.knowledge_base.search(message)
        
        if kb_results and kb_results[0]['score'] > 0.8:
            # é«˜ç½®ä¿¡åº¦åŒ¹é…ï¼Œç›´æ¥è¿”å›çŸ¥è¯†åº“ç­”æ¡ˆ
            return {
                'answer': kb_results[0]['content'],
                'source': 'knowledge_base',
                'confidence': kb_results[0]['score']
            }
        else:
            # ä½ç½®ä¿¡åº¦ï¼Œä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
            response = await self.dify_client.send_message_cached(
                inputs={
                    'question': message,
                    'context': json.dumps(context or {})
                },
                query='è¯·åŸºäºçŸ¥è¯†åº“å›ç­”ç”¨æˆ·é—®é¢˜',
                user='faq_assistant'
            )
            
            return {
                'answer': response.get('answer', ''),
                'source': 'llm_generated',
                'confidence': 0.6
            }
    
    async def _handle_technical_support(
        self, 
        message: str, 
        conversation_id: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å¤„ç†æŠ€æœ¯æ”¯æŒè¯·æ±‚
        
        Args:
            message (str): ç”¨æˆ·æ¶ˆæ¯
            conversation_id (str): å¯¹è¯ID
            context (Dict[str, Any]): ä¸Šä¸‹æ–‡
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        # ä½¿ç”¨Agentåº”ç”¨å¤„ç†æŠ€æœ¯æ”¯æŒ
        inputs = {
            'user_message': message,
            'user_context': json.dumps(context or {}),
            'conversation_history': await self._get_conversation_history(conversation_id)
        }
        
        # æµå¼å¤„ç†æŠ€æœ¯æ”¯æŒè¯·æ±‚
        response_parts = []
        async for event in self.dify_client.send_message_stream(
            inputs=inputs,
            query='è¯·å¸®åŠ©ç”¨æˆ·è§£å†³æŠ€æœ¯é—®é¢˜',
            user=f'tech_support_{conversation_id}',
            conversation_id=conversation_id
        ):
            if event.get('event') == 'message':
                response_parts.append(event.get('answer', ''))
            elif event.get('event') == 'message_end':
                break
        
        full_response = ''.join(response_parts)
        
        return {
            'answer': full_response,
            'source': 'agent_support',
            'requires_followup': True
        }
    
    async def _handle_complaint(
        self, 
        customer_id: str, 
        message: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å¤„ç†æŠ•è¯‰
        
        Args:
            customer_id (str): å®¢æˆ·ID
            message (str): æŠ•è¯‰å†…å®¹
            context (Dict[str, Any]): ä¸Šä¸‹æ–‡
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        # åˆ›å»ºå·¥å•
        ticket = await self.ticket_system.create_ticket(
            customer_id=customer_id,
            category='complaint',
            description=message,
            priority='high',
            context=context
        )
        
        # ç”Ÿæˆå›å¤
        response = f"""
æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼Œæˆ‘ä»¬éå¸¸é‡è§†æ‚¨çš„æ„è§ã€‚

æ‚¨çš„æŠ•è¯‰å·²ç»è®°å½•ï¼Œå·¥å•å·ä¸ºï¼š{ticket['ticket_id']}

æˆ‘ä»¬çš„å®¢æœä¸“å‘˜å°†åœ¨2å°æ—¶å†…ä¸æ‚¨è”ç³»ï¼Œä¸ºæ‚¨è§£å†³é—®é¢˜ã€‚

å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·æ‹¨æ‰“å®¢æœçƒ­çº¿ï¼š400-123-4567
        """.strip()
        
        return {
            'answer': response,
            'source': 'ticket_system',
            'ticket_id': ticket['ticket_id'],
            'escalated': True
        }

class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.conversations = {}
    
    async def get_or_create_conversation(self, customer_id: str) -> Dict[str, Any]:
        """è·å–æˆ–åˆ›å»ºå¯¹è¯"""
        if customer_id not in self.conversations:
            self.conversations[customer_id] = {
                'id': f"conv_{customer_id}_{int(time.time())}",
                'customer_id': customer_id,
                'created_at': time.time(),
                'messages': []
            }
        return self.conversations[customer_id]
    
    async def add_message(self, conversation_id: str, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        for conv in self.conversations.values():
            if conv['id'] == conversation_id:
                conv['messages'].append({
                    'role': role,
                    'content': content,
                    'timestamp': time.time()
                })
                break

class KnowledgeBaseManager:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    async def search(self, query: str) -> List[Dict[str, Any]]:
        """æœç´¢çŸ¥è¯†åº“"""
        # æ¨¡æ‹ŸçŸ¥è¯†åº“æœç´¢
        return [
            {
                'content': 'è¿™æ˜¯çŸ¥è¯†åº“ä¸­çš„ç­”æ¡ˆ...',
                'score': 0.9,
                'source': 'kb_001'
            }
        ]

class TicketSystem:
    """å·¥å•ç³»ç»Ÿ"""
    
    async def create_ticket(self, **kwargs) -> Dict[str, Any]:
        """åˆ›å»ºå·¥å•"""
        ticket_id = f"TK{int(time.time())}"
        return {
            'ticket_id': ticket_id,
            'status': 'open',
            'created_at': time.time(),
            **kwargs
        }
```

### 5.2 æ–‡æ¡£å¤„ç†å·¥ä½œæµ

```python
class DocumentProcessingWorkflow:
    """
    æ–‡æ¡£å¤„ç†å·¥ä½œæµ
    
    åŠŸèƒ½ï¼šè‡ªåŠ¨åŒ–æ–‡æ¡£å¤„ç†æµç¨‹
    æµç¨‹ï¼šä¸Šä¼  -> è§£æ -> åˆ†æ -> æ‘˜è¦ -> å­˜å‚¨
    """
    
    def __init__(self, dify_client: OptimizedDifyClient):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å·¥ä½œæµ
        
        Args:
            dify_client (OptimizedDifyClient): Difyå®¢æˆ·ç«¯
        """
        self.dify_client = dify_client
        self.file_processor = FileProcessor()
        self.document_analyzer = DocumentAnalyzer()
        self.storage_manager = StorageManager()
    
    async def process_document(
        self, 
        file_path: str, 
        user_id: str,
        processing_options: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£
        
        Args:
            file_path (str): æ–‡ä»¶è·¯å¾„
            user_id (str): ç”¨æˆ·ID
            processing_options (Dict[str, Any]): å¤„ç†é€‰é¡¹
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        processing_id = f"proc_{int(time.time())}_{user_id}"
        
        try:
            # 1. æ–‡ä»¶é¢„å¤„ç†
            file_info = await self.file_processor.process_file(file_path)
            
            # 2. ä½¿ç”¨å·¥ä½œæµå¤„ç†æ–‡æ¡£
            workflow_inputs = {
                'document_content': file_info['content'],
                'document_type': file_info['type'],
                'file_name': file_info['name'],
                'processing_options': json.dumps(processing_options or {})
            }
            
            # 3. è°ƒç”¨Difyå·¥ä½œæµ
            processing_result = {}
            async for event in self.dify_client.send_message_stream(
                inputs=workflow_inputs,
                query='å¤„ç†æ–‡æ¡£',
                user=user_id
            ):
                if event.get('event') == 'workflow_finished':
                    processing_result = event.get('data', {})
                    break
                elif event.get('event') == 'node_finished':
                    # è®°å½•èŠ‚ç‚¹å®ŒæˆçŠ¶æ€
                    node_data = event.get('data', {})
                    print(f"èŠ‚ç‚¹ {node_data.get('title')} å®Œæˆ")
            
            # 4. åå¤„ç†
            final_result = await self._post_process_result(
                processing_result, 
                file_info, 
                processing_id
            )
            
            # 5. å­˜å‚¨ç»“æœ
            await self.storage_manager.store_result(
                processing_id, 
                final_result
            )
            
            return {
                'success': True,
                'processing_id': processing_id,
                'result': final_result
            }
            
        except Exception as e:
            logger.exception(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return {
                'success': False,
                'processing_id': processing_id,
                'error': str(e)
            }
    
    async def _post_process_result(
        self, 
        workflow_result: Dict[str, Any], 
        file_info: Dict[str, Any],
        processing_id: str
    ) -> Dict[str, Any]:
        """
        åå¤„ç†å·¥ä½œæµç»“æœ
        
        Args:
            workflow_result (Dict[str, Any]): å·¥ä½œæµç»“æœ
            file_info (Dict[str, Any]): æ–‡ä»¶ä¿¡æ¯
            processing_id (str): å¤„ç†ID
            
        Returns:
            Dict[str, Any]: æœ€ç»ˆç»“æœ
        """
        return {
            'processing_id': processing_id,
            'file_info': file_info,
            'summary': workflow_result.get('summary', ''),
            'keywords': workflow_result.get('keywords', ''),
            'analysis': workflow_result.get('analysis', {}),
            'processed_at': time.time()
        }

class FileProcessor:
    """æ–‡ä»¶å¤„ç†å™¨"""
    
    async def process_file(self, file_path: str) -> Dict[str, Any]:
        """å¤„ç†æ–‡ä»¶"""
        # æ¨¡æ‹Ÿæ–‡ä»¶å¤„ç†
        return {
            'name': os.path.basename(file_path),
            'type': 'pdf',
            'content': 'æ–‡æ¡£å†…å®¹...',
            'size': 1024
        }

class DocumentAnalyzer:
    """æ–‡æ¡£åˆ†æå™¨"""
    
    async def analyze_document(self, content: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£"""
        return {
            'word_count': len(content.split()),
            'language': 'zh',
            'topics': ['ä¸»é¢˜1', 'ä¸»é¢˜2']
        }

class StorageManager:
    """å­˜å‚¨ç®¡ç†å™¨"""
    
    async def store_result(self, processing_id: str, result: Dict[str, Any]):
        """å­˜å‚¨ç»“æœ"""
        # æ¨¡æ‹Ÿå­˜å‚¨æ“ä½œ
        print(f"å­˜å‚¨å¤„ç†ç»“æœ: {processing_id}")
```

## 6. æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†Difyæ¡†æ¶çš„å®Œæ•´ä½¿ç”¨æŒ‡å—ï¼Œæ¶µç›–äº†ï¼š

1. **æ¡†æ¶æ¦‚è¿°**ï¼šæ ¸å¿ƒç‰¹ç‚¹å’Œç»„ä»¶æ¶æ„
2. **APIæ¥å£è¯¦è§£**ï¼šä¸‰å±‚APIè®¾è®¡å’Œæ ¸å¿ƒæ¥å£å®ç°
3. **åº”ç”¨å¼€å‘ç¤ºä¾‹**ï¼šChatã€Agentã€Workflowåº”ç”¨åˆ›å»º
4. **æœ€ä½³å®è·µ**ï¼šé”™è¯¯å¤„ç†ã€ç¼“å­˜ç­–ç•¥ã€æ€§èƒ½ä¼˜åŒ–
5. **å®æˆ˜æ¡ˆä¾‹**ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿå’Œæ–‡æ¡£å¤„ç†å·¥ä½œæµ

é€šè¿‡è¿™äº›å†…å®¹ï¼Œå¼€å‘è€…å¯ä»¥ï¼š
- å¿«é€Ÿç†è§£Difyæ¡†æ¶çš„è®¾è®¡ç†å¿µ
- æŒæ¡APIæ¥å£çš„ä½¿ç”¨æ–¹æ³•
- å­¦ä¼šåˆ›å»ºä¸åŒç±»å‹çš„AIåº”ç”¨
- åº”ç”¨æœ€ä½³å®è·µä¼˜åŒ–åº”ç”¨æ€§èƒ½
- å‚è€ƒå®æˆ˜æ¡ˆä¾‹æ„å»ºå¤æ‚ç³»ç»Ÿ

Difyæ¡†æ¶ä¸ºAIåº”ç”¨å¼€å‘æä¾›äº†å¼ºå¤§è€Œçµæ´»çš„åŸºç¡€è®¾æ–½ï¼Œé€šè¿‡åˆç†ä½¿ç”¨å…¶å„é¡¹åŠŸèƒ½ï¼Œå¯ä»¥å¿«é€Ÿæ„å»ºå‡ºé«˜è´¨é‡çš„AIåº”ç”¨ã€‚
