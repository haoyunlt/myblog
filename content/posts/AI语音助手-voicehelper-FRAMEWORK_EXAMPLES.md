---
title: "VoiceHelper æ¡†æ¶ä½¿ç”¨ç¤ºä¾‹ä¸é›†æˆæŒ‡å—"
date: 2025-09-28T00:47:16+08:00
draft: false
tags: ['æºç åˆ†æ', 'æŠ€æœ¯æ–‡æ¡£']
categories: ['AIè¯­éŸ³åŠ©æ‰‹']
description: "VoiceHelper æ¡†æ¶ä½¿ç”¨ç¤ºä¾‹ä¸é›†æˆæŒ‡å—çš„æ·±å…¥æŠ€æœ¯åˆ†ææ–‡æ¡£"
keywords: ['æºç åˆ†æ', 'æŠ€æœ¯æ–‡æ¡£']
author: "æŠ€æœ¯åˆ†æå¸ˆ"
weight: 1
---

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº†VoiceHelperæ¡†æ¶çš„è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹ï¼Œæ¶µç›–å¿«é€Ÿéƒ¨ç½²ã€SDKé›†æˆã€å¤šå¹³å°å¼€å‘å’Œå®é™…åº”ç”¨åœºæ™¯ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿä¸Šæ‰‹å¹¶æ·±åº¦é›†æˆVoiceHelperå¹³å°ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. Dockerä¸€é”®éƒ¨ç½²

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/voicehelper/voicehelper.git
cd voicehelper

# é…ç½®ç¯å¢ƒå˜é‡
cp shared/configs/env.example .env

# ç¼–è¾‘ç¯å¢ƒé…ç½®
vim .env
```

#### ç¯å¢ƒé…ç½®ç¤ºä¾‹

```bash
# .env é…ç½®æ–‡ä»¶
# åŸºç¡€é…ç½®
NODE_ENV=production
LOG_LEVEL=info

# æ•°æ®åº“é…ç½®
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=voicehelper
POSTGRES_USER=voicehelper_user
POSTGRES_PASSWORD=your_secure_password

# Redisé…ç½®
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=your_redis_password

# Neo4jé…ç½®ï¼ˆå›¾æ•°æ®åº“ï¼‰
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password

# MinIOé…ç½®ï¼ˆå¯¹è±¡å­˜å‚¨ï¼‰
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=your_minio_password

# AIæœåŠ¡APIå¯†é’¥
OPENAI_API_KEY=your_openai_api_key
AZURE_OPENAI_ENDPOINT=your_azure_endpoint
AZURE_OPENAI_API_KEY=your_azure_key

# æœåŠ¡ç«¯å£é…ç½®
BACKEND_PORT=8080
ALGO_PORT=8070
FRONTEND_PORT=3000

# JWTé…ç½®
JWT_SECRET=your_jwt_secret_key
JWT_EXPIRES_IN=24h

# æ–‡ä»¶ä¸Šä¼ é…ç½®
MAX_FILE_SIZE=100MB
ALLOWED_FILE_TYPES=pdf,docx,txt,md,html
```

#### å¯åŠ¨æœåŠ¡

```bash
# ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
docker-compose -f docker-compose.prod.yml up -d

# å¼€å‘ç¯å¢ƒéƒ¨ç½²
docker-compose -f docker-compose.dev.yml up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# å®æ—¶æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# æŸ¥çœ‹ç‰¹å®šæœåŠ¡æ—¥å¿—
docker-compose logs -f backend
docker-compose logs -f algo
docker-compose logs -f frontend
```

#### å¥åº·æ£€æŸ¥

```bash
# æ£€æŸ¥åç«¯æœåŠ¡
curl http://localhost:8080/health

# æ£€æŸ¥ç®—æ³•æœåŠ¡
curl http://localhost:8070/health

# æ£€æŸ¥å‰ç«¯æœåŠ¡
curl http://localhost:3000
```

### 2. å¼€å‘ç¯å¢ƒæ‰‹åŠ¨æ­å»º

#### Goåç«¯æœåŠ¡

```bash
cd backend

# å®‰è£…ä¾èµ–
go mod download

# è®¾ç½®å¼€å‘ç¯å¢ƒå˜é‡
export GO_ENV=development
export DB_HOST=localhost
export DB_PORT=5432
export REDIS_HOST=localhost
export REDIS_PORT=6379

# è¿è¡Œæ•°æ®åº“è¿ç§»
make migrate-up

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
make dev

# æˆ–è€…ç›´æ¥è¿è¡Œ
go run cmd/gateway/main.go
```

#### Pythonç®—æ³•æœåŠ¡

```bash
cd algo

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è®¾ç½®ç¯å¢ƒå˜é‡
export PYTHONPATH=${PWD}
export ALGO_ENV=development

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
python app/main.py

# æˆ–ä½¿ç”¨uvicorn
uvicorn app.main:app --host 0.0.0.0 --port 8070 --reload
```

#### Next.jså‰ç«¯æœåŠ¡

```bash
cd platforms/web

# å®‰è£…ä¾èµ–
npm install

# è®¾ç½®ç¯å¢ƒå˜é‡
cp .env.example .env.local
# ç¼–è¾‘ .env.local æ–‡ä»¶

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
npm run dev

# æ„å»ºç”Ÿäº§ç‰ˆæœ¬
npm run build
npm start
```

## ğŸ“± SDKé›†æˆç¤ºä¾‹

### JavaScript/TypeScript SDK

#### å®‰è£…å’Œåˆå§‹åŒ–

```bash
npm install @voicehelper/javascript-sdk
```

```typescript
import { VoiceHelperClient, VoiceHelperConfig } from '@voicehelper/javascript-sdk';

const config: VoiceHelperConfig = {
  apiKey: 'your-api-key',
  baseURL: 'https://api.voicehelper.ai',
  timeout: 30000,
  retryAttempts: 3,
  enableLogging: true
};

const client = new VoiceHelperClient(config);
```

#### åŸºç¡€èŠå¤©åŠŸèƒ½

```typescript
// ç®€å•æ–‡æœ¬èŠå¤©
async function simpleChat() {
  try {
    const response = await client.chat.send({
      message: 'ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µ',
      conversation_id: 'conv_123',
      model: 'gpt-3.5-turbo'
    });
    
    console.log('AIå›å¤:', response.data.message);
    console.log('å‚è€ƒèµ„æ–™:', response.data.references);
  } catch (error) {
    console.error('èŠå¤©å¤±è´¥:', error);
  }
}

// æµå¼èŠå¤©
async function streamingChat() {
  try {
    const stream = await client.chat.createStream({
      message: 'è¯·è¯¦ç»†ä»‹ç»æœºå™¨å­¦ä¹ çš„å‘å±•å†ç¨‹',
      conversation_id: 'conv_456',
      retrieval_config: {
        mode: 'hybrid',
        top_k: 10,
        collection: 'ai_knowledge'
      }
    });

    console.log('å¼€å§‹æ¥æ”¶å›å¤...');
    
    for await (const chunk of stream) {
      switch (chunk.type) {
        case 'retrieval_start':
          console.log('ğŸ” å¼€å§‹æ£€ç´¢ç›¸å…³èµ„æ–™...');
          break;
          
        case 'retrieval_result':
          console.log(`ğŸ“š æ‰¾åˆ° ${chunk.data.results.length} æ¡ç›¸å…³èµ„æ–™`);
          break;
          
        case 'generation_start':
          console.log('ğŸ¤– AIå¼€å§‹ç”Ÿæˆå›å¤...');
          break;
          
        case 'generation_chunk':
          process.stdout.write(chunk.data.text);
          break;
          
        case 'generation_done':
          console.log(`\nâœ… å›å¤å®Œæˆï¼Œè€—æ—¶: ${chunk.data.total_time_ms}ms`);
          console.log('ğŸ“– å‚è€ƒèµ„æ–™:', chunk.data.context_sources);
          break;
          
        case 'error':
          console.error('âŒ é”™è¯¯:', chunk.data.error);
          break;
      }
    }
  } catch (error) {
    console.error('æµå¼èŠå¤©å¤±è´¥:', error);
  }
}
```

#### æ–‡æ¡£ç®¡ç†åŠŸèƒ½

```typescript
// æ–‡æ¡£ä¸Šä¼ å…¥åº“
async function uploadDocuments() {
  try {
    const files = [
      {
        filename: 'company_handbook.pdf',
        content: await fs.readFile('path/to/handbook.pdf'),
        contentType: 'application/pdf'
      },
      {
        filename: 'product_guide.docx',
        content: await fs.readFile('path/to/guide.docx'),
        contentType: 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
      }
    ];

    const ingestResult = await client.documents.ingest({
      files,
      collection_name: 'company_knowledge',
      chunk_size: 1000,
      chunk_overlap: 200,
      metadata: {
        department: 'HR',
        category: 'policy',
        version: '2024.1'
      }
    });

    console.log('å…¥åº“ä»»åŠ¡ID:', ingestResult.task_id);

    // æŸ¥è¯¢å…¥åº“è¿›åº¦
    const interval = setInterval(async () => {
      try {
        const status = await client.documents.getTaskStatus(ingestResult.task_id);
        console.log(`å¤„ç†è¿›åº¦: ${status.progress}% - ${status.status}`);

        if (status.status === 'completed') {
          console.log('âœ… æ–‡æ¡£å…¥åº“å®Œæˆ!');
          console.log('å¤„ç†ç»“æœ:', status.result);
          clearInterval(interval);
        } else if (status.status === 'failed') {
          console.error('âŒ æ–‡æ¡£å…¥åº“å¤±è´¥:', status.error);
          clearInterval(interval);
        }
      } catch (error) {
        console.error('æŸ¥è¯¢çŠ¶æ€å¤±è´¥:', error);
        clearInterval(interval);
      }
    }, 2000);

  } catch (error) {
    console.error('æ–‡æ¡£ä¸Šä¼ å¤±è´¥:', error);
  }
}

// æ–‡æ¡£æœç´¢
async function searchDocuments() {
  try {
    const searchResult = await client.documents.search({
      query: 'è¯·å‡æ”¿ç­–',
      collection_name: 'company_knowledge',
      top_k: 5,
      filters: {
        department: 'HR',
        category: 'policy'
      }
    });

    console.log(`æ‰¾åˆ° ${searchResult.total_found} æ¡ç›¸å…³æ–‡æ¡£:`);
    searchResult.results.forEach((result, index) => {
      console.log(`${index + 1}. ${result.title}`);
      console.log(`   ç›¸å…³åº¦: ${(result.score * 100).toFixed(1)}%`);
      console.log(`   æ‘˜è¦: ${result.content.substring(0, 100)}...`);
    });
  } catch (error) {
    console.error('æ–‡æ¡£æœç´¢å¤±è´¥:', error);
  }
}
```

#### è¯­éŸ³äº¤äº’åŠŸèƒ½

```typescript
// è¯­éŸ³èŠå¤©ç±»
class VoiceChat {
  private client: VoiceHelperClient;
  private ws: WebSocket | null = null;
  private mediaRecorder: MediaRecorder | null = null;
  private audioContext: AudioContext | null = null;

  constructor(client: VoiceHelperClient) {
    this.client = client;
  }

  async startVoiceSession(options?: {
    language?: string;
    voice_id?: string;
    conversation_id?: string;
  }) {
    try {
      // å»ºç«‹WebSocketè¿æ¥
      this.ws = await this.client.voice.connect({
        language: options?.language || 'zh-CN',
        voice_id: options?.voice_id || 'zh-CN-XiaoxiaoNeural',
        conversation_id: options?.conversation_id
      });

      // ç›‘å¬è¯­éŸ³å“åº”
      this.ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        this.handleVoiceMessage(data);
      };

      this.ws.onerror = (error) => {
        console.error('WebSocketé”™è¯¯:', error);
      };

      this.ws.onclose = () => {
        console.log('è¯­éŸ³ä¼šè¯å·²ç»“æŸ');
        this.cleanup();
      };

      console.log('âœ… è¯­éŸ³ä¼šè¯å·²å»ºç«‹');
    } catch (error) {
      console.error('å»ºç«‹è¯­éŸ³ä¼šè¯å¤±è´¥:', error);
    }
  }

  private handleVoiceMessage(data: any) {
    switch (data.type) {
      case 'session_initialized':
        console.log('ğŸ™ï¸ è¯­éŸ³ä¼šè¯åˆå§‹åŒ–å®Œæˆ:', data.session_id);
        break;

      case 'asr_partial':
        // å®æ—¶æ˜¾ç¤ºè¯­éŸ³è¯†åˆ«ç»“æœ
        this.updateTranscript(data.text, false);
        break;

      case 'asr_final':
        // æœ€ç»ˆè¯­éŸ³è¯†åˆ«ç»“æœ
        console.log('ğŸ¯ è¯†åˆ«å®Œæˆ:', data.text);
        this.updateTranscript(data.text, true);
        break;

      case 'processing_start':
        console.log('ğŸ¤” AIæ­£åœ¨æ€è€ƒ...');
        break;

      case 'llm_response_chunk':
        // å®æ—¶æ˜¾ç¤ºAIæ–‡æœ¬å›å¤
        this.displayResponse(data.text, false);
        break;

      case 'llm_response_final':
        console.log('ğŸ’¬ AIå›å¤å®Œæˆ');
        this.displayResponse(data.text, true);
        if (data.references) {
          console.log('ğŸ“š å‚è€ƒèµ„æ–™:', data.references);
        }
        break;

      case 'tts_start':
        console.log('ğŸ”Š å¼€å§‹è¯­éŸ³åˆæˆ...');
        break;

      case 'tts_audio':
        // æ’­æ”¾TTSéŸ³é¢‘
        this.playAudio(data.audio_data, data.audio_format);
        break;

      case 'tts_complete':
        console.log('ğŸµ è¯­éŸ³æ’­æ”¾å®Œæˆ');
        break;

      case 'error':
        console.error('âŒ è¯­éŸ³å¤„ç†é”™è¯¯:', data.error);
        break;
    }
  }

  async startRecording() {
    try {
      // è¯·æ±‚éº¦å…‹é£æƒé™
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      // åˆ›å»ºå½•éŸ³å™¨
      this.mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus',
        audioBitsPerSecond: 64000
      });

      // å¤„ç†å½•éŸ³æ•°æ®
      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0 && this.ws) {
          this.sendAudioData(event.data);
        }
      };

      // å¼€å§‹å½•éŸ³ï¼Œæ¯100mså‘é€ä¸€æ¬¡æ•°æ®
      this.mediaRecorder.start(100);
      console.log('ğŸ™ï¸ å¼€å§‹å½•éŸ³...');

    } catch (error) {
      console.error('å¯åŠ¨å½•éŸ³å¤±è´¥:', error);
    }
  }

  stopRecording() {
    if (this.mediaRecorder) {
      this.mediaRecorder.stop();
      this.mediaRecorder = null;
      console.log('â¹ï¸ å½•éŸ³å·²åœæ­¢');
    }
  }

  private async sendAudioData(audioBlob: Blob) {
    try {
      const arrayBuffer = await audioBlob.arrayBuffer();
      const base64 = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)));
      
      if (this.ws && this.ws.readyState === WebSocket.OPEN) {
        this.ws.send(JSON.stringify({
          type: 'audio_chunk',
          audio_chunk: base64,
          timestamp: Date.now()
        }));
      }
    } catch (error) {
      console.error('å‘é€éŸ³é¢‘æ•°æ®å¤±è´¥:', error);
    }
  }

  private async playAudio(base64Audio: string, format: string) {
    try {
      // è§£ç base64éŸ³é¢‘
      const audioData = atob(base64Audio);
      const arrayBuffer = new ArrayBuffer(audioData.length);
      const view = new Uint8Array(arrayBuffer);
      
      for (let i = 0; i < audioData.length; i++) {
        view[i] = audioData.charCodeAt(i);
      }

      // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
      if (!this.audioContext) {
        this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      }

      // è§£ç å¹¶æ’­æ”¾éŸ³é¢‘
      const audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
      const source = this.audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(this.audioContext.destination);
      source.start();

    } catch (error) {
      console.error('éŸ³é¢‘æ’­æ”¾å¤±è´¥:', error);
    }
  }

  private updateTranscript(text: string, isFinal: boolean) {
    const element = document.getElementById('transcript');
    if (element) {
      element.textContent = text;
      element.className = isFinal ? 'final' : 'partial';
    }
  }

  private displayResponse(text: string, isFinal: boolean) {
    const element = document.getElementById('response');
    if (element) {
      if (isFinal) {
        element.textContent = text;
      } else {
        element.textContent += text;
      }
    }
  }

  private cleanup() {
    if (this.mediaRecorder) {
      this.mediaRecorder.stop();
    }
    if (this.audioContext) {
      this.audioContext.close();
    }
    if (this.ws) {
      this.ws.close();
    }
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const voiceChat = new VoiceChat(client);

// æŒ‰é’®äº‹ä»¶
document.getElementById('start-voice')?.addEventListener('click', () => {
  voiceChat.startVoiceSession({
    language: 'zh-CN',
    conversation_id: 'voice_conv_123'
  });
});

document.getElementById('start-recording')?.addEventListener('click', () => {
  voiceChat.startRecording();
});

document.getElementById('stop-recording')?.addEventListener('click', () => {
  voiceChat.stopRecording();
});
```

### Python SDKé›†æˆ

#### å®‰è£…å’ŒåŸºç¡€é…ç½®

```bash
pip install voicehelper-sdk
```

```python
from voicehelper_sdk import VoiceHelperClient, VoiceHelperConfig
import asyncio
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å®¢æˆ·ç«¯é…ç½®
config = VoiceHelperConfig(
    api_key="your-api-key",
    base_url="https://api.voicehelper.ai",
    timeout=30.0,
    max_retries=3,
    enable_logging=True
)

client = VoiceHelperClient(config)
```

#### å¼‚æ­¥èŠå¤©åŠŸèƒ½

```python
async def async_chat_example():
    """å¼‚æ­¥èŠå¤©ç¤ºä¾‹"""
    try:
        # ç®€å•èŠå¤©
        response = await client.chat.send_message(
            message="è¯·ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†",
            conversation_id="conv_python_123",
            model="gpt-3.5-turbo",
            temperature=0.7
        )
        
        print("AIå›å¤:", response.message)
        print("å“åº”æ—¶é—´:", response.response_time_ms, "ms")
        print("ä½¿ç”¨æ¨¡å‹:", response.model_used)
        
        if response.references:
            print("\nå‚è€ƒèµ„æ–™:")
            for i, ref in enumerate(response.references, 1):
                print(f"{i}. {ref.title}")
                print(f"   æ¥æº: {ref.source}")
                print(f"   ç›¸å…³åº¦: {ref.relevance_score:.2f}")
        
    except Exception as e:
        logger.error(f"èŠå¤©å¤±è´¥: {e}")

# æµå¼èŠå¤©
async def streaming_chat_example():
    """æµå¼èŠå¤©ç¤ºä¾‹"""
    try:
        stream = client.chat.create_stream(
            message="è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯Transformeræ¶æ„",
            conversation_id="conv_stream_456", 
            retrieval_config={
                "mode": "hybrid",
                "top_k": 8,
                "collection": "ai_papers"
            }
        )
        
        print("å¼€å§‹æ¥æ”¶æµå¼å›å¤...")
        full_response = ""
        
        async for chunk in stream:
            if chunk.type == "retrieval_start":
                print("ğŸ” å¼€å§‹æ£€ç´¢...")
                
            elif chunk.type == "retrieval_result":
                results = chunk.data.get("results", [])
                print(f"ğŸ“š æ£€ç´¢åˆ° {len(results)} æ¡ç›¸å…³æ–‡æ¡£")
                
            elif chunk.type == "generation_start":
                print("ğŸ¤– å¼€å§‹ç”Ÿæˆå›å¤...")
                
            elif chunk.type == "generation_chunk":
                text = chunk.data.get("text", "")
                print(text, end="", flush=True)
                full_response += text
                
            elif chunk.type == "generation_done":
                print(f"\n\nâœ… ç”Ÿæˆå®Œæˆ!")
                print(f"æ€»è€—æ—¶: {chunk.data.get('total_time_ms')}ms")
                print(f"ç”Ÿæˆtokenæ•°: {chunk.data.get('token_count', 0)}")
                
            elif chunk.type == "error":
                print(f"âŒ é”™è¯¯: {chunk.data.get('error')}")
                break
                
        print(f"\nå®Œæ•´å›å¤é•¿åº¦: {len(full_response)} å­—ç¬¦")
        
    except Exception as e:
        logger.error(f"æµå¼èŠå¤©å¤±è´¥: {e}")

# æ‰¹é‡æ–‡æ¡£å¤„ç†
async def batch_document_processing():
    """æ‰¹é‡æ–‡æ¡£å¤„ç†ç¤ºä¾‹"""
    try:
        # å‡†å¤‡æ–‡æ¡£æ–‡ä»¶
        documents = []
        
        # ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–æ–‡æ¡£
        import os
        doc_folder = "path/to/documents"
        
        for filename in os.listdir(doc_folder):
            if filename.endswith(('.pdf', '.docx', '.txt')):
                file_path = os.path.join(doc_folder, filename)
                with open(file_path, 'rb') as f:
                    content = f.read()
                
                documents.append({
                    "filename": filename,
                    "content": content,
                    "metadata": {
                        "source": "company_docs",
                        "upload_date": "2024-01-15",
                        "department": "product"
                    }
                })
        
        print(f"å‡†å¤‡å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£...")
        
        # æ‰¹é‡å…¥åº“
        ingest_result = await client.documents.batch_ingest(
            files=documents,
            collection_name="company_knowledge_v2",
            chunk_size=1200,
            chunk_overlap=150,
            processing_options={
                "enable_ocr": True,
                "extract_images": True,
                "enable_table_extraction": True
            }
        )
        
        print(f"æ‰¹é‡å…¥åº“ä»»åŠ¡å·²å¯åŠ¨: {ingest_result.task_id}")
        
        # ç›‘æ§å¤„ç†è¿›åº¦
        while True:
            status = await client.documents.get_task_status(ingest_result.task_id)
            
            print(f"è¿›åº¦: {status.progress}% - {status.status}")
            print(f"å·²å¤„ç†: {status.processed_files}/{status.total_files} ä¸ªæ–‡ä»¶")
            
            if status.status == "completed":
                print("ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
                result = status.result
                print(f"æˆåŠŸå¤„ç†: {result.get('documents_processed', 0)} ä¸ªæ–‡æ¡£")
                print(f"ç”Ÿæˆåˆ†å—: {result.get('chunks_created', 0)} ä¸ª")
                print(f"ç´¢å¼•å‘é‡: {result.get('vectors_indexed', 0)} ä¸ª")
                print(f"å¤„ç†æ—¶é—´: {result.get('processing_time_seconds', 0)} ç§’")
                break
                
            elif status.status == "failed":
                print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {status.error}")
                break
                
            elif status.status == "processing":
                # æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
                if hasattr(status, 'detailed_progress'):
                    for file_progress in status.detailed_progress:
                        print(f"  - {file_progress['filename']}: {file_progress['stage']}")
            
            await asyncio.sleep(3)  # æ¯3ç§’æ£€æŸ¥ä¸€æ¬¡
            
    except Exception as e:
        logger.error(f"æ‰¹é‡æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")

# æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
class IntelligentQASystem:
    """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, client: VoiceHelperClient):
        self.client = client
        self.conversation_history = {}
    
    async def answer_question(
        self,
        question: str,
        user_id: str,
        context: dict = None
    ) -> dict:
        """å›ç­”é—®é¢˜"""
        try:
            # è·å–æˆ–åˆ›å»ºå¯¹è¯å†å²
            conversation_id = f"qa_{user_id}"
            
            if conversation_id not in self.conversation_history:
                self.conversation_history[conversation_id] = []
            
            # æ„å»ºå¢å¼ºçš„é—®é¢˜ä¸Šä¸‹æ–‡
            enhanced_question = self._build_enhanced_question(
                question, 
                self.conversation_history[conversation_id],
                context
            )
            
            # è°ƒç”¨VoiceHelperè¿›è¡Œé—®ç­”
            response = await self.client.chat.send_message(
                message=enhanced_question,
                conversation_id=conversation_id,
                retrieval_config={
                    "mode": "graph",  # ä½¿ç”¨å›¾æ£€ç´¢æ¨¡å¼
                    "top_k": 5,
                    "enable_reasoning": True,
                    "collection": "knowledge_base"
                },
                generation_config={
                    "temperature": 0.3,
                    "max_tokens": 1000,
                    "enable_citations": True
                }
            )
            
            # ä¿å­˜å¯¹è¯å†å²
            self.conversation_history[conversation_id].extend([
                {"role": "user", "content": question},
                {"role": "assistant", "content": response.message}
            ])
            
            # é™åˆ¶å†å²é•¿åº¦
            if len(self.conversation_history[conversation_id]) > 20:
                self.conversation_history[conversation_id] = \
                    self.conversation_history[conversation_id][-20:]
            
            # æ„å»ºç»“æ„åŒ–å›å¤
            structured_response = {
                "answer": response.message,
                "confidence": response.confidence_score,
                "sources": [
                    {
                        "title": ref.title,
                        "content": ref.content[:200] + "..." if len(ref.content) > 200 else ref.content,
                        "relevance": ref.relevance_score,
                        "source": ref.source
                    }
                    for ref in response.references[:3]
                ],
                "response_time_ms": response.response_time_ms,
                "follow_up_questions": self._generate_follow_up_questions(question, response.message)
            }
            
            return structured_response
            
        except Exception as e:
            logger.error(f"é—®ç­”å¤±è´¥: {e}")
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "error": str(e)
            }
    
    def _build_enhanced_question(
        self, 
        question: str, 
        history: list,
        context: dict = None
    ) -> str:
        """æ„å»ºå¢å¼ºé—®é¢˜ä¸Šä¸‹æ–‡"""
        context_parts = [question]
        
        # æ·»åŠ å¯¹è¯å†å²ä¸Šä¸‹æ–‡
        if history:
            recent_history = history[-4:]  # æœ€è¿‘2è½®å¯¹è¯
            history_context = "\n".join([
                f"{msg['role']}: {msg['content']}" 
                for msg in recent_history
            ])
            context_parts.append(f"å¯¹è¯å†å²:\n{history_context}")
        
        # æ·»åŠ é¢å¤–ä¸Šä¸‹æ–‡
        if context:
            context_info = []
            for key, value in context.items():
                context_info.append(f"{key}: {value}")
            context_parts.append(f"ä¸Šä¸‹æ–‡ä¿¡æ¯:\n" + "\n".join(context_info))
        
        return "\n\n".join(context_parts)
    
    def _generate_follow_up_questions(self, original_question: str, answer: str) -> list:
        """ç”Ÿæˆç›¸å…³é—®é¢˜å»ºè®®"""
        # è¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„é—®é¢˜ç”Ÿæˆé€»è¾‘
        follow_ups = [
            "èƒ½å¦æä¾›æ›´å¤šç»†èŠ‚ï¼Ÿ",
            "æœ‰ä»€ä¹ˆå®é™…åº”ç”¨æ¡ˆä¾‹å—ï¼Ÿ", 
            "è¿˜æœ‰å…¶ä»–ç›¸å…³çš„æ¦‚å¿µå—ï¼Ÿ"
        ]
        return follow_ups

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    try:
        # åŸºç¡€èŠå¤©
        await async_chat_example()
        
        # æµå¼èŠå¤©
        await streaming_chat_example()
        
        # æ‰¹é‡æ–‡æ¡£å¤„ç†
        await batch_document_processing()
        
        # æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
        qa_system = IntelligentQASystem(client)
        
        result = await qa_system.answer_question(
            question="ä»€ä¹ˆæ˜¯GraphRAGï¼Ÿå®ƒä¸ä¼ ç»ŸRAGæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            user_id="user_123",
            context={
                "domain": "AIæŠ€æœ¯",
                "expertise_level": "intermediate"
            }
        )
        
        print("\næ™ºèƒ½é—®ç­”ç»“æœ:")
        print(f"å›ç­”: {result['answer']}")
        print(f"ç½®ä¿¡åº¦: {result['confidence']}")
        print(f"å“åº”æ—¶é—´: {result['response_time_ms']}ms")
        print("å‚è€ƒæ¥æº:")
        for source in result['sources']:
            print(f"  - {source['title']} (ç›¸å…³åº¦: {source['relevance']})")
        
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸŒ å¤šå¹³å°é›†æˆç¤ºä¾‹

### React Webåº”ç”¨é›†æˆ

```jsx
// React Hooké›†æˆç¤ºä¾‹
import React, { useState, useEffect, useCallback } from 'react';
import { VoiceHelperClient } from '@voicehelper/javascript-sdk';

// è‡ªå®šä¹‰Hook
const useVoiceHelper = (apiKey) => {
  const [client, setClient] = useState(null);
  const [isConnected, setIsConnected] = useState(false);
  const [error, setError] = useState(null);

  useEffect(() => {
    const initClient = async () => {
      try {
        const voiceHelperClient = new VoiceHelperClient({
          apiKey,
          baseURL: process.env.REACT_APP_VOICEHELPER_API_URL
        });

        await voiceHelperClient.connect();
        setClient(voiceHelperClient);
        setIsConnected(true);
        setError(null);
      } catch (err) {
        setError(err.message);
        setIsConnected(false);
      }
    };

    if (apiKey) {
      initClient();
    }
  }, [apiKey]);

  return { client, isConnected, error };
};

// èŠå¤©ç»„ä»¶
const ChatComponent = ({ apiKey, conversationId }) => {
  const { client, isConnected, error } = useVoiceHelper(apiKey);
  const [messages, setMessages] = useState([]);
  const [inputText, setInputText] = useState('');
  const [isLoading, setIsLoading] = useState(false);

  const sendMessage = useCallback(async () => {
    if (!client || !inputText.trim()) return;

    const userMessage = {
      id: Date.now(),
      role: 'user',
      content: inputText,
      timestamp: new Date()
    };

    setMessages(prev => [...prev, userMessage]);
    setInputText('');
    setIsLoading(true);

    try {
      const stream = await client.chat.createStream({
        message: inputText,
        conversation_id: conversationId,
        retrieval_config: {
          mode: 'hybrid',
          top_k: 5
        }
      });

      let aiMessage = {
        id: Date.now() + 1,
        role: 'assistant',
        content: '',
        references: [],
        timestamp: new Date()
      };

      setMessages(prev => [...prev, aiMessage]);

      for await (const chunk of stream) {
        if (chunk.type === 'generation_chunk') {
          aiMessage.content += chunk.data.text;
          setMessages(prev => {
            const updated = [...prev];
            updated[updated.length - 1] = { ...aiMessage };
            return updated;
          });
        } else if (chunk.type === 'retrieval_result') {
          aiMessage.references = chunk.data.results;
        }
      }
    } catch (err) {
      console.error('å‘é€æ¶ˆæ¯å¤±è´¥:', err);
    } finally {
      setIsLoading(false);
    }
  }, [client, inputText, conversationId]);

  if (error) {
    return <div className="error">è¿æ¥å¤±è´¥: {error}</div>;
  }

  if (!isConnected) {
    return <div className="loading">è¿æ¥ä¸­...</div>;
  }

  return (
    <div className="chat-container">
      <div className="messages">
        {messages.map(message => (
          <div key={message.id} className={`message ${message.role}`}>
            <div className="content">{message.content}</div>
            {message.references && message.references.length > 0 && (
              <div className="references">
                <h4>å‚è€ƒèµ„æ–™:</h4>
                {message.references.map((ref, index) => (
                  <div key={index} className="reference">
                    <strong>{ref.title}</strong>
                    <p>{ref.content.substring(0, 100)}...</p>
                  </div>
                ))}
              </div>
            )}
          </div>
        ))}
      </div>
      
      <div className="input-area">
        <input
          type="text"
          value={inputText}
          onChange={(e) => setInputText(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && sendMessage()}
          placeholder="è¾“å…¥æ¶ˆæ¯..."
          disabled={isLoading}
        />
        <button onClick={sendMessage} disabled={isLoading || !inputText.trim()}>
          {isLoading ? 'å‘é€ä¸­...' : 'å‘é€'}
        </button>
      </div>
    </div>
  );
};

// ä¸»åº”ç”¨
const App = () => {
  const [apiKey, setApiKey] = useState('');
  const [conversationId] = useState(`conv_${Date.now()}`);

  return (
    <div className="app">
      <header>
        <h1>VoiceHelper èŠå¤©ç¤ºä¾‹</h1>
        <input
          type="text"
          placeholder="è¾“å…¥APIå¯†é’¥"
          value={apiKey}
          onChange={(e) => setApiKey(e.target.value)}
        />
      </header>
      
      {apiKey && (
        <ChatComponent 
          apiKey={apiKey} 
          conversationId={conversationId}
        />
      )}
    </div>
  );
};

export default App;
```

### å¾®ä¿¡å°ç¨‹åºé›†æˆ

```javascript
// å¾®ä¿¡å°ç¨‹åºé›†æˆç¤ºä¾‹
// app.js
App({
  globalData: {
    voiceHelperConfig: {
      apiKey: 'your-api-key',
      baseURL: 'https://api.voicehelper.ai',
      timeout: 30000
    },
    userInfo: null
  },

  onLaunch() {
    // åˆå§‹åŒ–VoiceHelperå®¢æˆ·ç«¯
    this.initVoiceHelper();
    
    // è·å–ç”¨æˆ·ä¿¡æ¯
    this.getUserInfo();
  },

  async initVoiceHelper() {
    try {
      const { VoiceHelperMiniProgram } = require('./utils/voicehelper-sdk.js');
      
      this.voiceHelper = new VoiceHelperMiniProgram(this.globalData.voiceHelperConfig);
      await this.voiceHelper.initialize();
      
      console.log('VoiceHelperåˆå§‹åŒ–æˆåŠŸ');
    } catch (error) {
      console.error('VoiceHelperåˆå§‹åŒ–å¤±è´¥:', error);
      wx.showToast({
        title: 'æœåŠ¡åˆå§‹åŒ–å¤±è´¥',
        icon: 'none'
      });
    }
  },

  getUserInfo() {
    wx.getUserProfile({
      desc: 'ç”¨äºä¸ªæ€§åŒ–æœåŠ¡',
      success: (res) => {
        this.globalData.userInfo = res.userInfo;
      },
      fail: (err) => {
        console.log('è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥:', err);
      }
    });
  }
});

// pages/chat/chat.js
Page({
  data: {
    messages: [],
    inputText: '',
    isRecording: false,
    isConnected: false,
    currentTranscript: '',
    scrollIntoView: ''
  },

  onLoad(options) {
    this.conversationId = `miniprogram_${Date.now()}`;
    this.connectVoiceHelper();
  },

  async connectVoiceHelper() {
    try {
      const app = getApp();
      
      if (!app.voiceHelper) {
        await app.initVoiceHelper();
      }

      // å»ºç«‹WebSocketè¿æ¥
      await app.voiceHelper.connect({
        conversation_id: this.conversationId
      });

      this.setData({ isConnected: true });

      // ç›‘å¬è¯­éŸ³è¯†åˆ«ç»“æœ
      app.voiceHelper.onASRResult((result) => {
        this.setData({
          currentTranscript: result.text
        });

        if (result.is_final) {
          this.addMessage('user', result.text);
        }
      });

      // ç›‘å¬AIå›å¤
      app.voiceHelper.onAIResponse((response) => {
        this.addMessage('assistant', response.text, response.references);
        
        // æ’­æ”¾TTSéŸ³é¢‘
        if (response.audio_data) {
          this.playTTSAudio(response.audio_data);
        }
      });

      // ç›‘å¬è¿æ¥çŠ¶æ€
      app.voiceHelper.onConnectionChange((status) => {
        this.setData({ isConnected: status === 'connected' });
      });

    } catch (error) {
      console.error('è¿æ¥VoiceHelperå¤±è´¥:', error);
      wx.showToast({
        title: 'è¿æ¥æœåŠ¡å¤±è´¥',
        icon: 'none'
      });
    }
  },

  // å‘é€æ–‡æœ¬æ¶ˆæ¯
  async sendTextMessage() {
    const message = this.data.inputText.trim();
    if (!message) return;

    this.addMessage('user', message);
    this.setData({ inputText: '' });

    try {
      const app = getApp();
      await app.voiceHelper.sendMessage(message);
    } catch (error) {
      console.error('å‘é€æ¶ˆæ¯å¤±è´¥:', error);
      wx.showToast({
        title: 'å‘é€å¤±è´¥',
        icon: 'none'
      });
    }
  },

  // å¼€å§‹å½•éŸ³
  startRecording() {
    if (!this.data.isConnected) {
      wx.showToast({
        title: 'è¯·å…ˆè¿æ¥æœåŠ¡',
        icon: 'none'
      });
      return;
    }

    wx.authorize({
      scope: 'scope.record',
      success: () => {
        this.recorderManager = wx.getRecorderManager();
        
        this.recorderManager.onStart(() => {
          this.setData({ 
            isRecording: true,
            currentTranscript: 'æ­£åœ¨å½•éŸ³...'
          });
        });

        this.recorderManager.onFrameRecorded((res) => {
          // å®æ—¶å‘é€éŸ³é¢‘å¸§
          const app = getApp();
          app.voiceHelper.sendAudioFrame(res.frameBuffer);
        });

        this.recorderManager.onStop(() => {
          this.setData({ 
            isRecording: false,
            currentTranscript: ''
          });
        });

        // å¼€å§‹å½•éŸ³
        this.recorderManager.start({
          duration: 60000,
          sampleRate: 16000,
          numberOfChannels: 1,
          encodeBitRate: 48000,
          format: 'mp3',
          frameSize: 50
        });
      },
      fail: () => {
        wx.showToast({
          title: 'éœ€è¦å½•éŸ³æƒé™',
          icon: 'none'
        });
      }
    });
  },

  // åœæ­¢å½•éŸ³
  stopRecording() {
    if (this.recorderManager) {
      this.recorderManager.stop();
    }
  },

  // æ’­æ”¾TTSéŸ³é¢‘
  playTTSAudio(base64AudioData) {
    try {
      // ä¿å­˜éŸ³é¢‘æ–‡ä»¶
      const fs = wx.getFileSystemManager();
      const audioPath = `${wx.env.USER_DATA_PATH}/tts_${Date.now()}.mp3`;
      
      fs.writeFile({
        filePath: audioPath,
        data: base64AudioData,
        encoding: 'base64',
        success: () => {
          // æ’­æ”¾éŸ³é¢‘
          const innerAudioContext = wx.createInnerAudioContext();
          innerAudioContext.src = audioPath;
          innerAudioContext.play();
          
          innerAudioContext.onEnded(() => {
            // æ’­æ”¾å®Œæˆååˆ é™¤ä¸´æ—¶æ–‡ä»¶
            fs.unlink({
              filePath: audioPath,
              success: () => console.log('ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å·²åˆ é™¤'),
              fail: (err) => console.warn('åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥:', err)
            });
          });
        },
        fail: (err) => {
          console.error('ä¿å­˜éŸ³é¢‘æ–‡ä»¶å¤±è´¥:', err);
        }
      });
    } catch (error) {
      console.error('æ’­æ”¾TTSéŸ³é¢‘å¤±è´¥:', error);
    }
  },

  // æ·»åŠ æ¶ˆæ¯
  addMessage(role, content, references = null) {
    const message = {
      id: Date.now(),
      role,
      content,
      references,
      timestamp: new Date().toLocaleTimeString()
    };

    const messages = this.data.messages;
    messages.push(message);

    this.setData({
      messages,
      scrollIntoView: `msg-${message.id}`
    });
  },

  // è¾“å…¥æ¡†å˜åŒ–
  onInputChange(e) {
    this.setData({
      inputText: e.detail.value
    });
  }
});
```

```xml
<!-- pages/chat/chat.wxml -->
<view class="chat-container">
  <!-- è¿æ¥çŠ¶æ€ -->
  <view class="status-bar">
    <text class="status {{isConnected ? 'connected' : 'disconnected'}}">
      {{isConnected ? 'å·²è¿æ¥' : 'æœªè¿æ¥'}}
    </text>
  </view>

  <!-- æ¶ˆæ¯åˆ—è¡¨ -->
  <scroll-view class="messages" scroll-y="true" scroll-into-view="{{scrollIntoView}}">
    <view wx:for="{{messages}}" wx:key="id" id="msg-{{item.id}}" class="message {{item.role}}">
      <view class="message-content">
        <text>{{item.content}}</text>
      </view>
      
      <view wx:if="{{item.references}}" class="references">
        <text class="references-title">å‚è€ƒèµ„æ–™:</text>
        <view wx:for="{{item.references}}" wx:key="index" wx:for-item="ref" class="reference">
          <text class="ref-title">{{ref.title}}</text>
          <text class="ref-content">{{ref.content}}</text>
        </view>
      </view>
      
      <text class="timestamp">{{item.timestamp}}</text>
    </view>
  </scroll-view>

  <!-- å®æ—¶è½¬å½•æ˜¾ç¤º -->
  <view wx:if="{{currentTranscript}}" class="transcript">
    <text>{{currentTranscript}}</text>
  </view>

  <!-- è¾“å…¥åŒºåŸŸ -->
  <view class="input-area">
    <input 
      type="text" 
      placeholder="è¾“å…¥æ¶ˆæ¯..."
      value="{{inputText}}"
      bindinput="onInputChange"
      bindconfirm="sendTextMessage"
      disabled="{{!isConnected}}"
    />
    
    <button 
      class="send-btn" 
      bindtap="sendTextMessage"
      disabled="{{!isConnected || !inputText}}"
    >
      å‘é€
    </button>
  </view>

  <!-- è¯­éŸ³æ§åˆ¶æŒ‰é’® -->
  <view class="voice-controls">
    <button 
      class="voice-btn {{isRecording ? 'recording' : ''}}"
      bindtouchstart="startRecording"
      bindtouchend="stopRecording"
      disabled="{{!isConnected}}"
    >
      {{isRecording ? 'å½•éŸ³ä¸­...' : 'æŒ‰ä½è¯´è¯'}}
    </button>
  </view>
</view>
```

### React Nativeé›†æˆ

```javascript
// React Nativeé›†æˆç¤ºä¾‹
import React, { useState, useEffect, useRef } from 'react';
import {
  View,
  Text,
  TextInput,
  TouchableOpacity,
  ScrollView,
  Alert,
  StyleSheet,
  Platform
} from 'react-native';
import { VoiceHelperReactNative } from '@voicehelper/react-native-sdk';

const ChatScreen = () => {
  const [messages, setMessages] = useState([]);
  const [inputText, setInputText] = useState('');
  const [isRecording, setIsRecording] = useState(false);
  const [isConnected, setIsConnected] = useState(false);
  const [currentTranscript, setCurrentTranscript] = useState('');
  
  const voiceHelperRef = useRef(null);
  const scrollViewRef = useRef(null);

  useEffect(() => {
    initializeVoiceHelper();
    
    return () => {
      if (voiceHelperRef.current) {
        voiceHelperRef.current.disconnect();
      }
    };
  }, []);

  const initializeVoiceHelper = async () => {
    try {
      const voiceHelper = new VoiceHelperReactNative({
        apiKey: 'your-api-key',
        baseURL: 'https://api.voicehelper.ai',
        timeout: 30000
      });

      await voiceHelper.initialize();
      voiceHelperRef.current = voiceHelper;

      // è®¾ç½®äº‹ä»¶ç›‘å¬
      voiceHelper.onConnectionChange((status) => {
        setIsConnected(status === 'connected');
      });

      voiceHelper.onASRResult((result) => {
        setCurrentTranscript(result.text);
        
        if (result.is_final) {
          addMessage('user', result.text);
          setCurrentTranscript('');
        }
      });

      voiceHelper.onAIResponse((response) => {
        addMessage('assistant', response.text, response.references);
      });

      voiceHelper.onError((error) => {
        Alert.alert('é”™è¯¯', error.message);
      });

      // è¿æ¥æœåŠ¡
      await voiceHelper.connect({
        conversation_id: `rn_${Date.now()}`
      });

    } catch (error) {
      console.error('åˆå§‹åŒ–å¤±è´¥:', error);
      Alert.alert('åˆå§‹åŒ–å¤±è´¥', error.message);
    }
  };

  const sendTextMessage = async () => {
    if (!inputText.trim() || !isConnected) return;

    addMessage('user', inputText);
    setInputText('');

    try {
      await voiceHelperRef.current.sendMessage(inputText);
    } catch (error) {
      console.error('å‘é€æ¶ˆæ¯å¤±è´¥:', error);
      Alert.alert('å‘é€å¤±è´¥', error.message);
    }
  };

  const startRecording = async () => {
    try {
      await voiceHelperRef.current.startRecording({
        language: 'zh-CN',
        enableVAD: true,
        enableNoiseReduction: true
      });
      setIsRecording(true);
    } catch (error) {
      console.error('å¼€å§‹å½•éŸ³å¤±è´¥:', error);
      Alert.alert('å½•éŸ³å¤±è´¥', error.message);
    }
  };

  const stopRecording = async () => {
    try {
      await voiceHelperRef.current.stopRecording();
      setIsRecording(false);
    } catch (error) {
      console.error('åœæ­¢å½•éŸ³å¤±è´¥:', error);
    }
  };

  const addMessage = (role, content, references = null) => {
    const message = {
      id: Date.now(),
      role,
      content,
      references,
      timestamp: new Date().toLocaleTimeString()
    };

    setMessages(prev => [...prev, message]);
    
    // æ»šåŠ¨åˆ°åº•éƒ¨
    setTimeout(() => {
      scrollViewRef.current?.scrollToEnd({ animated: true });
    }, 100);
  };

  return (
    <View style={styles.container}>
      {/* çŠ¶æ€æ  */}
      <View style={styles.statusBar}>
        <Text style={[styles.statusText, { color: isConnected ? 'green' : 'red' }]}>
          {isConnected ? 'å·²è¿æ¥' : 'æœªè¿æ¥'}
        </Text>
      </View>

      {/* æ¶ˆæ¯åˆ—è¡¨ */}
      <ScrollView 
        ref={scrollViewRef}
        style={styles.messagesList}
        contentContainerStyle={styles.messagesContent}
      >
        {messages.map(message => (
          <View key={message.id} style={[
            styles.messageBubble,
            message.role === 'user' ? styles.userMessage : styles.aiMessage
          ]}>
            <Text style={[
              styles.messageText,
              { color: message.role === 'user' ? 'white' : 'black' }
            ]}>
              {message.content}
            </Text>
            
            {message.references && message.references.length > 0 && (
              <View style={styles.references}>
                <Text style={styles.referencesTitle}>å‚è€ƒèµ„æ–™:</Text>
                {message.references.map((ref, index) => (
                  <View key={index} style={styles.reference}>
                    <Text style={styles.refTitle}>{ref.title}</Text>
                    <Text style={styles.refContent}>
                      {ref.content.substring(0, 100)}...
                    </Text>
                  </View>
                ))}
              </View>
            )}
            
            <Text style={styles.timestamp}>{message.timestamp}</Text>
          </View>
        ))}
      </ScrollView>

      {/* å®æ—¶è½¬å½•æ˜¾ç¤º */}
      {currentTranscript ? (
        <View style={styles.transcriptBar}>
          <Text style={styles.transcriptText}>{currentTranscript}</Text>
        </View>
      ) : null}

      {/* è¾“å…¥åŒºåŸŸ */}
      <View style={styles.inputArea}>
        <TextInput
          style={styles.textInput}
          value={inputText}
          onChangeText={setInputText}
          placeholder="è¾“å…¥æ¶ˆæ¯..."
          editable={isConnected}
          onSubmitEditing={sendTextMessage}
        />
        
        <TouchableOpacity
          style={[styles.sendButton, { opacity: (!isConnected || !inputText) ? 0.5 : 1 }]}
          onPress={sendTextMessage}
          disabled={!isConnected || !inputText}
        >
          <Text style={styles.sendButtonText}>å‘é€</Text>
        </TouchableOpacity>
      </View>

      {/* è¯­éŸ³æ§åˆ¶ */}
      <View style={styles.voiceControls}>
        <TouchableOpacity
          style={[
            styles.voiceButton,
            { 
              backgroundColor: isRecording ? '#FF4444' : '#007AFF',
              opacity: !isConnected ? 0.5 : 1 
            }
          ]}
          onPressIn={startRecording}
          onPressOut={stopRecording}
          disabled={!isConnected}
        >
          <Text style={styles.voiceButtonText}>
            {isRecording ? 'æ¾å¼€ç»“æŸ' : 'æŒ‰ä½è¯´è¯'}
          </Text>
        </TouchableOpacity>
      </View>
    </View>
  );
};

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#F5F5F5'
  },
  statusBar: {
    paddingHorizontal: 16,
    paddingVertical: 8,
    backgroundColor: 'white',
    borderBottomWidth: 1,
    borderBottomColor: '#E0E0E0'
  },
  statusText: {
    fontSize: 14,
    fontWeight: 'bold'
  },
  messagesList: {
    flex: 1,
    paddingHorizontal: 16
  },
  messagesContent: {
    paddingVertical: 16
  },
  messageBubble: {
    marginBottom: 12,
    padding: 12,
    borderRadius: 12,
    maxWidth: '80%'
  },
  userMessage: {
    backgroundColor: '#007AFF',
    alignSelf: 'flex-end'
  },
  aiMessage: {
    backgroundColor: 'white',
    alignSelf: 'flex-start',
    borderWidth: 1,
    borderColor: '#E0E0E0'
  },
  messageText: {
    fontSize: 16,
    lineHeight: 22
  },
  references: {
    marginTop: 8,
    paddingTop: 8,
    borderTopWidth: 1,
    borderTopColor: 'rgba(255,255,255,0.3)'
  },
  referencesTitle: {
    fontSize: 12,
    fontWeight: 'bold',
    color: 'rgba(0,0,0,0.6)',
    marginBottom: 4
  },
  reference: {
    marginBottom: 4
  },
  refTitle: {
    fontSize: 12,
    fontWeight: 'bold',
    color: 'rgba(0,0,0,0.8)'
  },
  refContent: {
    fontSize: 11,
    color: 'rgba(0,0,0,0.6)'
  },
  timestamp: {
    fontSize: 11,
    color: 'rgba(0,0,0,0.5)',
    marginTop: 4
  },
  transcriptBar: {
    backgroundColor: '#FFF3CD',
    paddingHorizontal: 16,
    paddingVertical: 8,
    borderTopWidth: 1,
    borderTopColor: '#E0E0E0'
  },
  transcriptText: {
    fontSize: 14,
    color: '#856404',
    fontStyle: 'italic'
  },
  inputArea: {
    flexDirection: 'row',
    paddingHorizontal: 16,
    paddingVertical: 8,
    backgroundColor: 'white',
    alignItems: 'center'
  },
  textInput: {
    flex: 1,
    height: 40,
    borderWidth: 1,
    borderColor: '#E0E0E0',
    borderRadius: 20,
    paddingHorizontal: 16,
    fontSize: 16,
    marginRight: 8
  },
  sendButton: {
    backgroundColor: '#007AFF',
    paddingHorizontal: 20,
    paddingVertical: 10,
    borderRadius: 20
  },
  sendButtonText: {
    color: 'white',
    fontSize: 16,
    fontWeight: 'bold'
  },
  voiceControls: {
    paddingHorizontal: 16,
    paddingVertical: 16,
    backgroundColor: 'white',
    alignItems: 'center'
  },
  voiceButton: {
    width: 120,
    height: 120,
    borderRadius: 60,
    justifyContent: 'center',
    alignItems: 'center'
  },
  voiceButtonText: {
    color: 'white',
    fontSize: 16,
    fontWeight: 'bold',
    textAlign: 'center'
  }
});

export default ChatScreen;
```

---

## ğŸ¯ æ€»ç»“

æœ¬æ¡†æ¶ä½¿ç”¨æŒ‡å—æä¾›äº†VoiceHelperå¹³å°çš„å…¨é¢é›†æˆæ–¹æ¡ˆï¼š

### ğŸš€ éƒ¨ç½²é€‰é¡¹
- **Dockerä¸€é”®éƒ¨ç½²**: ç”Ÿäº§ç¯å¢ƒå¿«é€Ÿå¯åŠ¨
- **å¼€å‘ç¯å¢ƒæ­å»º**: æœ¬åœ°å¼€å‘å’Œè°ƒè¯•
- **äº‘åŸç”Ÿéƒ¨ç½²**: Kubernetesé›†ç¾¤éƒ¨ç½²

### ğŸ”§ SDKæ”¯æŒ  
- **JavaScript/TypeScript**: Webåº”ç”¨å’ŒNode.jsæœåŠ¡
- **Python**: æœåŠ¡ç«¯åº”ç”¨å’ŒAIå·¥ä½œæµ
- **React Native**: è·¨å¹³å°ç§»åŠ¨åº”ç”¨
- **å¾®ä¿¡å°ç¨‹åº**: åŸç”Ÿå°ç¨‹åºå¼€å‘

### ğŸ“± å¤šå¹³å°è¦†ç›–
- **Webåº”ç”¨**: Reactã€Vueã€Angularç­‰ä¸»æµæ¡†æ¶
- **ç§»åŠ¨åº”ç”¨**: iOSã€AndroidåŸç”Ÿå’Œè·¨å¹³å°
- **æ¡Œé¢åº”ç”¨**: Electronã€Tauriç­‰æ¡Œé¢æ¡†æ¶
- **å°ç¨‹åº**: å¾®ä¿¡ã€æ”¯ä»˜å®ã€ç™¾åº¦ç­‰å°ç¨‹åºå¹³å°

### ğŸŒŸ æ ¸å¿ƒèƒ½åŠ›
- **å¤šæ¨¡æ€äº¤äº’**: æ–‡æœ¬å’Œè¯­éŸ³æ— ç¼åˆ‡æ¢
- **æ™ºèƒ½æ£€ç´¢**: RAGå’ŒGraphRAGåŒé‡åŠ æŒ  
- **å®æ—¶é€šä¿¡**: WebSocketä½å»¶è¿Ÿäº¤äº’
- **ä¼ä¸šçº§**: é«˜å¯ç”¨ã€é«˜æ€§èƒ½ã€é«˜å®‰å…¨

é€šè¿‡æœ¬æŒ‡å—çš„ç¤ºä¾‹ä»£ç å’Œæœ€ä½³å®è·µï¼Œå¼€å‘è€…å¯ä»¥å¿«é€Ÿé›†æˆVoiceHelperå¹³å°ï¼Œæ„å»ºåŠŸèƒ½å¼ºå¤§çš„æ™ºèƒ½å¯¹è¯åº”ç”¨ã€‚
