---
title: "LangChain-09-Memory"
date: 2025-10-04T21:26:31+08:00
draft: false
tags:
  - LangChain
  - æ¶æ„è®¾è®¡
  - æ¦‚è§ˆ
  - æºç åˆ†æ
categories:
  - LangChain
  - AIæ¡†æ¶
  - Python
series: "langchain-source-analysis"
description: "LangChain æºç å‰–æ - 09-Memory"
author: "æºç åˆ†æ"
weight: 500
ShowToc: true
TocOpen: true

---

# LangChain-09-Memory

## æ¨¡å—æ¦‚è§ˆ

## æ¨¡å—åŸºæœ¬ä¿¡æ¯

**æ¨¡å—åç§°**: langchain-memory
**æ¨¡å—è·¯å¾„**: `libs/langchain/langchain/memory/`
**æ ¸å¿ƒèŒè´£**: æä¾›å¯¹è¯è®°å¿†ç®¡ç†ï¼Œè®© LLM åº”ç”¨èƒ½å¤Ÿè®°ä½å†å²å¯¹è¯å¹¶ç»´æŠ¤ä¸Šä¸‹æ–‡

## 1. æ¨¡å—èŒè´£

### 1.1 æ ¸å¿ƒèŒè´£

Memory æ¨¡å—ä¸º LangChain åº”ç”¨æä¾›å¯¹è¯è®°å¿†èƒ½åŠ›ï¼Œä¸»è¦åŠŸèƒ½ï¼š

1. **å¯¹è¯å†å²å­˜å‚¨**: ä¿å­˜ç”¨æˆ·å’Œ AI çš„å†å²æ¶ˆæ¯
2. **ä¸Šä¸‹æ–‡ç®¡ç†**: æ§åˆ¶ä¼ é€’ç»™ LLM çš„å†å²ä¿¡æ¯é‡
3. **å¤šç§è®°å¿†ç­–ç•¥**: ç¼“å†²ã€çª—å£ã€æ‘˜è¦ã€å‘é‡æ£€ç´¢ç­‰
4. **å®ä½“è·Ÿè¸ª**: è®°ä½å¯¹è¯ä¸­çš„å…³é”®å®ä½“
5. **è®°å¿†æŒä¹…åŒ–**: æ”¯æŒä¿å­˜å’Œæ¢å¤è®°å¿†
6. **å¤šä¼šè¯ç®¡ç†**: æ”¯æŒå¤šä¸ªç‹¬ç«‹çš„å¯¹è¯ä¼šè¯

### 1.2 æ ¸å¿ƒæ¦‚å¿µ

```
ç”¨æˆ·æ¶ˆæ¯
  â†“
Memory (ä¿å­˜å†å²)
  â†“
åŠ è½½ç›¸å…³å†å²
  â†“
æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„æç¤ºè¯
  â†“
LLM ç”Ÿæˆå“åº”
  â†“
Memory (ä¿å­˜ AI å“åº”)
```

**å…³é”®æœ¯è¯­**:

- **Memory**: è®°å¿†æŠ½è±¡ï¼Œç®¡ç†å¯¹è¯å†å²
- **ChatMessageHistory**: æ¶ˆæ¯å­˜å‚¨åç«¯
- **ConversationBufferMemory**: ç¼“å†²æ‰€æœ‰å†å²
- **ConversationWindowMemory**: åªä¿ç•™æœ€è¿‘ k è½®
- **ConversationSummaryMemory**: æ‘˜è¦å‹ç¼©å†å²
- **VectorStoreBackedMemory**: åŸºäºå‘é‡æ£€ç´¢çš„è®°å¿†

### 1.3 è®°å¿†ç±»å‹å¯¹æ¯”

| è®°å¿†ç±»å‹ | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | åŠ£åŠ¿ | Tokenæ¶ˆè€— |
|---------|---------|------|------|----------|
| **ConversationBufferMemory** | çŸ­å¯¹è¯ | å®Œæ•´ä¸Šä¸‹æ–‡ | Tokenå¿«é€Ÿå¢é•¿ | é«˜ |
| **ConversationWindowMemory** | ä¸€èˆ¬å¯¹è¯ | Tokenå¯æ§ | ä¸¢å¤±æ—©æœŸä¿¡æ¯ | ä¸­ |
| **ConversationSummaryMemory** | é•¿å¯¹è¯ | å‹ç¼©å†å² | æ‘˜è¦å¯èƒ½ä¸å‡†ç¡® | ä½ |
| **ConversationSummaryBufferMemory** | é•¿å¯¹è¯+ç²¾ç¡®æ€§ | å¹³è¡¡å‹ç¼©å’Œç»†èŠ‚ | éœ€è¦é¢å¤–æ‘˜è¦è°ƒç”¨ | ä¸­ |
| **VectorStoreBackedMemory** | å¤æ‚å¯¹è¯ | æ™ºèƒ½æ£€ç´¢ç›¸å…³å†å² | éœ€è¦å‘é‡å­˜å‚¨ | ä½ |
| **EntityMemory** | å®ä½“å¯†é›†å¯¹è¯ | è·Ÿè¸ªå®ä½“ä¿¡æ¯ | ç»´æŠ¤å¼€é”€å¤§ | ä¸­ |

### 1.4 è¾“å…¥/è¾“å‡º

**è¾“å…¥**:

- **save_context**: `{"input": "user message", "output": "ai response"}`

**è¾“å‡º**:

- **load_memory_variables**: `{"history": "formatted_history"}` æˆ– `{"history": list[BaseMessage]}`

### 1.5 ä¸Šä¸‹æ¸¸ä¾èµ–

**ä¸Šæ¸¸è°ƒç”¨è€…**:

- Chainsï¼ˆå¯¹è¯é“¾ï¼‰
- èŠå¤©åº”ç”¨
- ä»£ç†ï¼ˆå¸¦è®°å¿†çš„ä»£ç†ï¼‰

**ä¸‹æ¸¸ä¾èµ–**:

- `langchain_core.messages`: æ¶ˆæ¯ç±»å‹
- `langchain_core.chat_history`: èŠå¤©å†å²æŠ½è±¡
- å‘é‡å­˜å‚¨ï¼ˆVectorStoreBackedMemoryï¼‰
- LLMï¼ˆç”¨äºæ‘˜è¦ï¼‰

## 2. æ¨¡å—çº§æ¶æ„å›¾

```mermaid
flowchart TB
    subgraph Base["åŸºç¡€æŠ½è±¡å±‚"]
        BM[BaseMemory<br/>è®°å¿†åŸºç±»]
        BCMH[BaseChatMessageHistory<br/>æ¶ˆæ¯å†å²åŸºç±»]
    end

    subgraph Simple["ç®€å•è®°å¿†"]
        CBM[ConversationBufferMemory<br/>ç¼“å†²è®°å¿†]
        CWM[ConversationWindowMemory<br/>çª—å£è®°å¿†]
    end

    subgraph Advanced["é«˜çº§è®°å¿†"]
        CSM[ConversationSummaryMemory<br/>æ‘˜è¦è®°å¿†]
        CSBM[ConversationSummaryBufferMemory<br/>æ‘˜è¦ç¼“å†²è®°å¿†]
        VSBM[VectorStoreBackedMemory<br/>å‘é‡æ£€ç´¢è®°å¿†]
        EM[EntityMemory<br/>å®ä½“è®°å¿†]
    end

    subgraph Storage["å­˜å‚¨åç«¯"]
        CMH[ChatMessageHistory<br/>å†…å­˜å­˜å‚¨]
        REDIS[RedisChatMessageHistory<br/>Rediså­˜å‚¨]
        POSTGRES[PostgresChatMessageHistory<br/>PostgreSQLå­˜å‚¨]
        FILE[FileChatMessageHistory<br/>æ–‡ä»¶å­˜å‚¨]
    end

    subgraph Integration["é›†æˆ"]
        CHAIN[ConversationChain<br/>å¯¹è¯é“¾]
        RWMH[RunnableWithMessageHistory<br/>LCELé›†æˆ]
    end

    BM --> CBM
    BM --> CWM
    BM --> CSM
    BM --> CSBM
    BM --> VSBM
    BM --> EM

    BCMH --> CMH
    BCMH --> REDIS
    BCMH --> POSTGRES
    BCMH --> FILE

    CBM --> CMH
    CWM --> CMH
    CSM --> CMH

    CBM --> CHAIN
    CHAIN --> RWMH

    style Base fill:#e1f5ff
    style Simple fill:#fff4e1
    style Advanced fill:#e8f5e9
    style Storage fill:#f3e5f5
    style Integration fill:#fff3e0
```

### æ¶æ„å›¾è¯¦ç»†è¯´æ˜

**1. åŸºç¡€æŠ½è±¡å±‚**

- **BaseMemory**: æ‰€æœ‰è®°å¿†çš„åŸºç±»

  ```python
  class BaseMemory(ABC):
      @property
      @abstractmethod
      def memory_variables(self) -> list[str]:
          """è®°å¿†æä¾›çš„å˜é‡ååˆ—è¡¨"""

      @abstractmethod
      def load_memory_variables(self, inputs: dict[str, Any]) -> dict[str, Any]:
          """åŠ è½½è®°å¿†å˜é‡"""

      @abstractmethod
      def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -> None:
          """ä¿å­˜å¯¹è¯ä¸Šä¸‹æ–‡"""

      def clear(self) -> None:
          """æ¸…é™¤è®°å¿†"""
```

- **BaseChatMessageHistory**: æ¶ˆæ¯å†å²å­˜å‚¨æŠ½è±¡

  ```python
  class BaseChatMessageHistory(ABC):
      messages: list[BaseMessage]  # æ¶ˆæ¯åˆ—è¡¨

      def add_user_message(self, message: str) -> None:
          """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯"""

      def add_ai_message(self, message: str) -> None:
          """æ·»åŠ AIæ¶ˆæ¯"""

      def clear(self) -> None:
          """æ¸…ç©ºå†å²"""
```

**2. ç®€å•è®°å¿†å®ç°**

- **ConversationBufferMemory**: ç¼“å†²æ‰€æœ‰å†å²
  - ä¿ç•™å®Œæ•´å¯¹è¯å†å²
  - Token æ¶ˆè€—éšå¯¹è¯å¢é•¿çº¿æ€§å¢åŠ 
  - é€‚åˆçŸ­å¯¹è¯æˆ–éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡çš„åœºæ™¯

  ```python
  class ConversationBufferMemory(BaseMemory):
      chat_memory: BaseChatMessageHistory  # æ¶ˆæ¯å­˜å‚¨
      return_messages: bool = False  # è¿”å›æ¶ˆæ¯å¯¹è±¡è¿˜æ˜¯å­—ç¬¦ä¸²

      def load_memory_variables(self, inputs: dict) -> dict:
          """åŠ è½½æ‰€æœ‰å†å²æ¶ˆæ¯"""
          if self.return_messages:
              return {"history": self.chat_memory.messages}
          else:
              return {"history": self._get_buffer_string()}

      def save_context(self, inputs: dict, outputs: dict) -> None:
          """ä¿å­˜è¾“å…¥å’Œè¾“å‡º"""
          self.chat_memory.add_user_message(inputs["input"])
          self.chat_memory.add_ai_message(outputs["output"])
```

- **ConversationWindowMemory**: æ»‘åŠ¨çª—å£è®°å¿†
  - åªä¿ç•™æœ€è¿‘ k è½®å¯¹è¯
  - Token æ¶ˆè€—å›ºå®š
  - é€‚åˆä¸€èˆ¬é•¿åº¦å¯¹è¯

  ```python
  class ConversationWindowMemory(BaseMemory):
      k: int = 5  # ä¿ç•™æœ€è¿‘5è½®å¯¹è¯

      def load_memory_variables(self, inputs: dict) -> dict:
          """åŠ è½½æœ€è¿‘kè½®å¯¹è¯"""
          messages = self.chat_memory.messages[-self.k*2:]  # kè½®=kå¯¹æ¶ˆæ¯
          return {"history": messages}
```

**3. é«˜çº§è®°å¿†å®ç°**

- **ConversationSummaryMemory**: æ‘˜è¦è®°å¿†
  - ä½¿ç”¨ LLM å‹ç¼©å†å²ä¸ºæ‘˜è¦
  - èŠ‚çœ Token
  - é€‚åˆé•¿å¯¹è¯

  ```python
  class ConversationSummaryMemory(BaseMemory):
      llm: BaseLanguageModel  # ç”¨äºç”Ÿæˆæ‘˜è¦çš„LLM
      buffer: str = ""  # å½“å‰æ‘˜è¦

      def predict_new_summary(
          self,
          messages: list[BaseMessage],
          existing_summary: str
      ) -> str:
          """ç”Ÿæˆæ–°æ‘˜è¦"""
          # æç¤ºè¯ï¼šæ ¹æ®ç°æœ‰æ‘˜è¦å’Œæ–°æ¶ˆæ¯ï¼Œç”Ÿæˆæ›´æ–°çš„æ‘˜è¦
          prompt = f"""
          Current summary: {existing_summary}
          New messages: {messages}
          Updated summary:
          """
          return self.llm.predict(prompt)

      def save_context(self, inputs: dict, outputs: dict) -> None:
          """ä¿å­˜å¹¶æ›´æ–°æ‘˜è¦"""
          # æ·»åŠ æ–°æ¶ˆæ¯
          self.chat_memory.add_user_message(inputs["input"])
          self.chat_memory.add_ai_message(outputs["output"])

          # æ›´æ–°æ‘˜è¦
          new_messages = self.chat_memory.messages[-2:]
          self.buffer = self.predict_new_summary(new_messages, self.buffer)
```

- **ConversationSummaryBufferMemory**: æ··åˆè®°å¿†
  - æœ€è¿‘æ¶ˆæ¯ä¿æŒåŸæ ·
  - è¾ƒæ—©æ¶ˆæ¯å‹ç¼©ä¸ºæ‘˜è¦
  - å¹³è¡¡ç»†èŠ‚å’Œå‹ç¼©

  ```python
  class ConversationSummaryBufferMemory(BaseMemory):
      max_token_limit: int = 2000  # Tokenä¸Šé™

      def load_memory_variables(self, inputs: dict) -> dict:
          """è¿”å›æ‘˜è¦+æœ€è¿‘æ¶ˆæ¯"""
          return {
              "history": self.moving_summary_buffer + recent_messages
          }
```

- **VectorStoreBackedMemory**: å‘é‡æ£€ç´¢è®°å¿†
  - åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³å†å²
  - é€‚åˆå¤æ‚ã€éçº¿æ€§å¯¹è¯

  ```python
  class VectorStoreBackedMemory(BaseMemory):
      vectorstore: VectorStore
      k: int = 4  # æ£€ç´¢æ•°é‡

      def load_memory_variables(self, inputs: dict) -> dict:
          """æ£€ç´¢ç›¸å…³å†å²"""
          query = inputs["input"]
          docs = self.vectorstore.similarity_search(query, k=self.k)
          return {"history": docs}

      def save_context(self, inputs: dict, outputs: dict) -> None:
          """ä¿å­˜åˆ°å‘é‡å­˜å‚¨"""
          text = f"Human: {inputs['input']}\nAI: {outputs['output']}"
          self.vectorstore.add_texts([text])
```

- **EntityMemory**: å®ä½“è®°å¿†
  - æå–å’Œè·Ÿè¸ªå¯¹è¯ä¸­çš„å®ä½“
  - ç»´æŠ¤å®ä½“çŸ¥è¯†å›¾è°±

  ```python
  class EntityMemory(BaseMemory):
      entity_store: dict[str, str]  # å®ä½“å­˜å‚¨

      def save_context(self, inputs: dict, outputs: dict) -> None:
          """æå–å¹¶ä¿å­˜å®ä½“"""
          entities = self._extract_entities(inputs["input"], outputs["output"])
          for entity, info in entities.items():
              self.entity_store[entity] = info
```

**4. å­˜å‚¨åç«¯**

- **ChatMessageHistory**: å†…å­˜å­˜å‚¨ï¼ˆé»˜è®¤ï¼‰
  - å­˜å‚¨åœ¨å†…å­˜ä¸­
  - è¿›ç¨‹é‡å¯åä¸¢å¤±

- **RedisChatMessageHistory**: Redis å­˜å‚¨
  - æŒä¹…åŒ–
  - æ”¯æŒåˆ†å¸ƒå¼

- **PostgresChatMessageHistory**: PostgreSQL å­˜å‚¨
  - å…³ç³»æ•°æ®åº“å­˜å‚¨
  - æ”¯æŒå¤æ‚æŸ¥è¯¢

- **FileChatMessageHistory**: æ–‡ä»¶å­˜å‚¨
  - æœ¬åœ°æ–‡ä»¶æŒä¹…åŒ–
  - ç®€å•æ˜“ç”¨

**5. é›†æˆæ–¹å¼**

- **ConversationChain**: ä¼ ç»Ÿå¯¹è¯é“¾ï¼ˆå·²åºŸå¼ƒï¼‰

  ```python
  chain = ConversationChain(
      llm=llm,
      memory=ConversationBufferMemory()
  )
```

- **RunnableWithMessageHistory**: LCEL é›†æˆï¼ˆæ¨èï¼‰

  ```python
  chain_with_history = RunnableWithMessageHistory(
      runnable=chain,
      get_session_history=get_chat_history,
      input_messages_key="input",
      history_messages_key="history"
  )
```

## 3. æ ¸å¿ƒ API è¯¦è§£

### 3.1 ConversationBufferMemory - å®Œæ•´å†å²è®°å¿†

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from langchain.memory import ConversationBufferMemory

# åˆ›å»ºè®°å¿†
memory = ConversationBufferMemory()

# ä¿å­˜å¯¹è¯
memory.save_context(
    {"input": "Hi, I'm Alice"},
    {"output": "Hello Alice! Nice to meet you."}
)

memory.save_context(
    {"input": "What's my name?"},
    {"output": "Your name is Alice."}
)

# åŠ è½½è®°å¿†
print(memory.load_memory_variables({}))
# {
#   "history": "Human: Hi, I'm Alice\nAI: Hello Alice! Nice to meet you.\nHuman: What's my name?\nAI: Your name is Alice."
# }

# è¿”å›æ¶ˆæ¯å¯¹è±¡
memory_with_messages = ConversationBufferMemory(return_messages=True)
memory_with_messages.save_context({"input": "Hi"}, {"output": "Hello"})
print(memory_with_messages.load_memory_variables({}))
# {
#   "history": [
#       HumanMessage(content="Hi"),
#       AIMessage(content="Hello")
#   ]
# }

# åœ¨å¯¹è¯é“¾ä¸­ä½¿ç”¨
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI

conversation = ConversationChain(
    llm=ChatOpenAI(),
    memory=ConversationBufferMemory()
)

# å¯¹è¯1
response1 = conversation.predict(input="Hi, I'm Bob")
# "Hello Bob! How can I help you today?"

# å¯¹è¯2ï¼ˆè®°å¾—å‰é¢çš„å¯¹è¯ï¼‰
response2 = conversation.predict(input="What's my name?")
# "Your name is Bob."
```

### 3.2 ConversationWindowMemory - æ»‘åŠ¨çª—å£è®°å¿†

```python
from langchain.memory import ConversationWindowMemory

# åªä¿ç•™æœ€è¿‘2è½®å¯¹è¯
memory = ConversationWindowMemory(k=2)

# æ¨¡æ‹Ÿ5è½®å¯¹è¯
conversations = [
    ("Hi", "Hello"),
    ("My name is Alice", "Nice to meet you, Alice"),
    ("I like pizza", "Pizza is great!"),
    ("What's the weather?", "It's sunny today"),
    ("Thanks", "You're welcome!")
]

for user_msg, ai_msg in conversations:
    memory.save_context({"input": user_msg}, {"output": ai_msg})

# åªä¼šçœ‹åˆ°æœ€å2è½®
print(memory.load_memory_variables({}))
# {
#   "history": "Human: What's the weather?\nAI: It's sunny today\nHuman: Thanks\nAI: You're welcome!"
# }

# åœ¨é“¾ä¸­ä½¿ç”¨
conversation = ConversationChain(
    llm=ChatOpenAI(),
    memory=ConversationWindowMemory(k=3)  # åªè®°ä½æœ€è¿‘3è½®
)
```

### 3.3 ConversationSummaryMemory - æ‘˜è¦è®°å¿†

```python
from langchain.memory import ConversationSummaryMemory
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0)
memory = ConversationSummaryMemory(llm=llm)

# é•¿å¯¹è¯
memory.save_context(
    {"input": "Hi, I'm planning a trip to Japan"},
    {"output": "That sounds exciting! Japan is a wonderful destination. When are you planning to go?"}
)

memory.save_context(
    {"input": "I'm thinking March or April. What's the best time?"},
    {"output": "March and April are great times to visit Japan! You'll be there during cherry blossom season."}
)

memory.save_context(
    {"input": "Where should I visit in Tokyo?"},
    {"output": "In Tokyo, you should visit Shibuya, Shinjuku, Asakusa for the Senso-ji Temple, and Akihabara."}
)

# åŠ è½½æ‘˜è¦ï¼ˆè€Œä¸æ˜¯å®Œæ•´å†å²ï¼‰
print(memory.load_memory_variables({}))
# {
#   "history": "The human is planning a trip to Japan in March or April to see cherry blossoms. They've been given recommendations for places to visit in Tokyo including Shibuya, Shinjuku, Asakusa, and Akihabara."
# }
```

### 3.4 VectorStoreBackedMemory - å‘é‡æ£€ç´¢è®°å¿†

```python
from langchain.memory import VectorStoreRetrieverMemory
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts([], embeddings)

# åˆ›å»ºè®°å¿†
memory = VectorStoreRetrieverMemory(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 2})
)

# ä¿å­˜å¤šä¸ªå¯¹è¯ç‰‡æ®µ
memory.save_context(
    {"input": "My favorite color is blue"},
    {"output": "That's nice! Blue is a calming color."}
)

memory.save_context(
    {"input": "I have a dog named Max"},
    {"output": "Dogs are wonderful pets! Max sounds lovely."}
)

memory.save_context(
    {"input": "I work as a software engineer"},
    {"output": "That's a great profession!"}
)

# åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³è®°å¿†
# é—®å…³äºå® ç‰©çš„é—®é¢˜ï¼Œä¼šæ£€ç´¢åˆ°å…³äºç‹—çš„è®°å¿†
result = memory.load_memory_variables({"input": "Tell me about my pet"})
print(result)
# ä¼šæ£€ç´¢åˆ°ï¼š"I have a dog named Max"

# é—®å…³äºå·¥ä½œçš„é—®é¢˜
result = memory.load_memory_variables({"input": "What do I do for a living?"})
print(result)
# ä¼šæ£€ç´¢åˆ°ï¼š"I work as a software engineer"
```

### 3.5 RunnableWithMessageHistory - LCEL é›†æˆï¼ˆæ¨èï¼‰

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# 1. åˆ›å»ºèŠå¤©é“¾
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

chain = prompt | ChatOpenAI() | StrOutputParser()

# 2. ä¼šè¯å†å²å­˜å‚¨
store = {}  # session_id -> ChatMessageHistory

def get_session_history(session_id: str) -> ChatMessageHistory:
    """è·å–æˆ–åˆ›å»ºä¼šè¯å†å²"""
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# 3. åŒ…è£…é“¾ä»¥æ”¯æŒå†å²
chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# 4. ä½¿ç”¨ï¼ˆä¼ å…¥ session_idï¼‰
config = {"configurable": {"session_id": "user123"}}

response1 = chain_with_history.invoke(
    {"input": "Hi, I'm Alice"},
    config=config
)
print(response1)  # "Hello Alice! How can I help you?"

response2 = chain_with_history.invoke(
    {"input": "What's my name?"},
    config=config
)
print(response2)  # "Your name is Alice."

# 5. ä¸åŒä¼šè¯ç‹¬ç«‹è®°å¿†
config2 = {"configurable": {"session_id": "user456"}}

response3 = chain_with_history.invoke(
    {"input": "What's my name?"},
    config=config2
)
print(response3)  # "I don't know your name. Could you tell me?"
```

### 3.6 æŒä¹…åŒ–è®°å¿†

```python
# Redis æŒä¹…åŒ–
from langchain_community.chat_message_histories import RedisChatMessageHistory

history = RedisChatMessageHistory(
    session_id="user123",
    url="redis://localhost:6379"
)

chain_with_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: RedisChatMessageHistory(
        session_id=session_id,
        url="redis://localhost:6379"
    ),
    input_messages_key="input",
    history_messages_key="history"
)

# æ–‡ä»¶æŒä¹…åŒ–
from langchain_community.chat_message_histories import FileChatMessageHistory

def get_file_history(session_id: str):
    return FileChatMessageHistory(f"./chat_histories/{session_id}.json")

chain_with_history = RunnableWithMessageHistory(
    chain,
    get_file_history,
    input_messages_key="input",
    history_messages_key="history"
)
```

## 4. æ ¸å¿ƒæµç¨‹æ—¶åºå›¾

### 4.1 ConversationBufferMemory å·¥ä½œæµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User as ç”¨æˆ·
    participant Chain as ConversationChain
    participant Memory as ConversationBufferMemory
    participant History as ChatMessageHistory
    participant LLM as ChatModel

    User->>Chain: invoke("What's my name?")
    activate Chain

    Chain->>Memory: load_memory_variables({})
    activate Memory
    Memory->>History: get messages
    History-->>Memory: [msg1, msg2, ...]
    Memory->>Memory: æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    Memory-->>Chain: {"history": "..."}
    deactivate Memory

    Chain->>Chain: æ„å»ºæç¤ºè¯
    Note over Chain: System: ...<br/>History: ...<br/>Human: What's my name?

    Chain->>LLM: invoke(prompt)
    LLM-->>Chain: "Your name is Alice"

    Chain->>Memory: save_context(<br/>  {"input": "What's my name?"},<br/>  {"output": "Your name is Alice"}<br/>)
    activate Memory
    Memory->>History: add_user_message("What's my name?")
    Memory->>History: add_ai_message("Your name is Alice")
    deactivate Memory

    Chain-->>User: "Your name is Alice"
    deactivate Chain
```

### 4.2 ConversationSummaryMemory æ‘˜è¦æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant Chain as Chain
    participant Memory as ConversationSummaryMemory
    participant LLM as LLM (for summary)
    participant History as ChatMessageHistory

    Chain->>Memory: save_context(input, output)
    activate Memory

    Memory->>History: add messages
    History-->>Memory: ok

    Memory->>Memory: æ£€æŸ¥æ˜¯å¦éœ€è¦æ‘˜è¦
    Note over Memory: æ¶ˆæ¯æ•°é‡è¾¾åˆ°é˜ˆå€¼

    Memory->>LLM: ç”Ÿæˆæ‘˜è¦æç¤ºè¯
    Note over LLM: Current summary: ...<br/>New messages: ...<br/>Generate updated summary

    activate LLM
    LLM->>LLM: ç”Ÿæˆå‹ç¼©æ‘˜è¦
    LLM-->>Memory: "User is planning..."
    deactivate LLM

    Memory->>Memory: æ›´æ–°æ‘˜è¦ç¼“å†²
    Memory->>History: æ¸…é™¤å·²æ‘˜è¦çš„æ¶ˆæ¯

    Memory-->>Chain: ok
    deactivate Memory

    Note over Memory: ä¸‹æ¬¡åŠ è½½æ—¶è¿”å›æ‘˜è¦+æœ€è¿‘æ¶ˆæ¯
```

### 4.3 RunnableWithMessageHistory å®Œæ•´æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User as ç”¨æˆ·
    participant RWMH as RunnableWithMessageHistory
    participant Store as Session Store
    participant History as ChatMessageHistory
    participant Chain as Runnable Chain
    participant LLM as ChatModel

    User->>RWMH: invoke(<br/>  {"input": "Hi"},<br/>  config={"session_id": "user123"}<br/>)
    activate RWMH

    RWMH->>Store: get_session_history("user123")
    activate Store

    alt ä¼šè¯å­˜åœ¨
        Store-->>RWMH: existing_history
    else æ–°ä¼šè¯
        Store->>History: create new ChatMessageHistory()
        Store-->>RWMH: new_history
    end
    deactivate Store

    RWMH->>History: get messages
    History-->>RWMH: list[BaseMessage]

    RWMH->>RWMH: æ„å»ºè¾“å…¥
    Note over RWMH: {<br/>  "input": "Hi",<br/>  "history": [...]<br/>}

    RWMH->>Chain: invoke(input_with_history)
    activate Chain
    Chain->>LLM: process
    LLM-->>Chain: response
    Chain-->>RWMH: "Hello!"
    deactivate Chain

    RWMH->>History: add_user_message("Hi")
    RWMH->>History: add_ai_message("Hello!")

    RWMH-->>User: "Hello!"
    deactivate RWMH
```

## 5. æœ€ä½³å®è·µ

### 5.1 é€‰æ‹©åˆé€‚çš„è®°å¿†ç±»å‹

**çŸ­å¯¹è¯ï¼ˆ< 10è½®ï¼‰**: ConversationBufferMemory

```python
memory = ConversationBufferMemory(return_messages=True)
```

**ä¸€èˆ¬å¯¹è¯ï¼ˆ10-50è½®ï¼‰**: ConversationWindowMemory

```python
memory = ConversationWindowMemory(k=5)  # ä¿ç•™æœ€è¿‘5è½®
```

**é•¿å¯¹è¯ï¼ˆ> 50è½®ï¼‰**: ConversationSummaryBufferMemory

```python
memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=2000
)
```

**éçº¿æ€§å¯¹è¯ï¼ˆéœ€è¦æ£€ç´¢å†å²ï¼‰**: VectorStoreRetrieverMemory

```python
memory = VectorStoreRetrieverMemory(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
)
```

### 5.2 å¤šä¼šè¯ç®¡ç†

```python
from typing import Dict
from langchain_community.chat_message_histories import ChatMessageHistory

class SessionManager:
    """ä¼šè¯ç®¡ç†å™¨"""

    def __init__(self):
        self.sessions: Dict[str, ChatMessageHistory] = {}

    def get_history(self, session_id: str) -> ChatMessageHistory:
        """è·å–æˆ–åˆ›å»ºä¼šè¯"""
        if session_id not in self.sessions:
            self.sessions[session_id] = ChatMessageHistory()
        return self.sessions[session_id]

    def clear_session(self, session_id: str):
        """æ¸…é™¤ä¼šè¯"""
        if session_id in self.sessions:
            self.sessions[session_id].clear()

    def delete_session(self, session_id: str):
        """åˆ é™¤ä¼šè¯"""
        if session_id in self.sessions:
            del self.sessions[session_id]

# ä½¿ç”¨
manager = SessionManager()

chain_with_history = RunnableWithMessageHistory(
    chain,
    manager.get_history,
    input_messages_key="input",
    history_messages_key="history"
)
```

### 5.3 é™åˆ¶å†å²é•¿åº¦

```python
from langchain.memory import ConversationBufferMemory

class TruncatedBufferMemory(ConversationBufferMemory):
    """é™åˆ¶æ¶ˆæ¯æ•°é‡çš„ç¼“å†²è®°å¿†"""
    max_messages: int = 20

    def save_context(self, inputs: dict, outputs: dict) -> None:
        """ä¿å­˜å¹¶æˆªæ–­"""
        super().save_context(inputs, outputs)

        # æˆªæ–­åˆ°æœ€å¤§æ¶ˆæ¯æ•°
        messages = self.chat_memory.messages
        if len(messages) > self.max_messages:
            self.chat_memory.messages = messages[-self.max_messages:]

memory = TruncatedBufferMemory(max_messages=10)
```

### 5.4 è‡ªå®šä¹‰è®°å¿†æ ¼å¼

```python
from langchain.memory import ConversationBufferMemory

class CustomFormattedMemory(ConversationBufferMemory):
    """è‡ªå®šä¹‰æ ¼å¼åŒ–è®°å¿†"""

    def _get_buffer_string(self) -> str:
        """è‡ªå®šä¹‰æ ¼å¼"""
        messages = self.chat_memory.messages
        formatted = []

        for msg in messages:
            if msg.type == "human":
                formatted.append(f"ğŸ‘¤ User: {msg.content}")
            elif msg.type == "ai":
                formatted.append(f"ğŸ¤– Assistant: {msg.content}")

        return "\n".join(formatted)

memory = CustomFormattedMemory()
```

### 5.5 æ€§èƒ½ä¼˜åŒ–

**1. å¼‚æ­¥ä¿å­˜**:

```python
import asyncio

async def async_save_conversation(memory, inputs, outputs):
    """å¼‚æ­¥ä¿å­˜å¯¹è¯"""
    await asyncio.to_thread(
        memory.save_context,
        inputs,
        outputs
    )

# ä½¿ç”¨
asyncio.create_task(async_save_conversation(memory, inputs, outputs))
```

**2. æ‰¹é‡ä¿å­˜**:

```python
class BatchMemory:
    """æ‰¹é‡ä¿å­˜è®°å¿†"""

    def __init__(self, memory, batch_size=10):
        self.memory = memory
        self.batch_size = batch_size
        self.buffer = []

    def add(self, inputs, outputs):
        """æ·»åŠ åˆ°ç¼“å†²"""
        self.buffer.append((inputs, outputs))

        if len(self.buffer) >= self.batch_size:
            self.flush()

    def flush(self):
        """æ‰¹é‡ä¿å­˜"""
        for inputs, outputs in self.buffer:
            self.memory.save_context(inputs, outputs)
        self.buffer.clear()
```

**3. Redis è¿æ¥æ± **:

```python
import redis

# ä½¿ç”¨è¿æ¥æ± 
pool = redis.ConnectionPool(
    host='localhost',
    port=6379,
    max_connections=10
)

def get_redis_history(session_id: str):
    return RedisChatMessageHistory(
        session_id=session_id,
        url="redis://localhost:6379",
        ttl=3600  # 1å°æ—¶è¿‡æœŸ
    )
```

## 6. å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 6.1 Token æ¶ˆè€—è¿‡å¿«

**é—®é¢˜**: å¯¹è¯å†å²å¯¼è‡´ Token å¿«é€Ÿå¢é•¿

**è§£å†³æ–¹æ¡ˆ**:

```python
# æ–¹æ¡ˆ1: ä½¿ç”¨çª—å£è®°å¿†
memory = ConversationWindowMemory(k=3)

# æ–¹æ¡ˆ2: ä½¿ç”¨æ‘˜è¦è®°å¿†
memory = ConversationSummaryMemory(llm=llm)

# æ–¹æ¡ˆ3: è‡ªå®šä¹‰æˆªæ–­
class TokenLimitedMemory(ConversationBufferMemory):
    max_tokens: int = 1000

    def load_memory_variables(self, inputs: dict) -> dict:
        """é™åˆ¶Tokenæ•°é‡"""
        messages = self.chat_memory.messages
        total_tokens = 0
        truncated_messages = []

        # ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹
        for msg in reversed(messages):
            msg_tokens = len(msg.content.split())  # ç®€åŒ–è®¡ç®—
            if total_tokens + msg_tokens > self.max_tokens:
                break
            truncated_messages.insert(0, msg)
            total_tokens += msg_tokens

        self.chat_memory.messages = truncated_messages
        return super().load_memory_variables(inputs)
```

### 6.2 å¤šç”¨æˆ·å¹¶å‘

**é—®é¢˜**: å¤šä¸ªç”¨æˆ·åŒæ—¶è®¿é—®ï¼Œè®°å¿†æ··ä¹±

**è§£å†³æ–¹æ¡ˆ**:

```python
# ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„ä¼šè¯ç®¡ç†
from threading import Lock

class ThreadSafeSessionManager:
    def __init__(self):
        self.sessions = {}
        self.locks = {}
        self.global_lock = Lock()

    def get_history(self, session_id: str):
        with self.global_lock:
            if session_id not in self.locks:
                self.locks[session_id] = Lock()

        with self.locks[session_id]:
            if session_id not in self.sessions:
                self.sessions[session_id] = ChatMessageHistory()
            return self.sessions[session_id]
```

### 6.3 è®°å¿†æŒä¹…åŒ–å¤±è´¥

**é—®é¢˜**: Redis/æ•°æ®åº“è¿æ¥å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:

```python
# ä½¿ç”¨å›é€€æœºåˆ¶
class FallbackMemory:
    def __init__(self, primary, fallback):
        self.primary = primary
        self.fallback = fallback

    def save_context(self, inputs, outputs):
        try:
            self.primary.save_context(inputs, outputs)
        except Exception as e:
            logger.warning(f"Primary storage failed: {e}, using fallback")
            self.fallback.save_context(inputs, outputs)

# ä½¿ç”¨
memory = FallbackMemory(
    primary=RedisChatMessageHistory(...),
    fallback=ChatMessageHistory()  # å†…å­˜å›é€€
)
```

## 7. ä¸å…¶ä»–æ¨¡å—çš„åä½œ

- **Prompts**: é€šè¿‡ MessagesPlaceholder æ³¨å…¥å†å²
- **Language Models**: æ¥æ”¶åŒ…å«å†å²çš„æç¤ºè¯
- **Chains**: ConversationChain é›†æˆè®°å¿†
- **Runnables**: RunnableWithMessageHistory æä¾› LCEL æ”¯æŒ
- **VectorStores**: VectorStoreBackedMemory ä½¿ç”¨å‘é‡æ£€ç´¢

## 8. æ€»ç»“

Memory æ¨¡å—ä¸º LangChain æä¾›äº†çµæ´»çš„å¯¹è¯è®°å¿†ç®¡ç†èƒ½åŠ›ã€‚å…³é”®ç‰¹æ€§ï¼š

1. **å¤šç§è®°å¿†ç­–ç•¥**: Bufferã€Windowã€Summaryã€Vector
2. **çµæ´»å­˜å‚¨**: å†…å­˜ã€Redisã€PostgreSQLã€æ–‡ä»¶
3. **LCEL é›†æˆ**: RunnableWithMessageHistory
4. **ä¼šè¯ç®¡ç†**: æ”¯æŒå¤šç”¨æˆ·å¤šä¼šè¯
5. **å¯æ‰©å±•**: æ˜“äºè‡ªå®šä¹‰è®°å¿†é€»è¾‘

**å…³é”®åŸåˆ™**:

- æ ¹æ®å¯¹è¯é•¿åº¦é€‰æ‹©åˆé€‚çš„è®°å¿†ç±»å‹
- ä½¿ç”¨ RunnableWithMessageHistoryï¼ˆLCELï¼‰è€Œéæ—§çš„ ConversationChain
- ç”Ÿäº§ç¯å¢ƒä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨ï¼ˆRedis/PostgreSQLï¼‰
- é™åˆ¶ Token æ¶ˆè€—ï¼ˆçª—å£/æ‘˜è¦ï¼‰
- å¤šç”¨æˆ·åœºæ™¯ä½¿ç”¨ session_id éš”ç¦»

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-03
**ç›¸å…³æ–‡æ¡£**:

- LangChain-00-æ€»è§ˆ.md
- LangChain-04-Prompts-æ¦‚è§ˆ.md
- LangChain-03-LanguageModels-æ¦‚è§ˆ.md

---

## APIæ¥å£

## æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£è¯¦ç»†æè¿° **Memory æ¨¡å—**çš„å¯¹å¤– APIï¼ŒåŒ…æ‹¬å¯¹è¯è®°å¿†ã€ç¼“å†²åŒºç®¡ç†ã€å‘é‡å­˜å‚¨è®°å¿†ã€æ‘˜è¦è®°å¿†ç­‰æ ¸å¿ƒæ¥å£çš„æ‰€æœ‰å…¬å¼€æ–¹æ³•å’Œå‚æ•°è§„æ ¼ã€‚

---

## 1. BaseMemory æ ¸å¿ƒ API

### 1.1 åŸºç¡€æ¥å£

#### åŸºæœ¬ä¿¡æ¯
- **ç±»å**ï¼š`BaseMemory`
- **åŠŸèƒ½**ï¼šæ‰€æœ‰è®°å¿†ç³»ç»Ÿçš„æŠ½è±¡åŸºç±»
- **æ ¸å¿ƒèŒè´£**ï¼šå­˜å‚¨å¯¹è¯å†å²ã€ç®¡ç†ä¸Šä¸‹æ–‡ã€æä¾›è®°å¿†æ£€ç´¢

#### æ ¸å¿ƒæ–¹æ³•

```python
class BaseMemory(Serializable, ABC):
    """è®°å¿†åŸºç±»ã€‚"""

    @property
    @abstractmethod
    def memory_variables(self) -> List[str]:
        """è¿”å›è®°å¿†å˜é‡åˆ—è¡¨ã€‚"""

    @abstractmethod
    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """åŠ è½½è®°å¿†å˜é‡ã€‚"""

    @abstractmethod
    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
        """ä¿å­˜å¯¹è¯ä¸Šä¸‹æ–‡ã€‚"""

    def clear(self) -> None:
        """æ¸…é™¤è®°å¿†å†…å®¹ã€‚"""
        pass
```

**æ–¹æ³•è¯¦è§£**ï¼š

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|-----|------|---------|------|
| memory_variables | å±æ€§ | `List[str]` | è®°å¿†ç³»ç»Ÿæä¾›çš„å˜é‡ååˆ—è¡¨ |
| load_memory_variables | `inputs: Dict[str, Any]` | `Dict[str, Any]` | æ ¹æ®è¾“å…¥åŠ è½½ç›¸å…³è®°å¿† |
| save_context | `inputs: Dict`, `outputs: Dict` | `None` | ä¿å­˜å¯¹è¯è½®æ¬¡åˆ°è®°å¿† |
| clear | æ—  | `None` | æ¸…ç©ºæ‰€æœ‰è®°å¿†å†…å®¹ |

---

## 2. ConversationBufferMemory API

### 2.1 åŸºç¡€ç¼“å†²åŒºè®°å¿†

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šå­˜å‚¨å®Œæ•´çš„å¯¹è¯å†å²
- **ç‰¹ç‚¹**ï¼šç®€å•ç›´æ¥ï¼Œä¿ç•™æ‰€æœ‰å¯¹è¯å†…å®¹
- **é€‚ç”¨åœºæ™¯**ï¼šçŸ­å¯¹è¯ã€éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡çš„åœºæ™¯

#### æ„é€ å‚æ•°

```python
class ConversationBufferMemory(BaseChatMemory):
    def __init__(
        self,
        human_prefix: str = "Human",
        ai_prefix: str = "AI",
        memory_key: str = "history",
        return_messages: bool = False,
        input_key: Optional[str] = None,
        output_key: Optional[str] = None,
        **kwargs: Any
    ):
        """å¯¹è¯ç¼“å†²åŒºè®°å¿†æ„é€ å‡½æ•°ã€‚"""
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|------|--------|------|
| human_prefix | `str` | `"Human"` | äººç±»æ¶ˆæ¯å‰ç¼€ |
| ai_prefix | `str` | `"AI"` | AIæ¶ˆæ¯å‰ç¼€ |
| memory_key | `str` | `"history"` | è®°å¿†å˜é‡çš„é”®å |
| return_messages | `bool` | `False` | æ˜¯å¦è¿”å›æ¶ˆæ¯å¯¹è±¡è€Œéå­—ç¬¦ä¸² |
| input_key | `str` | `None` | æŒ‡å®šè¾“å…¥é”®ï¼ˆå¤šè¾“å…¥æ—¶ä½¿ç”¨ï¼‰ |
| output_key | `str` | `None` | æŒ‡å®šè¾“å‡ºé”®ï¼ˆå¤šè¾“å‡ºæ—¶ä½¿ç”¨ï¼‰ |

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain.memory import ConversationBufferMemory

# åˆ›å»ºç¼“å†²åŒºè®°å¿†
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# ä¿å­˜å¯¹è¯
memory.save_context(
    {"input": "ä½ å¥½ï¼Œæˆ‘æ˜¯å°æ˜"},
    {"output": "ä½ å¥½å°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚"}
)

memory.save_context(
    {"input": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"},
    {"output": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšã€‚"}
)

# åŠ è½½è®°å¿†
memory_vars = memory.load_memory_variables({})
print(memory_vars["chat_history"])
# [
#   HumanMessage(content="ä½ å¥½ï¼Œæˆ‘æ˜¯å°æ˜"),
#   AIMessage(content="ä½ å¥½å°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚"),
#   HumanMessage(content="ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"),
#   AIMessage(content="ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšã€‚")
# ]
```

#### æ ¸å¿ƒå®ç°

```python
def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    """åŠ è½½è®°å¿†å˜é‡ã€‚"""
    if self.return_messages:
        # è¿”å›æ¶ˆæ¯å¯¹è±¡åˆ—è¡¨
        return {self.memory_key: self.chat_memory.messages}
    else:
        # è¿”å›æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        buffer = self.buffer
        return {self.memory_key: buffer}

def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
    """ä¿å­˜å¯¹è¯ä¸Šä¸‹æ–‡ã€‚"""
    input_str = inputs[self.input_key or list(inputs.keys())[0]]
    output_str = outputs[self.output_key or list(outputs.keys())[0]]

    # æ·»åŠ åˆ°èŠå¤©è®°å¿†
    self.chat_memory.add_user_message(input_str)
    self.chat_memory.add_ai_message(output_str)

@property
def buffer(self) -> str:
    """è·å–æ ¼å¼åŒ–çš„ç¼“å†²åŒºå†…å®¹ã€‚"""
    return get_buffer_string(
        self.chat_memory.messages,
        human_prefix=self.human_prefix,
        ai_prefix=self.ai_prefix
    )
```

---

## 3. ConversationBufferWindowMemory API

### 3.1 çª—å£ç¼“å†²åŒºè®°å¿†

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šåªä¿ç•™æœ€è¿‘çš„Kè½®å¯¹è¯
- **ç‰¹ç‚¹**ï¼šå›ºå®šçª—å£å¤§å°ï¼Œè‡ªåŠ¨æ·˜æ±°æ—§å¯¹è¯
- **é€‚ç”¨åœºæ™¯**ï¼šé•¿å¯¹è¯ã€å†…å­˜é™åˆ¶åœºæ™¯

#### æ„é€ å‚æ•°

```python
class ConversationBufferWindowMemory(BaseChatMemory):
    def __init__(
        self,
        k: int = 5,
        human_prefix: str = "Human",
        ai_prefix: str = "AI",
        memory_key: str = "history",
        return_messages: bool = False,
        **kwargs: Any
    ):
        """çª—å£ç¼“å†²åŒºè®°å¿†æ„é€ å‡½æ•°ã€‚"""
        super().__init__(**kwargs)
        self.k = k  # çª—å£å¤§å°
        self.human_prefix = human_prefix
        self.ai_prefix = ai_prefix
        self.memory_key = memory_key
        self.return_messages = return_messages
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain.memory import ConversationBufferWindowMemory

# åˆ›å»ºçª—å£è®°å¿†ï¼ˆåªä¿ç•™æœ€è¿‘3è½®å¯¹è¯ï¼‰
memory = ConversationBufferWindowMemory(
    k=3,  # ä¿ç•™3è½®å¯¹è¯ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰
    memory_key="chat_history",
    return_messages=True
)

# æ·»åŠ å¤šè½®å¯¹è¯
conversations = [
    ("ç¬¬1è½®ï¼šä½ å¥½", "ä½ å¥½ï¼"),
    ("ç¬¬2è½®ï¼šå¤©æ°”", "ä»Šå¤©å¤©æ°”å¾ˆå¥½"),
    ("ç¬¬3è½®ï¼šæ—¶é—´", "ç°åœ¨æ˜¯ä¸‹åˆ3ç‚¹"),
    ("ç¬¬4è½®ï¼šè®¡åˆ’", "æˆ‘ä»¬æ¥åˆ¶å®šè®¡åˆ’å§"),
    ("ç¬¬5è½®ï¼šæ€»ç»“", "è®©æˆ‘æ€»ç»“ä¸€ä¸‹")
]

for human_msg, ai_msg in conversations:
    memory.save_context({"input": human_msg}, {"output": ai_msg})

# æ£€æŸ¥è®°å¿†å†…å®¹ï¼ˆåªæœ‰æœ€è¿‘3è½®ï¼‰
memory_vars = memory.load_memory_variables({})
messages = memory_vars["chat_history"]
print(f"ä¿ç•™çš„æ¶ˆæ¯æ•°é‡: {len(messages)}")  # 6æ¡æ¶ˆæ¯ï¼ˆ3è½®å¯¹è¯ï¼‰

for msg in messages:
    print(f"{msg.__class__.__name__}: {msg.content}")
# HumanMessage: ç¬¬3è½®ï¼šæ—¶é—´
# AIMessage: ç°åœ¨æ˜¯ä¸‹åˆ3ç‚¹
# HumanMessage: ç¬¬4è½®ï¼šè®¡åˆ’
# AIMessage: æˆ‘ä»¬æ¥åˆ¶å®šè®¡åˆ’å§
# HumanMessage: ç¬¬5è½®ï¼šæ€»ç»“
# AIMessage: è®©æˆ‘æ€»ç»“ä¸€ä¸‹
```

#### çª—å£ç®¡ç†å®ç°

```python
def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
    """ä¿å­˜ä¸Šä¸‹æ–‡å¹¶ç»´æŠ¤çª—å£å¤§å°ã€‚"""
    # æ·»åŠ æ–°æ¶ˆæ¯
    super().save_context(inputs, outputs)

    # ç»´æŠ¤çª—å£å¤§å°
    self._prune_messages()

def _prune_messages(self) -> None:
    """ä¿®å‰ªæ¶ˆæ¯ä»¥ç»´æŠ¤çª—å£å¤§å°ã€‚"""
    messages = self.chat_memory.messages

    # è®¡ç®—åº”ä¿ç•™çš„æ¶ˆæ¯æ•°é‡ï¼ˆkè½®å¯¹è¯ = 2*kæ¡æ¶ˆæ¯ï¼‰
    max_messages = 2 * self.k

    if len(messages) > max_messages:
        # åªä¿ç•™æœ€æ–°çš„æ¶ˆæ¯
        self.chat_memory.messages = messages[-max_messages:]

@property
def buffer(self) -> str:
    """è·å–çª—å£å†…çš„ç¼“å†²åŒºå†…å®¹ã€‚"""
    return get_buffer_string(
        self.chat_memory.messages,
        human_prefix=self.human_prefix,
        ai_prefix=self.ai_prefix
    )
```

---

## 4. ConversationSummaryMemory API

### 4.1 æ‘˜è¦è®°å¿†

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šå°†å¯¹è¯å†å²å‹ç¼©ä¸ºæ‘˜è¦
- **ç‰¹ç‚¹**ï¼šèŠ‚çœå†…å­˜ï¼Œä¿ç•™å…³é”®ä¿¡æ¯
- **é€‚ç”¨åœºæ™¯**ï¼šé•¿æœŸå¯¹è¯ã€å†…å­˜æ•æ„Ÿåº”ç”¨

#### æ„é€ å‚æ•°

```python
class ConversationSummaryMemory(BaseChatMemory):
    def __init__(
        self,
        llm: BaseLanguageModel,
        memory_key: str = "history",
        return_messages: bool = False,
        buffer: str = "",
        prompt: BasePromptTemplate = SUMMARY_PROMPT,
        **kwargs: Any
    ):
        """æ‘˜è¦è®°å¿†æ„é€ å‡½æ•°ã€‚"""
        super().__init__(**kwargs)
        self.llm = llm
        self.memory_key = memory_key
        self.return_messages = return_messages
        self.buffer = buffer
        self.prompt = prompt
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain.memory import ConversationSummaryMemory
from langchain_openai import OpenAI

# åˆ›å»ºæ‘˜è¦è®°å¿†
llm = OpenAI(temperature=0)
memory = ConversationSummaryMemory(
    llm=llm,
    memory_key="chat_summary"
)

# æ·»åŠ å¯¹è¯å†å²
memory.save_context(
    {"input": "æˆ‘æƒ³äº†è§£æœºå™¨å­¦ä¹ "},
    {"output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚"}
)

memory.save_context(
    {"input": "æœ‰å“ªäº›ä¸»è¦çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ"},
    {"output": "ä¸»è¦åŒ…æ‹¬ç›‘ç£å­¦ä¹ ï¼ˆå¦‚å†³ç­–æ ‘ã€éšæœºæ£®æ—ï¼‰ã€æ— ç›‘ç£å­¦ä¹ ï¼ˆå¦‚èšç±»ã€é™ç»´ï¼‰å’Œå¼ºåŒ–å­¦ä¹ ã€‚"}
)

memory.save_context(
    {"input": "ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"},
    {"output": "ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡æ³¨æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œæ— ç›‘ç£å­¦ä¹ ä»æœªæ ‡æ³¨æ•°æ®ä¸­å‘ç°æ¨¡å¼ã€‚"}
)

# è·å–æ‘˜è¦
memory_vars = memory.load_memory_variables({})
print(memory_vars["chat_summary"])
# "ç”¨æˆ·è¯¢é—®äº†æœºå™¨å­¦ä¹ çš„åŸºç¡€æ¦‚å¿µï¼ŒAIè§£é‡Šäº†æœºå™¨å­¦ä¹ çš„å®šä¹‰ã€ä¸»è¦ç®—æ³•åˆ†ç±»ï¼Œ
#  ä»¥åŠç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«ã€‚å¯¹è¯æ¶µç›–äº†æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µã€‚"
```

#### æ‘˜è¦ç”Ÿæˆå®ç°

```python
def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
    """ä¿å­˜ä¸Šä¸‹æ–‡å¹¶æ›´æ–°æ‘˜è¦ã€‚"""
    # æ·»åŠ æ–°æ¶ˆæ¯åˆ°ä¸´æ—¶ç¼“å†²åŒº
    super().save_context(inputs, outputs)

    # ç”Ÿæˆæ–°çš„æ‘˜è¦
    self._update_summary()

def _update_summary(self) -> None:
    """æ›´æ–°å¯¹è¯æ‘˜è¦ã€‚"""
    messages = self.chat_memory.messages

    if len(messages) >= 2:  # è‡³å°‘æœ‰ä¸€è½®å¯¹è¯
        # æ„å»ºæ‘˜è¦æç¤º
        new_lines = get_buffer_string(messages)

        if self.buffer:
            # æœ‰ç°æœ‰æ‘˜è¦ï¼Œè¿›è¡Œå¢é‡æ›´æ–°
            prompt_input = {
                "summary": self.buffer,
                "new_lines": new_lines
            }
            prompt = self.prompt
        else:
            # é¦–æ¬¡ç”Ÿæˆæ‘˜è¦
            prompt_input = {"new_lines": new_lines}
            prompt = SUMMARY_PROMPT

        # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
        self.buffer = self.llm.predict(prompt.format(**prompt_input))

        # æ¸…ç©ºæ¶ˆæ¯ç¼“å†²åŒºï¼ˆå·²ç»æ‘˜è¦åŒ–ï¼‰
        self.chat_memory.clear()

def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    """åŠ è½½æ‘˜è¦è®°å¿†ã€‚"""
    if self.return_messages:
        # å°†æ‘˜è¦è½¬æ¢ä¸ºç³»ç»Ÿæ¶ˆæ¯
        if self.buffer:
            return {self.memory_key: [SystemMessage(content=self.buffer)]}
        else:
            return {self.memory_key: []}
    else:
        return {self.memory_key: self.buffer}
```

#### é»˜è®¤æ‘˜è¦æç¤º

```python
SUMMARY_PROMPT = PromptTemplate(
    input_variables=["summary", "new_lines"],
    template="""
è¯·ç®€æ´åœ°æ€»ç»“ä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼š

ç°æœ‰æ‘˜è¦ï¼š
{summary}

æ–°çš„å¯¹è¯å†…å®¹ï¼š
{new_lines}

æ–°çš„æ‘˜è¦ï¼š
""".strip()
)
```

---

## 5. ConversationTokenBufferMemory API

### 5.1 ä»¤ç‰Œç¼“å†²åŒºè®°å¿†

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šåŸºäºä»¤ç‰Œæ•°é‡é™åˆ¶è®°å¿†å¤§å°
- **ç‰¹ç‚¹**ï¼šç²¾ç¡®æ§åˆ¶è®°å¿†çš„ä»¤ç‰Œæ¶ˆè€—
- **é€‚ç”¨åœºæ™¯**ï¼šAPIæˆæœ¬æ•æ„Ÿã€æœ‰ä¸¥æ ¼ä»¤ç‰Œé™åˆ¶çš„åœºæ™¯

#### æ„é€ å‚æ•°

```python
class ConversationTokenBufferMemory(BaseChatMemory):
    def __init__(
        self,
        llm: BaseLanguageModel,
        max_token_limit: int = 2000,
        return_messages: bool = False,
        memory_key: str = "history",
        **kwargs: Any
    ):
        """ä»¤ç‰Œç¼“å†²åŒºè®°å¿†æ„é€ å‡½æ•°ã€‚"""
        super().__init__(**kwargs)
        self.llm = llm
        self.max_token_limit = max_token_limit
        self.return_messages = return_messages
        self.memory_key = memory_key
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain.memory import ConversationTokenBufferMemory
from langchain_openai import OpenAI

# åˆ›å»ºä»¤ç‰Œé™åˆ¶è®°å¿†
llm = OpenAI()
memory = ConversationTokenBufferMemory(
    llm=llm,
    max_token_limit=100,  # é™åˆ¶100ä¸ªä»¤ç‰Œ
    memory_key="chat_history"
)

# æ·»åŠ å¯¹è¯ï¼ˆä¼šè‡ªåŠ¨ç®¡ç†ä»¤ç‰Œæ•°é‡ï¼‰
memory.save_context(
    {"input": "è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„å‘å±•å†å²å’Œä¸»è¦é‡Œç¨‹ç¢‘"},
    {"output": "æ·±åº¦å­¦ä¹ èµ·æºäº1940å¹´ä»£çš„æ„ŸçŸ¥æœºæ¦‚å¿µï¼Œç»å†äº†å¤šæ¬¡èµ·ä¼..."}
)

# æ£€æŸ¥å½“å‰ä»¤ç‰Œä½¿ç”¨æƒ…å†µ
current_tokens = memory._get_current_token_count()
print(f"å½“å‰ä»¤ç‰Œæ•°: {current_tokens}/{memory.max_token_limit}")

# ç»§ç»­æ·»åŠ å¯¹è¯ï¼Œè¶…å‡ºé™åˆ¶æ—¶ä¼šè‡ªåŠ¨åˆ é™¤æ—§æ¶ˆæ¯
memory.save_context(
    {"input": "æ·±åº¦å­¦ä¹ æœ‰å“ªäº›ä¸»è¦åº”ç”¨é¢†åŸŸï¼Ÿ"},
    {"output": "æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨..."}
)
```

#### ä»¤ç‰Œç®¡ç†å®ç°

```python
def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
    """ä¿å­˜ä¸Šä¸‹æ–‡å¹¶ç®¡ç†ä»¤ç‰Œé™åˆ¶ã€‚"""
    # æ·»åŠ æ–°æ¶ˆæ¯
    super().save_context(inputs, outputs)

    # ä¿®å‰ªæ¶ˆæ¯ä»¥ç¬¦åˆä»¤ç‰Œé™åˆ¶
    self._prune_messages_to_token_limit()

def _prune_messages_to_token_limit(self) -> None:
    """ä¿®å‰ªæ¶ˆæ¯ä»¥æ»¡è¶³ä»¤ç‰Œé™åˆ¶ã€‚"""
    while self._get_current_token_count() > self.max_token_limit:
        if len(self.chat_memory.messages) <= 2:
            # è‡³å°‘ä¿ç•™ä¸€è½®å¯¹è¯
            break

        # åˆ é™¤æœ€æ—§çš„æ¶ˆæ¯å¯¹ï¼ˆäººç±»+AIï¼‰
        self.chat_memory.messages = self.chat_memory.messages[2:]

def _get_current_token_count(self) -> int:
    """è®¡ç®—å½“å‰æ¶ˆæ¯çš„ä»¤ç‰Œæ•°é‡ã€‚"""
    buffer = get_buffer_string(self.chat_memory.messages)
    return self.llm.get_num_tokens(buffer)

@property
def buffer(self) -> str:
    """è·å–å½“å‰ç¼“å†²åŒºå†…å®¹ã€‚"""
    return get_buffer_string(self.chat_memory.messages)
```

---

## 6. VectorStoreRetrieverMemory API

### 6.1 å‘é‡å­˜å‚¨è®°å¿†

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šä½¿ç”¨å‘é‡å­˜å‚¨è¿›è¡Œè¯­ä¹‰è®°å¿†æ£€ç´¢
- **ç‰¹ç‚¹**ï¼šåŸºäºç›¸ä¼¼æ€§æ£€ç´¢ç›¸å…³å†å²
- **é€‚ç”¨åœºæ™¯**ï¼šé•¿æœŸè®°å¿†ã€è¯­ä¹‰ç›¸å…³çš„ä¸Šä¸‹æ–‡æ£€ç´¢

#### æ„é€ å‚æ•°

```python
class VectorStoreRetrieverMemory(BaseMemory):
    def __init__(
        self,
        retriever: VectorStoreRetriever,
        memory_key: str = "history",
        input_key: Optional[str] = None,
        return_docs: bool = False,
        **kwargs: Any
    ):
        """å‘é‡å­˜å‚¨æ£€ç´¢è®°å¿†æ„é€ å‡½æ•°ã€‚"""
        self.retriever = retriever
        self.memory_key = memory_key
        self.input_key = input_key
        self.return_docs = return_docs
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain.memory import VectorStoreRetrieverMemory
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)

# åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

# åˆ›å»ºå‘é‡å­˜å‚¨è®°å¿†
memory = VectorStoreRetrieverMemory(
    retriever=retriever,
    memory_key="relevant_history"
)

# ä¿å­˜å¯¹è¯å†å²
memory.save_context(
    {"input": "æˆ‘å¯¹æœºå™¨å­¦ä¹ å¾ˆæ„Ÿå…´è¶£"},
    {"output": "æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªå¾ˆæœ‰å‰æ™¯çš„é¢†åŸŸï¼Œå»ºè®®ä»åŸºç¡€ç®—æ³•å¼€å§‹å­¦ä¹ "}
)

memory.save_context(
    {"input": "Pythonæœ‰å“ªäº›æœºå™¨å­¦ä¹ åº“ï¼Ÿ"},
    {"output": "ä¸»è¦æœ‰scikit-learnã€TensorFlowã€PyTorchç­‰ä¼˜ç§€çš„åº“"}
)

memory.save_context(
    {"input": "ä»Šå¤©å¤©æ°”çœŸå¥½"},
    {"output": "æ˜¯çš„ï¼Œé€‚åˆå‡ºå»æ•£æ­¥"}
)

# åŸºäºæŸ¥è¯¢æ£€ç´¢ç›¸å…³å†å²
relevant_memory = memory.load_memory_variables(
    {"input": "æ¨èä¸€äº›æ·±åº¦å­¦ä¹ èµ„æº"}
)
print(relevant_memory["relevant_history"])
# ä¼šè¿”å›ä¸"æ·±åº¦å­¦ä¹ "ç›¸å…³çš„å†å²å¯¹è¯ï¼Œå¦‚æœºå™¨å­¦ä¹ å’ŒPythonåº“çš„è®¨è®º
```

#### å‘é‡æ£€ç´¢å®ç°

```python
def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
    """ä¿å­˜å¯¹è¯åˆ°å‘é‡å­˜å‚¨ã€‚"""
    input_str = inputs[self.input_key or list(inputs.keys())[0]]
    output_str = outputs[list(outputs.keys())[0]]

    # æ„å»ºæ–‡æ¡£å†…å®¹
    document_content = f"Human: {input_str}\nAI: {output_str}"

    # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
    self.retriever.vectorstore.add_texts(
        texts=[document_content],
        metadatas=[{
            "input": input_str,
            "output": output_str,
            "timestamp": time.time()
        }]
    )

def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    """åŸºäºè¾“å…¥æ£€ç´¢ç›¸å…³è®°å¿†ã€‚"""
    query = inputs[self.input_key or list(inputs.keys())[0]]

    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    docs = self.retriever.get_relevant_documents(query)

    if self.return_docs:
        return {self.memory_key: docs}
    else:
        # æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
        memory_content = "\n\n".join([doc.page_content for doc in docs])
        return {self.memory_key: memory_content}

@property
def memory_variables(self) -> List[str]:
    """è¿”å›è®°å¿†å˜é‡åˆ—è¡¨ã€‚"""
    return [self.memory_key]

def clear(self) -> None:
    """æ¸…é™¤å‘é‡å­˜å‚¨ä¸­çš„æ‰€æœ‰è®°å¿†ã€‚"""
    # æ³¨æ„ï¼šè¿™ä¼šåˆ é™¤å‘é‡å­˜å‚¨ä¸­çš„æ‰€æœ‰æ–‡æ¡£
    if hasattr(self.retriever.vectorstore, 'delete'):
        self.retriever.vectorstore.delete()
```

---

## 7. ConversationEntityMemory API

### 7.1 å®ä½“è®°å¿†

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šæå–å’Œè®°ä½å¯¹è¯ä¸­çš„å®ä½“ä¿¡æ¯
- **ç‰¹ç‚¹**ï¼šç»“æ„åŒ–å­˜å‚¨å®ä½“åŠå…¶å±æ€§
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦è®°ä½äººç‰©ã€åœ°ç‚¹ã€äº‹ä»¶ç­‰å®ä½“ä¿¡æ¯çš„å¯¹è¯

#### æ„é€ å‚æ•°

```python
class ConversationEntityMemory(BaseChatMemory):
    def __init__(
        self,
        llm: BaseLanguageModel,
        entity_extraction_prompt: BasePromptTemplate = ENTITY_EXTRACTION_PROMPT,
        entity_summarization_prompt: BasePromptTemplate = ENTITY_SUMMARIZATION_PROMPT,
        entity_cache: Optional[List[str]] = None,
        k: int = 3,
        memory_key: str = "entities",
        **kwargs: Any
    ):
        """å®ä½“è®°å¿†æ„é€ å‡½æ•°ã€‚"""
        super().__init__(**kwargs)
        self.llm = llm
        self.entity_extraction_prompt = entity_extraction_prompt
        self.entity_summarization_prompt = entity_summarization_prompt
        self.entity_cache = entity_cache or []
        self.k = k  # è¿”å›çš„ç›¸å…³å®ä½“æ•°é‡
        self.memory_key = memory_key
        self.entity_store: Dict[str, str] = {}  # å®ä½“å­˜å‚¨
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain.memory import ConversationEntityMemory
from langchain_openai import OpenAI

# åˆ›å»ºå®ä½“è®°å¿†
llm = OpenAI(temperature=0)
memory = ConversationEntityMemory(
    llm=llm,
    memory_key="entity_info"
)

# ä¿å­˜åŒ…å«å®ä½“çš„å¯¹è¯
memory.save_context(
    {"input": "æˆ‘å«å¼ ä¸‰ï¼Œä»Šå¹´30å²ï¼Œä½åœ¨åŒ—äº¬ï¼Œåœ¨é˜¿é‡Œå·´å·´å·¥ä½œ"},
    {"output": "å¾ˆé«˜å…´è®¤è¯†ä½ å¼ ä¸‰ï¼ä½ åœ¨é˜¿é‡Œå·´å·´åšä»€ä¹ˆå·¥ä½œå‘¢ï¼Ÿ"}
)

memory.save_context(
    {"input": "æˆ‘æ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œä¸»è¦è´Ÿè´£åç«¯å¼€å‘"},
    {"output": "è½¯ä»¶å·¥ç¨‹å¸ˆæ˜¯ä¸ªå¾ˆæœ‰å‰æ™¯çš„èŒä¸šï¼Œåç«¯å¼€å‘éœ€è¦æŒæ¡å“ªäº›æŠ€æœ¯å‘¢ï¼Ÿ"}
)

# æ£€ç´¢ä¸ç‰¹å®šè¾“å…¥ç›¸å…³çš„å®ä½“ä¿¡æ¯
entity_info = memory.load_memory_variables(
    {"input": "å¼ ä¸‰çš„å·¥ä½œç»å†å¦‚ä½•ï¼Ÿ"}
)
print(entity_info["entity_info"])
# "å¼ ä¸‰: 30å²ï¼Œä½åœ¨åŒ—äº¬ï¼Œåœ¨é˜¿é‡Œå·´å·´æ‹…ä»»è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£åç«¯å¼€å‘"
```

#### å®ä½“æå–å’Œç®¡ç†

```python
def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
    """ä¿å­˜ä¸Šä¸‹æ–‡å¹¶æå–å®ä½“ã€‚"""
    # ä¿å­˜åˆ°èŠå¤©è®°å¿†
    super().save_context(inputs, outputs)

    # æå–æ–°å®ä½“
    input_str = inputs[list(inputs.keys())[0]]
    output_str = outputs[list(outputs.keys())[0]]

    # ä»è¾“å…¥å’Œè¾“å‡ºä¸­æå–å®ä½“
    text = f"{input_str}\n{output_str}"
    entities = self._extract_entities(text)

    # æ›´æ–°å®ä½“å­˜å‚¨
    for entity in entities:
        self._update_entity_info(entity, text)

def _extract_entities(self, text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–å®ä½“ã€‚"""
    prompt = self.entity_extraction_prompt.format(text=text)
    result = self.llm.predict(prompt)

    # è§£æLLMè¿”å›çš„å®ä½“åˆ—è¡¨
    entities = [entity.strip() for entity in result.split(',') if entity.strip()]
    return entities

def _update_entity_info(self, entity: str, context: str) -> None:
    """æ›´æ–°å®ä½“ä¿¡æ¯ã€‚"""
    if entity in self.entity_store:
        # æ›´æ–°ç°æœ‰å®ä½“ä¿¡æ¯
        existing_info = self.entity_store[entity]
        prompt = self.entity_summarization_prompt.format(
            entity=entity,
            existing_info=existing_info,
            new_context=context
        )
        updated_info = self.llm.predict(prompt)
        self.entity_store[entity] = updated_info
    else:
        # åˆ›å»ºæ–°å®ä½“ä¿¡æ¯
        prompt = f"æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œæ€»ç»“å…³äº{entity}çš„ä¿¡æ¯ï¼š\n{context}"
        entity_info = self.llm.predict(prompt)
        self.entity_store[entity] = entity_info

def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    """åŠ è½½ç›¸å…³å®ä½“ä¿¡æ¯ã€‚"""
    input_str = inputs[list(inputs.keys())[0]]

    # ä»è¾“å…¥ä¸­æå–å®ä½“
    relevant_entities = self._extract_entities(input_str)

    # è·å–ç›¸å…³å®ä½“ä¿¡æ¯
    entity_summaries = []
    for entity in relevant_entities[:self.k]:
        if entity in self.entity_store:
            entity_summaries.append(f"{entity}: {self.entity_store[entity]}")

    return {self.memory_key: "\n".join(entity_summaries)}
```

---

## 8. ç»„åˆè®°å¿† API

### 8.1 CombinedMemory

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šç»„åˆå¤šç§è®°å¿†ç±»å‹
- **ç‰¹ç‚¹**ï¼šåŒæ—¶ä½¿ç”¨å¤šä¸ªè®°å¿†ç³»ç»Ÿ
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦ä¸åŒç±»å‹è®°å¿†äº’è¡¥çš„å¤æ‚åº”ç”¨

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain.memory import (
    CombinedMemory,
    ConversationBufferMemory,
    ConversationSummaryMemory,
    VectorStoreRetrieverMemory
)

# åˆ›å»ºå¤šä¸ªè®°å¿†ç»„ä»¶
buffer_memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

summary_memory = ConversationSummaryMemory(
    llm=llm,
    memory_key="conversation_summary"
)

vector_memory = VectorStoreRetrieverMemory(
    retriever=retriever,
    memory_key="relevant_context"
)

# ç»„åˆè®°å¿†
combined_memory = CombinedMemory(
    memories=[buffer_memory, summary_memory, vector_memory]
)

# ä½¿ç”¨ç»„åˆè®°å¿†
combined_memory.save_context(
    {"input": "æˆ‘æƒ³å­¦ä¹ æ·±åº¦å­¦ä¹ "},
    {"output": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯..."}
)

# è·å–æ‰€æœ‰è®°å¿†ç±»å‹çš„ä¿¡æ¯
all_memory = combined_memory.load_memory_variables({
    "input": "æœ‰ä»€ä¹ˆæ·±åº¦å­¦ä¹ çš„å­¦ä¹ å»ºè®®å—ï¼Ÿ"
})

print("èŠå¤©å†å²:", all_memory["chat_history"])
print("å¯¹è¯æ‘˜è¦:", all_memory["conversation_summary"])
print("ç›¸å…³ä¸Šä¸‹æ–‡:", all_memory["relevant_context"])
```

#### ç»„åˆè®°å¿†å®ç°

```python
class CombinedMemory(BaseMemory):
    """ç»„åˆå¤šä¸ªè®°å¿†ç³»ç»Ÿã€‚"""

    def __init__(self, memories: List[BaseMemory]):
        self.memories = memories

    @property
    def memory_variables(self) -> List[str]:
        """è¿”å›æ‰€æœ‰è®°å¿†çš„å˜é‡åˆ—è¡¨ã€‚"""
        variables = []
        for memory in self.memories:
            variables.extend(memory.memory_variables)
        return variables

    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """åŠ è½½æ‰€æœ‰è®°å¿†çš„å˜é‡ã€‚"""
        memory_data = {}
        for memory in self.memories:
            memory_vars = memory.load_memory_variables(inputs)
            memory_data.update(memory_vars)
        return memory_data

    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
        """ä¿å­˜ä¸Šä¸‹æ–‡åˆ°æ‰€æœ‰è®°å¿†ã€‚"""
        for memory in self.memories:
            memory.save_context(inputs, outputs)

    def clear(self) -> None:
        """æ¸…é™¤æ‰€æœ‰è®°å¿†ã€‚"""
        for memory in self.memories:
            memory.clear()
```

---

## 9. è®°å¿†ç®¡ç†å·¥å…· API

### 9.1 è®°å¿†ç»Ÿè®¡å’Œç›‘æ§

```python
class MemoryManager:
    """è®°å¿†ç®¡ç†å™¨ã€‚"""

    def __init__(self, memory: BaseMemory):
        self.memory = memory
        self.stats = {
            "total_contexts_saved": 0,
            "total_memory_loads": 0,
            "memory_size_bytes": 0,
            "last_accessed": None
        }

    def save_context_with_stats(
        self,
        inputs: Dict[str, Any],
        outputs: Dict[str, str]
    ) -> None:
        """ä¿å­˜ä¸Šä¸‹æ–‡å¹¶æ›´æ–°ç»Ÿè®¡ã€‚"""
        start_time = time.time()

        # ä¿å­˜ä¸Šä¸‹æ–‡
        self.memory.save_context(inputs, outputs)

        # æ›´æ–°ç»Ÿè®¡
        self.stats["total_contexts_saved"] += 1
        self.stats["last_accessed"] = time.time()
        self.stats["save_time"] = time.time() - start_time

        # ä¼°ç®—å†…å­˜å¤§å°
        self._update_memory_size()

    def load_memory_with_stats(
        self,
        inputs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åŠ è½½è®°å¿†å¹¶æ›´æ–°ç»Ÿè®¡ã€‚"""
        start_time = time.time()

        # åŠ è½½è®°å¿†
        memory_vars = self.memory.load_memory_variables(inputs)

        # æ›´æ–°ç»Ÿè®¡
        self.stats["total_memory_loads"] += 1
        self.stats["last_accessed"] = time.time()
        self.stats["load_time"] = time.time() - start_time

        return memory_vars

    def _update_memory_size(self) -> None:
        """æ›´æ–°è®°å¿†å¤§å°ä¼°ç®—ã€‚"""
        if hasattr(self.memory, 'buffer'):
            self.stats["memory_size_bytes"] = len(self.memory.buffer.encode('utf-8'))
        elif hasattr(self.memory, 'chat_memory'):
            total_size = 0
            for msg in self.memory.chat_memory.messages:
                total_size += len(msg.content.encode('utf-8'))
            self.stats["memory_size_bytes"] = total_size

    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        return {
            **self.stats,
            "memory_type": type(self.memory).__name__,
            "memory_variables": self.memory.memory_variables
        }

    def optimize_memory(self) -> None:
        """ä¼˜åŒ–è®°å¿†æ€§èƒ½ã€‚"""
        if isinstance(self.memory, ConversationBufferMemory):
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢ä¸ºçª—å£è®°å¿†
            if hasattr(self.memory, 'chat_memory'):
                message_count = len(self.memory.chat_memory.messages)
                if message_count > 100:  # æ¶ˆæ¯è¿‡å¤š
                    print("å»ºè®®ä½¿ç”¨ConversationBufferWindowMemoryä»¥æé«˜æ€§èƒ½")

        elif isinstance(self.memory, ConversationSummaryMemory):
            # æ£€æŸ¥æ‘˜è¦æ˜¯å¦è¿‡é•¿
            if len(self.memory.buffer) > 2000:
                print("æ‘˜è¦è¿‡é•¿ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆæˆ–åˆ†æ®µæ‘˜è¦")
```

---

## 10. æœ€ä½³å®è·µä¸é…ç½®

### 10.1 è®°å¿†ç±»å‹é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èè®°å¿†ç±»å‹ | é…ç½®å»ºè®® |
|-----|-------------|---------|
| çŸ­å¯¹è¯ | `ConversationBufferMemory` | ç®€å•ç›´æ¥ï¼Œä¿ç•™å®Œæ•´å†å² |
| é•¿å¯¹è¯ | `ConversationBufferWindowMemory` | k=5-10ï¼Œå¹³è¡¡æ€§èƒ½å’Œä¸Šä¸‹æ–‡ |
| æˆæœ¬æ•æ„Ÿ | `ConversationTokenBufferMemory` | æ ¹æ®æ¨¡å‹å®šä»·è®¾ç½®tokené™åˆ¶ |
| é•¿æœŸè®°å¿† | `ConversationSummaryMemory` | ä½¿ç”¨é«˜è´¨é‡LLMç”Ÿæˆæ‘˜è¦ |
| è¯­ä¹‰æ£€ç´¢ | `VectorStoreRetrieverMemory` | é€‰æ‹©åˆé€‚çš„embeddingæ¨¡å‹ |
| å®ä½“è¿½è¸ª | `ConversationEntityMemory` | é€‚ç”¨äºå®¢æœã€ä¸ªäººåŠ©æ‰‹ç­‰åœºæ™¯ |
| å¤æ‚åº”ç”¨ | `CombinedMemory` | ç»„åˆå¤šç§è®°å¿†ç±»å‹ |

### 10.2 æ€§èƒ½ä¼˜åŒ–é…ç½®

```python
def create_optimized_memory(
    conversation_length: str,
    cost_sensitivity: str,
    semantic_search: bool = False
) -> BaseMemory:
    """æ ¹æ®éœ€æ±‚åˆ›å»ºä¼˜åŒ–çš„è®°å¿†é…ç½®ã€‚"""

    if conversation_length == "short" and cost_sensitivity == "low":
        # çŸ­å¯¹è¯ï¼Œæˆæœ¬ä¸æ•æ„Ÿ
        return ConversationBufferMemory(
            memory_key="history",
            return_messages=True
        )

    elif conversation_length == "long" and cost_sensitivity == "high":
        # é•¿å¯¹è¯ï¼Œæˆæœ¬æ•æ„Ÿ
        return ConversationTokenBufferMemory(
            llm=llm,
            max_token_limit=1000,
            memory_key="history"
        )

    elif semantic_search:
        # éœ€è¦è¯­ä¹‰æ£€ç´¢
        return VectorStoreRetrieverMemory(
            retriever=retriever,
            memory_key="relevant_history"
        )

    else:
        # é»˜è®¤é…ç½®ï¼šçª—å£è®°å¿†
        return ConversationBufferWindowMemory(
            k=5,
            memory_key="history",
            return_messages=True
        )

# ä½¿ç”¨ç¤ºä¾‹
memory = create_optimized_memory(
    conversation_length="long",
    cost_sensitivity="high",
    semantic_search=False
)
```

---

## 11. æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº† **Memory æ¨¡å—**çš„æ ¸å¿ƒ APIï¼š

### ä¸»è¦è®°å¿†ç±»å‹
1. **ConversationBufferMemory**ï¼šå®Œæ•´å¯¹è¯å†å²å­˜å‚¨
2. **ConversationBufferWindowMemory**ï¼šå›ºå®šçª—å£å¤§å°çš„è®°å¿†
3. **ConversationSummaryMemory**ï¼šåŸºäºLLMçš„å¯¹è¯æ‘˜è¦
4. **ConversationTokenBufferMemory**ï¼šåŸºäºä»¤ç‰Œé™åˆ¶çš„è®°å¿†
5. **VectorStoreRetrieverMemory**ï¼šåŸºäºå‘é‡æ£€ç´¢çš„è¯­ä¹‰è®°å¿†
6. **ConversationEntityMemory**ï¼šå®ä½“æå–å’Œè¿½è¸ªè®°å¿†

### æ ¸å¿ƒåŠŸèƒ½
1. **ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šsave_contextå’Œload_memory_variables
2. **è®°å¿†æ£€ç´¢**ï¼šåŸºäºè¾“å…¥æ£€ç´¢ç›¸å…³å†å²ä¿¡æ¯
3. **å†…å­˜ä¼˜åŒ–**ï¼šä¸åŒç­–ç•¥çš„å†…å­˜ä½¿ç”¨ä¼˜åŒ–
4. **ç»„åˆä½¿ç”¨**ï¼šCombinedMemoryæ”¯æŒå¤šç§è®°å¿†ç±»å‹ç»„åˆ

æ¯ä¸ª API å‡åŒ…å«ï¼š

- å®Œæ•´çš„æ„é€ å‚æ•°å’Œé…ç½®é€‰é¡¹
- è¯¦ç»†çš„ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µ
- æ ¸å¿ƒå®ç°é€»è¾‘å’Œç®—æ³•è¯´æ˜
- æ€§èƒ½ä¼˜åŒ–å»ºè®®å’Œé€‰æ‹©æŒ‡å—

Memory æ¨¡å—æ˜¯æ„å»ºæœ‰çŠ¶æ€å¯¹è¯ç³»ç»Ÿçš„å…³é”®ç»„ä»¶ï¼Œæ­£ç¡®é€‰æ‹©å’Œé…ç½®è®°å¿†ç±»å‹å¯¹æé«˜å¯¹è¯è´¨é‡å’Œç³»ç»Ÿæ€§èƒ½è‡³å…³é‡è¦ã€‚

---

## æ•°æ®ç»“æ„

## æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£è¯¦ç»†æè¿° **Memory æ¨¡å—**çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ŒåŒ…æ‹¬è®°å¿†ç±»å±‚æ¬¡ã€æ¶ˆæ¯å­˜å‚¨ã€ç¼“å†²åŒºç®¡ç†ã€å®ä½“æå–ã€å‘é‡æ£€ç´¢ç­‰ã€‚æ‰€æœ‰ç»“æ„å‡é…å¤‡ UML ç±»å›¾å’Œè¯¦ç»†çš„å­—æ®µè¯´æ˜ã€‚

---

## 1. Memory ç±»å±‚æ¬¡ç»“æ„

### 1.1 æ ¸å¿ƒè®°å¿†ç»§æ‰¿ä½“ç³»

```mermaid
classDiagram
    class BaseMemory {
        <<abstract>>
        +memory_variables: List[str]
        +load_memory_variables(inputs: Dict[str, Any]) Dict[str, Any]
        +save_context(inputs: Dict[str, Any], outputs: Dict[str, str]) None
        +clear() None
    }

    class BaseChatMemory {
        <<abstract>>
        +chat_memory: BaseChatMessageHistory
        +output_key: Optional[str]
        +input_key: Optional[str]
        +return_messages: bool
        +_get_input_output(inputs: Dict, outputs: Dict) Tuple[str, str]
    }

    class ConversationBufferMemory {
        +human_prefix: str
        +ai_prefix: str
        +memory_key: str
        +buffer: str
        +load_memory_variables(inputs: Dict) Dict[str, Any]
        +save_context(inputs: Dict, outputs: Dict) None
    }

    class ConversationBufferWindowMemory {
        +k: int
        +human_prefix: str
        +ai_prefix: str
        +memory_key: str
        +load_memory_variables(inputs: Dict) Dict[str, Any]
        +_prune_messages() None
    }

    class ConversationSummaryMemory {
        +llm: BaseLanguageModel
        +buffer: str
        +prompt: BasePromptTemplate
        +memory_key: str
        +_update_summary() None
        +predict_new_summary(messages: List, existing_summary: str) str
    }

    class ConversationTokenBufferMemory {
        +llm: BaseLanguageModel
        +max_token_limit: int
        +memory_key: str
        +_get_current_token_count() int
        +_prune_messages_to_token_limit() None
    }

    class VectorStoreRetrieverMemory {
        +retriever: VectorStoreRetriever
        +memory_key: str
        +input_key: Optional[str]
        +return_docs: bool
        +load_memory_variables(inputs: Dict) Dict[str, Any]
    }

    class ConversationEntityMemory {
        +llm: BaseLanguageModel
        +entity_store: Dict[str, str]
        +entity_cache: List[str]
        +k: int
        +_extract_entities(text: str) List[str]
        +_update_entity_info(entity: str, context: str) None
    }

    class CombinedMemory {
        +memories: List[BaseMemory]
        +load_memory_variables(inputs: Dict) Dict[str, Any]
        +save_context(inputs: Dict, outputs: Dict) None
    }

    BaseMemory <|-- BaseChatMemory
    BaseMemory <|-- VectorStoreRetrieverMemory
    BaseMemory <|-- CombinedMemory
    BaseChatMemory <|-- ConversationBufferMemory
    BaseChatMemory <|-- ConversationBufferWindowMemory
    BaseChatMemory <|-- ConversationSummaryMemory
    BaseChatMemory <|-- ConversationTokenBufferMemory
    BaseChatMemory <|-- ConversationEntityMemory
```

**å›¾è§£è¯´æ˜**ï¼š

1. **æŠ½è±¡åŸºç±»**ï¼š
   - `BaseMemory`ï¼šæ‰€æœ‰è®°å¿†ç³»ç»Ÿçš„æ ¹åŸºç±»
   - `BaseChatMemory`ï¼šåŸºäºèŠå¤©æ¶ˆæ¯çš„è®°å¿†åŸºç±»

2. **ç¼“å†²åŒºè®°å¿†**ï¼š
   - `ConversationBufferMemory`ï¼šå®Œæ•´å¯¹è¯å†å²
   - `ConversationBufferWindowMemory`ï¼šå›ºå®šçª—å£å¤§å°
   - `ConversationTokenBufferMemory`ï¼šåŸºäºä»¤ç‰Œé™åˆ¶

3. **æ™ºèƒ½è®°å¿†**ï¼š
   - `ConversationSummaryMemory`ï¼šåŸºäºæ‘˜è¦çš„å‹ç¼©è®°å¿†
   - `ConversationEntityMemory`ï¼šå®ä½“æå–å’Œè¿½è¸ª
   - `VectorStoreRetrieverMemory`ï¼šå‘é‡æ£€ç´¢è®°å¿†

4. **ç»„åˆè®°å¿†**ï¼š
   - `CombinedMemory`ï¼šå¤šç§è®°å¿†ç±»å‹çš„ç»„åˆ

---

## 2. æ¶ˆæ¯å­˜å‚¨æ•°æ®ç»“æ„

### 2.1 ChatMessageHistory ç»“æ„

```python
class BaseChatMessageHistory(ABC):
    """èŠå¤©æ¶ˆæ¯å†å²åŸºç±»ã€‚"""

    messages: List[BaseMessage]  # æ¶ˆæ¯åˆ—è¡¨

    @abstractmethod
    def add_user_message(self, message: str) -> None:
        """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ã€‚"""

    @abstractmethod
    def add_ai_message(self, message: str) -> None:
        """æ·»åŠ AIæ¶ˆæ¯ã€‚"""

    def add_message(self, message: BaseMessage) -> None:
        """æ·»åŠ ä»»æ„ç±»å‹æ¶ˆæ¯ã€‚"""
        self.messages.append(message)

    def clear(self) -> None:
        """æ¸…ç©ºæ¶ˆæ¯å†å²ã€‚"""
        self.messages = []

class ChatMessageHistory(BaseChatMessageHistory):
    """å†…å­˜ä¸­çš„èŠå¤©æ¶ˆæ¯å†å²ã€‚"""

    def __init__(self, messages: Optional[List[BaseMessage]] = None):
        self.messages = messages or []

    def add_user_message(self, message: str) -> None:
        """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ã€‚"""
        self.messages.append(HumanMessage(content=message))

    def add_ai_message(self, message: str) -> None:
        """æ·»åŠ AIæ¶ˆæ¯ã€‚"""
        self.messages.append(AIMessage(content=message))
```

**æ¶ˆæ¯ç±»å‹ç»“æ„**ï¼š

```python
class BaseMessage:
    """æ¶ˆæ¯åŸºç±»ã€‚"""
    content: str                    # æ¶ˆæ¯å†…å®¹
    additional_kwargs: dict         # é¢å¤–å‚æ•°
    response_metadata: dict         # å“åº”å…ƒæ•°æ®

class HumanMessage(BaseMessage):
    """äººç±»æ¶ˆæ¯ã€‚"""
    type: str = "human"

class AIMessage(BaseMessage):
    """AIæ¶ˆæ¯ã€‚"""
    type: str = "ai"

class SystemMessage(BaseMessage):
    """ç³»ç»Ÿæ¶ˆæ¯ã€‚"""
    type: str = "system"

class FunctionMessage(BaseMessage):
    """å‡½æ•°æ¶ˆæ¯ã€‚"""
    type: str = "function"
    name: str                       # å‡½æ•°åç§°

class ToolMessage(BaseMessage):
    """å·¥å…·æ¶ˆæ¯ã€‚"""
    type: str = "tool"
    tool_call_id: str              # å·¥å…·è°ƒç”¨ID
```

---

### 2.2 æŒä¹…åŒ–æ¶ˆæ¯å†å²

```python
class FileChatMessageHistory(BaseChatMessageHistory):
    """åŸºäºæ–‡ä»¶çš„æ¶ˆæ¯å†å²ã€‚"""

    def __init__(self, file_path: str):
        self.file_path = file_path
        self._messages: List[BaseMessage] = []
        self._load_messages()

    def _load_messages(self) -> None:
        """ä»æ–‡ä»¶åŠ è½½æ¶ˆæ¯ã€‚"""
        if os.path.exists(self.file_path):
            with open(self.file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self._messages = [self._deserialize_message(msg) for msg in data]

    def _save_messages(self) -> None:
        """ä¿å­˜æ¶ˆæ¯åˆ°æ–‡ä»¶ã€‚"""
        with open(self.file_path, 'w', encoding='utf-8') as f:
            data = [self._serialize_message(msg) for msg in self._messages]
            json.dump(data, f, ensure_ascii=False, indent=2)

    def add_message(self, message: BaseMessage) -> None:
        """æ·»åŠ æ¶ˆæ¯å¹¶æŒä¹…åŒ–ã€‚"""
        self._messages.append(message)
        self._save_messages()

    @property
    def messages(self) -> List[BaseMessage]:
        """è·å–æ¶ˆæ¯åˆ—è¡¨ã€‚"""
        return self._messages

    def clear(self) -> None:
        """æ¸…ç©ºæ¶ˆæ¯å¹¶åˆ é™¤æ–‡ä»¶ã€‚"""
        self._messages = []
        if os.path.exists(self.file_path):
            os.remove(self.file_path)

class RedisChatMessageHistory(BaseChatMessageHistory):
    """åŸºäºRedisçš„æ¶ˆæ¯å†å²ã€‚"""

    def __init__(self, session_id: str, url: str = "redis://localhost:6379"):
        import redis
        self.redis_client = redis.from_url(url)
        self.session_id = session_id
        self.key = f"chat_history:{session_id}"

    @property
    def messages(self) -> List[BaseMessage]:
        """ä»Redisè·å–æ¶ˆæ¯ã€‚"""
        messages_data = self.redis_client.lrange(self.key, 0, -1)
        return [json.loads(msg.decode()) for msg in messages_data]

    def add_message(self, message: BaseMessage) -> None:
        """æ·»åŠ æ¶ˆæ¯åˆ°Redisã€‚"""
        message_data = json.dumps(self._serialize_message(message))
        self.redis_client.rpush(self.key, message_data)

        # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆå¯é€‰ï¼‰
        self.redis_client.expire(self.key, 86400)  # 24å°æ—¶

    def clear(self) -> None:
        """æ¸…ç©ºRedisä¸­çš„æ¶ˆæ¯ã€‚"""
        self.redis_client.delete(self.key)
```

---

## 3. ç¼“å†²åŒºç®¡ç†æ•°æ®ç»“æ„

### 3.1 ConversationBufferMemory ç»“æ„

```python
class ConversationBufferMemory(BaseChatMemory):
    """å¯¹è¯ç¼“å†²åŒºè®°å¿†æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        chat_memory: Optional[BaseChatMessageHistory] = None,
        output_key: Optional[str] = None,
        input_key: Optional[str] = None,
        return_messages: bool = False,
        human_prefix: str = "Human",
        ai_prefix: str = "AI",
        memory_key: str = "history"
    ):
        # æ ¸å¿ƒå­—æ®µ
        self.chat_memory = chat_memory or ChatMessageHistory()
        self.output_key = output_key
        self.input_key = input_key
        self.return_messages = return_messages
        self.human_prefix = human_prefix
        self.ai_prefix = ai_prefix
        self.memory_key = memory_key

        # ç»Ÿè®¡ä¿¡æ¯
        self._message_count = 0
        self._total_tokens = 0
        self._created_at = time.time()
        self._last_accessed = None

    @property
    def buffer(self) -> str:
        """è·å–æ ¼å¼åŒ–çš„ç¼“å†²åŒºå†…å®¹ã€‚"""
        return get_buffer_string(
            self.chat_memory.messages,
            human_prefix=self.human_prefix,
            ai_prefix=self.ai_prefix
        )

    @property
    def memory_variables(self) -> List[str]:
        """è¿”å›è®°å¿†å˜é‡åˆ—è¡¨ã€‚"""
        return [self.memory_key]

    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        return {
            "message_count": len(self.chat_memory.messages),
            "buffer_size_chars": len(self.buffer),
            "created_at": self._created_at,
            "last_accessed": self._last_accessed,
            "memory_type": "ConversationBufferMemory"
        }
```

**ç¼“å†²åŒºæ ¼å¼åŒ–å‡½æ•°**ï¼š

```python
def get_buffer_string(
    messages: List[BaseMessage],
    human_prefix: str = "Human",
    ai_prefix: str = "AI"
) -> str:
    """å°†æ¶ˆæ¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ã€‚"""
    string_messages = []

    for message in messages:
        if isinstance(message, HumanMessage):
            role = human_prefix
        elif isinstance(message, AIMessage):
            role = ai_prefix
        elif isinstance(message, SystemMessage):
            role = "System"
        else:
            role = message.__class__.__name__

        string_messages.append(f"{role}: {message.content}")

    return "\n".join(string_messages)
```

---

### 3.2 ConversationBufferWindowMemory ç»“æ„

```python
class ConversationBufferWindowMemory(BaseChatMemory):
    """çª—å£ç¼“å†²åŒºè®°å¿†æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        k: int = 5,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.k = k  # çª—å£å¤§å°ï¼ˆè½®æ¬¡æ•°ï¼‰

        # çª—å£ç®¡ç†
        self._window_stats = {
            "max_window_size": k,
            "current_window_size": 0,
            "total_messages_processed": 0,
            "messages_pruned": 0
        }

    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
        """ä¿å­˜ä¸Šä¸‹æ–‡å¹¶ç»´æŠ¤çª—å£ã€‚"""
        # æ·»åŠ æ–°æ¶ˆæ¯
        super().save_context(inputs, outputs)
        self._window_stats["total_messages_processed"] += 2

        # ç»´æŠ¤çª—å£å¤§å°
        self._prune_messages()

    def _prune_messages(self) -> None:
        """ä¿®å‰ªæ¶ˆæ¯ä»¥ç»´æŠ¤çª—å£å¤§å°ã€‚"""
        messages = self.chat_memory.messages
        max_messages = 2 * self.k  # kè½®å¯¹è¯ = 2kæ¡æ¶ˆæ¯

        if len(messages) > max_messages:
            # è®¡ç®—éœ€è¦åˆ é™¤çš„æ¶ˆæ¯æ•°
            messages_to_remove = len(messages) - max_messages

            # åˆ é™¤æœ€æ—§çš„æ¶ˆæ¯
            self.chat_memory.messages = messages[messages_to_remove:]

            # æ›´æ–°ç»Ÿè®¡
            self._window_stats["messages_pruned"] += messages_to_remove

        self._window_stats["current_window_size"] = len(self.chat_memory.messages) // 2

    def get_window_info(self) -> Dict[str, Any]:
        """è·å–çª—å£ä¿¡æ¯ã€‚"""
        return {
            **self._window_stats,
            "window_utilization": self._window_stats["current_window_size"] / self.k,
            "pruning_efficiency": (
                self._window_stats["messages_pruned"] /
                self._window_stats["total_messages_processed"]
                if self._window_stats["total_messages_processed"] > 0 else 0
            )
        }
```

---

## 4. æ™ºèƒ½è®°å¿†æ•°æ®ç»“æ„

### 4.1 ConversationSummaryMemory ç»“æ„

```python
class ConversationSummaryMemory(BaseChatMemory):
    """å¯¹è¯æ‘˜è¦è®°å¿†æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        llm: BaseLanguageModel,
        buffer: str = "",
        prompt: BasePromptTemplate = SUMMARY_PROMPT,
        summary_message_cls: Type[BaseMessage] = SystemMessage,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.llm = llm
        self.buffer = buffer  # å½“å‰æ‘˜è¦å†…å®¹
        self.prompt = prompt
        self.summary_message_cls = summary_message_cls

        # æ‘˜è¦ç»Ÿè®¡
        self._summary_stats = {
            "summary_count": 0,
            "total_summary_tokens": 0,
            "original_message_count": 0,
            "compression_ratio": 0.0,
            "last_summary_time": None
        }

    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
        """ä¿å­˜ä¸Šä¸‹æ–‡å¹¶æ›´æ–°æ‘˜è¦ã€‚"""
        # æ·»åŠ åˆ°ä¸´æ—¶æ¶ˆæ¯å­˜å‚¨
        super().save_context(inputs, outputs)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
        if len(self.chat_memory.messages) >= 2:
            self._update_summary()

    def _update_summary(self) -> None:
        """æ›´æ–°å¯¹è¯æ‘˜è¦ã€‚"""
        messages = self.chat_memory.messages
        new_lines = get_buffer_string(messages)

        # æ„å»ºæ‘˜è¦æç¤º
        if self.buffer:
            # å¢é‡æ‘˜è¦æ›´æ–°
            prompt_input = {
                "summary": self.buffer,
                "new_lines": new_lines
            }
        else:
            # é¦–æ¬¡ç”Ÿæˆæ‘˜è¦
            prompt_input = {"new_lines": new_lines}

        # ç”Ÿæˆæ–°æ‘˜è¦
        start_time = time.time()
        new_summary = self.llm.predict(self.prompt.format(**prompt_input))
        summary_time = time.time() - start_time

        # æ›´æ–°æ‘˜è¦å’Œç»Ÿè®¡
        original_tokens = self.llm.get_num_tokens(new_lines)
        summary_tokens = self.llm.get_num_tokens(new_summary)

        self.buffer = new_summary
        self._summary_stats.update({
            "summary_count": self._summary_stats["summary_count"] + 1,
            "total_summary_tokens": summary_tokens,
            "original_message_count": len(messages),
            "compression_ratio": summary_tokens / original_tokens if original_tokens > 0 else 0,
            "last_summary_time": summary_time
        })

        # æ¸…ç©ºä¸´æ—¶æ¶ˆæ¯ï¼ˆå·²æ‘˜è¦åŒ–ï¼‰
        self.chat_memory.clear()

    def get_summary_stats(self) -> Dict[str, Any]:
        """è·å–æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        return {
            **self._summary_stats,
            "current_summary_length": len(self.buffer),
            "average_compression_ratio": self._summary_stats["compression_ratio"]
        }

# é»˜è®¤æ‘˜è¦æç¤ºæ¨¡æ¿
SUMMARY_PROMPT = PromptTemplate(
    input_variables=["summary", "new_lines"],
    template="""
æ¸è¿›å¼æ€»ç»“ä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œåœ¨ä¹‹å‰æ‘˜è¦çš„åŸºç¡€ä¸Šæ·»åŠ æ–°ä¿¡æ¯ï¼š

ç°æœ‰æ‘˜è¦ï¼š
{summary}

æ–°çš„å¯¹è¯å†…å®¹ï¼š
{new_lines}

æ–°çš„æ‘˜è¦ï¼š
""".strip()
)
```

---

### 4.2 ConversationEntityMemory ç»“æ„

```python
class ConversationEntityMemory(BaseChatMemory):
    """å¯¹è¯å®ä½“è®°å¿†æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        llm: BaseLanguageModel,
        entity_extraction_prompt: BasePromptTemplate = ENTITY_EXTRACTION_PROMPT,
        entity_summarization_prompt: BasePromptTemplate = ENTITY_SUMMARIZATION_PROMPT,
        entity_cache: Optional[List[str]] = None,
        k: int = 3,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.llm = llm
        self.entity_extraction_prompt = entity_extraction_prompt
        self.entity_summarization_prompt = entity_summarization_prompt
        self.entity_cache = entity_cache or []
        self.k = k

        # å®ä½“å­˜å‚¨å’Œç»Ÿè®¡
        self.entity_store: Dict[str, EntityInfo] = {}
        self._entity_stats = {
            "total_entities": 0,
            "active_entities": 0,
            "entity_updates": 0,
            "extraction_calls": 0
        }

    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
        """ä¿å­˜ä¸Šä¸‹æ–‡å¹¶æå–å®ä½“ã€‚"""
        super().save_context(inputs, outputs)

        # æå–å’Œæ›´æ–°å®ä½“
        input_str = inputs[self.input_key or list(inputs.keys())[0]]
        output_str = outputs[self.output_key or list(outputs.keys())[0]]

        context_text = f"{input_str}\n{output_str}"
        self._extract_and_update_entities(context_text)

    def _extract_and_update_entities(self, text: str) -> None:
        """æå–å¹¶æ›´æ–°å®ä½“ä¿¡æ¯ã€‚"""
        # æå–å®ä½“
        entities = self._extract_entities(text)
        self._entity_stats["extraction_calls"] += 1

        # æ›´æ–°æ¯ä¸ªå®ä½“çš„ä¿¡æ¯
        for entity in entities:
            self._update_entity_info(entity, text)

    def _extract_entities(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“ã€‚"""
        prompt = self.entity_extraction_prompt.format(text=text)
        result = self.llm.predict(prompt)

        # è§£æå®ä½“åˆ—è¡¨
        entities = [e.strip() for e in result.split(',') if e.strip()]
        return entities

    def _update_entity_info(self, entity: str, context: str) -> None:
        """æ›´æ–°å®ä½“ä¿¡æ¯ã€‚"""
        if entity in self.entity_store:
            # æ›´æ–°ç°æœ‰å®ä½“
            entity_info = self.entity_store[entity]
            entity_info.update_info(context, self.llm, self.entity_summarization_prompt)
            self._entity_stats["entity_updates"] += 1
        else:
            # åˆ›å»ºæ–°å®ä½“
            entity_info = EntityInfo(entity)
            entity_info.initialize_info(context, self.llm)
            self.entity_store[entity] = entity_info
            self._entity_stats["total_entities"] += 1

        # æ›´æ–°å®ä½“ç¼“å­˜
        if entity not in self.entity_cache:
            self.entity_cache.append(entity)

        self._entity_stats["active_entities"] = len(self.entity_store)

class EntityInfo:
    """å®ä½“ä¿¡æ¯æ•°æ®ç»“æ„ã€‚"""

    def __init__(self, name: str):
        self.name = name
        self.info = ""
        self.contexts: List[str] = []
        self.created_at = time.time()
        self.last_updated = time.time()
        self.update_count = 0

    def initialize_info(self, context: str, llm: BaseLanguageModel) -> None:
        """åˆå§‹åŒ–å®ä½“ä¿¡æ¯ã€‚"""
        prompt = f"æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œæ€»ç»“å…³äº{self.name}çš„ä¿¡æ¯ï¼š\n{context}"
        self.info = llm.predict(prompt)
        self.contexts.append(context)
        self.update_count += 1

    def update_info(
        self,
        new_context: str,
        llm: BaseLanguageModel,
        summarization_prompt: BasePromptTemplate
    ) -> None:
        """æ›´æ–°å®ä½“ä¿¡æ¯ã€‚"""
        prompt = summarization_prompt.format(
            entity=self.name,
            existing_info=self.info,
            new_context=new_context
        )

        self.info = llm.predict(prompt)
        self.contexts.append(new_context)
        self.last_updated = time.time()
        self.update_count += 1

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ã€‚"""
        return {
            "name": self.name,
            "info": self.info,
            "context_count": len(self.contexts),
            "created_at": self.created_at,
            "last_updated": self.last_updated,
            "update_count": self.update_count
        }
```

---

## 5. å‘é‡æ£€ç´¢è®°å¿†ç»“æ„

### 5.1 VectorStoreRetrieverMemory ç»“æ„

```python
class VectorStoreRetrieverMemory(BaseMemory):
    """å‘é‡å­˜å‚¨æ£€ç´¢è®°å¿†æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        retriever: VectorStoreRetriever,
        memory_key: str = "history",
        input_key: Optional[str] = None,
        return_docs: bool = False,
        exclude_input_keys: Optional[List[str]] = None
    ):
        self.retriever = retriever
        self.memory_key = memory_key
        self.input_key = input_key
        self.return_docs = return_docs
        self.exclude_input_keys = exclude_input_keys or []

        # æ£€ç´¢ç»Ÿè®¡
        self._retrieval_stats = {
            "total_contexts_saved": 0,
            "total_retrievals": 0,
            "average_retrieval_time": 0.0,
            "total_documents_stored": 0,
            "cache_hits": 0
        }

        # ç®€å•çš„æŸ¥è¯¢ç¼“å­˜
        self._query_cache: Dict[str, List[Document]] = {}
        self._cache_max_size = 100

    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
        """ä¿å­˜å¯¹è¯åˆ°å‘é‡å­˜å‚¨ã€‚"""
        # æ„å»ºæ–‡æ¡£å†…å®¹
        input_str = inputs[self.input_key or list(inputs.keys())[0]]
        output_str = outputs[list(outputs.keys())[0]]

        document_content = f"Human: {input_str}\nAI: {output_str}"

        # åˆ›å»ºæ–‡æ¡£å…ƒæ•°æ®
        metadata = {
            "input": input_str,
            "output": output_str,
            "timestamp": time.time(),
            "conversation_id": self._generate_conversation_id(inputs)
        }

        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.retriever.vectorstore.add_texts(
            texts=[document_content],
            metadatas=[metadata]
        )

        # æ›´æ–°ç»Ÿè®¡
        self._retrieval_stats["total_contexts_saved"] += 1
        self._retrieval_stats["total_documents_stored"] += 1

        # æ¸…ç©ºç¼“å­˜ï¼ˆæ–°æ–‡æ¡£å¯èƒ½å½±å“æ£€ç´¢ç»“æœï¼‰
        self._query_cache.clear()

    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºè¾“å…¥æ£€ç´¢ç›¸å…³è®°å¿†ã€‚"""
        query = inputs[self.input_key or list(inputs.keys())[0]]

        # æ£€æŸ¥ç¼“å­˜
        if query in self._query_cache:
            docs = self._query_cache[query]
            self._retrieval_stats["cache_hits"] += 1
        else:
            # æ‰§è¡Œæ£€ç´¢
            start_time = time.time()
            docs = self.retriever.get_relevant_documents(query)
            retrieval_time = time.time() - start_time

            # æ›´æ–°ç»Ÿè®¡
            self._retrieval_stats["total_retrievals"] += 1
            total_time = (
                self._retrieval_stats["average_retrieval_time"] *
                (self._retrieval_stats["total_retrievals"] - 1) +
                retrieval_time
            )
            self._retrieval_stats["average_retrieval_time"] = (
                total_time / self._retrieval_stats["total_retrievals"]
            )

            # ç¼“å­˜ç»“æœ
            if len(self._query_cache) < self._cache_max_size:
                self._query_cache[query] = docs

        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        if self.return_docs:
            return {self.memory_key: docs}
        else:
            memory_content = self._format_documents(docs)
            return {self.memory_key: memory_content}

    def _format_documents(self, docs: List[Document]) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£ä¸ºå­—ç¬¦ä¸²ã€‚"""
        if not docs:
            return ""

        formatted_docs = []
        for doc in docs:
            # æå–åŸå§‹å¯¹è¯å†…å®¹
            content = doc.page_content
            metadata = doc.metadata

            # æ·»åŠ æ—¶é—´æˆ³ä¿¡æ¯
            if "timestamp" in metadata:
                timestamp = datetime.fromtimestamp(metadata["timestamp"])
                time_str = timestamp.strftime("%Y-%m-%d %H:%M")
                formatted_docs.append(f"[{time_str}] {content}")
            else:
                formatted_docs.append(content)

        return "\n\n".join(formatted_docs)

    def _generate_conversation_id(self, inputs: Dict[str, Any]) -> str:
        """ç”Ÿæˆå¯¹è¯IDã€‚"""
        # ç®€å•çš„ä¼šè¯IDç”Ÿæˆç­–ç•¥
        return hashlib.md5(str(inputs).encode()).hexdigest()[:8]

    @property
    def memory_variables(self) -> List[str]:
        """è¿”å›è®°å¿†å˜é‡åˆ—è¡¨ã€‚"""
        return [self.memory_key]

    def clear(self) -> None:
        """æ¸…é™¤å‘é‡å­˜å‚¨ä¸­çš„è®°å¿†ã€‚"""
        # æ³¨æ„ï¼šè¿™å¯èƒ½ä¼šå½±å“å…¶ä»–ä½¿ç”¨åŒä¸€å‘é‡å­˜å‚¨çš„ç»„ä»¶
        if hasattr(self.retriever.vectorstore, 'delete_collection'):
            self.retriever.vectorstore.delete_collection()

        # æ¸…ç©ºç¼“å­˜å’Œç»Ÿè®¡
        self._query_cache.clear()
        self._retrieval_stats = {
            "total_contexts_saved": 0,
            "total_retrievals": 0,
            "average_retrieval_time": 0.0,
            "total_documents_stored": 0,
            "cache_hits": 0
        }

    def get_retrieval_stats(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        cache_hit_rate = (
            self._retrieval_stats["cache_hits"] /
            max(self._retrieval_stats["total_retrievals"], 1)
        )

        return {
            **self._retrieval_stats,
            "cache_hit_rate": cache_hit_rate,
            "cache_size": len(self._query_cache),
            "memory_type": "VectorStoreRetrieverMemory"
        }
```

---

## 6. ç»„åˆè®°å¿†æ•°æ®ç»“æ„

### 6.1 CombinedMemory ç»“æ„

```python
class CombinedMemory(BaseMemory):
    """ç»„åˆè®°å¿†æ•°æ®ç»“æ„ã€‚"""

    def __init__(self, memories: List[BaseMemory]):
        self.memories = memories
        self._validate_memories()

        # ç»„åˆç»Ÿè®¡
        self._combined_stats = {
            "memory_count": len(memories),
            "memory_types": [type(mem).__name__ for mem in memories],
            "total_operations": 0,
            "load_times": [],
            "save_times": []
        }

    def _validate_memories(self) -> None:
        """éªŒè¯è®°å¿†ç»„åˆçš„æœ‰æ•ˆæ€§ã€‚"""
        if not self.memories:
            raise ValueError("è‡³å°‘éœ€è¦ä¸€ä¸ªè®°å¿†ç»„ä»¶")

        # æ£€æŸ¥è®°å¿†å˜é‡åå†²çª
        all_variables = []
        for memory in self.memories:
            variables = memory.memory_variables
            for var in variables:
                if var in all_variables:
                    raise ValueError(f"è®°å¿†å˜é‡åå†²çª: {var}")
                all_variables.append(var)

    @property
    def memory_variables(self) -> List[str]:
        """è¿”å›æ‰€æœ‰è®°å¿†çš„å˜é‡åˆ—è¡¨ã€‚"""
        variables = []
        for memory in self.memories:
            variables.extend(memory.memory_variables)
        return variables

    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """åŠ è½½æ‰€æœ‰è®°å¿†çš„å˜é‡ã€‚"""
        start_time = time.time()
        memory_data = {}

        for memory in self.memories:
            try:
                memory_vars = memory.load_memory_variables(inputs)
                memory_data.update(memory_vars)
            except Exception as e:
                # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†å…¶ä»–è®°å¿†
                print(f"è®°å¿† {type(memory).__name__} åŠ è½½å¤±è´¥: {e}")

        # æ›´æ–°ç»Ÿè®¡
        load_time = time.time() - start_time
        self._combined_stats["load_times"].append(load_time)
        self._combined_stats["total_operations"] += 1

        return memory_data

    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
        """ä¿å­˜ä¸Šä¸‹æ–‡åˆ°æ‰€æœ‰è®°å¿†ã€‚"""
        start_time = time.time()

        for memory in self.memories:
            try:
                memory.save_context(inputs, outputs)
            except Exception as e:
                # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†å…¶ä»–è®°å¿†
                print(f"è®°å¿† {type(memory).__name__} ä¿å­˜å¤±è´¥: {e}")

        # æ›´æ–°ç»Ÿè®¡
        save_time = time.time() - start_time
        self._combined_stats["save_times"].append(save_time)
        self._combined_stats["total_operations"] += 1

    def clear(self) -> None:
        """æ¸…é™¤æ‰€æœ‰è®°å¿†ã€‚"""
        for memory in self.memories:
            try:
                memory.clear()
            except Exception as e:
                print(f"è®°å¿† {type(memory).__name__} æ¸…é™¤å¤±è´¥: {e}")

    def get_memory_by_type(self, memory_type: Type[BaseMemory]) -> Optional[BaseMemory]:
        """æ ¹æ®ç±»å‹è·å–è®°å¿†ç»„ä»¶ã€‚"""
        for memory in self.memories:
            if isinstance(memory, memory_type):
                return memory
        return None

    def get_combined_stats(self) -> Dict[str, Any]:
        """è·å–ç»„åˆè®°å¿†ç»Ÿè®¡ã€‚"""
        avg_load_time = (
            sum(self._combined_stats["load_times"]) /
            len(self._combined_stats["load_times"])
            if self._combined_stats["load_times"] else 0
        )

        avg_save_time = (
            sum(self._combined_stats["save_times"]) /
            len(self._combined_stats["save_times"])
            if self._combined_stats["save_times"] else 0
        )

        return {
            **self._combined_stats,
            "average_load_time": avg_load_time,
            "average_save_time": avg_save_time,
            "memory_variables": self.memory_variables
        }
```

---

## 7. æ€§èƒ½ç›‘æ§æ•°æ®ç»“æ„

### 7.1 MemoryMetrics ç»“æ„

```python
class MemoryMetrics:
    """è®°å¿†æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨ã€‚"""

    def __init__(self, memory: BaseMemory):
        self.memory = memory
        self.memory_type = type(memory).__name__

        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = {
            "load_count": 0,
            "save_count": 0,
            "clear_count": 0,
            "total_load_time": 0.0,
            "total_save_time": 0.0,
            "load_times": deque(maxlen=100),  # æœ€è¿‘100æ¬¡æ“ä½œ
            "save_times": deque(maxlen=100),
            "memory_sizes": deque(maxlen=100),
            "error_count": 0,
            "last_error": None
        }

        # å†…å­˜ä½¿ç”¨ç›‘æ§
        self.memory_usage = {
            "peak_memory_mb": 0.0,
            "current_memory_mb": 0.0,
            "memory_growth_rate": 0.0
        }

    def record_load_operation(self, execution_time: float, memory_size: int) -> None:
        """è®°å½•åŠ è½½æ“ä½œã€‚"""
        self.metrics["load_count"] += 1
        self.metrics["total_load_time"] += execution_time
        self.metrics["load_times"].append(execution_time)
        self.metrics["memory_sizes"].append(memory_size)

        self._update_memory_usage(memory_size)

    def record_save_operation(self, execution_time: float) -> None:
        """è®°å½•ä¿å­˜æ“ä½œã€‚"""
        self.metrics["save_count"] += 1
        self.metrics["total_save_time"] += execution_time
        self.metrics["save_times"].append(execution_time)

    def record_error(self, error: Exception) -> None:
        """è®°å½•é”™è¯¯ã€‚"""
        self.metrics["error_count"] += 1
        self.metrics["last_error"] = {
            "type": type(error).__name__,
            "message": str(error),
            "timestamp": time.time()
        }

    def _update_memory_usage(self, current_size: int) -> None:
        """æ›´æ–°å†…å­˜ä½¿ç”¨æƒ…å†µã€‚"""
        size_mb = current_size / (1024 * 1024)
        self.memory_usage["current_memory_mb"] = size_mb

        if size_mb > self.memory_usage["peak_memory_mb"]:
            self.memory_usage["peak_memory_mb"] = size_mb

        # è®¡ç®—å†…å­˜å¢é•¿ç‡
        if len(self.metrics["memory_sizes"]) >= 2:
            recent_sizes = list(self.metrics["memory_sizes"])[-10:]
            if len(recent_sizes) >= 2:
                growth_rate = (recent_sizes[-1] - recent_sizes[0]) / len(recent_sizes)
                self.memory_usage["memory_growth_rate"] = growth_rate

    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦ã€‚"""
        load_times = list(self.metrics["load_times"])
        save_times = list(self.metrics["save_times"])

        return {
            "memory_type": self.memory_type,
            "operation_counts": {
                "loads": self.metrics["load_count"],
                "saves": self.metrics["save_count"],
                "errors": self.metrics["error_count"]
            },
            "timing_stats": {
                "avg_load_time": (
                    sum(load_times) / len(load_times) if load_times else 0
                ),
                "avg_save_time": (
                    sum(save_times) / len(save_times) if save_times else 0
                ),
                "p95_load_time": (
                    sorted(load_times)[int(0.95 * len(load_times))]
                    if len(load_times) >= 20 else 0
                ),
                "p95_save_time": (
                    sorted(save_times)[int(0.95 * len(save_times))]
                    if len(save_times) >= 20 else 0
                )
            },
            "memory_usage": self.memory_usage,
            "error_rate": (
                self.metrics["error_count"] /
                max(self.metrics["load_count"] + self.metrics["save_count"], 1)
            )
        }
```

---

## 8. æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº† **Memory æ¨¡å—**çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼š

1. **ç±»å±‚æ¬¡ç»“æ„**ï¼šä»BaseMemoryåˆ°å„ç§å…·ä½“å®ç°çš„å®Œæ•´ç»§æ‰¿å…³ç³»
2. **æ¶ˆæ¯å­˜å‚¨**ï¼šChatMessageHistoryå’Œå„ç§æŒä¹…åŒ–å®ç°
3. **ç¼“å†²åŒºç®¡ç†**ï¼šä¸åŒç¼“å†²ç­–ç•¥çš„æ•°æ®ç»“æ„å’Œç®—æ³•
4. **æ™ºèƒ½è®°å¿†**ï¼šæ‘˜è¦è®°å¿†å’Œå®ä½“è®°å¿†çš„å¤æ‚æ•°æ®ç»“æ„
5. **å‘é‡æ£€ç´¢**ï¼šåŸºäºå‘é‡å­˜å‚¨çš„è¯­ä¹‰è®°å¿†ç»“æ„
6. **ç»„åˆè®°å¿†**ï¼šå¤šç§è®°å¿†ç±»å‹çš„ç»„åˆå’Œç®¡ç†
7. **æ€§èƒ½ç›‘æ§**ï¼šè®°å¿†ç³»ç»Ÿçš„æŒ‡æ ‡æ”¶é›†å’Œåˆ†æ

æ‰€æœ‰æ•°æ®ç»“æ„å‡åŒ…å«ï¼š

- å®Œæ•´çš„å­—æ®µå®šä¹‰å’Œç±»å‹è¯´æ˜
- è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯å’Œç›‘æ§æœºåˆ¶
- æ€§èƒ½ä¼˜åŒ–ç­–ç•¥å’Œç¼“å­˜æœºåˆ¶
- é”™è¯¯å¤„ç†å’Œæ¢å¤èƒ½åŠ›
- å¯æ‰©å±•çš„æ¶æ„è®¾è®¡

è¿™äº›ç»“æ„ä¸ºæ„å»ºé«˜æ•ˆã€å¯é çš„å¯¹è¯è®°å¿†ç³»ç»Ÿæä¾›äº†å®Œæ•´çš„æ•°æ®æ¨¡å‹åŸºç¡€ï¼Œæ”¯æŒä»ç®€å•ç¼“å†²åˆ°å¤æ‚è¯­ä¹‰æ£€ç´¢çš„å„ç§è®°å¿†éœ€æ±‚ã€‚

---

## æ—¶åºå›¾

## æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£é€šè¿‡è¯¦ç»†çš„æ—¶åºå›¾å±•ç¤º **Memory æ¨¡å—**åœ¨å„ç§åœºæ™¯ä¸‹çš„æ‰§è¡Œæµç¨‹ï¼ŒåŒ…æ‹¬å¯¹è¯è®°å¿†å­˜å‚¨ã€ç¼“å†²åŒºç®¡ç†ã€æ‘˜è¦ç”Ÿæˆã€å®ä½“æå–ã€å‘é‡æ£€ç´¢ç­‰å¤æ‚äº¤äº’è¿‡ç¨‹ã€‚

---

## 1. åŸºç¡€è®°å¿†æ“ä½œåœºæ™¯

### 1.1 ConversationBufferMemory åŸºç¡€æ“ä½œæµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Memory as ConversationBufferMemory
    participant ChatHistory as ChatMessageHistory
    participant Formatter as BufferFormatter

    User->>Memory: save_context({"input": "ä½ å¥½"}, {"output": "ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ "})

    Memory->>Memory: æå–è¾“å…¥è¾“å‡º<br/>input_str = "ä½ å¥½"<br/>output_str = "ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ "

    Memory->>ChatHistory: add_user_message("ä½ å¥½")
    ChatHistory->>ChatHistory: åˆ›å»ºHumanMessage<br/>messages.append(HumanMessage("ä½ å¥½"))

    Memory->>ChatHistory: add_ai_message("ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ")
    ChatHistory->>ChatHistory: åˆ›å»ºAIMessage<br/>messages.append(AIMessage("ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ "))

    ChatHistory-->>Memory: æ¶ˆæ¯ä¿å­˜å®Œæˆ

    User->>Memory: load_memory_variables({})

    Memory->>Memory: æ£€æŸ¥return_messagesé…ç½®<br/>return_messages = False

    alt return_messages = True
        Memory-->>User: {"history": [HumanMessage, AIMessage]}
    else return_messages = False
        Memory->>Formatter: get_buffer_string(messages, human_prefix, ai_prefix)
        Formatter->>Formatter: æ ¼å¼åŒ–æ¶ˆæ¯<br/>"Human: ä½ å¥½\nAI: ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ "
        Formatter-->>Memory: formatted_string
        Memory-->>User: {"history": "Human: ä½ å¥½\nAI: ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ "}
    end
```

**å…³é”®æ­¥éª¤è¯´æ˜**ï¼š

1. **æ¶ˆæ¯å­˜å‚¨**ï¼ˆæ­¥éª¤ 3-6ï¼‰ï¼š
   - å°†ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºHumanMessageå¯¹è±¡
   - å°†AIè¾“å‡ºè½¬æ¢ä¸ºAIMessageå¯¹è±¡
   - æŒ‰æ—¶é—´é¡ºåºæ·»åŠ åˆ°æ¶ˆæ¯å†å²

2. **æ¶ˆæ¯æ ¼å¼åŒ–**ï¼ˆæ­¥éª¤ 11-14ï¼‰ï¼š
   - æ ¹æ®return_messagesé…ç½®å†³å®šè¿”å›æ ¼å¼
   - å­—ç¬¦ä¸²æ ¼å¼ï¼šä½¿ç”¨å‰ç¼€æ ¼å¼åŒ–ä¸ºå¯è¯»æ–‡æœ¬
   - æ¶ˆæ¯æ ¼å¼ï¼šç›´æ¥è¿”å›æ¶ˆæ¯å¯¹è±¡åˆ—è¡¨

**æ€§èƒ½ç‰¹å¾**ï¼š

- å­˜å‚¨æ“ä½œï¼šO(1) æ—¶é—´å¤æ‚åº¦
- æ£€ç´¢æ“ä½œï¼šO(n) æ—¶é—´å¤æ‚åº¦ï¼ˆnä¸ºæ¶ˆæ¯æ•°é‡ï¼‰
- å†…å­˜ä½¿ç”¨ï¼šéšå¯¹è¯é•¿åº¦çº¿æ€§å¢é•¿

---

### 1.2 ConversationBufferWindowMemory çª—å£ç®¡ç†æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Memory as ConversationBufferWindowMemory
    participant ChatHistory
    participant WindowManager as WindowManager

    Note over Memory: é…ç½®ï¼šk=3 (ä¿ç•™3è½®å¯¹è¯)

    loop æ·»åŠ 5è½®å¯¹è¯
        User->>Memory: save_context({"input": f"ç¬¬{i}è½®é—®é¢˜"}, {"output": f"ç¬¬{i}è½®å›ç­”"})

        Memory->>ChatHistory: æ·»åŠ ç”¨æˆ·å’ŒAIæ¶ˆæ¯
        ChatHistory-->>Memory: æ¶ˆæ¯å·²æ·»åŠ 

        Memory->>WindowManager: æ£€æŸ¥çª—å£å¤§å°<br/>current_messages = len(messages)

        alt current_messages > 2 * k (6æ¡æ¶ˆæ¯)
            WindowManager->>WindowManager: è®¡ç®—éœ€è¦åˆ é™¤çš„æ¶ˆæ¯æ•°<br/>to_remove = current_messages - 6

            WindowManager->>ChatHistory: åˆ é™¤æœ€æ—§çš„æ¶ˆæ¯<br/>messages = messages[to_remove:]

            WindowManager->>WindowManager: æ›´æ–°ç»Ÿè®¡ä¿¡æ¯<br/>messages_pruned += to_remove

            WindowManager-->>Memory: çª—å£ç»´æŠ¤å®Œæˆ<br/>å½“å‰ä¿ç•™: æœ€è¿‘3è½®å¯¹è¯
        else current_messages <= 6
            WindowManager-->>Memory: æ— éœ€ä¿®å‰ªï¼Œç»§ç»­æ·»åŠ 
        end
    end

    User->>Memory: load_memory_variables({})

    Memory->>Memory: è·å–å½“å‰çª—å£å†…å®¹<br/>åªåŒ…å«æœ€è¿‘3è½®å¯¹è¯

    Memory-->>User: {"history": "æœ€è¿‘3è½®å¯¹è¯çš„æ ¼å¼åŒ–æ–‡æœ¬"}
```

**çª—å£ç®¡ç†ç®—æ³•**ï¼š

```python
def _prune_messages(self) -> None:
    """çª—å£ä¿®å‰ªç®—æ³•ã€‚"""
    messages = self.chat_memory.messages
    max_messages = 2 * self.k  # kè½®å¯¹è¯ = 2kæ¡æ¶ˆæ¯

    if len(messages) > max_messages:
        # è®¡ç®—éœ€è¦åˆ é™¤çš„æ¶ˆæ¯æ•°
        messages_to_remove = len(messages) - max_messages

        # ç¡®ä¿åˆ é™¤å¶æ•°ä¸ªæ¶ˆæ¯ï¼ˆä¿æŒé—®ç­”å¯¹å®Œæ•´ï¼‰
        if messages_to_remove % 2 != 0:
            messages_to_remove += 1

        # åˆ é™¤æœ€æ—§çš„æ¶ˆæ¯
        self.chat_memory.messages = messages[messages_to_remove:]

        # æ›´æ–°ç»Ÿè®¡
        self.window_stats["messages_pruned"] += messages_to_remove
```

**çª—å£æ•ˆæœç¤ºä¾‹**ï¼š

```
è½®æ¬¡1: Human: é—®é¢˜1, AI: å›ç­”1
è½®æ¬¡2: Human: é—®é¢˜2, AI: å›ç­”2
è½®æ¬¡3: Human: é—®é¢˜3, AI: å›ç­”3  â† çª—å£å¼€å§‹
è½®æ¬¡4: Human: é—®é¢˜4, AI: å›ç­”4  â† ä¿ç•™
è½®æ¬¡5: Human: é—®é¢˜5, AI: å›ç­”5  â† ä¿ç•™
```

---

## 2. æ™ºèƒ½è®°å¿†åœºæ™¯

### 2.1 ConversationSummaryMemory æ‘˜è¦ç”Ÿæˆæµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Memory as ConversationSummaryMemory
    participant ChatHistory
    participant LLM as LanguageModel
    participant SummaryManager

    User->>Memory: save_context({"input": "ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ "}, {"output": "æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦åˆ†æ”¯..."})

    Memory->>ChatHistory: æ·»åŠ æ¶ˆæ¯åˆ°ä¸´æ—¶å­˜å‚¨
    ChatHistory-->>Memory: æ¶ˆæ¯å·²æ·»åŠ 

    Memory->>Memory: æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦<br/>len(messages) >= 2 ?

    alt éœ€è¦ç”Ÿæˆæ‘˜è¦
        Memory->>SummaryManager: å¯åŠ¨æ‘˜è¦æ›´æ–°æµç¨‹

        SummaryManager->>SummaryManager: è·å–æ–°å¯¹è¯å†…å®¹<br/>new_lines = format_messages(messages)

        alt å­˜åœ¨ç°æœ‰æ‘˜è¦
            SummaryManager->>SummaryManager: æ„å»ºå¢é‡æ‘˜è¦æç¤º<br/>prompt = SUMMARY_PROMPT.format(<br/>  summary=existing_summary,<br/>  new_lines=new_lines<br/>)
        else é¦–æ¬¡ç”Ÿæˆæ‘˜è¦
            SummaryManager->>SummaryManager: æ„å»ºåˆå§‹æ‘˜è¦æç¤º<br/>prompt = "æ€»ç»“ä»¥ä¸‹å¯¹è¯ï¼š\n" + new_lines
        end

        SummaryManager->>LLM: predict(summary_prompt)
        Note over LLM: LLMåˆ†æå¯¹è¯å†…å®¹<br/>ç”Ÿæˆç®€æ´æ‘˜è¦
        LLM-->>SummaryManager: "ç”¨æˆ·è¯¢é—®äº†æœºå™¨å­¦ä¹ æ¦‚å¿µï¼ŒAIè§£é‡Šäº†åŸºæœ¬å®šä¹‰å’Œåº”ç”¨é¢†åŸŸ"

        SummaryManager->>SummaryManager: æ›´æ–°æ‘˜è¦ç¼“å†²åŒº<br/>buffer = new_summary

        SummaryManager->>SummaryManager: è®¡ç®—å‹ç¼©ç»Ÿè®¡<br/>original_tokens = count_tokens(new_lines)<br/>summary_tokens = count_tokens(new_summary)<br/>compression_ratio = summary_tokens / original_tokens

        SummaryManager->>ChatHistory: æ¸…ç©ºä¸´æ—¶æ¶ˆæ¯<br/>messages.clear()

        SummaryManager-->>Memory: æ‘˜è¦æ›´æ–°å®Œæˆ
    end

    User->>Memory: load_memory_variables({})

    Memory->>Memory: æ£€æŸ¥è¿”å›æ ¼å¼<br/>return_messages = ?

    alt return_messages = True
        Memory-->>User: {"history": [SystemMessage(content=summary)]}
    else return_messages = False
        Memory-->>User: {"history": summary_buffer}
    end
```

**æ‘˜è¦æç¤ºæ¨¡æ¿**ï¼š

```python
SUMMARY_PROMPT = PromptTemplate(
    input_variables=["summary", "new_lines"],
    template="""
æ¸è¿›å¼æ€»ç»“ä»¥ä¸‹å¯¹è¯ï¼Œåœ¨ç°æœ‰æ‘˜è¦åŸºç¡€ä¸Šæ•´åˆæ–°ä¿¡æ¯ï¼š

ç°æœ‰æ‘˜è¦ï¼š
{summary}

æ–°çš„å¯¹è¯å†…å®¹ï¼š
{new_lines}

æ›´æ–°åçš„æ‘˜è¦ï¼ˆä¿æŒç®€æ´ï¼Œçªå‡ºå…³é”®ä¿¡æ¯ï¼‰ï¼š
""".strip()
)
```

**æ‘˜è¦æ•ˆæœå¯¹æ¯”**ï¼š

| åŸå§‹å¯¹è¯é•¿åº¦ | æ‘˜è¦é•¿åº¦ | å‹ç¼©æ¯” | ä¿¡æ¯ä¿ç•™åº¦ |
|-------------|---------|--------|-----------|
| 500 tokens | 50 tokens | 10:1 | 85% |
| 1000 tokens | 80 tokens | 12.5:1 | 80% |
| 2000 tokens | 120 tokens | 16.7:1 | 75% |

---

### 2.2 ConversationEntityMemory å®ä½“æå–æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Memory as ConversationEntityMemory
    participant ChatHistory
    participant EntityExtractor
    participant LLM
    participant EntityStore

    User->>Memory: save_context({<br/>  "input": "æˆ‘å«å¼ ä¸‰ï¼Œåœ¨åŒ—äº¬å·¥ä½œï¼Œæ˜¯è½¯ä»¶å·¥ç¨‹å¸ˆ"<br/>}, {<br/>  "output": "å¾ˆé«˜å…´è®¤è¯†ä½ å¼ ä¸‰ï¼ä½ åœ¨åŒ—äº¬å“ªä¸ªå…¬å¸å·¥ä½œï¼Ÿ"<br/>})

    Memory->>ChatHistory: ä¿å­˜å¯¹è¯æ¶ˆæ¯
    ChatHistory-->>Memory: æ¶ˆæ¯å·²ä¿å­˜

    Memory->>EntityExtractor: å¯åŠ¨å®ä½“æå–<br/>context = "æˆ‘å«å¼ ä¸‰ï¼Œåœ¨åŒ—äº¬å·¥ä½œ...å¾ˆé«˜å…´è®¤è¯†ä½ å¼ ä¸‰ï¼..."

    EntityExtractor->>LLM: è°ƒç”¨å®ä½“æå–<br/>prompt = ENTITY_EXTRACTION_PROMPT.format(text=context)

    LLM->>LLM: åˆ†ææ–‡æœ¬å†…å®¹<br/>è¯†åˆ«äººåã€åœ°åã€èŒä¸šç­‰å®ä½“
    LLM-->>EntityExtractor: "å¼ ä¸‰, åŒ—äº¬, è½¯ä»¶å·¥ç¨‹å¸ˆ"

    EntityExtractor->>EntityExtractor: è§£æå®ä½“åˆ—è¡¨<br/>entities = ["å¼ ä¸‰", "åŒ—äº¬", "è½¯ä»¶å·¥ç¨‹å¸ˆ"]

    loop å¤„ç†æ¯ä¸ªå®ä½“
        EntityExtractor->>EntityStore: æ£€æŸ¥å®ä½“æ˜¯å¦å­˜åœ¨<br/>entity = "å¼ ä¸‰"

        alt å®ä½“å·²å­˜åœ¨
            EntityStore->>EntityStore: è·å–ç°æœ‰ä¿¡æ¯<br/>existing_info = "å¼ ä¸‰ç›¸å…³çš„å·²çŸ¥ä¿¡æ¯"

            EntityStore->>LLM: æ›´æ–°å®ä½“ä¿¡æ¯<br/>prompt = ENTITY_SUMMARIZATION_PROMPT.format(<br/>  entity="å¼ ä¸‰",<br/>  existing_info=existing_info,<br/>  new_context=context<br/>)

            LLM-->>EntityStore: "å¼ ä¸‰ï¼šä½åœ¨åŒ—äº¬ï¼ŒèŒä¸šæ˜¯è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæ€§æ ¼å‹å¥½"

            EntityStore->>EntityStore: æ›´æ–°å®ä½“è®°å½•<br/>entity_store["å¼ ä¸‰"] = updated_info

        else æ–°å®ä½“
            EntityStore->>LLM: åˆå§‹åŒ–å®ä½“ä¿¡æ¯<br/>prompt = f"æ ¹æ®ä¸Šä¸‹æ–‡æ€»ç»“å…³äº{entity}çš„ä¿¡æ¯ï¼š\n{context}"

            LLM-->>EntityStore: "å¼ ä¸‰ï¼šä½åœ¨åŒ—äº¬çš„è½¯ä»¶å·¥ç¨‹å¸ˆ"

            EntityStore->>EntityStore: åˆ›å»ºæ–°å®ä½“è®°å½•<br/>entity_store["å¼ ä¸‰"] = new_info
        end
    end

    EntityExtractor-->>Memory: å®ä½“æå–å’Œæ›´æ–°å®Œæˆ

    User->>Memory: load_memory_variables({"input": "å¼ ä¸‰çš„å·¥ä½œæƒ…å†µå¦‚ä½•ï¼Ÿ"})

    Memory->>EntityExtractor: ä»æŸ¥è¯¢ä¸­æå–ç›¸å…³å®ä½“<br/>query_entities = extract_entities("å¼ ä¸‰çš„å·¥ä½œæƒ…å†µå¦‚ä½•ï¼Ÿ")

    EntityExtractor-->>Memory: ["å¼ ä¸‰"]

    Memory->>EntityStore: è·å–ç›¸å…³å®ä½“ä¿¡æ¯<br/>entity_info = entity_store["å¼ ä¸‰"]

    EntityStore-->>Memory: "å¼ ä¸‰ï¼šä½åœ¨åŒ—äº¬çš„è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæ€§æ ¼å‹å¥½"

    Memory-->>User: {"entities": "å¼ ä¸‰ï¼šä½åœ¨åŒ—äº¬çš„è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæ€§æ ¼å‹å¥½"}
```

**å®ä½“æå–æç¤ºæ¨¡æ¿**ï¼š

```python
ENTITY_EXTRACTION_PROMPT = PromptTemplate(
    input_variables=["text"],
    template="""
ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æ‰€æœ‰é‡è¦çš„å®ä½“ï¼ˆäººåã€åœ°åã€ç»„ç»‡ã€äº§å“ç­‰ï¼‰ï¼Œç”¨é€—å·åˆ†éš”ï¼š

æ–‡æœ¬ï¼š
{text}

å®ä½“ï¼š
""".strip()
)

ENTITY_SUMMARIZATION_PROMPT = PromptTemplate(
    input_variables=["entity", "existing_info", "new_context"],
    template="""
åŸºäºæ–°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ›´æ–°å…³äºå®ä½“"{entity}"çš„æ€»ç»“ï¼š

ç°æœ‰ä¿¡æ¯ï¼š
{existing_info}

æ–°çš„ä¸Šä¸‹æ–‡ï¼š
{new_context}

æ›´æ–°åçš„å®ä½“ä¿¡æ¯ï¼š
""".strip()
)
```

---

## 3. å‘é‡æ£€ç´¢è®°å¿†åœºæ™¯

### 3.1 VectorStoreRetrieverMemory è¯­ä¹‰æ£€ç´¢æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Memory as VectorStoreRetrieverMemory
    participant VectorStore
    participant Embeddings
    participant Retriever
    participant Cache as QueryCache

    User->>Memory: save_context({<br/>  "input": "Pythonæœ‰å“ªäº›æœºå™¨å­¦ä¹ åº“ï¼Ÿ"<br/>}, {<br/>  "output": "ä¸»è¦æœ‰scikit-learnã€TensorFlowã€PyTorchç­‰"<br/>})

    Memory->>Memory: æ„å»ºæ–‡æ¡£å†…å®¹<br/>content = "Human: Pythonæœ‰å“ªäº›æœºå™¨å­¦ä¹ åº“ï¼Ÿ\nAI: ä¸»è¦æœ‰scikit-learnã€TensorFlowã€PyTorchç­‰"

    Memory->>Memory: åˆ›å»ºå…ƒæ•°æ®<br/>metadata = {<br/>  "input": "Pythonæœ‰å“ªäº›æœºå™¨å­¦ä¹ åº“ï¼Ÿ",<br/>  "output": "ä¸»è¦æœ‰scikit-learnã€TensorFlowã€PyTorchç­‰",<br/>  "timestamp": 1699123456.789,<br/>  "conversation_id": "abc12345"<br/>}

    Memory->>VectorStore: add_texts([content], [metadata])

    VectorStore->>Embeddings: embed_documents([content])
    Embeddings-->>VectorStore: [embedding_vector]

    VectorStore->>VectorStore: å­˜å‚¨å‘é‡å’Œæ–‡æ¡£<br/>vector_id = store_vector(embedding, content, metadata)

    VectorStore-->>Memory: æ–‡æ¡£å­˜å‚¨å®Œæˆ<br/>document_id = vector_id

    Memory->>Cache: æ¸…ç©ºæŸ¥è¯¢ç¼“å­˜<br/>æ–°æ–‡æ¡£å¯èƒ½å½±å“æ£€ç´¢ç»“æœ

    User->>Memory: load_memory_variables({<br/>  "input": "æ¨èä¸€äº›æ·±åº¦å­¦ä¹ æ¡†æ¶"<br/>})

    Memory->>Memory: ç”Ÿæˆç¼“å­˜é”®<br/>cache_key = hash("æ¨èä¸€äº›æ·±åº¦å­¦ä¹ æ¡†æ¶")

    Memory->>Cache: æ£€æŸ¥æŸ¥è¯¢ç¼“å­˜<br/>get(cache_key)

    alt ç¼“å­˜å‘½ä¸­
        Cache-->>Memory: cached_documents
        Memory->>Memory: æ›´æ–°ç¼“å­˜ç»Ÿè®¡<br/>cache_hits += 1
    else ç¼“å­˜æœªå‘½ä¸­
        Cache-->>Memory: None

        Memory->>Retriever: get_relevant_documents("æ¨èä¸€äº›æ·±åº¦å­¦ä¹ æ¡†æ¶")

        Retriever->>Embeddings: embed_query("æ¨èä¸€äº›æ·±åº¦å­¦ä¹ æ¡†æ¶")
        Embeddings-->>Retriever: query_vector

        Retriever->>VectorStore: similarity_search_by_vector(query_vector, k=3)

        VectorStore->>VectorStore: è®¡ç®—å‘é‡ç›¸ä¼¼åº¦<br/>æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£
        VectorStore-->>Retriever: [<br/>  Document(content="Human: Pythonæœ‰å“ªäº›æœºå™¨å­¦ä¹ åº“ï¼Ÿ...", metadata={...}),<br/>  Document(...)<br/>]

        Retriever-->>Memory: relevant_documents

        Memory->>Cache: ç¼“å­˜æŸ¥è¯¢ç»“æœ<br/>put(cache_key, relevant_documents)

        Memory->>Memory: æ›´æ–°æ£€ç´¢ç»Ÿè®¡<br/>total_retrievals += 1<br/>avg_retrieval_time = ...
    end

    Memory->>Memory: æ ¼å¼åŒ–æ£€ç´¢ç»“æœ<br/>format_documents(documents)

    alt return_docs = True
        Memory-->>User: {"history": [Document1, Document2, ...]}
    else return_docs = False
        Memory->>Memory: è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼<br/>"[2023-11-05 10:30] Human: Pythonæœ‰å“ªäº›æœºå™¨å­¦ä¹ åº“ï¼Ÿ\nAI: ä¸»è¦æœ‰scikit-learn..."
        Memory-->>User: {"history": formatted_text}
    end
```

**å‘é‡æ£€ç´¢ä¼˜åŒ–**ï¼š

```python
class VectorStoreRetrieverMemory:
    def __init__(self, retriever, cache_size=100):
        self.retriever = retriever
        self._query_cache = {}
        self._cache_max_size = cache_size
        self._retrieval_stats = {
            "cache_hits": 0,
            "total_retrievals": 0,
            "avg_retrieval_time": 0.0
        }

    def _get_cache_key(self, query: str) -> str:
        """ç”ŸæˆæŸ¥è¯¢ç¼“å­˜é”®ã€‚"""
        return hashlib.md5(query.encode()).hexdigest()

    def _should_cache_result(self, docs: List[Document]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç¼“å­˜ç»“æœã€‚"""
        # åªç¼“å­˜æœ‰æ„ä¹‰çš„æ£€ç´¢ç»“æœ
        return len(docs) > 0 and all(
            hasattr(doc, 'metadata') and 'timestamp' in doc.metadata
            for doc in docs
        )
```

---

## 4. ç»„åˆè®°å¿†åœºæ™¯

### 4.1 CombinedMemory å¤šè®°å¿†ååŒæµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Combined as CombinedMemory
    participant BufferMem as ConversationBufferMemory
    participant SummaryMem as ConversationSummaryMemory
    participant VectorMem as VectorStoreRetrieverMemory
    participant Validator as MemoryValidator

    Note over Combined: ç»„åˆä¸‰ç§è®°å¿†ç±»å‹ï¼š<br/>ç¼“å†²åŒº + æ‘˜è¦ + å‘é‡æ£€ç´¢

    User->>Combined: save_context({<br/>  "input": "è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åå‘ä¼ æ’­ç®—æ³•"<br/>}, {<br/>  "output": "åå‘ä¼ æ’­æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•..."<br/>})

    Combined->>Validator: éªŒè¯è¾“å…¥æœ‰æ•ˆæ€§<br/>æ£€æŸ¥inputså’Œoutputsæ ¼å¼

    par å¹¶è¡Œä¿å­˜åˆ°æ‰€æœ‰è®°å¿†
        Combined->>BufferMem: save_context(inputs, outputs)
        BufferMem->>BufferMem: æ·»åŠ åˆ°æ¶ˆæ¯å†å²<br/>ä¿æŒå®Œæ•´å¯¹è¯è®°å½•
        BufferMem-->>Combined: ç¼“å†²åŒºä¿å­˜å®Œæˆ
    and
        Combined->>SummaryMem: save_context(inputs, outputs)
        SummaryMem->>SummaryMem: æ·»åŠ åˆ°ä¸´æ—¶å­˜å‚¨<br/>è§¦å‘æ‘˜è¦æ›´æ–°
        SummaryMem-->>Combined: æ‘˜è¦è®°å¿†ä¿å­˜å®Œæˆ
    and
        Combined->>VectorMem: save_context(inputs, outputs)
        VectorMem->>VectorMem: å‘é‡åŒ–å¹¶å­˜å‚¨<br/>æ”¯æŒè¯­ä¹‰æ£€ç´¢
        VectorMem-->>Combined: å‘é‡è®°å¿†ä¿å­˜å®Œæˆ
    end

    Combined->>Combined: æ›´æ–°ç»„åˆç»Ÿè®¡<br/>save_operations += 1<br/>save_time = ...

    User->>Combined: load_memory_variables({<br/>  "input": "æ·±åº¦å­¦ä¹ ä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜å¦‚ä½•è§£å†³ï¼Ÿ"<br/>})

    Combined->>Combined: å¹¶è¡ŒåŠ è½½æ‰€æœ‰è®°å¿†ç±»å‹

    par å¹¶è¡ŒåŠ è½½è®°å¿†
        Combined->>BufferMem: load_memory_variables(inputs)
        BufferMem-->>Combined: {"chat_history": "æœ€è¿‘çš„å®Œæ•´å¯¹è¯å†å²"}
    and
        Combined->>SummaryMem: load_memory_variables(inputs)
        SummaryMem-->>Combined: {"conversation_summary": "å¯¹è¯æ‘˜è¦å†…å®¹"}
    and
        Combined->>VectorMem: load_memory_variables(inputs)
        VectorMem->>VectorMem: åŸºäº"æ¢¯åº¦æ¶ˆå¤±é—®é¢˜"<br/>æ£€ç´¢ç›¸å…³å†å²å¯¹è¯
        VectorMem-->>Combined: {"relevant_context": "ç›¸å…³çš„å†å²è®¨è®º"}
    end

    Combined->>Combined: åˆå¹¶æ‰€æœ‰è®°å¿†æ•°æ®<br/>memory_data = {<br/>  "chat_history": "...",<br/>  "conversation_summary": "...",<br/>  "relevant_context": "..."<br/>}

    Combined->>Combined: æ£€æŸ¥å˜é‡åå†²çª<br/>ç¡®ä¿æ²¡æœ‰é‡å¤çš„memory_key

    Combined-->>User: {<br/>  "chat_history": "æœ€è¿‘å¯¹è¯",<br/>  "conversation_summary": "å¯¹è¯æ‘˜è¦",<br/>  "relevant_context": "ç›¸å…³å†å²"<br/>}
```

**ç»„åˆè®°å¿†ä¼˜åŠ¿**ï¼š

1. **äº’è¡¥æ€§**ï¼š
   - ç¼“å†²åŒºè®°å¿†ï¼šä¿ç•™æœ€è¿‘å®Œæ•´å¯¹è¯
   - æ‘˜è¦è®°å¿†ï¼šå‹ç¼©é•¿æœŸå¯¹è¯å†å²
   - å‘é‡è®°å¿†ï¼šæä¾›è¯­ä¹‰ç›¸å…³çš„å†å²ä¸Šä¸‹æ–‡

2. **å®¹é”™æ€§**ï¼š
   - å•ä¸ªè®°å¿†ç»„ä»¶å¤±è´¥ä¸å½±å“æ•´ä½“
   - é”™è¯¯éš”ç¦»å’Œæ¢å¤æœºåˆ¶

3. **çµæ´»æ€§**ï¼š
   - å¯æ ¹æ®éœ€è¦åŠ¨æ€ç»„åˆä¸åŒè®°å¿†ç±»å‹
   - æ”¯æŒè®°å¿†ç»„ä»¶çš„çƒ­æ’æ‹”

---

## 5. æ€§èƒ½ä¼˜åŒ–åœºæ™¯

### 5.1 è®°å¿†ç¼“å­˜å’Œæ‰¹é‡æ“ä½œ

```mermaid
sequenceDiagram
    autonumber
    participant App as Application
    participant Manager as MemoryManager
    participant Memory as ConversationBufferMemory
    participant Cache as MemoryCache
    participant Monitor as PerformanceMonitor

    App->>Manager: æ‰¹é‡ä¿å­˜å¯¹è¯<br/>batch_save_contexts([<br/>  (inputs1, outputs1),<br/>  (inputs2, outputs2),<br/>  ...<br/>])

    Manager->>Monitor: å¼€å§‹æ€§èƒ½ç›‘æ§<br/>start_batch_operation()

    Manager->>Cache: æ£€æŸ¥æ‰¹é‡ç¼“å­˜ç­–ç•¥<br/>should_use_batch_cache()

    alt ä½¿ç”¨æ‰¹é‡ä¼˜åŒ–
        Manager->>Manager: åˆ†æ‰¹å¤„ç†<br/>batch_size = 10

        loop å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
            Manager->>Memory: æ‰¹é‡ä¿å­˜ä¸Šä¸‹æ–‡<br/>batch_save_context(batch)

            Memory->>Memory: ä¼˜åŒ–æ¶ˆæ¯æ·»åŠ <br/>æ‰¹é‡åˆ›å»ºæ¶ˆæ¯å¯¹è±¡<br/>å‡å°‘å•æ¬¡æ“ä½œå¼€é”€

            Memory-->>Manager: æ‰¹æ¬¡ä¿å­˜å®Œæˆ

            Manager->>Monitor: è®°å½•æ‰¹æ¬¡æ€§èƒ½<br/>batch_time, memory_usage
        end

    else é€ä¸ªå¤„ç†
        loop å¤„ç†æ¯ä¸ªå¯¹è¯
            Manager->>Memory: save_context(inputs, outputs)
            Memory-->>Manager: å•æ¬¡ä¿å­˜å®Œæˆ
        end
    end

    Manager->>Cache: æ›´æ–°ç¼“å­˜ç»Ÿè®¡<br/>batch_operations += 1

    Manager->>Monitor: ç»“æŸæ€§èƒ½ç›‘æ§<br/>end_batch_operation()

    Monitor->>Monitor: åˆ†ææ€§èƒ½æ•°æ®<br/>è®¡ç®—ååé‡ã€å»¶è¿Ÿåˆ†å¸ƒ

    Monitor-->>Manager: æ€§èƒ½æŠ¥å‘Š<br/>{<br/>  "throughput": "100 contexts/sec",<br/>  "avg_latency": "10ms",<br/>  "memory_efficiency": "85%"<br/>}

    Manager-->>App: æ‰¹é‡æ“ä½œå®Œæˆ<br/>performance_stats
```

**æ‰¹é‡ä¼˜åŒ–ç­–ç•¥**ï¼š

```python
class BatchMemoryManager:
    def __init__(self, memory: BaseMemory, batch_size: int = 50):
        self.memory = memory
        self.batch_size = batch_size
        self.pending_contexts = []

    def add_context(self, inputs: Dict, outputs: Dict) -> None:
        """æ·»åŠ ä¸Šä¸‹æ–‡åˆ°å¾…å¤„ç†é˜Ÿåˆ—ã€‚"""
        self.pending_contexts.append((inputs, outputs))

        if len(self.pending_contexts) >= self.batch_size:
            self.flush_batch()

    def flush_batch(self) -> None:
        """æ‰¹é‡å¤„ç†å¾…å¤„ç†çš„ä¸Šä¸‹æ–‡ã€‚"""
        if not self.pending_contexts:
            return

        start_time = time.time()

        # æ‰¹é‡å¤„ç†
        for inputs, outputs in self.pending_contexts:
            self.memory.save_context(inputs, outputs)

        batch_time = time.time() - start_time

        # æ›´æ–°ç»Ÿè®¡
        self._update_batch_stats(len(self.pending_contexts), batch_time)

        # æ¸…ç©ºé˜Ÿåˆ—
        self.pending_contexts.clear()
```

---

### 5.2 è®°å¿†å‹ç¼©å’Œæ¸…ç†

```mermaid
sequenceDiagram
    autonumber
    participant Scheduler as MemoryScheduler
    participant Analyzer as MemoryAnalyzer
    participant Buffer as ConversationBufferMemory
    participant Compressor as MemoryCompressor
    participant Cleaner as MemoryCleaner

    Scheduler->>Analyzer: å®šæœŸåˆ†æè®°å¿†ä½¿ç”¨æƒ…å†µ<br/>analyze_memory_usage()

    Analyzer->>Buffer: è·å–è®°å¿†ç»Ÿè®¡<br/>get_memory_stats()

    Buffer-->>Analyzer: {<br/>  "message_count": 1000,<br/>  "memory_size_mb": 50,<br/>  "oldest_message_age": 86400<br/>}

    Analyzer->>Analyzer: åˆ†æè®°å¿†å¥åº·çŠ¶æ€<br/>åˆ¤æ–­æ˜¯å¦éœ€è¦ä¼˜åŒ–

    alt å†…å­˜ä½¿ç”¨è¿‡é«˜
        Analyzer->>Compressor: å¯åŠ¨è®°å¿†å‹ç¼©<br/>compress_memory(strategy="summary")

        Compressor->>Compressor: é€‰æ‹©å‹ç¼©ç­–ç•¥<br/>- è½¬æ¢ä¸ºæ‘˜è¦è®°å¿†<br/>- åˆ é™¤å†—ä½™æ¶ˆæ¯<br/>- åˆå¹¶ç›¸ä¼¼å¯¹è¯

        Compressor->>Buffer: æ‰§è¡Œå‹ç¼©æ“ä½œ<br/>convert_to_summary_memory()

        Buffer->>Buffer: ç”Ÿæˆå¯¹è¯æ‘˜è¦<br/>æ¸…ç†åŸå§‹æ¶ˆæ¯

        Buffer-->>Compressor: å‹ç¼©å®Œæˆ<br/>å†…å­˜å‡å°‘80%

        Compressor-->>Analyzer: å‹ç¼©æ“ä½œå®Œæˆ

    else æ¶ˆæ¯è¿‡æœŸ
        Analyzer->>Cleaner: å¯åŠ¨è¿‡æœŸæ¸…ç†<br/>clean_expired_messages(max_age=7*24*3600)

        Cleaner->>Buffer: æ‰«æè¿‡æœŸæ¶ˆæ¯<br/>find_messages_older_than(max_age)

        Buffer-->>Cleaner: expired_messages = [msg1, msg2, ...]

        Cleaner->>Cleaner: è¯„ä¼°æ¶ˆæ¯é‡è¦æ€§<br/>ä¿ç•™é‡è¦çš„å†å²å¯¹è¯

        Cleaner->>Buffer: åˆ é™¤è¿‡æœŸæ¶ˆæ¯<br/>remove_messages(expired_messages)

        Buffer-->>Cleaner: æ¸…ç†å®Œæˆ

        Cleaner-->>Analyzer: è¿‡æœŸæ¸…ç†å®Œæˆ
    end

    Analyzer->>Analyzer: æ›´æ–°ä¼˜åŒ–ç»Ÿè®¡<br/>optimization_count += 1<br/>memory_saved = ...

    Analyzer-->>Scheduler: è®°å¿†ä¼˜åŒ–å®Œæˆ<br/>ä¼˜åŒ–æŠ¥å‘Š
```

**è®°å¿†ä¼˜åŒ–ç­–ç•¥**ï¼š

| è§¦å‘æ¡ä»¶ | ä¼˜åŒ–ç­–ç•¥ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|---------|---------|------|---------|
| å†…å­˜ > 100MB | è½¬æ¢ä¸ºæ‘˜è¦è®°å¿† | å‡å°‘90%å†…å­˜ | é•¿æœŸå¯¹è¯ |
| æ¶ˆæ¯ > 1000æ¡ | çª—å£æˆªæ–­ | ä¿æŒå›ºå®šå¤§å° | å®æ—¶å¯¹è¯ |
| æ¶ˆæ¯ > 7å¤© | è¿‡æœŸæ¸…ç† | åˆ é™¤æ— ç”¨å†å² | ä¸´æ—¶ä¼šè¯ |
| ç›¸ä¼¼åº¦ > 0.9 | å»é‡åˆå¹¶ | å‡å°‘å†—ä½™ | é‡å¤å¯¹è¯ |

---

## 6. é”™è¯¯å¤„ç†å’Œæ¢å¤åœºæ™¯

### 6.1 è®°å¿†æ•…éšœæ¢å¤æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant App
    participant Memory as ConversationSummaryMemory
    participant LLM
    participant ErrorHandler
    participant BackupMemory as ConversationBufferMemory
    participant Recovery as RecoveryManager

    App->>Memory: save_context(inputs, outputs)

    Memory->>LLM: è°ƒç”¨æ‘˜è¦ç”Ÿæˆ<br/>predict(summary_prompt)

    LLM-->>Memory: APIError("Rate limit exceeded")

    Memory->>ErrorHandler: å¤„ç†LLMè°ƒç”¨å¤±è´¥<br/>handle_llm_error(error)

    ErrorHandler->>ErrorHandler: åˆ†æé”™è¯¯ç±»å‹<br/>error_type = "rate_limit"

    alt å¯é‡è¯•é”™è¯¯
        ErrorHandler->>ErrorHandler: å®æ–½é€€é¿é‡è¯•<br/>retry_with_backoff(max_retries=3)

        loop é‡è¯•æœºåˆ¶
            ErrorHandler->>LLM: é‡æ–°è°ƒç”¨LLM<br/>wait_time = 2^attempt seconds

            alt é‡è¯•æˆåŠŸ
                LLM-->>ErrorHandler: æ‘˜è¦ç”ŸæˆæˆåŠŸ
                ErrorHandler-->>Memory: æ¢å¤æ­£å¸¸æ“ä½œ
                break
            else é‡è¯•å¤±è´¥
                ErrorHandler->>ErrorHandler: å¢åŠ ç­‰å¾…æ—¶é—´<br/>ç»§ç»­é‡è¯•
            end
        end

    else ä¸å¯é‡è¯•é”™è¯¯
        ErrorHandler->>Recovery: å¯åŠ¨é™çº§ç­–ç•¥<br/>fallback_to_buffer_memory()

        Recovery->>BackupMemory: åˆ‡æ¢åˆ°ç¼“å†²åŒºè®°å¿†<br/>ä¿å­˜å½“å‰å¯¹è¯

        BackupMemory->>BackupMemory: ç›´æ¥å­˜å‚¨æ¶ˆæ¯<br/>æ— éœ€LLMå¤„ç†

        BackupMemory-->>Recovery: å¤‡ç”¨å­˜å‚¨æˆåŠŸ

        Recovery->>Recovery: è®°å½•æ•…éšœä¿¡æ¯<br/>failure_log = {<br/>  "timestamp": now(),<br/>  "error_type": "llm_failure",<br/>  "fallback_used": "buffer_memory"<br/>}

        Recovery-->>ErrorHandler: é™çº§å¤„ç†å®Œæˆ
    end

    alt æ¢å¤æˆåŠŸ
        ErrorHandler-->>Memory: æ“ä½œå®Œæˆ
        Memory-->>App: save_contextæˆåŠŸ
    else å®Œå…¨å¤±è´¥
        ErrorHandler->>Recovery: å¯åŠ¨æ•°æ®æ¢å¤<br/>recover_from_backup()

        Recovery->>Recovery: ä»å¤‡ä»½æ¢å¤è®°å¿†çŠ¶æ€<br/>load_last_known_good_state()

        Recovery-->>ErrorHandler: æ¢å¤å®Œæˆï¼ˆå¯èƒ½ä¸¢å¤±éƒ¨åˆ†æ•°æ®ï¼‰

        ErrorHandler-->>Memory: è¿”å›é”™è¯¯ä¿¡æ¯
        Memory-->>App: MemoryException("è®°å¿†ç³»ç»Ÿæš‚æ—¶ä¸å¯ç”¨")
    end
```

**é”™è¯¯æ¢å¤ç­–ç•¥**ï¼š

```python
class MemoryErrorHandler:
    def __init__(self, memory: BaseMemory, backup_memory: Optional[BaseMemory] = None):
        self.memory = memory
        self.backup_memory = backup_memory or ConversationBufferMemory()
        self.error_stats = defaultdict(int)
        self.recovery_strategies = {
            "rate_limit": self._handle_rate_limit,
            "network_error": self._handle_network_error,
            "memory_full": self._handle_memory_full,
            "corruption": self._handle_corruption
        }

    def handle_error(self, error: Exception, operation: str, *args, **kwargs):
        """ç»Ÿä¸€é”™è¯¯å¤„ç†å…¥å£ã€‚"""
        error_type = self._classify_error(error)
        self.error_stats[error_type] += 1

        if error_type in self.recovery_strategies:
            return self.recovery_strategies[error_type](error, operation, *args, **kwargs)
        else:
            return self._handle_unknown_error(error, operation, *args, **kwargs)

    def _handle_rate_limit(self, error, operation, *args, **kwargs):
        """å¤„ç†APIé™æµé”™è¯¯ã€‚"""
        max_retries = 3
        base_delay = 1.0

        for attempt in range(max_retries):
            delay = base_delay * (2 ** attempt)
            time.sleep(delay)

            try:
                return getattr(self.memory, operation)(*args, **kwargs)
            except Exception as retry_error:
                if attempt == max_retries - 1:
                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨è®°å¿†
                    return self._fallback_to_backup(operation, *args, **kwargs)
                continue

    def _fallback_to_backup(self, operation, *args, **kwargs):
        """å›é€€åˆ°å¤‡ç”¨è®°å¿†ã€‚"""
        try:
            return getattr(self.backup_memory, operation)(*args, **kwargs)
        except Exception as backup_error:
            raise MemoryException(f"ä¸»è®°å¿†å’Œå¤‡ç”¨è®°å¿†éƒ½å¤±è´¥: {backup_error}")
```

---

## 7. æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†å±•ç¤ºäº† **Memory æ¨¡å—**çš„å…³é”®æ‰§è¡Œæ—¶åºï¼š

1. **åŸºç¡€è®°å¿†æ“ä½œ**ï¼šConversationBufferMemoryå’ŒConversationBufferWindowMemoryçš„å­˜å‚¨å’Œæ£€ç´¢æµç¨‹
2. **æ™ºèƒ½è®°å¿†å¤„ç†**ï¼šConversationSummaryMemoryçš„æ‘˜è¦ç”Ÿæˆå’ŒConversationEntityMemoryçš„å®ä½“æå–
3. **å‘é‡æ£€ç´¢è®°å¿†**ï¼šVectorStoreRetrieverMemoryçš„è¯­ä¹‰æ£€ç´¢å’Œç¼“å­˜æœºåˆ¶
4. **ç»„åˆè®°å¿†ååŒ**ï¼šCombinedMemoryçš„å¤šè®°å¿†ç±»å‹å¹¶è¡Œå¤„ç†
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ‰¹é‡æ“ä½œã€è®°å¿†å‹ç¼©å’Œæ¸…ç†çš„ä¼˜åŒ–ç­–ç•¥
6. **é”™è¯¯å¤„ç†**ï¼šè®°å¿†ç³»ç»Ÿçš„æ•…éšœæ¢å¤å’Œé™çº§å¤„ç†

æ¯å¼ æ—¶åºå›¾åŒ…å«ï¼š

- è¯¦ç»†çš„å‚ä¸è€…äº¤äº’è¿‡ç¨‹
- å…³é”®ç®—æ³•å’Œå¤„ç†é€»è¾‘
- æ€§èƒ½ä¼˜åŒ–ç‚¹å’Œç¼“å­˜ç­–ç•¥
- é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
- ç»Ÿè®¡ä¿¡æ¯æ”¶é›†å’Œç›‘æ§

è¿™äº›æ—¶åºå›¾å¸®åŠ©å¼€å‘è€…æ·±å…¥ç†è§£è®°å¿†ç³»ç»Ÿçš„å†…éƒ¨å·¥ä½œæœºåˆ¶ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€å¯é çš„å¯¹è¯è®°å¿†ç³»ç»Ÿæä¾›æŒ‡å¯¼ã€‚Memoryæ¨¡å—æ˜¯æ„å»ºæœ‰çŠ¶æ€å¯¹è¯åº”ç”¨çš„æ ¸å¿ƒç»„ä»¶ï¼Œæ­£ç¡®ç†è§£å…¶æ‰§è¡Œæµç¨‹å¯¹æé«˜å¯¹è¯è´¨é‡å’Œç³»ç»Ÿæ€§èƒ½è‡³å…³é‡è¦ã€‚

---
