---
title: "VoiceHelperæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ - æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§"
date: "2025-09-22T14:00:00+08:00"
draft: false
description: "VoiceHelperç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–ç­–ç•¥å’Œç›‘æ§ä½“ç³»ï¼Œæ¶µç›–æ€§èƒ½è°ƒä¼˜ã€ç›‘æ§ç³»ç»Ÿã€å‘Šè­¦æœºåˆ¶ç­‰å…³é”®æŠ€æœ¯"
slug: "voicehelper-performance-optimization"
author: "tommie blog"
categories: ["voicehelper", "AI", "æ€§èƒ½ä¼˜åŒ–"]
tags: ["VoiceHelper", "æ€§èƒ½ä¼˜åŒ–", "ç›‘æ§ç³»ç»Ÿ", "Prometheus", "Grafana", "æ€§èƒ½è°ƒä¼˜"]
showComments: false
toc: true
tocOpen: false
showReadingTime: true
showWordCount: true
pinned: true
weight: 10
# æ€§èƒ½ä¼˜åŒ–é…ç½®
paginated: true
lazyLoad: true
performanceOptimized: true
---

# VoiceHelperæ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»VoiceHelperæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ç³»ç»Ÿçš„æ€§èƒ½ä¼˜åŒ–ç­–ç•¥å’Œç›‘æ§ä½“ç³»ï¼Œæ¶µç›–æ€§èƒ½è°ƒä¼˜ã€ç›‘æ§ç³»ç»Ÿã€å‘Šè­¦æœºåˆ¶ç­‰å…³é”®æŠ€æœ¯ã€‚

## 8. æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

### 8.1 ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### 8.1.0 æ€§èƒ½ä¼˜åŒ–æ¶æ„æ€»è§ˆ

```mermaid
graph TB
    subgraph "å‰ç«¯æ€§èƒ½ä¼˜åŒ–"
        FE[å‰ç«¯åº”ç”¨]
        CDN[CDNåŠ é€Ÿ]
        Cache[æµè§ˆå™¨ç¼“å­˜]
        Compress[èµ„æºå‹ç¼©]
        LazyLoad[æ‡’åŠ è½½]
    end
    
    subgraph "åç«¯æ€§èƒ½ä¼˜åŒ–"
        BE[åç«¯æœåŠ¡]
        Pool[è¿æ¥æ± ]
        Async[å¼‚æ­¥å¤„ç†]
        Batch[æ‰¹é‡æ“ä½œ]
        Circuit[ç†”æ–­å™¨]
    end
    
    subgraph "æ•°æ®åº“ä¼˜åŒ–"
        DB[(æ•°æ®åº“)]
        Index[ç´¢å¼•ä¼˜åŒ–]
        Partition[åˆ†åŒºè¡¨]
        ReadReplica[è¯»å†™åˆ†ç¦»]
        QueryOpt[æŸ¥è¯¢ä¼˜åŒ–]
    end
    
    subgraph "ç¼“å­˜ä¼˜åŒ–"
        L1[L1æœ¬åœ°ç¼“å­˜]
        L2[L2åˆ†å¸ƒå¼ç¼“å­˜]
        Preload[ç¼“å­˜é¢„çƒ­]
        Eviction[ç¼“å­˜æ·˜æ±°]
    end
    
    subgraph "ç›‘æ§ç³»ç»Ÿ"
        Metrics[æŒ‡æ ‡æ”¶é›†]
        Alert[å‘Šè­¦ç³»ç»Ÿ]
        Dashboard[ç›‘æ§é¢æ¿]
        Trace[é“¾è·¯è¿½è¸ª]
    end
    
    FE --> CDN
    FE --> Cache
    FE --> Compress
    FE --> LazyLoad
    
    BE --> Pool
    BE --> Async
    BE --> Batch
    BE --> Circuit
    
    DB --> Index
    DB --> Partition
    DB --> ReadReplica
    DB --> QueryOpt
    
    BE --> L1
    BE --> L2
    L2 --> Preload
    L2 --> Eviction
    
    BE --> Metrics
    Metrics --> Alert
    Metrics --> Dashboard
    BE --> Trace
    
    style FE fill:#e1f5fe
    style BE fill:#f3e5f5
    style DB fill:#fff3e0
    style Metrics fill:#e8f5e8
```
  </div>
</div>

#### 8.1.1 å‰ç«¯æ€§èƒ½ä¼˜åŒ–

```typescript
// å‰ç«¯æ€§èƒ½ä¼˜åŒ–é…ç½®
// æ–‡ä»¶è·¯å¾„: frontend/next.config.js
const nextConfig = {
  // å¯ç”¨å®éªŒæ€§åŠŸèƒ½
  experimental: {
    optimizeCss: true,
    optimizePackageImports: ['@mui/material', 'lodash'],
  },
  
  // å‹ç¼©é…ç½®
  compress: true,
  
  // å›¾ç‰‡ä¼˜åŒ–
  images: {
    domains: ['cdn.voicehelper.com'],
    formats: ['image/webp', 'image/avif'],
    minimumCacheTTL: 60 * 60 * 24 * 30, // 30å¤©
  },
  
  // Webpackä¼˜åŒ–
  webpack: (config, { dev, isServer }) => {
    // ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–
    if (!dev && !isServer) {
      config.optimization.splitChunks = {
        chunks: 'all',
        cacheGroups: {
          vendor: {
            test: /[\\/]node_modules[\\/]/,
            name: 'vendors',
            chunks: 'all',
          },
          common: {
            name: 'common',
            minChunks: 2,
            chunks: 'all',
            enforce: true,
          },
        },
      }
    }
    
    return config
  },
  
  // å¤´éƒ¨ä¼˜åŒ–
  async headers() {
    return [
      {
        source: '/(.*)',
        headers: [
          {
            key: 'X-DNS-Prefetch-Control',
            value: 'on'
          },
          {
            key: 'X-Frame-Options',
            value: 'DENY'
          },
          {
            key: 'X-Content-Type-Options',
            value: 'nosniff'
          },
        ],
      },
      {
        source: '/static/(.*)',
        headers: [
          {
            key: 'Cache-Control',
            value: 'public, max-age=31536000, immutable',
          },
        ],
      },
    ]
  },
}

module.exports = nextConfig
```

```typescript
// å‰ç«¯æ€§èƒ½ç›‘æ§Hook
// æ–‡ä»¶è·¯å¾„: frontend/hooks/usePerformanceMonitor.ts
import { useEffect, useCallback } from 'react'

interface PerformanceMetrics {
  fcp: number // First Contentful Paint
  lcp: number // Largest Contentful Paint
  fid: number // First Input Delay
  cls: number // Cumulative Layout Shift
  ttfb: number // Time to First Byte
}

export function usePerformanceMonitor() {
  const reportMetrics = useCallback((metrics: Partial<PerformanceMetrics>) => {
    // å‘é€æ€§èƒ½æ•°æ®åˆ°ç›‘æ§æœåŠ¡
    fetch('/api/performance/metrics', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        ...metrics,
        timestamp: Date.now(),
        userAgent: navigator.userAgent,
        url: window.location.href,
      }),
    }).catch(console.error)
  }, [])

  useEffect(() => {
    // ç›‘æ§Core Web Vitals
    const observer = new PerformanceObserver((list) => {
      for (const entry of list.getEntries()) {
        switch (entry.entryType) {
          case 'paint':
            if (entry.name === 'first-contentful-paint') {
              reportMetrics({ fcp: entry.startTime })
            }
            break
          case 'largest-contentful-paint':
            reportMetrics({ lcp: entry.startTime })
            break
          case 'first-input':
            reportMetrics({ fid: entry.processingStart - entry.startTime })
            break
          case 'layout-shift':
            if (!entry.hadRecentInput) {
              reportMetrics({ cls: entry.value })
            }
            break
        }
      }
    })

    // ç›‘æ§ä¸åŒç±»å‹çš„æ€§èƒ½æŒ‡æ ‡
    observer.observe({ entryTypes: ['paint', 'largest-contentful-paint', 'first-input', 'layout-shift'] })

    // ç›‘æ§å¯¼èˆªæ€§èƒ½
    const navObserver = new PerformanceObserver((list) => {
      for (const entry of list.getEntries()) {
        if (entry.entryType === 'navigation') {
          const navEntry = entry as PerformanceNavigationTiming
          reportMetrics({
            ttfb: navEntry.responseStart - navEntry.requestStart,
          })
        }
      }
    })

    navObserver.observe({ entryTypes: ['navigation'] })

    return () => {
      observer.disconnect()
      navObserver.disconnect()
    }
  }, [reportMetrics])

  // ç›‘æ§èµ„æºåŠ è½½æ€§èƒ½
  useEffect(() => {
    const resourceObserver = new PerformanceObserver((list) => {
      for (const entry of list.getEntries()) {
        if (entry.entryType === 'resource') {
          const resourceEntry = entry as PerformanceResourceTiming
          
          // ç›‘æ§æ…¢èµ„æº
          if (resourceEntry.duration > 1000) {
            fetch('/api/performance/slow-resources', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                name: resourceEntry.name,
                duration: resourceEntry.duration,
                size: resourceEntry.transferSize,
                type: resourceEntry.initiatorType,
                timestamp: Date.now(),
              }),
            }).catch(console.error)
          }
        }
      }
    })

    resourceObserver.observe({ entryTypes: ['resource'] })

    return () => resourceObserver.disconnect()
  }, [])
}

// ç»„ä»¶çº§æ€§èƒ½ç›‘æ§
export function withPerformanceMonitoring<T extends object>(
  WrappedComponent: React.ComponentType<T>,
  componentName: string
) {
  return function PerformanceMonitoredComponent(props: T) {
    useEffect(() => {
      const startTime = performance.now()
      
      return () => {
        const endTime = performance.now()
        const renderTime = endTime - startTime
        
        // æŠ¥å‘Šç»„ä»¶æ¸²æŸ“æ—¶é—´
        if (renderTime > 16) { // è¶…è¿‡ä¸€å¸§çš„æ—¶é—´
          fetch('/api/performance/component-render', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              component: componentName,
              renderTime,
              timestamp: Date.now(),
            }),
          }).catch(console.error)
        }
      }
    }, [])

    return <WrappedComponent {...props} />
  }
}
```

#### 8.1.2 åç«¯æ€§èƒ½ä¼˜åŒ–

```go
// åç«¯æ€§èƒ½ä¼˜åŒ–ä¸­é—´ä»¶
// æ–‡ä»¶è·¯å¾„: backend/internal/middleware/performance.go
package middleware

import (
    "context"
    "runtime"
    "time"
    
    "github.com/gin-gonic/gin"
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    // HTTPè¯·æ±‚æŒ‡æ ‡
    httpRequestsTotal = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Total number of HTTP requests",
        },
        []string{"method", "endpoint", "status"},
    )
    
    httpRequestDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "http_request_duration_seconds",
            Help:    "HTTP request duration in seconds",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "endpoint"},
    )
    
    // ç³»ç»Ÿèµ„æºæŒ‡æ ‡
    memoryUsage = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "memory_usage_bytes",
            Help: "Memory usage in bytes",
        },
        []string{"type"},
    )
    
    goroutineCount = promauto.NewGauge(
        prometheus.GaugeOpts{
            Name: "goroutines_count",
            Help: "Number of goroutines",
        },
    )
)

// æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶
func PerformanceMonitoring() gin.HandlerFunc {
    return func(c *gin.Context) {
        start := time.Now()
        
        // å¤„ç†è¯·æ±‚
        c.Next()
        
        // è®°å½•æŒ‡æ ‡
        duration := time.Since(start)
        method := c.Request.Method
        endpoint := c.FullPath()
        status := c.Writer.Status()
        
        httpRequestsTotal.WithLabelValues(method, endpoint, string(rune(status))).Inc()
        httpRequestDuration.WithLabelValues(method, endpoint).Observe(duration.Seconds())
        
        // è®°å½•æ…¢è¯·æ±‚
        if duration > time.Second {
            logger.Warn("Slow request detected",
                "method", method,
                "endpoint", endpoint,
                "duration", duration,
                "status", status,
            )
        }
    }
}

// ç³»ç»Ÿèµ„æºç›‘æ§
func StartResourceMonitoring(ctx context.Context) {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            var m runtime.MemStats
            runtime.ReadMemStats(&m)
            
            // æ›´æ–°å†…å­˜æŒ‡æ ‡
            memoryUsage.WithLabelValues("heap_alloc").Set(float64(m.HeapAlloc))
            memoryUsage.WithLabelValues("heap_sys").Set(float64(m.HeapSys))
            memoryUsage.WithLabelValues("stack_sys").Set(float64(m.StackSys))
            
            // æ›´æ–°åç¨‹æ•°é‡
            goroutineCount.Set(float64(runtime.NumGoroutine()))
        }
    }
}

// è¿æ¥æ± ä¼˜åŒ–
type ConnectionPool struct {
    pool chan *Connection
    maxConnections int
    activeConnections int
    mu sync.RWMutex
}

func NewConnectionPool(maxConnections int) *ConnectionPool {
    return &ConnectionPool{
        pool: make(chan *Connection, maxConnections),
        maxConnections: maxConnections,
    }
}

func (cp *ConnectionPool) Get() (*Connection, error) {
    select {
    case conn := <-cp.pool:
        return conn, nil
    default:
        cp.mu.Lock()
        if cp.activeConnections < cp.maxConnections {
            cp.activeConnections++
            cp.mu.Unlock()
            return cp.createConnection()
        }
        cp.mu.Unlock()
        
        // ç­‰å¾…å¯ç”¨è¿æ¥
        select {
        case conn := <-cp.pool:
            return conn, nil
        case <-time.After(5 * time.Second):
            return nil, errors.New("connection pool timeout")
        }
    }
}

func (cp *ConnectionPool) Put(conn *Connection) {
    if conn.IsValid() {
        select {
        case cp.pool <- conn:
        default:
            // æ± å·²æ»¡ï¼Œå…³é—­è¿æ¥
            conn.Close()
            cp.mu.Lock()
            cp.activeConnections--
            cp.mu.Unlock()
        }
    } else {
        conn.Close()
        cp.mu.Lock()
        cp.activeConnections--
        cp.mu.Unlock()
    }
}
```

#### 8.1.3 æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–

```sql
-- PostgreSQLæ€§èƒ½ä¼˜åŒ–é…ç½®
-- æ–‡ä»¶è·¯å¾„: database/performance_optimization.sql

-- åˆ›å»ºç´¢å¼•ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_messages_session_created 
ON messages(session_id, created_at DESC);

CREATE INDEX CONCURRENTLY idx_documents_user_status 
ON documents(user_id, status) WHERE status IN ('completed', 'processing');

CREATE INDEX CONCURRENTLY idx_sessions_user_updated 
ON sessions(user_id, updated_at DESC);

-- åˆ†åŒºè¡¨ä¼˜åŒ–
CREATE TABLE messages_2024_01 PARTITION OF messages
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE messages_2024_02 PARTITION OF messages
FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- æŸ¥è¯¢ä¼˜åŒ–è§†å›¾
CREATE MATERIALIZED VIEW user_session_stats AS
SELECT 
    user_id,
    COUNT(*) as total_sessions,
    COUNT(*) FILTER (WHERE status = 'active') as active_sessions,
    MAX(updated_at) as last_activity,
    AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_session_duration
FROM sessions
GROUP BY user_id;

-- å®šæœŸåˆ·æ–°ç‰©åŒ–è§†å›¾
CREATE OR REPLACE FUNCTION refresh_user_session_stats()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY user_session_stats;
END;
$$ LANGUAGE plpgsql;

-- åˆ›å»ºå®šæ—¶ä»»åŠ¡
SELECT cron.schedule('refresh-stats', '0 */6 * * *', 'SELECT refresh_user_session_stats();');

-- æŸ¥è¯¢æ€§èƒ½ç›‘æ§
CREATE OR REPLACE FUNCTION log_slow_queries()
RETURNS event_trigger AS $$
BEGIN
    -- è®°å½•æ…¢æŸ¥è¯¢
    INSERT INTO slow_query_log (query, duration, timestamp)
    SELECT query, total_time, now()
    FROM pg_stat_statements
    WHERE total_time > 1000; -- è¶…è¿‡1ç§’çš„æŸ¥è¯¢
END;
$$ LANGUAGE plpgsql;

-- è‡ªåŠ¨VACUUMå’ŒANALYZE
ALTER TABLE messages SET (
    autovacuum_vacuum_scale_factor = 0.1,
    autovacuum_analyze_scale_factor = 0.05
);

ALTER TABLE sessions SET (
    autovacuum_vacuum_scale_factor = 0.2,
    autovacuum_analyze_scale_factor = 0.1
);
```

```go
// æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
// æ–‡ä»¶è·¯å¾„: backend/internal/database/pool.go
package database

import (
    "database/sql"
    "time"
    
    _ "github.com/lib/pq"
)

type DatabaseConfig struct {
    DSN             string
    MaxOpenConns    int
    MaxIdleConns    int
    ConnMaxLifetime time.Duration
    ConnMaxIdleTime time.Duration
}

func NewOptimizedDB(config *DatabaseConfig) (*sql.DB, error) {
    db, err := sql.Open("postgres", config.DSN)
    if err != nil {
        return nil, err
    }
    
    // è¿æ¥æ± ä¼˜åŒ–é…ç½®
    db.SetMaxOpenConns(config.MaxOpenConns)     // æœ€å¤§æ‰“å¼€è¿æ¥æ•°
    db.SetMaxIdleConns(config.MaxIdleConns)     // æœ€å¤§ç©ºé—²è¿æ¥æ•°
    db.SetConnMaxLifetime(config.ConnMaxLifetime) // è¿æ¥æœ€å¤§ç”Ÿå­˜æ—¶é—´
    db.SetConnMaxIdleTime(config.ConnMaxIdleTime) // è¿æ¥æœ€å¤§ç©ºé—²æ—¶é—´
    
    // æµ‹è¯•è¿æ¥
    if err := db.Ping(); err != nil {
        return nil, err
    }
    
    return db, nil
}

// æŸ¥è¯¢ä¼˜åŒ–å™¨
type QueryOptimizer struct {
    db *sql.DB
    cache map[string]*sql.Stmt
    mu sync.RWMutex
}

func NewQueryOptimizer(db *sql.DB) *QueryOptimizer {
    return &QueryOptimizer{
        db: db,
        cache: make(map[string]*sql.Stmt),
    }
}

func (qo *QueryOptimizer) PrepareQuery(name, query string) error {
    stmt, err := qo.db.Prepare(query)
    if err != nil {
        return err
    }
    
    qo.mu.Lock()
    qo.cache[name] = stmt
    qo.mu.Unlock()
    
    return nil
}

func (qo *QueryOptimizer) ExecuteQuery(name string, args ...interface{}) (*sql.Rows, error) {
    qo.mu.RLock()
    stmt, exists := qo.cache[name]
    qo.mu.RUnlock()
    
    if !exists {
        return nil, fmt.Errorf("prepared statement %s not found", name)
    }
    
    return stmt.Query(args...)
}

// æ‰¹é‡æ“ä½œä¼˜åŒ–
func (qo *QueryOptimizer) BatchInsert(table string, columns []string, values [][]interface{}) error {
    if len(values) == 0 {
        return nil
    }
    
    // æ„å»ºæ‰¹é‡æ’å…¥SQL
    placeholders := make([]string, len(values))
    args := make([]interface{}, 0, len(values)*len(columns))
    
    for i, row := range values {
        placeholder := make([]string, len(columns))
        for j := range columns {
            placeholder[j] = fmt.Sprintf("$%d", len(args)+j+1)
        }
        placeholders[i] = fmt.Sprintf("(%s)", strings.Join(placeholder, ","))
        args = append(args, row...)
    }
    
    query := fmt.Sprintf("INSERT INTO %s (%s) VALUES %s",
        table,
        strings.Join(columns, ","),
        strings.Join(placeholders, ","))
    
    _, err := qo.db.Exec(query, args...)
    return err
}
```

### 8.2 ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

#### 8.2.1 å¤šçº§ç¼“å­˜æ¶æ„

```go
// å¤šçº§ç¼“å­˜å®ç°
// æ–‡ä»¶è·¯å¾„: backend/internal/cache/multilevel_cache.go
package cache

import (
    "context"
    "encoding/json"
    "fmt"
    "sync"
    "time"
    
    "github.com/go-redis/redis/v8"
    lru "github.com/hashicorp/golang-lru"
)

type MultiLevelCache struct {
    l1Cache *lru.Cache      // æœ¬åœ°LRUç¼“å­˜
    l2Cache *redis.Client   // Redisåˆ†å¸ƒå¼ç¼“å­˜
    stats   *CacheStats
    mu      sync.RWMutex
}

type CacheStats struct {
    L1Hits   int64
    L1Misses int64
    L2Hits   int64
    L2Misses int64
}

func NewMultiLevelCache(l1Size int, redisClient *redis.Client) (*MultiLevelCache, error) {
    l1Cache, err := lru.New(l1Size)
    if err != nil {
        return nil, err
    }
    
    return &MultiLevelCache{
        l1Cache: l1Cache,
        l2Cache: redisClient,
        stats:   &CacheStats{},
    }, nil
}

func (mc *MultiLevelCache) Get(ctx context.Context, key string) (interface{}, error) {
    // å…ˆæŸ¥L1ç¼“å­˜
    if value, ok := mc.l1Cache.Get(key); ok {
        mc.mu.Lock()
        mc.stats.L1Hits++
        mc.mu.Unlock()
        return value, nil
    }
    
    mc.mu.Lock()
    mc.stats.L1Misses++
    mc.mu.Unlock()
    
    // æŸ¥L2ç¼“å­˜
    data, err := mc.l2Cache.Get(ctx, key).Result()
    if err == redis.Nil {
        mc.mu.Lock()
        mc.stats.L2Misses++
        mc.mu.Unlock()
        return nil, ErrCacheMiss
    } else if err != nil {
        return nil, err
    }
    
    mc.mu.Lock()
    mc.stats.L2Hits++
    mc.mu.Unlock()
    
    // ååºåˆ—åŒ–æ•°æ®
    var value interface{}
    if err := json.Unmarshal([]byte(data), &value); err != nil {
        return nil, err
    }
    
    // å›å¡«L1ç¼“å­˜
    mc.l1Cache.Add(key, value)
    
    return value, nil
}

func (mc *MultiLevelCache) Set(ctx context.Context, key string, value interface{}, expiration time.Duration) error {
    // è®¾ç½®L1ç¼“å­˜
    mc.l1Cache.Add(key, value)
    
    // åºåˆ—åŒ–æ•°æ®
    data, err := json.Marshal(value)
    if err != nil {
        return err
    }
    
    // è®¾ç½®L2ç¼“å­˜
    return mc.l2Cache.Set(ctx, key, data, expiration).Err()
}

func (mc *MultiLevelCache) Delete(ctx context.Context, key string) error {
    // åˆ é™¤L1ç¼“å­˜
    mc.l1Cache.Remove(key)
    
    // åˆ é™¤L2ç¼“å­˜
    return mc.l2Cache.Del(ctx, key).Err()
}

func (mc *MultiLevelCache) GetStats() CacheStats {
    mc.mu.RLock()
    defer mc.mu.RUnlock()
    return *mc.stats
}

// ç¼“å­˜é¢„çƒ­
func (mc *MultiLevelCache) Warmup(ctx context.Context, keys []string, loader func(string) (interface{}, error)) error {
    for _, key := range keys {
        // æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
        if _, err := mc.Get(ctx, key); err == nil {
            continue
        }
        
        // åŠ è½½æ•°æ®
        value, err := loader(key)
        if err != nil {
            continue
        }
        
        // è®¾ç½®ç¼“å­˜
        mc.Set(ctx, key, value, time.Hour)
    }
    
    return nil
}
```

#### 8.2.2 æ™ºèƒ½ç¼“å­˜ç­–ç•¥

```go
// æ™ºèƒ½ç¼“å­˜ç­–ç•¥
// æ–‡ä»¶è·¯å¾„: backend/internal/cache/smart_cache.go
package cache

import (
    "context"
    "hash/fnv"
    "math"
    "sync"
    "time"
)

type SmartCache struct {
    cache     *MultiLevelCache
    analytics *CacheAnalytics
    strategy  CacheStrategy
}

type CacheAnalytics struct {
    accessCount map[string]int64
    accessTime  map[string]time.Time
    hitRate     map[string]float64
    mu          sync.RWMutex
}

type CacheStrategy interface {
    ShouldCache(key string, value interface{}) bool
    GetTTL(key string, value interface{}) time.Duration
    GetPriority(key string) int
}

type AdaptiveCacheStrategy struct {
    analytics *CacheAnalytics
}

func (acs *AdaptiveCacheStrategy) ShouldCache(key string, value interface{}) bool {
    acs.analytics.mu.RLock()
    defer acs.analytics.mu.RUnlock()
    
    // åŸºäºè®¿é—®é¢‘ç‡å†³å®šæ˜¯å¦ç¼“å­˜
    accessCount := acs.analytics.accessCount[key]
    hitRate := acs.analytics.hitRate[key]
    
    // è®¿é—®æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ä¸”å‘½ä¸­ç‡é«˜äº50%æ‰ç¼“å­˜
    return accessCount > 5 && hitRate > 0.5
}

func (acs *AdaptiveCacheStrategy) GetTTL(key string, value interface{}) time.Duration {
    acs.analytics.mu.RLock()
    defer acs.analytics.mu.RUnlock()
    
    accessCount := acs.analytics.accessCount[key]
    lastAccess := acs.analytics.accessTime[key]
    
    // åŸºäºè®¿é—®é¢‘ç‡å’Œæœ€è¿‘è®¿é—®æ—¶é—´è®¡ç®—TTL
    baseTTL := time.Hour
    
    // è®¿é—®é¢‘ç‡è¶Šé«˜ï¼ŒTTLè¶Šé•¿
    frequencyMultiplier := math.Log(float64(accessCount + 1))
    
    // æœ€è¿‘è®¿é—®æ—¶é—´è¶Šè¿‘ï¼ŒTTLè¶Šé•¿
    timeSinceAccess := time.Since(lastAccess)
    timeMultiplier := math.Max(0.1, 1.0 - timeSinceAccess.Hours()/24.0)
    
    ttl := time.Duration(float64(baseTTL) * frequencyMultiplier * timeMultiplier)
    
    // é™åˆ¶TTLèŒƒå›´
    if ttl < time.Minute {
        ttl = time.Minute
    } else if ttl > time.Hour*24 {
        ttl = time.Hour * 24
    }
    
    return ttl
}

func (acs *AdaptiveCacheStrategy) GetPriority(key string) int {
    acs.analytics.mu.RLock()
    defer acs.analytics.mu.RUnlock()
    
    accessCount := acs.analytics.accessCount[key]
    hitRate := acs.analytics.hitRate[key]
    
    // åŸºäºè®¿é—®é¢‘ç‡å’Œå‘½ä¸­ç‡è®¡ç®—ä¼˜å…ˆçº§
    return int(float64(accessCount) * hitRate)
}

// ç¼“å­˜åˆ†ç‰‡
type ShardedCache struct {
    shards []*MultiLevelCache
    count  int
}

func NewShardedCache(shardCount, l1Size int, redisClients []*redis.Client) (*ShardedCache, error) {
    if len(redisClients) != shardCount {
        return nil, fmt.Errorf("redis clients count mismatch")
    }
    
    shards := make([]*MultiLevelCache, shardCount)
    for i := 0; i < shardCount; i++ {
        cache, err := NewMultiLevelCache(l1Size, redisClients[i])
        if err != nil {
            return nil, err
        }
        shards[i] = cache
    }
    
    return &ShardedCache{
        shards: shards,
        count:  shardCount,
    }, nil
}

func (sc *ShardedCache) getShard(key string) *MultiLevelCache {
    h := fnv.New32a()
    h.Write([]byte(key))
    return sc.shards[h.Sum32()%uint32(sc.count)]
}

func (sc *ShardedCache) Get(ctx context.Context, key string) (interface{}, error) {
    return sc.getShard(key).Get(ctx, key)
}

func (sc *ShardedCache) Set(ctx context.Context, key string, value interface{}, expiration time.Duration) error {
    return sc.getShard(key).Set(ctx, key, value, expiration)
}

func (sc *ShardedCache) Delete(ctx context.Context, key string) error {
    return sc.getShard(key).Delete(ctx, key)
}
```

### 8.3 ç›‘æ§ç³»ç»Ÿå®ç°

#### 8.3.0 ç›‘æ§ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    subgraph "æ•°æ®é‡‡é›†å±‚"
        App[åº”ç”¨æœåŠ¡]
        NodeExporter[Node Exporter]
        DBExporter[DB Exporter]
        RedisExporter[Redis Exporter]
    end
    
    subgraph "æ•°æ®å­˜å‚¨å±‚"
        Prometheus[(Prometheus<br/>æ—¶åºæ•°æ®åº“)]
        InfluxDB[(InfluxDB<br/>å¤‡ç”¨å­˜å‚¨)]
    end
    
    subgraph "å¯è§†åŒ–å±‚"
        Grafana[Grafana<br/>ç›‘æ§é¢æ¿]
        Kibana[Kibana<br/>æ—¥å¿—åˆ†æ]
    end
    
    subgraph "å‘Šè­¦å±‚"
        AlertManager[AlertManager<br/>å‘Šè­¦ç®¡ç†]
        DingTalk[é’‰é’‰é€šçŸ¥]
        Email[é‚®ä»¶é€šçŸ¥]
        SMS[çŸ­ä¿¡é€šçŸ¥]
    end
    
    subgraph "æ—¥å¿—ç³»ç»Ÿ"
        Filebeat[Filebeat<br/>æ—¥å¿—é‡‡é›†]
        Logstash[Logstash<br/>æ—¥å¿—å¤„ç†]
        Elasticsearch[(Elasticsearch<br/>æ—¥å¿—å­˜å‚¨)]
    end
    
    App --> Prometheus
    NodeExporter --> Prometheus
    DBExporter --> Prometheus
    RedisExporter --> Prometheus
    
    Prometheus --> Grafana
    Prometheus --> AlertManager
    
    AlertManager --> DingTalk
    AlertManager --> Email
    AlertManager --> SMS
    
    App --> Filebeat
    Filebeat --> Logstash
    Logstash --> Elasticsearch
    Elasticsearch --> Kibana
    
    style Prometheus fill:#e1f5fe
    style Grafana fill:#f3e5f5
    style AlertManager fill:#fff3e0
    style Elasticsearch fill:#e8f5e8
```
  </div>
</div>

#### 8.3.1 Prometheusç›‘æ§é…ç½®

```yaml
# Prometheusé…ç½®æ–‡ä»¶
# æ–‡ä»¶è·¯å¾„: monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # VoiceHelperåç«¯æœåŠ¡
  - job_name: 'voicehelper-backend'
    static_configs:
      - targets: ['backend:8080']
    metrics_path: /metrics
    scrape_interval: 10s
    
  # VoiceHelper AIæœåŠ¡
  - job_name: 'voicehelper-ai'
    static_configs:
      - targets: ['ai-service:8000']
    metrics_path: /metrics
    scrape_interval: 15s
    
  # PostgreSQLç›‘æ§
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    
  # Redisç›‘æ§
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    
  # Milvusç›‘æ§
  - job_name: 'milvus'
    static_configs:
      - targets: ['milvus:9091']
    
  # ç³»ç»Ÿç›‘æ§
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    
  # Nginxç›‘æ§
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']

# è¿œç¨‹å†™å…¥é…ç½®ï¼ˆå¯é€‰ï¼‰
remote_write:
  - url: "https://prometheus-remote-write.example.com/api/v1/write"
    basic_auth:
      username: "user"
      password: "password"
```

```yaml
# å‘Šè­¦è§„åˆ™é…ç½®
# æ–‡ä»¶è·¯å¾„: monitoring/alert_rules.yml
groups:
  - name: voicehelper.rules
    rules:
      # æœåŠ¡å¯ç”¨æ€§å‘Šè­¦
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "Service {{ $labels.job }} has been down for more than 1 minute."
      
      # é«˜é”™è¯¯ç‡å‘Šè­¦
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second for {{ $labels.job }}."
      
      # é«˜å»¶è¿Ÿå‘Šè­¦
      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "95th percentile latency is {{ $value }}s for {{ $labels.job }}."
      
      # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is above 80% (current value: {{ $value }}%)"
      
      # CPUä½¿ç”¨ç‡å‘Šè­¦
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is above 80% (current value: {{ $value }}%)"
      
      # ç£ç›˜ç©ºé—´å‘Šè­¦
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Disk space low"
          description: "Disk space is below 10% (current value: {{ $value }}%)"
      
      # æ•°æ®åº“è¿æ¥æ•°å‘Šè­¦
      - alert: DatabaseConnectionsHigh
        expr: pg_stat_database_numbackends > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High database connections"
          description: "Database connections are above 80 (current value: {{ $value }})"
      
      # Rediså†…å­˜ä½¿ç”¨å‘Šè­¦
      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory usage high"
          description: "Redis memory usage is above 80% (current value: {{ $value }}%)"
```

#### 8.3.2 Grafanaä»ªè¡¨ç›˜é…ç½®

```json
{
  "dashboard": {
    "id": null,
    "title": "VoiceHelper System Overview",
    "tags": ["voicehelper", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ],
        "yAxes": [
          {
            "label": "Requests/sec"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 0
        }
      },
      {
        "id": 2,
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ],
        "yAxes": [
          {
            "label": "Seconds"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 0
        }
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "singlestat",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m]) * 100"
          }
        ],
        "valueName": "current",
        "format": "percent",
        "thresholds": "1,5",
        "colorBackground": true,
        "gridPos": {
          "h": 4,
          "w": 6,
          "x": 0,
          "y": 8
        }
      },
      {
        "id": 4,
        "title": "Active Sessions",
        "type": "singlestat",
        "targets": [
          {
            "expr": "active_sessions_total"
          }
        ],
        "valueName": "current",
        "format": "short",
        "gridPos": {
          "h": 4,
          "w": 6,
          "x": 6,
          "y": 8
        }
      },
      {
        "id": 5,
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "memory_usage_bytes{type=\"heap_alloc\"}",
            "legendFormat": "Heap Allocated"
          },
          {
            "expr": "memory_usage_bytes{type=\"heap_sys\"}",
            "legendFormat": "Heap System"
          }
        ],
        "yAxes": [
          {
            "label": "Bytes",
            "logBase": 1
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 8
        }
      },
      {
        "id": 6,
        "title": "Database Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "pg_stat_database_tup_fetched",
            "legendFormat": "Tuples Fetched"
          },
          {
            "expr": "pg_stat_database_tup_inserted",
            "legendFormat": "Tuples Inserted"
          },
          {
            "expr": "pg_stat_database_tup_updated",
            "legendFormat": "Tuples Updated"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 24,
          "x": 0,
          "y": 16
        }
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "5s"
  }
}
```

#### 8.3.3 è‡ªå®šä¹‰ç›‘æ§æŒ‡æ ‡

```go
// è‡ªå®šä¹‰ç›‘æ§æŒ‡æ ‡
// æ–‡ä»¶è·¯å¾„: backend/internal/metrics/custom_metrics.go
package metrics

import (
    "time"
    
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    // ä¸šåŠ¡æŒ‡æ ‡
    ActiveSessions = promauto.NewGauge(prometheus.GaugeOpts{
        Name: "active_sessions_total",
        Help: "Total number of active sessions",
    })
    
    MessageProcessingTime = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "message_processing_duration_seconds",
            Help:    "Time spent processing messages",
            Buckets: []float64{0.1, 0.5, 1.0, 2.0, 5.0, 10.0},
        },
        []string{"message_type"},
    )
    
    RAGRetrievalTime = promauto.NewHistogram(prometheus.HistogramOpts{
        Name:    "rag_retrieval_duration_seconds",
        Help:    "Time spent on RAG retrieval",
        Buckets: []float64{0.01, 0.05, 0.1, 0.2, 0.5, 1.0},
    })
    
    VectorSearchLatency = promauto.NewHistogram(prometheus.HistogramOpts{
        Name:    "vector_search_duration_seconds",
        Help:    "Vector search latency",
        Buckets: []float64{0.001, 0.005, 0.01, 0.05, 0.1, 0.2},
    })
    
    LLMTokensUsed = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "llm_tokens_used_total",
            Help: "Total number of LLM tokens used",
        },
        []string{"model", "user_tier"},
    )
    
    CacheHitRate = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "cache_hit_rate",
            Help: "Cache hit rate by cache level",
        },
        []string{"level"},
    )
    
    DocumentProcessingQueue = promauto.NewGauge(prometheus.GaugeOpts{
        Name: "document_processing_queue_size",
        Help: "Number of documents in processing queue",
    })
    
    UserActivityRate = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "user_activity_rate",
            Help: "User activity rate by time period",
        },
        []string{"period"},
    )
)

// æŒ‡æ ‡æ”¶é›†å™¨
type MetricsCollector struct {
    sessionManager *SessionManager
    cacheManager   *CacheManager
    queueManager   *QueueManager
}

func NewMetricsCollector(sm *SessionManager, cm *CacheManager, qm *QueueManager) *MetricsCollector {
    return &MetricsCollector{
        sessionManager: sm,
        cacheManager:   cm,
        queueManager:   qm,
    }
}

func (mc *MetricsCollector) StartCollection() {
    ticker := time.NewTicker(30 * time.Second)
    go func() {
        for range ticker.C {
            mc.collectMetrics()
        }
    }()
}

func (mc *MetricsCollector) collectMetrics() {
    // æ”¶é›†æ´»è·ƒä¼šè¯æ•°
    ActiveSessions.Set(float64(mc.sessionManager.GetActiveSessionCount()))
    
    // æ”¶é›†ç¼“å­˜å‘½ä¸­ç‡
    l1Stats := mc.cacheManager.GetL1Stats()
    l2Stats := mc.cacheManager.GetL2Stats()
    
    if l1Stats.Total > 0 {
        CacheHitRate.WithLabelValues("l1").Set(float64(l1Stats.Hits) / float64(l1Stats.Total))
    }
    
    if l2Stats.Total > 0 {
        CacheHitRate.WithLabelValues("l2").Set(float64(l2Stats.Hits) / float64(l2Stats.Total))
    }
    
    // æ”¶é›†é˜Ÿåˆ—å¤§å°
    DocumentProcessingQueue.Set(float64(mc.queueManager.GetQueueSize("document_processing")))
    
    // æ”¶é›†ç”¨æˆ·æ´»è·ƒåº¦
    hourlyActive := mc.sessionManager.GetActiveUsersInPeriod(time.Hour)
    dailyActive := mc.sessionManager.GetActiveUsersInPeriod(24 * time.Hour)
    
    UserActivityRate.WithLabelValues("hourly").Set(float64(hourlyActive))
    UserActivityRate.WithLabelValues("daily").Set(float64(dailyActive))
}

// æ€§èƒ½è¿½è¸ªè£…é¥°å™¨
func TrackPerformance(metric prometheus.Observer) func(func()) {
    return func(fn func()) {
        start := time.Now()
        defer func() {
            metric.Observe(time.Since(start).Seconds())
        }()
        fn()
    }
}

// ä½¿ç”¨ç¤ºä¾‹
func ProcessMessage(messageType string, processFn func()) {
    timer := MessageProcessingTime.WithLabelValues(messageType)
    TrackPerformance(timer)(processFn)
}
```

### 8.4 å‘Šè­¦ç³»ç»Ÿå®ç°

#### 8.4.1 AlertManageré…ç½®

```yaml
# AlertManageré…ç½®æ–‡ä»¶
# æ–‡ä»¶è·¯å¾„: monitoring/alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@voicehelper.com'
  smtp_auth_username: 'alerts@voicehelper.com'
  smtp_auth_password: 'your-app-password'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default'
  routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 5s
      repeat_interval: 30m
    - match:
        severity: warning
      receiver: 'warning-alerts'
      repeat_interval: 2h

receivers:
  - name: 'default'
    email_configs:
      - to: 'team@voicehelper.com'
        subject: '[VoiceHelper] Alert: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
          {{ end }}

  - name: 'critical-alerts'
    email_configs:
      - to: 'oncall@voicehelper.com'
        subject: '[CRITICAL] VoiceHelper Alert: {{ .GroupLabels.alertname }}'
        body: |
          ğŸš¨ CRITICAL ALERT ğŸš¨
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Severity: {{ .Labels.severity }}
          Service: {{ .Labels.job }}
          Time: {{ .StartsAt }}
          {{ end }}
    webhook_configs:
      - url: 'http://dingtalk-webhook:8080/webhook'
        send_resolved: true

  - name: 'warning-alerts'
    email_configs:
      - to: 'team@voicehelper.com'
        subject: '[WARNING] VoiceHelper Alert: {{ .GroupLabels.alertname }}'
        body: |
          âš ï¸ Warning Alert
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
```

#### 8.4.2 æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ

```go
// æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ
// æ–‡ä»¶è·¯å¾„: backend/internal/alerting/smart_alerting.go
package alerting

import (
    "context"
    "fmt"
    "math"
    "sync"
    "time"
)

type AlertLevel int

const (
    AlertLevelInfo AlertLevel = iota
    AlertLevelWarning
    AlertLevelCritical
)

type Alert struct {
    ID          string
    Name        string
    Level       AlertLevel
    Message     string
    Labels      map[string]string
    Annotations map[string]string
    StartsAt    time.Time
    EndsAt      time.Time
    Resolved    bool
}

type AlertRule struct {
    Name        string
    Expression  string
    Duration    time.Duration
    Level       AlertLevel
    Annotations map[string]string
}

type SmartAlertManager struct {
    rules           []AlertRule
    activeAlerts    map[string]*Alert
    alertHistory    []*Alert
    suppressions    map[string]time.Time
    escalationRules map[AlertLevel][]NotificationChannel
    mu              sync.RWMutex
}

type NotificationChannel interface {
    Send(alert *Alert) error
    GetName() string
}

func NewSmartAlertManager() *SmartAlertManager {
    return &SmartAlertManager{
        activeAlerts:    make(map[string]*Alert),
        suppressions:    make(map[string]time.Time),
        escalationRules: make(map[AlertLevel][]NotificationChannel),
    }
}

func (sam *SmartAlertManager) AddRule(rule AlertRule) {
    sam.mu.Lock()
    defer sam.mu.Unlock()
    sam.rules = append(sam.rules, rule)
}

func (sam *SmartAlertManager) AddNotificationChannel(level AlertLevel, channel NotificationChannel) {
    sam.mu.Lock()
    defer sam.mu.Unlock()
    sam.escalationRules[level] = append(sam.escalationRules[level], channel)
}

func (sam *SmartAlertManager) EvaluateRules(ctx context.Context, metrics map[string]float64) {
    sam.mu.Lock()
    defer sam.mu.Unlock()
    
    for _, rule := range sam.rules {
        if sam.evaluateExpression(rule.Expression, metrics) {
            sam.triggerAlert(rule)
        } else {
            sam.resolveAlert(rule.Name)
        }
    }
}

func (sam *SmartAlertManager) triggerAlert(rule AlertRule) {
    alertID := rule.Name
    
    // æ£€æŸ¥æ˜¯å¦è¢«æŠ‘åˆ¶
    if suppressUntil, exists := sam.suppressions[alertID]; exists {
        if time.Now().Before(suppressUntil) {
            return
        }
        delete(sam.suppressions, alertID)
    }
    
    // æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨æ´»è·ƒå‘Šè­¦
    if existingAlert, exists := sam.activeAlerts[alertID]; exists {
        // æ›´æ–°å‘Šè­¦æ—¶é—´
        existingAlert.EndsAt = time.Now().Add(rule.Duration)
        return
    }
    
    // åˆ›å»ºæ–°å‘Šè­¦
    alert := &Alert{
        ID:          alertID,
        Name:        rule.Name,
        Level:       rule.Level,
        Message:     rule.Annotations["summary"],
        Labels:      map[string]string{"alertname": rule.Name},
        Annotations: rule.Annotations,
        StartsAt:    time.Now(),
        EndsAt:      time.Now().Add(rule.Duration),
        Resolved:    false,
    }
    
    sam.activeAlerts[alertID] = alert
    sam.alertHistory = append(sam.alertHistory, alert)
    
    // å‘é€é€šçŸ¥
    go sam.sendNotifications(alert)
}

func (sam *SmartAlertManager) resolveAlert(alertName string) {
    if alert, exists := sam.activeAlerts[alertName]; exists {
        alert.Resolved = true
        alert.EndsAt = time.Now()
        delete(sam.activeAlerts, alertName)
        
        // å‘é€è§£å†³é€šçŸ¥
        go sam.sendResolutionNotification(alert)
    }
}

func (sam *SmartAlertManager) sendNotifications(alert *Alert) {
    channels := sam.escalationRules[alert.Level]
    
    for _, channel := range channels {
        if err := channel.Send(alert); err != nil {
            fmt.Printf("Failed to send alert via %s: %v\n", channel.GetName(), err)
        }
    }
    
    // æ™ºèƒ½æŠ‘åˆ¶ï¼šæ ¹æ®å‘Šè­¦é¢‘ç‡åŠ¨æ€è°ƒæ•´æŠ‘åˆ¶æ—¶é—´
    suppressDuration := sam.calculateSuppressionDuration(alert)
    sam.suppressions[alert.ID] = time.Now().Add(suppressDuration)
}

func (sam *SmartAlertManager) calculateSuppressionDuration(alert *Alert) time.Duration {
    // ç»Ÿè®¡æœ€è¿‘1å°æ—¶å†…åŒç±»å‘Šè­¦çš„é¢‘ç‡
    count := 0
    oneHourAgo := time.Now().Add(-time.Hour)
    
    for _, historyAlert := range sam.alertHistory {
        if historyAlert.Name == alert.Name && historyAlert.StartsAt.After(oneHourAgo) {
            count++
        }
    }
    
    // åŸºäºé¢‘ç‡è®¡ç®—æŠ‘åˆ¶æ—¶é—´
    baseDuration := 5 * time.Minute
    if count > 10 {
        // é«˜é¢‘å‘Šè­¦ï¼Œå»¶é•¿æŠ‘åˆ¶æ—¶é—´
        return time.Duration(math.Min(float64(baseDuration)*math.Pow(1.5, float64(count-10)), float64(time.Hour)))
    }
    
    return baseDuration
}

func (sam *SmartAlertManager) evaluateExpression(expression string, metrics map[string]float64) bool {
    // ç®€åŒ–çš„è¡¨è¾¾å¼æ±‚å€¼å™¨
    // å®é™…å®ç°ä¸­åº”è¯¥ä½¿ç”¨æ›´å®Œå–„çš„è¡¨è¾¾å¼è§£æå™¨
    switch expression {
    case "cpu_usage > 80":
        return metrics["cpu_usage"] > 80
    case "memory_usage > 80":
        return metrics["memory_usage"] > 80
    case "error_rate > 0.05":
        return metrics["error_rate"] > 0.05
    case "response_time > 1":
        return metrics["response_time"] > 1
    default:
        return false
    }
}

// å‘Šè­¦èšåˆ
func (sam *SmartAlertManager) AggregateAlerts() map[string][]*Alert {
    sam.mu.RLock()
    defer sam.mu.RUnlock()
    
    aggregated := make(map[string][]*Alert)
    
    for _, alert := range sam.activeAlerts {
        key := fmt.Sprintf("%s_%s", alert.Labels["service"], alert.Level)
        aggregated[key] = append(aggregated[key], alert)
    }
    
    return aggregated
}

// å‘Šè­¦ç»Ÿè®¡
func (sam *SmartAlertManager) GetAlertStats() map[string]interface{} {
    sam.mu.RLock()
    defer sam.mu.RUnlock()
    
    stats := map[string]interface{}{
        "active_alerts":    len(sam.activeAlerts),
        "total_alerts":     len(sam.alertHistory),
        "suppressed_alerts": len(sam.suppressions),
    }
    
    // æŒ‰çº§åˆ«ç»Ÿè®¡
    levelCounts := make(map[AlertLevel]int)
    for _, alert := range sam.activeAlerts {
        levelCounts[alert.Level]++
    }
    
    stats["by_level"] = levelCounts
    
    return stats
}
```

## ç›¸å…³æ–‡æ¡£

- [ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ](/posts/voicehelper-architecture-overview/)
- [å‰ç«¯æ¨¡å—æ·±åº¦è§£æ](/posts/voicehelper-frontend-modules/)
- [åç«¯æœåŠ¡æ ¸å¿ƒå®ç°](/posts/voicehelper-backend-services/)
- [AIç®—æ³•å¼•æ“æ·±åº¦åˆ†æ](/posts/voicehelper-ai-algorithms/)
- [æ•°æ®å­˜å‚¨æ¶æ„](/posts/voicehelper-data-storage/)
- [ç³»ç»Ÿäº¤äº’æ—¶åºå›¾](/posts/voicehelper-system-interactions/)
- [ç¬¬ä¸‰æ–¹é›†æˆä¸æ‰©å±•](/posts/voicehelper-third-party-integration/)
- [éƒ¨ç½²ä¸è¿ç»´](/posts/voicehelper-deployment-operations/)
- [æ€»ç»“ä¸æœ€ä½³å®è·µ](/posts/voicehelper-best-practices/)
- [é¡¹ç›®åŠŸèƒ½æ¸…å•](/posts/voicehelper-feature-inventory/)
- [ç‰ˆæœ¬è¿­ä»£å†ç¨‹](/posts/voicehelper-version-history/)
- [ç«äº‰åŠ›åˆ†æ](/posts/voicehelper-competitive-analysis/)
- [APIæ¥å£æ¸…å•](/posts/voicehelper-api-reference/)
- [é”™è¯¯ç ç³»ç»Ÿ](/posts/voicehelper-error-codes/)
- [ç‰ˆæœ¬è¿­ä»£è®¡åˆ’](/posts/voicehelper-version-roadmap/)
