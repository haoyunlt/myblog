---
title: "Kubernetes Scheduler è¯¦ç»†æºç å‰–æ"
date: 2025-09-28T00:47:17+08:00
draft: false
tags: ['å®¹å™¨ç¼–æ’', 'DevOps', 'Go', 'æºç åˆ†æ', 'Kubernetes']
categories: ['å®¹å™¨ç¼–æ’']
description: "Kubernetes Scheduler è¯¦ç»†æºç å‰–æçš„æ·±å…¥æŠ€æœ¯åˆ†ææ–‡æ¡£"
keywords: ['å®¹å™¨ç¼–æ’', 'DevOps', 'Go', 'æºç åˆ†æ', 'Kubernetes']
author: "æŠ€æœ¯åˆ†æå¸ˆ"
weight: 1
---

## ğŸ“š æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£æ·±å…¥åˆ†æ Kubernetes Scheduler çš„æ¶æ„è®¾è®¡ã€æºç å®ç°å’Œè°ƒåº¦ç®—æ³•ã€‚Scheduler æ˜¯ Kubernetes é›†ç¾¤çš„"è°ƒåº¦å¤§è„‘"ï¼Œè´Ÿè´£å°† Pod è°ƒåº¦åˆ°æœ€åˆé€‚çš„èŠ‚ç‚¹ä¸Šï¼Œæ˜¯å®ç°èµ„æºä¼˜åŒ–é…ç½®å’Œè´Ÿè½½å‡è¡¡çš„æ ¸å¿ƒç»„ä»¶ã€‚

## ğŸ—ï¸ Scheduler æ•´ä½“æ¶æ„

### 1.1 è°ƒåº¦å™¨æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "kube-scheduler æ¶æ„"
        subgraph "è°ƒåº¦æ ¸å¿ƒ (Scheduling Core)"
            SCHED[Scheduler<br/>è°ƒåº¦å™¨æ ¸å¿ƒ]
            QUEUE[Scheduling Queue<br/>è°ƒåº¦é˜Ÿåˆ—]
            CACHE[Node Cache<br/>èŠ‚ç‚¹ç¼“å­˜]
            SNAPSHOT[Cache Snapshot<br/>ç¼“å­˜å¿«ç…§]
        end
        
        subgraph "è°ƒåº¦æ¡†æ¶ (Scheduling Framework)"
            FRAMEWORK[Framework<br/>è°ƒåº¦æ¡†æ¶]
            PROFILES[Profiles<br/>è°ƒåº¦é…ç½®]
            PLUGINS[Plugins<br/>æ’ä»¶ç³»ç»Ÿ]
            EXTENDERS[Extenders<br/>æ‰©å±•å™¨]
        end
        
        subgraph "è°ƒåº¦é˜¶æ®µ (Scheduling Phases)"
            SORT[Sort<br/>æ’åºé˜¶æ®µ]
            PREFILTER[PreFilter<br/>é¢„è¿‡æ»¤é˜¶æ®µ]
            FILTER[Filter<br/>è¿‡æ»¤é˜¶æ®µ]
            PRESCORE[PreScore<br/>é¢„æ‰“åˆ†é˜¶æ®µ]
            SCORE[Score<br/>æ‰“åˆ†é˜¶æ®µ]
            RESERVE[Reserve<br/>é¢„ç•™é˜¶æ®µ]
            PERMIT[Permit<br/>è®¸å¯é˜¶æ®µ]
            PREBIND[PreBind<br/>é¢„ç»‘å®šé˜¶æ®µ]
            BIND[Bind<br/>ç»‘å®šé˜¶æ®µ]
            POSTBIND[PostBind<br/>åç»‘å®šé˜¶æ®µ]
        end
        
        subgraph "èµ„æºç®¡ç† (Resource Management)"
            NODE_INFO[Node Info<br/>èŠ‚ç‚¹ä¿¡æ¯]
            POD_INFO[Pod Info<br/>Pod ä¿¡æ¯]
            RESOURCE_TRACKER[Resource Tracker<br/>èµ„æºè·Ÿè¸ªå™¨]
            AFFINITY[Affinity Manager<br/>äº²å’Œæ€§ç®¡ç†å™¨]
        end
        
        subgraph "å¤–éƒ¨æ¥å£ (External Interfaces)"
            API_CLIENT[API Client<br/>API å®¢æˆ·ç«¯]
            INFORMERS[Informers<br/>é€šçŸ¥å™¨]
            EVENT_RECORDER[Event Recorder<br/>äº‹ä»¶è®°å½•å™¨]
            METRICS[Metrics<br/>æŒ‡æ ‡æ”¶é›†]
        end
    end
    
    %% æ ¸å¿ƒæµç¨‹
    SCHED --> QUEUE
    QUEUE --> FRAMEWORK
    FRAMEWORK --> PROFILES
    PROFILES --> PLUGINS
    
    %% è°ƒåº¦é˜¶æ®µæµç¨‹
    SORT --> PREFILTER
    PREFILTER --> FILTER
    FILTER --> PRESCORE
    PRESCORE --> SCORE
    SCORE --> RESERVE
    RESERVE --> PERMIT
    PERMIT --> PREBIND
    PREBIND --> BIND
    BIND --> POSTBIND
    
    %% æ•°æ®æµ
    CACHE --> SNAPSHOT
    SNAPSHOT --> FRAMEWORK
    NODE_INFO --> CACHE
    POD_INFO --> QUEUE
    
    %% å¤–éƒ¨äº¤äº’
    API_CLIENT --> INFORMERS
    INFORMERS --> CACHE
    FRAMEWORK --> EVENT_RECORDER
    FRAMEWORK --> METRICS
    
    %% æ ·å¼å®šä¹‰
    classDef core fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef framework fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef phases fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef resource fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef external fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    
    class SCHED,QUEUE,CACHE,SNAPSHOT core
    class FRAMEWORK,PROFILES,PLUGINS,EXTENDERS framework
    class SORT,PREFILTER,FILTER,PRESCORE,SCORE,RESERVE,PERMIT,PREBIND,BIND,POSTBIND phases
    class NODE_INFO,POD_INFO,RESOURCE_TRACKER,AFFINITY resource
    class API_CLIENT,INFORMERS,EVENT_RECORDER,METRICS external
```

### 1.2 è°ƒåº¦æµç¨‹æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant Queue as è°ƒåº¦é˜Ÿåˆ—
    participant Scheduler as è°ƒåº¦å™¨
    participant Framework as è°ƒåº¦æ¡†æ¶
    participant Plugins as æ’ä»¶ç³»ç»Ÿ
    participant Cache as èŠ‚ç‚¹ç¼“å­˜
    participant API as API Server

    Note over Queue,API: Pod è°ƒåº¦å®Œæ•´æµç¨‹

    Queue->>+Scheduler: 1. è·å–å¾…è°ƒåº¦ Pod
    Scheduler->>Scheduler: 2. é€‰æ‹©è°ƒåº¦æ¡†æ¶
    Scheduler->>+Cache: 3. è·å–é›†ç¾¤å¿«ç…§
    Cache-->>-Scheduler: è¿”å›èŠ‚ç‚¹ä¿¡æ¯å¿«ç…§

    Scheduler->>+Framework: 4. å¼€å§‹è°ƒåº¦å‘¨æœŸ
    
    %% Sort é˜¶æ®µ
    Framework->>+Plugins: 5. Sort: é˜Ÿåˆ—æ’åº
    Plugins-->>-Framework: æ’åºå®Œæˆ
    
    %% PreFilter é˜¶æ®µ
    Framework->>+Plugins: 6. PreFilter: é¢„è¿‡æ»¤æ£€æŸ¥
    Plugins->>Plugins: æ£€æŸ¥ Pod è§„æ ¼å’Œçº¦æŸ
    Plugins-->>-Framework: é¢„è¿‡æ»¤ç»“æœ
    
    %% Filter é˜¶æ®µ
    Framework->>+Plugins: 7. Filter: èŠ‚ç‚¹è¿‡æ»¤
    loop éå†æ‰€æœ‰èŠ‚ç‚¹
        Plugins->>Plugins: æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æ»¡è¶³è¦æ±‚
        Plugins->>Plugins: èµ„æºã€äº²å’Œæ€§ã€æ±¡ç‚¹æ£€æŸ¥
    end
    Plugins-->>-Framework: å¯è°ƒåº¦èŠ‚ç‚¹åˆ—è¡¨
    
    %% PreScore é˜¶æ®µ
    Framework->>+Plugins: 8. PreScore: é¢„æ‰“åˆ†å‡†å¤‡
    Plugins->>Plugins: å‡†å¤‡æ‰“åˆ†æ‰€éœ€æ•°æ®
    Plugins-->>-Framework: é¢„æ‰“åˆ†å®Œæˆ
    
    %% Score é˜¶æ®µ
    Framework->>+Plugins: 9. Score: èŠ‚ç‚¹æ‰“åˆ†
    loop éå†å¯è°ƒåº¦èŠ‚ç‚¹
        Plugins->>Plugins: è®¡ç®—èŠ‚ç‚¹åˆ†æ•°
        Plugins->>Plugins: èµ„æºåˆ©ç”¨ç‡ã€äº²å’Œæ€§æƒé‡
    end
    Plugins-->>-Framework: èŠ‚ç‚¹åˆ†æ•°åˆ—è¡¨
    
    Framework->>Framework: 10. é€‰æ‹©æœ€é«˜åˆ†èŠ‚ç‚¹
    
    %% Reserve é˜¶æ®µ
    Framework->>+Plugins: 11. Reserve: èµ„æºé¢„ç•™
    Plugins->>Cache: æ›´æ–°èŠ‚ç‚¹èµ„æºçŠ¶æ€
    Plugins-->>-Framework: é¢„ç•™æˆåŠŸ
    
    %% Permit é˜¶æ®µ
    Framework->>+Plugins: 12. Permit: è°ƒåº¦è®¸å¯
    Plugins->>Plugins: æ£€æŸ¥è°ƒåº¦è®¸å¯æ¡ä»¶
    Plugins-->>-Framework: è®¸å¯é€šè¿‡
    
    %% PreBind é˜¶æ®µ
    Framework->>+Plugins: 13. PreBind: é¢„ç»‘å®šæ“ä½œ
    Plugins->>API: æ‰§è¡Œé¢„ç»‘å®šæ“ä½œ
    Plugins-->>-Framework: é¢„ç»‘å®šå®Œæˆ
    
    %% Bind é˜¶æ®µ
    Framework->>+Plugins: 14. Bind: ç»‘å®š Pod åˆ°èŠ‚ç‚¹
    Plugins->>+API: æ›´æ–° Pod.spec.nodeName
    API-->>-Plugins: ç»‘å®šæˆåŠŸ
    Plugins-->>-Framework: ç»‘å®šå®Œæˆ
    
    %% PostBind é˜¶æ®µ
    Framework->>+Plugins: 15. PostBind: åç»‘å®šæ“ä½œ
    Plugins->>Plugins: æ‰§è¡Œæ¸…ç†å’Œåç»­æ“ä½œ
    Plugins-->>-Framework: åç»‘å®šå®Œæˆ
    
    Framework-->>-Scheduler: è°ƒåº¦å‘¨æœŸå®Œæˆ
    Scheduler-->>-Queue: Pod è°ƒåº¦å®Œæˆ

    Note over Queue,API: Pod æˆåŠŸè°ƒåº¦åˆ°ç›®æ ‡èŠ‚ç‚¹
```

## ğŸš€ å¯åŠ¨æµç¨‹è¯¦ç»†åˆ†æ

### 2.1 Scheduler å¯åŠ¨å…¥å£

```go
// cmd/kube-scheduler/scheduler.go
/*
Scheduler ä¸»å…¥å£æ–‡ä»¶
è´Ÿè´£åˆå§‹åŒ–å’Œå¯åŠ¨è°ƒåº¦å™¨æœåŠ¡

ä¸»è¦èŒè´£ï¼š
1. åˆ›å»º Cobra å‘½ä»¤å¯¹è±¡
2. è§£æå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®
3. å¯åŠ¨è°ƒåº¦å™¨ä¸»å¾ªç¯
*/
package main

import (
    "os"

    "k8s.io/component-base/cli"
    _ "k8s.io/component-base/logs/json/register" // JSON æ—¥å¿—æ ¼å¼æ³¨å†Œ
    _ "k8s.io/component-base/metrics/prometheus/clientgo"
    _ "k8s.io/component-base/metrics/prometheus/version" // ç‰ˆæœ¬æŒ‡æ ‡æ³¨å†Œ
    "k8s.io/kubernetes/cmd/kube-scheduler/app"
)

/*
main å‡½æ•°æ˜¯ Scheduler çš„ç¨‹åºå…¥å£ç‚¹

æ‰§è¡Œæµç¨‹ï¼š
1. åˆ›å»ºè°ƒåº¦å™¨å‘½ä»¤å¯¹è±¡
2. é€šè¿‡ CLI æ¡†æ¶æ‰§è¡Œå‘½ä»¤
3. æ ¹æ®æ‰§è¡Œç»“æœé€€å‡ºç¨‹åº

è¿”å›å€¼ï¼š
- ç¨‹åºé€€å‡ºç ï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼Œé 0 è¡¨ç¤ºå¤±è´¥ï¼‰
*/
func main() {
    // åˆ›å»ºè°ƒåº¦å™¨å‘½ä»¤å¯¹è±¡
    command := app.NewSchedulerCommand()
    
    // æ‰§è¡Œå‘½ä»¤ï¼Œå¯åŠ¨è°ƒåº¦å™¨
    code := cli.Run(command)
    
    // æ ¹æ®æ‰§è¡Œç»“æœé€€å‡ºç¨‹åº
    os.Exit(code)
}
```

### 2.2 è°ƒåº¦å™¨æ ¸å¿ƒç»“æ„

```go
// pkg/scheduler/scheduler.go
/*
Scheduler ç»“æ„ä½“å®šä¹‰äº†è°ƒåº¦å™¨çš„æ ¸å¿ƒç»„ä»¶å’ŒåŠŸèƒ½

ä¸»è¦åŠŸèƒ½ï¼š
1. ç›‘å¬æœªè°ƒåº¦çš„ Pod
2. ä¸º Pod å¯»æ‰¾åˆé€‚çš„èŠ‚ç‚¹
3. å°†ç»‘å®šä¿¡æ¯å†™å› API Server
4. å¤„ç†è°ƒåº¦å¤±è´¥å’Œé‡è¯•
*/

/*
Scheduler è°ƒåº¦å™¨æ ¸å¿ƒç»“æ„ä½“

å­—æ®µè¯´æ˜ï¼š
- Cache: èŠ‚ç‚¹å’Œ Pod ä¿¡æ¯çš„æœ¬åœ°ç¼“å­˜
- Extenders: å¤–éƒ¨è°ƒåº¦æ‰©å±•å™¨åˆ—è¡¨
- NextPod: è·å–ä¸‹ä¸€ä¸ªå¾…è°ƒåº¦ Pod çš„å‡½æ•°
- FailureHandler: è°ƒåº¦å¤±è´¥å¤„ç†å‡½æ•°
- SchedulePod: Pod è°ƒåº¦æ ¸å¿ƒå‡½æ•°
- SchedulingQueue: è°ƒåº¦é˜Ÿåˆ—ï¼Œå­˜å‚¨å¾…è°ƒåº¦çš„ Pod
- Profiles: è°ƒåº¦é…ç½®æ–‡ä»¶æ˜ å°„
- client: Kubernetes API å®¢æˆ·ç«¯
- percentageOfNodesToScore: å‚ä¸æ‰“åˆ†çš„èŠ‚ç‚¹ç™¾åˆ†æ¯”
*/
type Scheduler struct {
    // é¢„æœŸé€šè¿‡ Cache è¿›è¡Œçš„æ›´æ”¹å°†è¢« NodeLister å’Œ Algorithm è§‚å¯Ÿåˆ°
    Cache internalcache.Cache

    // å¤–éƒ¨è°ƒåº¦æ‰©å±•å™¨
    Extenders []fwk.Extender

    // NextPod åº”è¯¥æ˜¯ä¸€ä¸ªé˜»å¡å‡½æ•°ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ª Pod å¯ç”¨
    // æˆ‘ä»¬ä¸ä½¿ç”¨é€šé“ï¼Œå› ä¸ºè°ƒåº¦ä¸€ä¸ª Pod å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œ
    // æˆ‘ä»¬ä¸å¸Œæœ› Pod åœ¨é€šé“ä¸­ç­‰å¾…æ—¶å˜å¾—é™ˆæ—§
    NextPod func(logger klog.Logger) (*framework.QueuedPodInfo, error)

    // FailureHandler åœ¨è°ƒåº¦å¤±è´¥æ—¶è¢«è°ƒç”¨
    FailureHandler FailureHandlerFn

    // SchedulePod å°è¯•å°†ç»™å®šçš„ Pod è°ƒåº¦åˆ°èŠ‚ç‚¹åˆ—è¡¨ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹
    // æˆåŠŸæ—¶è¿”å›å¸¦æœ‰å»ºè®®ä¸»æœºåç§°çš„ ScheduleResult ç»“æ„ï¼Œ
    // å¦åˆ™å°†è¿”å›å¸¦æœ‰åŸå› çš„ FitError
    SchedulePod func(ctx context.Context, fwk framework.Framework, state fwk.CycleState, pod *v1.Pod) (ScheduleResult, error)

    // å…³é—­æ­¤é€šé“ä»¥å…³é—­è°ƒåº¦å™¨
    StopEverything <-chan struct{}

    // SchedulingQueue ä¿å­˜è¦è°ƒåº¦çš„ Pod
    SchedulingQueue internalqueue.SchedulingQueue

    // API è°ƒåº¦å™¨ï¼Œç”¨äºå¼‚æ­¥ API è°ƒç”¨
    APIDispatcher *apidispatcher.APIDispatcher

    // Profiles æ˜¯è°ƒåº¦é…ç½®æ–‡ä»¶
    Profiles profile.Map

    // Kubernetes API å®¢æˆ·ç«¯
    client clientset.Interface

    // èŠ‚ç‚¹ä¿¡æ¯å¿«ç…§
    nodeInfoSnapshot *internalcache.Snapshot

    // å‚ä¸æ‰“åˆ†çš„èŠ‚ç‚¹ç™¾åˆ†æ¯”
    percentageOfNodesToScore int32

    // ä¸‹ä¸€ä¸ªå¼€å§‹èŠ‚ç‚¹ç´¢å¼•ï¼ˆç”¨äºè´Ÿè½½å‡è¡¡ï¼‰
    nextStartNodeIndex int

    // æ—¥å¿—è®°å½•å™¨ï¼Œåˆ›å»º Scheduler æ—¶å¿…é¡»åˆå§‹åŒ–
    logger klog.Logger

    // å·²æ³¨å†Œçš„å¤„ç†å™¨ï¼Œç”¨äºæ£€æŸ¥æ‰€æœ‰å¤„ç†å™¨æ˜¯å¦å·²å®ŒæˆåŒæ­¥
    registeredHandlers []cache.ResourceEventHandlerRegistration

    // æ˜¯å¦å¯ç”¨æåèŠ‚ç‚¹åç§°æœŸæœ›åŠŸèƒ½
    nominatedNodeNameForExpectationEnabled bool
}

/*
applyDefaultHandlers åº”ç”¨é»˜è®¤çš„å¤„ç†å™¨

åŠŸèƒ½è¯´æ˜ï¼š
1. è®¾ç½®é»˜è®¤çš„ Pod è°ƒåº¦å‡½æ•°
2. è®¾ç½®é»˜è®¤çš„å¤±è´¥å¤„ç†å‡½æ•°
3. åˆå§‹åŒ–è°ƒåº¦å™¨çš„æ ¸å¿ƒåŠŸèƒ½
*/
func (sched *Scheduler) applyDefaultHandlers() {
    // è®¾ç½®é»˜è®¤çš„ Pod è°ƒåº¦å‡½æ•°
    sched.SchedulePod = sched.schedulePod
    
    // è®¾ç½®é»˜è®¤çš„å¤±è´¥å¤„ç†å‡½æ•°
    sched.FailureHandler = sched.handleSchedulingFailure
}
```

### 2.3 è°ƒåº¦å™¨è¿è¡Œä¸»å¾ªç¯

```go
/*
Run å¯åŠ¨è°ƒåº¦å™¨çš„ä¸»è¿è¡Œå¾ªç¯

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼Œç”¨äºæ§åˆ¶ç”Ÿå‘½å‘¨æœŸ

å·¥ä½œæµç¨‹ï¼š
1. ç­‰å¾…ç¼“å­˜åŒæ­¥å®Œæˆ
2. å¯åŠ¨è°ƒåº¦ä¸»å¾ªç¯
3. å¤„ç†åœæ­¢ä¿¡å·
4. æ¸…ç†èµ„æº
*/
func (sched *Scheduler) Run(ctx context.Context) {
    // ç­‰å¾…æ‰€æœ‰äº‹ä»¶å¤„ç†å™¨å®ŒæˆåŒæ­¥
    sched.logger.Info("ç­‰å¾…ç¼“å­˜åŒæ­¥")
    if !cache.WaitForCacheSync(ctx.Done(), sched.registeredHandlers...) {
        sched.logger.Error(nil, "æ— æ³•åŒæ­¥ç¼“å­˜")
        return
    }
    sched.logger.Info("ç¼“å­˜åŒæ­¥å®Œæˆï¼Œå¼€å§‹è°ƒåº¦")

    // å¯åŠ¨è°ƒåº¦ä¸»å¾ªç¯
    wait.UntilWithContext(ctx, sched.ScheduleOne, 0)
}

/*
ScheduleOne æ‰§è¡Œå•ä¸ª Pod çš„å®Œæ•´è°ƒåº¦å·¥ä½œæµç¨‹

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡

å·¥ä½œæµç¨‹ï¼š
1. ä»é˜Ÿåˆ—è·å–å¾…è°ƒåº¦çš„ Pod
2. é€‰æ‹©åˆé€‚çš„è°ƒåº¦æ¡†æ¶
3. æ‰§è¡Œè°ƒåº¦ç®—æ³•
4. å¤„ç†è°ƒåº¦ç»“æœ
5. æ›´æ–°è°ƒåº¦é˜Ÿåˆ—çŠ¶æ€
*/
func (sched *Scheduler) ScheduleOne(ctx context.Context) {
    logger := klog.FromContext(ctx)
    
    // 1. ä»è°ƒåº¦é˜Ÿåˆ—è·å–ä¸‹ä¸€ä¸ª Pod
    podInfo, err := sched.NextPod(logger)
    if err != nil {
        utilruntime.HandleErrorWithContext(ctx, err, "ä»è°ƒåº¦é˜Ÿåˆ—æ£€ç´¢ä¸‹ä¸€ä¸ª Pod æ—¶å‡ºé”™")
        return
    }
    
    // Pod å¯èƒ½ä¸º nilï¼Œå½“ schedulerQueue å…³é—­æ—¶
    if podInfo == nil || podInfo.Pod == nil {
        return
    }

    pod := podInfo.Pod
    logger = klog.LoggerWithValues(logger, "pod", klog.KObj(pod))
    ctx = klog.NewContext(ctx, logger)
    logger.V(4).Info("å‡†å¤‡å°è¯•è°ƒåº¦ Pod", "pod", klog.KObj(pod))

    // 2. ä¸º Pod é€‰æ‹©è°ƒåº¦æ¡†æ¶
    fwk, err := sched.frameworkForPod(pod)
    if err != nil {
        // è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºæˆ‘ä»¬åªæ¥å—è°ƒåº¦é‚£äº›æŒ‡å®šäº†
        // ä¸æŸä¸ªé…ç½®æ–‡ä»¶åŒ¹é…çš„è°ƒåº¦å™¨åç§°çš„ Pod
        logger.Error(err, "å‘ç”Ÿé”™è¯¯")
        sched.SchedulingQueue.Done(pod.UID)
        return
    }
    
    // 3. æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ­¤ Pod çš„è°ƒåº¦
    if sched.skipPodSchedule(ctx, fwk, pod) {
        // æˆ‘ä»¬ä¸å°†æ­¤ Pod æ”¾å›é˜Ÿåˆ—ï¼Œä½†å¿…é¡»æ¸…ç†æ­£åœ¨å¤„ç†çš„ Pod/äº‹ä»¶
        sched.SchedulingQueue.Done(pod.UID)
        return
    }

    logger.V(3).Info("å°è¯•è°ƒåº¦ Pod", "pod", klog.KObj(pod))

    // 4. æ‰§è¡Œè°ƒåº¦ç®—æ³•
    start := time.Now()
    state := framework.NewCycleState()
    state.SetRecordPluginMetrics(rand.Intn(100) < pluginMetricsSamplePercent)

    // åˆå§‹åŒ– Pod çš„è°ƒåº¦ä¸Šä¸‹æ–‡
    schedulingCycleCtx, cancel := context.WithCancel(ctx)
    defer cancel()

    // æ‰§è¡Œè°ƒåº¦
    scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)
    if err != nil {
        // 5. å¤„ç†è°ƒåº¦å¤±è´¥
        sched.handleSchedulingFailure(ctx, fwk, podInfo, err, v1.PodReasonUnschedulable, clearNominatedNode)
        return
    }
    
    // 6. è®°å½•è°ƒåº¦æˆåŠŸçš„æŒ‡æ ‡
    metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start))
    metrics.DeprecatedSchedulingAlgorithmLatency.Observe(metrics.SinceInMicroseconds(start))

    // 7. å‘Šè¯‰ç¼“å­˜å‡è®¾ä¸€ä¸ª Pod ç°åœ¨æ­£åœ¨è¿è¡Œåœ¨ç»™å®šèŠ‚ç‚¹ä¸Šï¼Œ
    // å³ä½¿å®ƒè¿˜æ²¡æœ‰è¢«ç»‘å®š
    // è¿™å…è®¸æˆ‘ä»¬åœ¨ç­‰å¾…ç»‘å®šå‘ç”Ÿæ—¶ç»§ç»­è°ƒåº¦ï¼Œè€Œä¸ä¼šè¿‡åº¦æäº¤èµ„æº
    assumedPodInfo := podInfo.DeepCopy()
    assumedPod := assumedPodInfo.Pod
    
    // å‡è®¾ï¼šåœ¨å®é™…ç»‘å®šä¹‹å‰ï¼Œæˆ‘ä»¬å‡è®¾ Pod å·²ç»è¿è¡Œåœ¨é€‰å®šçš„èŠ‚ç‚¹ä¸Š
    err = sched.assume(assumedPod, scheduleResult.SuggestedHost)
    if err != nil {
        // è¿™é€šå¸¸ä¸ä¼šå‘ç”Ÿï¼Œå› ä¸ºæˆ‘ä»¬åªæ˜¯ä»ç¼“å­˜ä¸­è·å–äº†èŠ‚ç‚¹
        // ä½†æ˜¯å¦‚æœèŠ‚ç‚¹è¢«åˆ é™¤äº†ï¼Œå¯èƒ½ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µ
        logger.Error(err, "è°ƒåº¦å™¨ç¼“å­˜ä¸­å‡è®¾ Pod å¤±è´¥")
        sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, err, v1.PodReasonSchedulerError, clearNominatedNode)
        return
    }

    // 8. å¼‚æ­¥è¿è¡Œ "permit" æ’ä»¶
    runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
    if !runPermitStatus.IsWait() && !runPermitStatus.IsSuccess() {
        var reason string
        if runPermitStatus.IsUnschedulable() {
            reason = v1.PodReasonUnschedulable
        } else {
            reason = v1.PodReasonSchedulerError
        }
        
        // æ’¤é”€å‡è®¾ï¼Œå› ä¸º permit æ’ä»¶æ‹’ç»äº†è°ƒåº¦
        if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
            logger.Error(forgetErr, "è°ƒåº¦å™¨ç¼“å­˜ä¸­å¿˜è®° Pod å¤±è´¥")
        }
        
        sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, runPermitStatus.AsError(), reason, clearNominatedNode)
        return
    }

    // 9. å¼‚æ­¥ç»‘å®š Pod åˆ°èŠ‚ç‚¹
    go func() {
        bindingCycleCtx, cancel := context.WithCancel(ctx)
        defer cancel()

        waitOnPermitStatus := fwk.WaitOnPermit(bindingCycleCtx, assumedPod)
        if !waitOnPermitStatus.IsSuccess() {
            var reason string
            if waitOnPermitStatus.IsUnschedulable() {
                reason = v1.PodReasonUnschedulable
            } else {
                reason = v1.PodReasonSchedulerError
            }
            
            // æ’¤é”€å‡è®¾
            if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
                logger.Error(forgetErr, "è°ƒåº¦å™¨ç¼“å­˜ä¸­å¿˜è®° Pod å¤±è´¥")
            }
            
            sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, waitOnPermitStatus.AsError(), reason, clearNominatedNode)
            return
        }

        // æ‰§è¡Œç»‘å®šæ“ä½œ
        err := sched.bind(bindingCycleCtx, fwk, assumedPod, scheduleResult.SuggestedHost, state)
        if err != nil {
            // ç»‘å®šå¤±è´¥ï¼Œæ’¤é”€å‡è®¾
            if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
                logger.Error(forgetErr, "è°ƒåº¦å™¨ç¼“å­˜ä¸­å¿˜è®° Pod å¤±è´¥")
            }
            
            sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, err, v1.PodReasonSchedulerError, clearNominatedNode)
            return
        }

        // ç»‘å®šæˆåŠŸï¼Œå®Œæˆè°ƒåº¦
        logger.V(2).Info("æˆåŠŸç»‘å®š Pod åˆ°èŠ‚ç‚¹", "pod", klog.KObj(pod), "node", scheduleResult.SuggestedHost)
    }()
}
```

## ğŸ¯ è°ƒåº¦ç®—æ³•æ ¸å¿ƒå®ç°

### 3.1 è°ƒåº¦ç®—æ³•æ¶æ„

```mermaid
graph TB
    subgraph "è°ƒåº¦ç®—æ³•æ¶æ„"
        subgraph "è°ƒåº¦ç­–ç•¥ (Scheduling Policies)"
            POLICY[Scheduling Policy<br/>è°ƒåº¦ç­–ç•¥]
            PREDICATES[Predicates<br/>æ–­è¨€å‡½æ•°]
            PRIORITIES[Priorities<br/>ä¼˜å…ˆçº§å‡½æ•°]
        end
        
        subgraph "è¿‡æ»¤é˜¶æ®µ (Filtering Phase)"
            NODE_FILTER[Node Filter<br/>èŠ‚ç‚¹è¿‡æ»¤å™¨]
            RESOURCE_FIT[Resource Fit<br/>èµ„æºé€‚é…]
            NODE_AFFINITY[Node Affinity<br/>èŠ‚ç‚¹äº²å’Œæ€§]
            POD_AFFINITY[Pod Affinity<br/>Pod äº²å’Œæ€§]
            TAINTS_TOLERATIONS[Taints & Tolerations<br/>æ±¡ç‚¹ä¸å®¹å¿]
            VOLUME_BINDING[Volume Binding<br/>å­˜å‚¨å·ç»‘å®š]
        end
        
        subgraph "æ‰“åˆ†é˜¶æ®µ (Scoring Phase)"
            NODE_SCORER[Node Scorer<br/>èŠ‚ç‚¹æ‰“åˆ†å™¨]
            RESOURCE_SCORE[Resource Score<br/>èµ„æºåˆ†æ•°]
            AFFINITY_SCORE[Affinity Score<br/>äº²å’Œæ€§åˆ†æ•°]
            IMAGE_LOCALITY[Image Locality<br/>é•œåƒæœ¬åœ°æ€§]
            TOPOLOGY_SPREAD[Topology Spread<br/>æ‹“æ‰‘åˆ†å¸ƒ]
        end
        
        subgraph "é€‰æ‹©é˜¶æ®µ (Selection Phase)"
            SCORE_NORMALIZE[Score Normalize<br/>åˆ†æ•°å½’ä¸€åŒ–]
            WEIGHT_CALCULATE[Weight Calculate<br/>æƒé‡è®¡ç®—]
            FINAL_SCORE[Final Score<br/>æœ€ç»ˆåˆ†æ•°]
            NODE_SELECT[Node Select<br/>èŠ‚ç‚¹é€‰æ‹©]
        end
    end
    
    %% ç­–ç•¥åˆ°è¿‡æ»¤
    POLICY --> PREDICATES
    PREDICATES --> NODE_FILTER
    
    %% è¿‡æ»¤é˜¶æ®µæµç¨‹
    NODE_FILTER --> RESOURCE_FIT
    NODE_FILTER --> NODE_AFFINITY
    NODE_FILTER --> POD_AFFINITY
    NODE_FILTER --> TAINTS_TOLERATIONS
    NODE_FILTER --> VOLUME_BINDING
    
    %% ç­–ç•¥åˆ°æ‰“åˆ†
    POLICY --> PRIORITIES
    PRIORITIES --> NODE_SCORER
    
    %% æ‰“åˆ†é˜¶æ®µæµç¨‹
    NODE_SCORER --> RESOURCE_SCORE
    NODE_SCORER --> AFFINITY_SCORE
    NODE_SCORER --> IMAGE_LOCALITY
    NODE_SCORER --> TOPOLOGY_SPREAD
    
    %% é€‰æ‹©é˜¶æ®µæµç¨‹
    RESOURCE_SCORE --> SCORE_NORMALIZE
    AFFINITY_SCORE --> SCORE_NORMALIZE
    IMAGE_LOCALITY --> SCORE_NORMALIZE
    TOPOLOGY_SPREAD --> SCORE_NORMALIZE
    
    SCORE_NORMALIZE --> WEIGHT_CALCULATE
    WEIGHT_CALCULATE --> FINAL_SCORE
    FINAL_SCORE --> NODE_SELECT
    
    %% æ ·å¼å®šä¹‰
    classDef policy fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef filtering fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef scoring fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef selection fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    
    class POLICY,PREDICATES,PRIORITIES policy
    class NODE_FILTER,RESOURCE_FIT,NODE_AFFINITY,POD_AFFINITY,TAINTS_TOLERATIONS,VOLUME_BINDING filtering
    class NODE_SCORER,RESOURCE_SCORE,AFFINITY_SCORE,IMAGE_LOCALITY,TOPOLOGY_SPREAD scoring
    class SCORE_NORMALIZE,WEIGHT_CALCULATE,FINAL_SCORE,NODE_SELECT selection
```

### 3.2 è°ƒåº¦ç®—æ³•æ ¸å¿ƒå®ç°

```go
// pkg/scheduler/schedule_one.go
/*
schedulePod æ˜¯è°ƒåº¦å•ä¸ª Pod çš„æ ¸å¿ƒç®—æ³•å®ç°

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- fwk: è°ƒåº¦æ¡†æ¶
- state: è°ƒåº¦å‘¨æœŸçŠ¶æ€
- pod: å¾…è°ƒåº¦çš„ Pod

è¿”å›å€¼ï¼š
- ScheduleResult: è°ƒåº¦ç»“æœï¼ŒåŒ…å«å»ºè®®çš„èŠ‚ç‚¹
- error: è°ƒåº¦è¿‡ç¨‹ä¸­çš„é”™è¯¯

è°ƒåº¦æµç¨‹ï¼š
1. è¿è¡Œ PreFilter æ’ä»¶
2. æŸ¥æ‰¾å¯è¡Œçš„èŠ‚ç‚¹ï¼ˆFilter é˜¶æ®µï¼‰
3. è¿è¡Œ PreScore æ’ä»¶
4. å¯¹å¯è¡ŒèŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†ï¼ˆScore é˜¶æ®µï¼‰
5. é€‰æ‹©æœ€ä½³èŠ‚ç‚¹
*/
func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
    trace := utiltrace.New("Scheduling", utiltrace.Field{Key: "namespace", Value: pod.Namespace}, utiltrace.Field{Key: "name", Value: pod.Name})
    defer trace.LogIfLong(100 * time.Millisecond)

    // 1. è¿è¡Œ PreFilter æ’ä»¶
    preFilterStatus := fwk.RunPreFilterPlugins(ctx, state, pod)
    if !preFilterStatus.IsSuccess() {
        return result, preFilterStatus.AsError()
    }

    // 2. æŸ¥æ‰¾å¯è¡Œçš„èŠ‚ç‚¹ï¼ˆFilter é˜¶æ®µï¼‰
    startTime := time.Now()
    feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
    if err != nil {
        return result, err
    }
    trace.Step("Computing predicates done")

    // 3. å¦‚æœæ²¡æœ‰å¯è¡ŒèŠ‚ç‚¹ï¼Œè¿”å›é”™è¯¯
    if len(feasibleNodes) == 0 {
        return result, &framework.FitError{
            Pod:         pod,
            NumAllNodes: sched.nodeInfoSnapshot.NumNodes(),
            Diagnosis:   diagnosis,
        }
    }

    // 4. è®°å½•è¿‡æ»¤é˜¶æ®µçš„æŒ‡æ ‡
    metrics.SchedulingAlgorithmPredicateEvaluationSecond.Observe(metrics.SinceInSeconds(startTime))
    metrics.DeprecatedSchedulingAlgorithmPredicateEvaluationSecond.Observe(metrics.SinceInMicroseconds(startTime))
    metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startTime))

    // 5. å¦‚æœåªæœ‰ä¸€ä¸ªå¯è¡ŒèŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›
    if len(feasibleNodes) == 1 {
        return ScheduleResult{
            SuggestedHost:  feasibleNodes[0].Name,
            EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap),
            FeasibleNodes:  1,
        }, nil
    }

    // 6. è¿è¡Œ PreScore æ’ä»¶
    preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, feasibleNodes)
    if !preScoreStatus.IsSuccess() {
        return result, preScoreStatus.AsError()
    }

    // 7. å¯¹å¯è¡ŒèŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†ï¼ˆScore é˜¶æ®µï¼‰
    startTime = time.Now()
    priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes)
    if err != nil {
        return result, err
    }
    trace.Step("Prioritizing done")

    // 8. è®°å½•æ‰“åˆ†é˜¶æ®µçš„æŒ‡æ ‡
    metrics.SchedulingAlgorithmPriorityEvaluationSecond.Observe(metrics.SinceInSeconds(startTime))
    metrics.DeprecatedSchedulingAlgorithmPriorityEvaluationSecond.Observe(metrics.SinceInMicroseconds(startTime))
    metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startTime))

    // 9. é€‰æ‹©æœ€ä½³èŠ‚ç‚¹
    host, err := selectHost(priorityList)
    trace.Step("Selecting host done")
    
    return ScheduleResult{
        SuggestedHost:  host,
        EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap),
        FeasibleNodes:  len(feasibleNodes),
    }, err
}

/*
findNodesThatFitPod æŸ¥æ‰¾é€‚åˆè°ƒåº¦ Pod çš„èŠ‚ç‚¹

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- fwk: è°ƒåº¦æ¡†æ¶
- state: è°ƒåº¦å‘¨æœŸçŠ¶æ€
- pod: å¾…è°ƒåº¦çš„ Pod

è¿”å›å€¼ï¼š
- []*v1.Node: å¯è¡ŒèŠ‚ç‚¹åˆ—è¡¨
- framework.Diagnosis: è¯Šæ–­ä¿¡æ¯
- error: æŸ¥æ‰¾è¿‡ç¨‹ä¸­çš„é”™è¯¯

è¿‡æ»¤æµç¨‹ï¼š
1. è·å–æ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯
2. å¹¶è¡Œè¿è¡Œ Filter æ’ä»¶
3. æ”¶é›†è¿‡æ»¤ç»“æœå’Œè¯Šæ–­ä¿¡æ¯
4. è¿”å›å¯è¡ŒèŠ‚ç‚¹åˆ—è¡¨
*/
func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) {
    diagnosis := framework.Diagnosis{
        NodeToStatusMap:      make(framework.NodeToStatusMap),
        UnschedulablePlugins: sets.NewString(),
    }

    // 1. è·å–æ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯
    allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List()
    if err != nil {
        return nil, diagnosis, err
    }

    // 2. å¦‚æœæ²¡æœ‰èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›
    if len(allNodes) == 0 {
        return nil, diagnosis, ErrNoNodesAvailable
    }

    // 3. å¹¶è¡Œè¿è¡Œ Filter æ’ä»¶
    feasibleNodes := make([]*v1.Node, 0, len(allNodes))
    
    if !fwk.HasFilterPlugins() {
        // å¦‚æœæ²¡æœ‰è¿‡æ»¤æ’ä»¶ï¼Œæ‰€æœ‰èŠ‚ç‚¹éƒ½æ˜¯å¯è¡Œçš„
        for _, nodeInfo := range allNodes {
            feasibleNodes = append(feasibleNodes, nodeInfo.Node())
        }
        return feasibleNodes, diagnosis, nil
    }

    // 4. è®¡ç®—éœ€è¦æ£€æŸ¥çš„èŠ‚ç‚¹æ•°é‡
    numNodesToFind := sched.numFeasibleNodesToFind(int32(len(allNodes)))

    // 5. å¹¶è¡Œè¿‡æ»¤èŠ‚ç‚¹
    ctx, cancel := context.WithCancel(ctx)
    defer cancel()
    
    feasibleNodesLen := int32(0)
    processedNodes := int32(0)
    
    checkNode := func(i int) {
        // æ£€æŸ¥æ˜¯å¦å·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„èŠ‚ç‚¹
        if sched.nextStartNodeIndex >= len(allNodes) {
            sched.nextStartNodeIndex = 0
        }
        
        nodeInfo := allNodes[(sched.nextStartNodeIndex+i)%len(allNodes)]
        node := nodeInfo.Node()
        
        // è¿è¡Œ Filter æ’ä»¶
        status := fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo)
        
        // åŸå­æ“ä½œå¢åŠ å·²å¤„ç†èŠ‚ç‚¹æ•°
        atomic.AddInt32(&processedNodes, 1)
        
        if status.IsSuccess() {
            // èŠ‚ç‚¹é€šè¿‡è¿‡æ»¤ï¼Œæ·»åŠ åˆ°å¯è¡ŒèŠ‚ç‚¹åˆ—è¡¨
            length := atomic.AddInt32(&feasibleNodesLen, 1)
            if length <= numNodesToFind {
                feasibleNodes = append(feasibleNodes, node)
            }
            
            // å¦‚æœæ‰¾åˆ°è¶³å¤Ÿçš„èŠ‚ç‚¹ï¼Œå–æ¶ˆå…¶ä»–åç¨‹
            if length > numNodesToFind {
                cancel()
            }
        } else {
            // èŠ‚ç‚¹æœªé€šè¿‡è¿‡æ»¤ï¼Œè®°å½•è¯Šæ–­ä¿¡æ¯
            diagnosis.NodeToStatusMap[node.Name] = status
            diagnosis.UnschedulablePlugins.Insert(status.FailedPlugin())
        }
    }

    // 6. ä½¿ç”¨å·¥ä½œæ± å¹¶è¡Œå¤„ç†èŠ‚ç‚¹
    fwk.Parallelizer().Until(ctx, len(allNodes), checkNode, metrics.Filter)
    
    // 7. æ›´æ–°ä¸‹ä¸€ä¸ªå¼€å§‹èŠ‚ç‚¹ç´¢å¼•ï¼ˆç”¨äºè´Ÿè½½å‡è¡¡ï¼‰
    sched.nextStartNodeIndex = (sched.nextStartNodeIndex + int(processedNodes)) % len(allNodes)

    // 8. è®°å½•è¿‡æ»¤æŒ‡æ ‡
    feasibleNodesCount := int(feasibleNodesLen)
    metrics.FeasibleNodes.Observe(float64(feasibleNodesCount))

    return feasibleNodes, diagnosis, nil
}

/*
prioritizeNodes å¯¹å¯è¡ŒèŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- extenders: å¤–éƒ¨æ‰©å±•å™¨åˆ—è¡¨
- fwk: è°ƒåº¦æ¡†æ¶
- state: è°ƒåº¦å‘¨æœŸçŠ¶æ€
- pod: å¾…è°ƒåº¦çš„ Pod
- feasibleNodes: å¯è¡ŒèŠ‚ç‚¹åˆ—è¡¨

è¿”å›å€¼ï¼š
- []framework.NodePluginScores: èŠ‚ç‚¹åˆ†æ•°åˆ—è¡¨
- error: æ‰“åˆ†è¿‡ç¨‹ä¸­çš„é”™è¯¯

æ‰“åˆ†æµç¨‹ï¼š
1. è¿è¡Œæ¡†æ¶å†…ç½®çš„ Score æ’ä»¶
2. è¿è¡Œå¤–éƒ¨æ‰©å±•å™¨çš„æ‰“åˆ†é€»è¾‘
3. åˆå¹¶å’Œå½’ä¸€åŒ–åˆ†æ•°
4. è¿”å›æœ€ç»ˆçš„èŠ‚ç‚¹åˆ†æ•°åˆ—è¡¨
*/
func prioritizeNodes(
    ctx context.Context,
    extenders []framework.Extender,
    fwk framework.Framework,
    state *framework.CycleState,
    pod *v1.Pod,
    feasibleNodes []*v1.Node,
) ([]framework.NodePluginScores, error) {
    // 1. å¦‚æœæ²¡æœ‰å¯è¡ŒèŠ‚ç‚¹ï¼Œè¿”å›ç©ºåˆ—è¡¨
    if len(feasibleNodes) == 0 {
        return nil, nil
    }

    // 2. è¿è¡Œæ¡†æ¶å†…ç½®çš„ Score æ’ä»¶
    scoresMap, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, feasibleNodes)
    if !scoreStatus.IsSuccess() {
        return nil, scoreStatus.AsError()
    }

    // 3. è¿è¡Œå¤–éƒ¨æ‰©å±•å™¨çš„æ‰“åˆ†é€»è¾‘
    if len(extenders) != 0 && feasibleNodes != nil {
        for i := range extenders {
            if !extenders[i].IsInterested(pod) {
                continue
            }
            
            // è°ƒç”¨å¤–éƒ¨æ‰©å±•å™¨
            prioritizedList, weight, err := extenders[i].Prioritize(pod, feasibleNodes)
            if err != nil {
                return nil, err
            }
            
            // å°†å¤–éƒ¨æ‰©å±•å™¨çš„åˆ†æ•°åˆå¹¶åˆ°æ€»åˆ†ä¸­
            for j := range feasibleNodes {
                host := feasibleNodes[j].Name
                score, ok := prioritizedList[host]
                if !ok {
                    continue
                }
                
                if scoresMap[host] == nil {
                    scoresMap[host] = make(framework.PluginToNodeScores)
                }
                scoresMap[host][extenders[i].Name()] = int64(score * weight)
            }
        }
    }

    // 4. å°†åˆ†æ•°æ˜ å°„è½¬æ¢ä¸ºèŠ‚ç‚¹åˆ†æ•°åˆ—è¡¨
    result := make([]framework.NodePluginScores, 0, len(feasibleNodes))
    for i := range feasibleNodes {
        nodeName := feasibleNodes[i].Name
        result = append(result, framework.NodePluginScores{
            Name:   nodeName,
            Scores: scoresMap[nodeName],
        })
    }

    return result, nil
}

/*
selectHost ä»æ‰“åˆ†ç»“æœä¸­é€‰æ‹©æœ€ä½³èŠ‚ç‚¹

å‚æ•°ï¼š
- nodeScoreList: èŠ‚ç‚¹åˆ†æ•°åˆ—è¡¨

è¿”å›å€¼ï¼š
- string: é€‰ä¸­çš„èŠ‚ç‚¹åç§°
- error: é€‰æ‹©è¿‡ç¨‹ä¸­çš„é”™è¯¯

é€‰æ‹©é€»è¾‘ï¼š
1. è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æ€»åˆ†
2. æ‰¾å‡ºæœ€é«˜åˆ†æ•°
3. å¦‚æœæœ‰å¤šä¸ªèŠ‚ç‚¹å…·æœ‰ç›¸åŒçš„æœ€é«˜åˆ†æ•°ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª
4. è¿”å›é€‰ä¸­çš„èŠ‚ç‚¹åç§°
*/
func selectHost(nodeScoreList []framework.NodePluginScores) (string, error) {
    if len(nodeScoreList) == 0 {
        return "", fmt.Errorf("ç©ºçš„ä¼˜å…ˆçº§åˆ—è¡¨")
    }
    
    // 1. è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æ€»åˆ†å¹¶æ‰¾å‡ºæœ€é«˜åˆ†
    maxScore := nodeScoreList[0].TotalScore()
    selected := nodeScoreList[0].Name
    cntOfMaxScore := 1
    
    for i := 1; i < len(nodeScoreList); i++ {
        score := nodeScoreList[i].TotalScore()
        if score > maxScore {
            // æ‰¾åˆ°æ›´é«˜åˆ†æ•°çš„èŠ‚ç‚¹
            maxScore = score
            selected = nodeScoreList[i].Name
            cntOfMaxScore = 1
        } else if score == maxScore {
            // æ‰¾åˆ°ç›¸åŒåˆ†æ•°çš„èŠ‚ç‚¹ï¼Œéšæœºé€‰æ‹©
            cntOfMaxScore++
            if rand.Intn(cntOfMaxScore) == 0 {
                selected = nodeScoreList[i].Name
            }
        }
    }
    
    return selected, nil
}
```

## ğŸ”Œ æ’ä»¶ç³»ç»Ÿè¯¦ç»†åˆ†æ

### 4.1 è°ƒåº¦æ¡†æ¶æ’ä»¶æ¶æ„

```mermaid
graph TB
    subgraph "è°ƒåº¦æ¡†æ¶æ’ä»¶ç³»ç»Ÿ"
        subgraph "æ’ä»¶æ¥å£ (Plugin Interfaces)"
            SORT_PLUGIN[Sort Plugin<br/>æ’åºæ’ä»¶æ¥å£]
            PREFILTER_PLUGIN[PreFilter Plugin<br/>é¢„è¿‡æ»¤æ’ä»¶æ¥å£]
            FILTER_PLUGIN[Filter Plugin<br/>è¿‡æ»¤æ’ä»¶æ¥å£]
            PRESCORE_PLUGIN[PreScore Plugin<br/>é¢„æ‰“åˆ†æ’ä»¶æ¥å£]
            SCORE_PLUGIN[Score Plugin<br/>æ‰“åˆ†æ’ä»¶æ¥å£]
            RESERVE_PLUGIN[Reserve Plugin<br/>é¢„ç•™æ’ä»¶æ¥å£]
            PERMIT_PLUGIN[Permit Plugin<br/>è®¸å¯æ’ä»¶æ¥å£]
            PREBIND_PLUGIN[PreBind Plugin<br/>é¢„ç»‘å®šæ’ä»¶æ¥å£]
            BIND_PLUGIN[Bind Plugin<br/>ç»‘å®šæ’ä»¶æ¥å£]
            POSTBIND_PLUGIN[PostBind Plugin<br/>åç»‘å®šæ’ä»¶æ¥å£]
        end
        
        subgraph "å†…ç½®æ’ä»¶ (Built-in Plugins)"
            NODE_RESOURCES[NodeResources<br/>èŠ‚ç‚¹èµ„æºæ’ä»¶]
            NODE_AFFINITY[NodeAffinity<br/>èŠ‚ç‚¹äº²å’Œæ€§æ’ä»¶]
            POD_AFFINITY[PodAffinity<br/>Pod äº²å’Œæ€§æ’ä»¶]
            TAINT_TOLERATION[TaintToleration<br/>æ±¡ç‚¹å®¹å¿æ’ä»¶]
            VOLUME_BINDING[VolumeBinding<br/>å­˜å‚¨å·ç»‘å®šæ’ä»¶]
            IMAGE_LOCALITY[ImageLocality<br/>é•œåƒæœ¬åœ°æ€§æ’ä»¶]
            TOPOLOGY_SPREAD[TopologySpread<br/>æ‹“æ‰‘åˆ†å¸ƒæ’ä»¶]
            DEFAULT_BINDER[DefaultBinder<br/>é»˜è®¤ç»‘å®šæ’ä»¶]
        end
        
        subgraph "æ’ä»¶æ³¨å†Œ (Plugin Registry)"
            REGISTRY[Plugin Registry<br/>æ’ä»¶æ³¨å†Œè¡¨]
            FACTORY[Plugin Factory<br/>æ’ä»¶å·¥å‚]
            CONFIG[Plugin Config<br/>æ’ä»¶é…ç½®]
        end
        
        subgraph "æ‰©å±•æœºåˆ¶ (Extension Mechanisms)"
            OUT_OF_TREE[Out-of-tree Plugins<br/>å¤–éƒ¨æ’ä»¶]
            CUSTOM_PLUGINS[Custom Plugins<br/>è‡ªå®šä¹‰æ’ä»¶]
            EXTENDERS[Scheduler Extenders<br/>è°ƒåº¦æ‰©å±•å™¨]
        end
    end
    
    %% æ’ä»¶æ¥å£å…³ç³»
    SORT_PLUGIN --> NODE_RESOURCES
    PREFILTER_PLUGIN --> NODE_RESOURCES
    FILTER_PLUGIN --> NODE_RESOURCES
    FILTER_PLUGIN --> NODE_AFFINITY
    FILTER_PLUGIN --> POD_AFFINITY
    FILTER_PLUGIN --> TAINT_TOLERATION
    FILTER_PLUGIN --> VOLUME_BINDING
    
    PRESCORE_PLUGIN --> NODE_RESOURCES
    SCORE_PLUGIN --> NODE_RESOURCES
    SCORE_PLUGIN --> NODE_AFFINITY
    SCORE_PLUGIN --> POD_AFFINITY
    SCORE_PLUGIN --> IMAGE_LOCALITY
    SCORE_PLUGIN --> TOPOLOGY_SPREAD
    
    BIND_PLUGIN --> DEFAULT_BINDER
    
    %% æ³¨å†Œå…³ç³»
    REGISTRY --> FACTORY
    FACTORY --> CONFIG
    CONFIG --> NODE_RESOURCES
    CONFIG --> NODE_AFFINITY
    CONFIG --> POD_AFFINITY
    CONFIG --> TAINT_TOLERATION
    CONFIG --> VOLUME_BINDING
    CONFIG --> IMAGE_LOCALITY
    CONFIG --> TOPOLOGY_SPREAD
    CONFIG --> DEFAULT_BINDER
    
    %% æ‰©å±•å…³ç³»
    REGISTRY --> OUT_OF_TREE
    REGISTRY --> CUSTOM_PLUGINS
    EXTENDERS --> REGISTRY
    
    %% æ ·å¼å®šä¹‰
    classDef interfaces fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef builtin fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef registry fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef extension fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    
    class SORT_PLUGIN,PREFILTER_PLUGIN,FILTER_PLUGIN,PRESCORE_PLUGIN,SCORE_PLUGIN,RESERVE_PLUGIN,PERMIT_PLUGIN,PREBIND_PLUGIN,BIND_PLUGIN,POSTBIND_PLUGIN interfaces
    class NODE_RESOURCES,NODE_AFFINITY,POD_AFFINITY,TAINT_TOLERATION,VOLUME_BINDING,IMAGE_LOCALITY,TOPOLOGY_SPREAD,DEFAULT_BINDER builtin
    class REGISTRY,FACTORY,CONFIG registry
    class OUT_OF_TREE,CUSTOM_PLUGINS,EXTENDERS extension
```

### 4.2 æ ¸å¿ƒæ’ä»¶å®ç°ç¤ºä¾‹

#### 4.2.1 NodeResources æ’ä»¶

```go
// pkg/scheduler/framework/plugins/noderesources/fit.go
/*
NodeResourcesFit æ’ä»¶å®ç°èŠ‚ç‚¹èµ„æºé€‚é…æ£€æŸ¥

ä¸»è¦åŠŸèƒ½ï¼š
1. æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æœ‰è¶³å¤Ÿçš„èµ„æºæ¥è¿è¡Œ Pod
2. æ”¯æŒå¤šç§èµ„æºç±»å‹ï¼ˆCPUã€å†…å­˜ã€å­˜å‚¨ç­‰ï¼‰
3. è€ƒè™‘å·²åˆ†é…å’Œè¯·æ±‚çš„èµ„æº
4. æä¾›èµ„æºåˆ©ç”¨ç‡æ‰“åˆ†
*/

/*
Fit ç»“æ„ä½“å®ç° PreFilterã€Filter å’Œ Score æ’ä»¶æ¥å£

å­—æ®µè¯´æ˜ï¼š
- handle: è°ƒåº¦æ¡†æ¶å¥æŸ„
- resourceAllocationScorer: èµ„æºåˆ†é…æ‰“åˆ†å™¨
*/
type Fit struct {
    handle framework.Handle
    resourceAllocationScorer resourceAllocationScorer
}

/*
Name è¿”å›æ’ä»¶åç§°

è¿”å›å€¼ï¼š
- string: æ’ä»¶åç§° "NodeResourcesFit"
*/
func (f *Fit) Name() string {
    return Name
}

/*
PreFilter é¢„è¿‡æ»¤é˜¶æ®µæ£€æŸ¥ Pod çš„èµ„æºè¯·æ±‚

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- state: è°ƒåº¦å‘¨æœŸçŠ¶æ€
- pod: å¾…è°ƒåº¦çš„ Pod

è¿”å›å€¼ï¼š
- *framework.PreFilterResult: é¢„è¿‡æ»¤ç»“æœ
- *framework.Status: æ‰§è¡ŒçŠ¶æ€

é¢„è¿‡æ»¤é€»è¾‘ï¼š
1. è®¡ç®— Pod çš„èµ„æºè¯·æ±‚
2. æ£€æŸ¥é›†ç¾¤æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ€»èµ„æº
3. ä¸ºåç»­è¿‡æ»¤é˜¶æ®µå‡†å¤‡æ•°æ®
*/
func (f *Fit) PreFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
    // 1. è®¡ç®— Pod çš„èµ„æºè¯·æ±‚
    podRequest := computePodResourceRequest(pod)
    if podRequest.IsEmpty() {
        // Pod æ²¡æœ‰èµ„æºè¯·æ±‚ï¼Œè·³è¿‡æ£€æŸ¥
        return nil, framework.NewStatus(framework.Success, "")
    }

    // 2. å°†èµ„æºè¯·æ±‚å­˜å‚¨åˆ°çŠ¶æ€ä¸­ï¼Œä¾›åç»­é˜¶æ®µä½¿ç”¨
    state.Write(preFilterStateKey, &preFilterState{
        Resource: podRequest,
    })

    // 3. æ£€æŸ¥é›†ç¾¤æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ€»èµ„æº
    nodes := f.handle.SnapshotSharedLister().NodeInfos()
    totalResource := framework.Resource{}
    
    for _, nodeInfo := range nodes {
        if nodeInfo.Node() == nil {
            continue
        }
        
        // ç´¯åŠ æ‰€æœ‰èŠ‚ç‚¹çš„å¯åˆ†é…èµ„æº
        totalResource.Add(nodeInfo.Allocatable)
    }

    // 4. æ£€æŸ¥æ€»èµ„æºæ˜¯å¦è¶³å¤Ÿ
    if !totalResource.Fits(podRequest) {
        return nil, framework.NewStatus(framework.UnschedulableAndUnresolvable, 
            fmt.Sprintf("é›†ç¾¤æ€»èµ„æºä¸è¶³: éœ€è¦ %v, å¯ç”¨ %v", podRequest, totalResource))
    }

    return nil, framework.NewStatus(framework.Success, "")
}

/*
Filter è¿‡æ»¤é˜¶æ®µæ£€æŸ¥å•ä¸ªèŠ‚ç‚¹æ˜¯å¦æœ‰è¶³å¤Ÿèµ„æº

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- state: è°ƒåº¦å‘¨æœŸçŠ¶æ€
- pod: å¾…è°ƒåº¦çš„ Pod
- nodeInfo: èŠ‚ç‚¹ä¿¡æ¯

è¿”å›å€¼ï¼š
- *framework.Status: è¿‡æ»¤ç»“æœçŠ¶æ€

è¿‡æ»¤é€»è¾‘ï¼š
1. ä»çŠ¶æ€ä¸­è·å– Pod èµ„æºè¯·æ±‚
2. æ£€æŸ¥èŠ‚ç‚¹å¯åˆ†é…èµ„æº
3. æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å‰©ä½™èµ„æº
4. è¿”å›è¿‡æ»¤ç»“æœ
*/
func (f *Fit) Filter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
    // 1. ä»çŠ¶æ€ä¸­è·å–é¢„è¿‡æ»¤ç»“æœ
    s, err := getPreFilterState(state)
    if err != nil {
        return framework.AsStatus(err)
    }

    // 2. æ£€æŸ¥èŠ‚ç‚¹èµ„æºæ˜¯å¦è¶³å¤Ÿ
    insufficientResources := fitsRequest(s.Resource, nodeInfo)
    
    if len(insufficientResources) != 0 {
        // 3. èµ„æºä¸è¶³ï¼Œè¿”å›å¤±è´¥çŠ¶æ€
        failureReasons := make([]string, 0, len(insufficientResources))
        for _, r := range insufficientResources {
            failureReasons = append(failureReasons, fmt.Sprintf("èµ„æºä¸è¶³: %v", r.String()))
        }
        
        return framework.NewStatus(framework.Unschedulable, strings.Join(failureReasons, ", "))
    }

    return framework.NewStatus(framework.Success, "")
}

/*
Score æ‰“åˆ†é˜¶æ®µè®¡ç®—èŠ‚ç‚¹èµ„æºåˆ©ç”¨ç‡åˆ†æ•°

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- state: è°ƒåº¦å‘¨æœŸçŠ¶æ€
- pod: å¾…è°ƒåº¦çš„ Pod
- nodeName: èŠ‚ç‚¹åç§°

è¿”å›å€¼ï¼š
- int64: èŠ‚ç‚¹åˆ†æ•°ï¼ˆ0-100ï¼‰
- *framework.Status: æ‰§è¡ŒçŠ¶æ€

æ‰“åˆ†é€»è¾‘ï¼š
1. è·å–èŠ‚ç‚¹ä¿¡æ¯
2. è®¡ç®—èµ„æºåˆ©ç”¨ç‡
3. æ ¹æ®é…ç½®çš„æ‰“åˆ†ç­–ç•¥è®¡ç®—åˆ†æ•°
4. è¿”å›æ ‡å‡†åŒ–åçš„åˆ†æ•°
*/
func (f *Fit) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
    // 1. è·å–èŠ‚ç‚¹ä¿¡æ¯
    nodeInfo, err := f.handle.SnapshotSharedLister().NodeInfos().Get(nodeName)
    if err != nil {
        return 0, framework.AsStatus(fmt.Errorf("è·å–èŠ‚ç‚¹ %q ä¿¡æ¯å¤±è´¥: %w", nodeName, err))
    }

    // 2. ä»çŠ¶æ€ä¸­è·å– Pod èµ„æºè¯·æ±‚
    s, err := getPreFilterState(state)
    if err != nil {
        return 0, framework.AsStatus(err)
    }

    // 3. ä½¿ç”¨èµ„æºåˆ†é…æ‰“åˆ†å™¨è®¡ç®—åˆ†æ•°
    return f.resourceAllocationScorer.score(pod, nodeInfo, s.Resource)
}

/*
ScoreExtensions è¿”å›æ‰“åˆ†æ‰©å±•æ¥å£

è¿”å›å€¼ï¼š
- framework.ScoreExtensions: æ‰“åˆ†æ‰©å±•æ¥å£
*/
func (f *Fit) ScoreExtensions() framework.ScoreExtensions {
    return f
}

/*
NormalizeScore æ ‡å‡†åŒ–åˆ†æ•°åˆ° 0-100 èŒƒå›´

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- state: è°ƒåº¦å‘¨æœŸçŠ¶æ€
- pod: å¾…è°ƒåº¦çš„ Pod
- scores: èŠ‚ç‚¹åˆ†æ•°æ˜ å°„

è¿”å›å€¼ï¼š
- *framework.Status: æ‰§è¡ŒçŠ¶æ€

æ ‡å‡†åŒ–é€»è¾‘ï¼š
1. æ‰¾å‡ºæœ€é«˜å’Œæœ€ä½åˆ†æ•°
2. è®¡ç®—åˆ†æ•°èŒƒå›´
3. å°†æ‰€æœ‰åˆ†æ•°æ ‡å‡†åŒ–åˆ° 0-100 èŒƒå›´
4. æ›´æ–°åˆ†æ•°æ˜ å°„
*/
func (f *Fit) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status {
    return helper.DefaultNormalizeScore(framework.MaxNodeScore, false, scores)
}

/*
computePodResourceRequest è®¡ç®— Pod çš„èµ„æºè¯·æ±‚

å‚æ•°ï¼š
- pod: Pod å¯¹è±¡

è¿”å›å€¼ï¼š
- *framework.Resource: Pod çš„æ€»èµ„æºè¯·æ±‚

è®¡ç®—é€»è¾‘ï¼š
1. éå† Pod ä¸­çš„æ‰€æœ‰å®¹å™¨
2. ç´¯åŠ æ¯ä¸ªå®¹å™¨çš„èµ„æºè¯·æ±‚
3. è€ƒè™‘ Init å®¹å™¨çš„èµ„æºéœ€æ±‚
4. è¿”å›æ€»çš„èµ„æºè¯·æ±‚
*/
func computePodResourceRequest(pod *v1.Pod) *framework.Resource {
    result := &framework.Resource{}
    
    // 1. è®¡ç®—æ™®é€šå®¹å™¨çš„èµ„æºè¯·æ±‚
    for _, container := range pod.Spec.Containers {
        result.Add(container.Resources.Requests)
    }
    
    // 2. è€ƒè™‘ Init å®¹å™¨çš„èµ„æºéœ€æ±‚
    // Init å®¹å™¨æ˜¯ä¸²è¡Œæ‰§è¡Œçš„ï¼Œæ‰€ä»¥å–æœ€å¤§å€¼
    for _, container := range pod.Spec.InitContainers {
        containerResource := framework.NewResource(container.Resources.Requests)
        result.SetMaxResource(containerResource)
    }
    
    // 3. æ·»åŠ  Pod çº§åˆ«çš„å¼€é”€
    if pod.Spec.Overhead != nil {
        result.Add(pod.Spec.Overhead)
    }
    
    return result
}

/*
fitsRequest æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦èƒ½æ»¡è¶³èµ„æºè¯·æ±‚

å‚æ•°ï¼š
- podRequest: Pod èµ„æºè¯·æ±‚
- nodeInfo: èŠ‚ç‚¹ä¿¡æ¯

è¿”å›å€¼ï¼š
- []InsufficientResource: ä¸è¶³çš„èµ„æºåˆ—è¡¨

æ£€æŸ¥é€»è¾‘ï¼š
1. è®¡ç®—èŠ‚ç‚¹å‰©ä½™å¯åˆ†é…èµ„æº
2. æ£€æŸ¥æ¯ç§èµ„æºç±»å‹æ˜¯å¦è¶³å¤Ÿ
3. è®°å½•ä¸è¶³çš„èµ„æº
4. è¿”å›ä¸è¶³èµ„æºåˆ—è¡¨
*/
func fitsRequest(podRequest *framework.Resource, nodeInfo *framework.NodeInfo) []InsufficientResource {
    insufficientResources := make([]InsufficientResource, 0, 4)
    
    // 1. è®¡ç®—èŠ‚ç‚¹å‰©ä½™èµ„æº
    allowedPodNumber := nodeInfo.Allocatable.AllowedPodNumber
    if len(nodeInfo.Pods)+1 > allowedPodNumber {
        insufficientResources = append(insufficientResources, InsufficientResource{
            ResourceName: v1.ResourcePods,
            Reason:       "èŠ‚ç‚¹ Pod æ•°é‡å·²è¾¾ä¸Šé™",
            Requested:    1,
            Used:         len(nodeInfo.Pods),
            Capacity:     allowedPodNumber,
        })
    }
    
    // 2. æ£€æŸ¥å„ç§èµ„æºç±»å‹
    if podRequest.MilliCPU > (nodeInfo.Allocatable.MilliCPU - nodeInfo.Requested.MilliCPU) {
        insufficientResources = append(insufficientResources, InsufficientResource{
            ResourceName: v1.ResourceCPU,
            Reason:       "CPU èµ„æºä¸è¶³",
            Requested:    podRequest.MilliCPU,
            Used:         nodeInfo.Requested.MilliCPU,
            Capacity:     nodeInfo.Allocatable.MilliCPU,
        })
    }
    
    if podRequest.Memory > (nodeInfo.Allocatable.Memory - nodeInfo.Requested.Memory) {
        insufficientResources = append(insufficientResources, InsufficientResource{
            ResourceName: v1.ResourceMemory,
            Reason:       "å†…å­˜èµ„æºä¸è¶³",
            Requested:    podRequest.Memory,
            Used:         nodeInfo.Requested.Memory,
            Capacity:     nodeInfo.Allocatable.Memory,
        })
    }
    
    // 3. æ£€æŸ¥æ‰©å±•èµ„æº
    for rName, rQuant := range podRequest.ScalarResources {
        if rQuant > (nodeInfo.Allocatable.ScalarResources[rName] - nodeInfo.Requested.ScalarResources[rName]) {
            insufficientResources = append(insufficientResources, InsufficientResource{
                ResourceName: rName,
                Reason:       fmt.Sprintf("%s èµ„æºä¸è¶³", rName),
                Requested:    rQuant,
                Used:         nodeInfo.Requested.ScalarResources[rName],
                Capacity:     nodeInfo.Allocatable.ScalarResources[rName],
            })
        }
    }
    
    return insufficientResources
}
```

## ğŸ“Š è°ƒåº¦å™¨ç›‘æ§å’Œæ€§èƒ½ä¼˜åŒ–

### 5.1 è°ƒåº¦å™¨æŒ‡æ ‡ç›‘æ§

```yaml
# Scheduler ç›‘æ§é…ç½®
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: kube-scheduler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: kube-scheduler
  endpoints:
  - port: https
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      serverName: kube-scheduler
      insecureSkipVerify: false
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    interval: 30s
    path: /metrics
    
---
# è°ƒåº¦å™¨å…³é”®æŒ‡æ ‡å‘Šè­¦è§„åˆ™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kube-scheduler-alerts
  namespace: kube-system
spec:
  groups:
  - name: kube-scheduler.rules
    rules:
    # Scheduler å¯ç”¨æ€§å‘Šè­¦
    - alert: KubeSchedulerDown
      expr: up{job="kube-scheduler"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes Scheduler ä¸å¯ç”¨"
        description: "Scheduler {{ $labels.instance }} å·²ç»å®•æœºè¶…è¿‡ 5 åˆ†é’Ÿ"
    
    # è°ƒåº¦å»¶è¿Ÿå‘Šè­¦
    - alert: KubeSchedulerHighLatency
      expr: |
        histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) by (le)) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Scheduler è°ƒåº¦å»¶è¿Ÿè¿‡é«˜"
        description: "Scheduler è°ƒåº¦ç®—æ³• 99% åˆ†ä½å»¶è¿Ÿä¸º {{ $value }} ç§’"
    
    # è°ƒåº¦å¤±è´¥ç‡å‘Šè­¦
    - alert: KubeSchedulerHighFailureRate
      expr: |
        sum(rate(scheduler_schedule_attempts_total{result="error",job="kube-scheduler"}[5m])) / sum(rate(scheduler_schedule_attempts_total{job="kube-scheduler"}[5m])) > 0.1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Scheduler è°ƒåº¦å¤±è´¥ç‡è¿‡é«˜"
        description: "Scheduler è°ƒåº¦å¤±è´¥ç‡ä¸º {{ $value | humanizePercentage }}"
    
    # è°ƒåº¦é˜Ÿåˆ—æ·±åº¦å‘Šè­¦
    - alert: KubeSchedulerQueueDepth
      expr: |
        scheduler_pending_pods{job="kube-scheduler",queue="active"} > 100
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Scheduler è°ƒåº¦é˜Ÿåˆ—æ·±åº¦è¿‡é«˜"
        description: "Scheduler æ´»è·ƒé˜Ÿåˆ—ä¸­æœ‰ {{ $value }} ä¸ªå¾…è°ƒåº¦ Pod"
    
    # ä¸å¯è°ƒåº¦ Pod å‘Šè­¦
    - alert: KubeSchedulerUnschedulablePods
      expr: |
        scheduler_pending_pods{job="kube-scheduler",queue="unschedulable"} > 50
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Scheduler ä¸å¯è°ƒåº¦ Pod è¿‡å¤š"
        description: "Scheduler ä¸å¯è°ƒåº¦é˜Ÿåˆ—ä¸­æœ‰ {{ $value }} ä¸ª Pod"
    
    # è°ƒåº¦å™¨æ’ä»¶é”™è¯¯å‘Šè­¦
    - alert: KubeSchedulerPluginErrors
      expr: |
        increase(scheduler_plugin_execution_duration_seconds_count{result="error",job="kube-scheduler"}[5m]) > 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Scheduler æ’ä»¶æ‰§è¡Œé”™è¯¯è¿‡å¤š"
        description: "Scheduler æ’ä»¶ {{ $labels.plugin }} åœ¨è¿‡å» 5 åˆ†é’Ÿå†…æ‰§è¡Œå¤±è´¥ {{ $value }} æ¬¡"
```

### 5.2 è°ƒåº¦å™¨æ€§èƒ½ä¼˜åŒ–é…ç½®

```yaml
# Scheduler é«˜æ€§èƒ½é…ç½®
apiVersion: v1
kind: Pod
metadata:
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - name: kube-scheduler
    image: k8s.gcr.io/kube-scheduler:v1.29.0
    command:
    - kube-scheduler
    
    # åŸºç¡€é…ç½®
    - --bind-address=0.0.0.0
    - --secure-port=10259
    - --port=0
    
    # é¢†å¯¼é€‰ä¸¾é…ç½®
    - --leader-elect=true
    - --leader-elect-lease-duration=15s
    - --leader-elect-renew-deadline=10s
    - --leader-elect-retry-period=2s
    
    # è°ƒåº¦æ€§èƒ½é…ç½®
    - --kube-api-qps=100                    # API è¯·æ±‚ QPS é™åˆ¶
    - --kube-api-burst=100                  # API è¯·æ±‚çªå‘é™åˆ¶
    - --percentage-of-nodes-to-score=50     # å‚ä¸æ‰“åˆ†çš„èŠ‚ç‚¹ç™¾åˆ†æ¯”
    
    # é…ç½®æ–‡ä»¶
    - --config=/etc/kubernetes/scheduler-config.yaml
    
    # è®¤è¯é…ç½®
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    
    # TLS é…ç½®
    - --tls-cert-file=/etc/kubernetes/pki/kube-scheduler.crt
    - --tls-private-key-file=/etc/kubernetes/pki/kube-scheduler.key
    
    # æ—¥å¿—é…ç½®
    - --v=2                                 # æ—¥å¿—çº§åˆ«
    - --logtostderr=true                    # è¾“å‡ºåˆ°æ ‡å‡†é”™è¯¯
    
    # èµ„æºé…ç½®
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
    
    # å­˜å‚¨å·æŒ‚è½½
    volumeMounts:
    - name: config
      mountPath: /etc/kubernetes/scheduler-config.yaml
      readOnly: true
    - name: kubeconfig
      mountPath: /etc/kubernetes/scheduler.conf
      readOnly: true
    - name: k8s-certs
      mountPath: /etc/kubernetes/pki
      readOnly: true
      
  # ä¸»æœºç½‘ç»œæ¨¡å¼
  hostNetwork: true
  
  # ä¼˜å…ˆçº§ç±»
  priorityClassName: system-node-critical
  
  # å­˜å‚¨å·å®šä¹‰
  volumes:
  - name: config
    configMap:
      name: scheduler-config
  - name: kubeconfig
    hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: File
  - name: k8s-certs
    hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate

---
# è°ƒåº¦å™¨é…ç½®æ–‡ä»¶
apiVersion: v1
kind: ConfigMap
metadata:
  name: scheduler-config
  namespace: kube-system
data:
  scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1beta3
    kind: KubeSchedulerConfiguration
    
    # è°ƒåº¦é…ç½®æ–‡ä»¶
    profiles:
    - schedulerName: default-scheduler
      plugins:
        # å¯ç”¨çš„æ’ä»¶
        queueSort:
          enabled:
          - name: PrioritySort
        preFilter:
          enabled:
          - name: NodeResourcesFit
          - name: NodeAffinity
          - name: PodTopologySpread
          - name: InterPodAffinity
          - name: VolumeBinding
          - name: NodePorts
        filter:
          enabled:
          - name: NodeUnschedulable
          - name: NodeName
          - name: TaintToleration
          - name: NodeAffinity
          - name: NodePorts
          - name: NodeResourcesFit
          - name: VolumeRestrictions
          - name: EBSLimits
          - name: GCEPDLimits
          - name: NodeVolumeLimits
          - name: AzureDiskLimits
          - name: VolumeBinding
          - name: VolumeZone
          - name: PodTopologySpread
          - name: InterPodAffinity
        postFilter:
          enabled:
          - name: DefaultPreemption
        preScore:
          enabled:
          - name: InterPodAffinity
          - name: PodTopologySpread
          - name: TaintToleration
          - name: NodeAffinity
          - name: NodeResourcesFit
        score:
          enabled:
          - name: NodeResourcesFit
            weight: 1
          - name: InterPodAffinity
            weight: 1
          - name: NodeAffinity
            weight: 1
          - name: PodTopologySpread
            weight: 2
          - name: TaintToleration
            weight: 1
          - name: ImageLocality
            weight: 1
        reserve:
          enabled:
          - name: VolumeBinding
        permit:
          enabled: []
        preBind:
          enabled:
          - name: VolumeBinding
        bind:
          enabled:
          - name: DefaultBinder
        postBind:
          enabled: []
      
      # æ’ä»¶é…ç½®
      pluginConfig:
      - name: NodeResourcesFit
        args:
          scoringStrategy:
            type: LeastAllocated    # æœ€å°‘åˆ†é…ç­–ç•¥
            resources:
            - name: cpu
              weight: 1
            - name: memory
              weight: 1
      
      - name: PodTopologySpread
        args:
          defaultingType: List
          defaultConstraints:
          - maxSkew: 3
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: ScheduleAnyway
          - maxSkew: 5
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: ScheduleAnyway
      
      - name: InterPodAffinity
        args:
          hardPodAffinityWeight: 1
      
      - name: NodeAffinity
        args:
          addedAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/arch
                  operator: In
                  values: ["amd64", "arm64"]
    
    # æ‰©å±•å™¨é…ç½®
    extenders: []
    
    # æ€§èƒ½é…ç½®
    percentageOfNodesToScore: 50    # å‚ä¸æ‰“åˆ†çš„èŠ‚ç‚¹ç™¾åˆ†æ¯”
    podInitialBackoffSeconds: 1     # Pod åˆå§‹é€€é¿æ—¶é—´
    podMaxBackoffSeconds: 10        # Pod æœ€å¤§é€€é¿æ—¶é—´
    
    # å¹¶è¡Œé…ç½®
    parallelism: 16                 # å¹¶è¡Œåº¦
    
    # é¢†å¯¼é€‰ä¸¾é…ç½®
    leaderElection:
      leaderElect: true
      leaseDuration: 15s
      renewDeadline: 10s
      retryPeriod: 2s
      resourceLock: leases
      resourceName: kube-scheduler
      resourceNamespace: kube-system
```

## ğŸ“š æ€»ç»“

### æ ¸å¿ƒç‰¹æ€§æ€»ç»“

1. **æ’ä»¶åŒ–æ¶æ„**ï¼šçµæ´»çš„æ’ä»¶ç³»ç»Ÿæ”¯æŒè‡ªå®šä¹‰è°ƒåº¦é€»è¾‘
2. **å¤šé˜¶æ®µè°ƒåº¦**ï¼šå®Œæ•´çš„è°ƒåº¦ç”Ÿå‘½å‘¨æœŸç®¡ç†
3. **é«˜æ€§èƒ½è®¾è®¡**ï¼šå¹¶è¡Œå¤„ç†å’Œä¼˜åŒ–çš„ç®—æ³•å®ç°
4. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒå¤–éƒ¨æ‰©å±•å™¨å’Œè‡ªå®šä¹‰æ’ä»¶
5. **ä¸°å¯Œçš„è°ƒåº¦ç­–ç•¥**ï¼šå†…ç½®å¤šç§è°ƒåº¦ç®—æ³•å’Œç­–ç•¥

### æœ€ä½³å®è·µå»ºè®®

1. **æ€§èƒ½è°ƒä¼˜**ï¼šåˆç†é…ç½®èŠ‚ç‚¹æ‰“åˆ†ç™¾åˆ†æ¯”å’Œå¹¶è¡Œåº¦
2. **ç›‘æ§å®Œå–„**ï¼šå»ºç«‹å…¨é¢çš„è°ƒåº¦æŒ‡æ ‡ç›‘æ§ä½“ç³»
3. **æ’ä»¶ä¼˜åŒ–**ï¼šæ ¹æ®é›†ç¾¤ç‰¹ç‚¹é€‰æ‹©å’Œé…ç½®åˆé€‚çš„æ’ä»¶
4. **èµ„æºç®¡ç†**ï¼šåˆç†è®¾ç½®èµ„æºè¯·æ±‚å’Œé™åˆ¶
5. **æ•…éšœå¤„ç†**ï¼šå®ç°å®Œå–„çš„è°ƒåº¦å¤±è´¥å¤„ç†å’Œé‡è¯•æœºåˆ¶

é€šè¿‡æ·±å…¥ç†è§£ Scheduler çš„æ¶æ„å’Œå®ç°ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°ä¼˜åŒ–é›†ç¾¤çš„èµ„æºåˆ†é…å’Œè°ƒåº¦æ€§èƒ½ï¼Œæ„å»ºé«˜æ•ˆç¨³å®šçš„ Kubernetes é›†ç¾¤ã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´09æœˆ27æ—¥  
**é€‚ç”¨ç‰ˆæœ¬**: Kubernetes 1.29+
