---
title: "VoiceHelperæºç å‰–æ - 08Voiceè¯­éŸ³æœåŠ¡"
date: 2025-10-10T08:00:00+08:00
draft: false
tags: ["æºç å‰–æ", "VoiceHelper", "è¯­éŸ³è¯†åˆ«", "è¯­éŸ³åˆæˆ", "ASR", "TTS", "æƒ…æ„Ÿè¯†åˆ«"]
categories: ["VoiceHelper", "æºç å‰–æ"]
description: "Voiceè¯­éŸ³æœåŠ¡è¯¦è§£ï¼šASRè¯­éŸ³è¯†åˆ«ï¼ˆWhisperï¼‰ã€TTSè¯­éŸ³åˆæˆï¼ˆEdge TTSï¼‰ã€VADè¯­éŸ³æ£€æµ‹ã€æƒ…æ„Ÿè¯†åˆ«ã€å®æ—¶è¯­éŸ³æµ"
weight: 9
---

# VoiceHelper-08-Voiceè¯­éŸ³æœåŠ¡

## 1. æ¨¡å—æ¦‚è§ˆ

### 1.1 èŒè´£è¾¹ç•Œ

**æ ¸å¿ƒèŒè´£**:
- **ASR(è¯­éŸ³è¯†åˆ«)**:å°†è¯­éŸ³è½¬æ¢ä¸ºæ–‡å­—,æ”¯æŒå¤šè¯­è¨€
- **TTS(è¯­éŸ³åˆæˆ)**:å°†æ–‡å­—è½¬æ¢ä¸ºè‡ªç„¶æµç•…çš„è¯­éŸ³
- **VAD(è¯­éŸ³æ´»åŠ¨æ£€æµ‹)**:å®æ—¶æ£€æµ‹éŸ³é¢‘ä¸­çš„è¯­éŸ³ç‰‡æ®µ
- **æƒ…æ„Ÿè¯†åˆ«**:ä»è¯­éŸ³ä¸­è¯†åˆ«è¯´è¯è€…çš„æƒ…æ„ŸçŠ¶æ€
- **å®æ—¶è¯­éŸ³æµ**:æ”¯æŒWebSocketåŒå‘è¯­éŸ³äº¤äº’
- **å¤šè¯­éŸ³æ”¯æŒ**:æä¾›å¤šç§è¯­éŸ³é€‰æ‹©(æ€§åˆ«ã€æ–¹è¨€ã€é£æ ¼)

**è¾“å…¥**:
- éŸ³é¢‘æ–‡ä»¶(wav/mp3/m4a/webmç­‰æ ¼å¼)
- å®æ—¶éŸ³é¢‘æµ(PCM 16kHz 16bit)
- æ–‡æœ¬(ç”¨äºTTSåˆæˆ)
- WebSocketè¿æ¥(å®æ—¶å¯¹è¯)

**è¾“å‡º**:
- è½¬å½•æ–‡æœ¬(å¸¦æ—¶é—´æˆ³ã€ç½®ä¿¡åº¦)
- åˆæˆè¯­éŸ³(MP3æ ¼å¼)
- VADæ£€æµ‹ç»“æœ(è¯­éŸ³ç‰‡æ®µæ—¶é—´èŒƒå›´)
- æƒ…æ„Ÿè¯†åˆ«ç»“æœ(æƒ…æ„Ÿç±»å‹ã€ç½®ä¿¡åº¦ã€å¼ºåº¦)
- å®æ—¶å¯¹è¯å“åº”(æ–‡æœ¬+è¯­éŸ³)

**ä¸Šä¸‹æ¸¸ä¾èµ–**:
- **ä¸Šæ¸¸**:APIç½‘å…³ã€å‰ç«¯å®¢æˆ·ç«¯
- **ä¸‹æ¸¸**:
  - Whisperæ¨¡å‹(ASR)
  - Edge TTS API(å¾®è½¯å…è´¹TTS)
  - Silero VADæ¨¡å‹(è¯­éŸ³æ´»åŠ¨æ£€æµ‹)
  - LLM RouteræœåŠ¡(å¯¹è¯ç”Ÿæˆ)
  - AgentæœåŠ¡(ä»»åŠ¡å¤„ç†)

**ç”Ÿå‘½å‘¨æœŸ**:
- **å¯åŠ¨**:åŠ è½½é…ç½® â†’ åˆå§‹åŒ–ASRæ¨¡å‹ â†’ åˆå§‹åŒ–VADæ¨¡å‹ â†’ åˆå§‹åŒ–TTSæœåŠ¡ â†’ ç›‘å¬HTTP+WebSocket(:8002)
- **è¿è¡Œ**:æ¥æ”¶è¯·æ±‚ â†’ éŸ³é¢‘å¤„ç† â†’ è¿”å›ç»“æœ
- **å…³é—­**:åœæ­¢æ¥æ”¶è¯·æ±‚ â†’ ç­‰å¾…ç°æœ‰ä»»åŠ¡å®Œæˆ â†’ é‡Šæ”¾æ¨¡å‹èµ„æº

---

### 1.2 æ¨¡å—æ¶æ„å›¾

```mermaid
flowchart TB
    subgraph "Voice Service :8002"
        direction TB
        
        subgraph "APIå±‚"
            API_ASR[ASR API<br/>è¯­éŸ³è½¬æ–‡å­—]
            API_TTS[TTS API<br/>æ–‡å­—è½¬è¯­éŸ³]
            API_VAD[VAD API<br/>è¯­éŸ³æ£€æµ‹]
            API_EMOTION[Emotion API<br/>æƒ…æ„Ÿè¯†åˆ«]
            API_WS[WebSocket API<br/>å®æ—¶è¯­éŸ³æµ]
        end
        
        subgraph "æœåŠ¡å±‚ Service Layer"
            SVC_ASR[ASRRouter<br/>ASRè·¯ç”±å™¨]
            SVC_TTS[TTSRouter<br/>TTSè·¯ç”±å™¨]
            SVC_VAD[VADDetector<br/>VADæ£€æµ‹å™¨]
            SVC_EMOTION[EmotionRecognizer<br/>æƒ…æ„Ÿè¯†åˆ«å™¨]
            SVC_WS[WebSocketVoiceHandler<br/>WebSocketå¤„ç†å™¨]
        end
        
        subgraph "ASR Providerå±‚"
            WHISPER[WhisperService<br/>Whisper ASR]
            FASTER_WHISPER[FasterWhisper<br/>ä¼˜åŒ–ç‰ˆWhisper]
            LOCAL_ASR[LocalASR<br/>æœ¬åœ°æ¨¡å‹]
        end
        
        subgraph "TTS Providerå±‚"
            EDGE_TTS[EdgeTTSService<br/>Edge TTS]
            OPENAI_TTS[OpenAI TTS<br/>é¢„ç•™]
            LOCAL_TTS[Local TTS<br/>é¢„ç•™]
        end
        
        subgraph "VADå±‚"
            SILERO_VAD[SileroVADDetector<br/>Silero VADæ¨¡å‹]
        end
        
        subgraph "æƒ…æ„Ÿå±‚"
            RULE_EMOTION[è§„åˆ™å¼•æ“<br/>éŸ³é¢‘ç‰¹å¾+æ–‡æœ¬]
            DL_EMOTION[æ·±åº¦å­¦ä¹ <br/>é¢„ç•™æ¥å£]
        end
        
        API_ASR --> SVC_ASR
        API_TTS --> SVC_TTS
        API_VAD --> SVC_VAD
        API_EMOTION --> SVC_EMOTION
        API_WS --> SVC_WS
        
        SVC_ASR --> WHISPER & FASTER_WHISPER & LOCAL_ASR
        SVC_TTS --> EDGE_TTS & OPENAI_TTS & LOCAL_TTS
        SVC_VAD --> SILERO_VAD
        SVC_EMOTION --> RULE_EMOTION & DL_EMOTION
        
        SVC_WS --> SVC_ASR
        SVC_WS --> SVC_TTS
        SVC_WS --> SVC_VAD
        SVC_WS --> SVC_EMOTION
    end
    
    subgraph "å¤–éƒ¨æ¨¡å‹/API"
        EXT_WHISPER[Whisperæ¨¡å‹<br/>faster-whisper]
        EXT_EDGE[Edge TTS API<br/>å¾®è½¯å…è´¹]
        EXT_SILERO[Silero VADæ¨¡å‹<br/>PyTorch]
    end
    
    subgraph "ä¸‹æ¸¸æœåŠ¡"
        LLM_SVC[LLM Router<br/>å¯¹è¯ç”Ÿæˆ]
        AGENT_SVC[Agent Service<br/>ä»»åŠ¡å¤„ç†]
    end
    
    WHISPER -.åŠ è½½.-> EXT_WHISPER
    EDGE_TTS -.è°ƒç”¨.-> EXT_EDGE
    SILERO_VAD -.åŠ è½½.-> EXT_SILERO
    
    SVC_WS -.è°ƒç”¨.-> LLM_SVC
    SVC_WS -.è°ƒç”¨.-> AGENT_SVC
    
    style SVC_ASR fill:#87CEEB
    style SVC_TTS fill:#FFB6C1
    style SVC_VAD fill:#98FB98
    style SVC_EMOTION fill:#DDA0DD
    style SVC_WS fill:#F0E68C
```

### æ¶æ„è¦ç‚¹è¯´æ˜

#### 1. å¤šProvideræ¶æ„
- **ASRRouter/TTSRouter**:ç»Ÿä¸€è·¯ç”±å±‚,æ”¯æŒå¤šä¸ªProvider
- **åŠ¨æ€Provideræ³¨å†Œ**:å¯è¿è¡Œæ—¶æ·»åŠ /åˆ‡æ¢Provider
- **é™çº§ç­–ç•¥**:ä¸»Providerå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨Provider

#### 2. æ ¸å¿ƒæ¨¡å—èŒè´£
- **ASRæ¨¡å—**:
  - æ”¯æŒWhisper(é«˜è´¨é‡,å¤šè¯­è¨€)
  - æ”¯æŒFasterWhisper(æ€§èƒ½ä¼˜åŒ–ç‰ˆ)
  - æ”¯æŒæœ¬åœ°æ¨¡å‹(ç¦»çº¿åœºæ™¯)
- **TTSæ¨¡å—**:
  - EdgeTTS(å…è´¹,é«˜è´¨é‡,å¤šè¯­éŸ³)
  - OpenAI TTS(é¢„ç•™,æ›´è‡ªç„¶)
  - æœ¬åœ°TTS(é¢„ç•™,ç¦»çº¿åœºæ™¯)
- **VADæ¨¡å—**:
  - Silero VAD(é«˜ç²¾åº¦F1>0.95,ä½å»¶è¿Ÿ<50ms)
  - æ”¯æŒå®æ—¶æ£€æµ‹
  - å¹³æ»‘å¤„ç†å‡å°‘è¯¯åˆ¤
- **æƒ…æ„Ÿè¯†åˆ«**:
  - éŸ³é¢‘ç‰¹å¾æå–(éŸ³é«˜ã€èƒ½é‡ã€è¯­é€Ÿã€è¿‡é›¶ç‡)
  - æ–‡æœ¬æƒ…æ„Ÿåˆ†æ(å…³é”®è¯åŒ¹é…)
  - è§„åˆ™å¼•æ“+æ·±åº¦å­¦ä¹ (é¢„ç•™)

#### 3. å®æ—¶è¯­éŸ³æµå¤„ç†
WebSocketè¿æ¥å»ºç«‹å:
1. **æ¥æ”¶éŸ³é¢‘**:å®¢æˆ·ç«¯æŒç»­å‘é€éŸ³é¢‘ç‰‡æ®µ(PCM 16kHz)
2. **VADæ£€æµ‹**:å®æ—¶æ£€æµ‹æ˜¯å¦ä¸ºè¯­éŸ³
3. **ç¼“å†²ç´¯ç§¯**:ç´¯ç§¯è¶³å¤ŸéŸ³é¢‘(çº¦3ç§’)è¿›è¡Œè¯†åˆ«
4. **ASRè½¬å½•**:è°ƒç”¨Whisperè¿›è¡Œè½¬å½•
5. **å¯¹è¯ç”Ÿæˆ**:è°ƒç”¨LLMç”Ÿæˆå›å¤
6. **TTSåˆæˆ**:å°†å›å¤è½¬ä¸ºè¯­éŸ³
7. **å‘é€å“åº”**:å‘é€æ–‡æœ¬+éŸ³é¢‘ç»™å®¢æˆ·ç«¯

#### 4. æ€§èƒ½ä¼˜åŒ–
- **å¼‚æ­¥å¤„ç†**:æ‰€æœ‰I/Oæ“ä½œå¼‚æ­¥åŒ–(asyncio)
- **æ¨¡å‹é‡åŒ–**:Whisperä½¿ç”¨int8é‡åŒ–(å‡å°‘50%å†…å­˜)
- **æµå¼è¾“å‡º**:TTSæµå¼åˆæˆ,é™ä½é¦–å­—å»¶è¿Ÿ
- **è¿æ¥æ± **:å¤ç”¨HTTPè¿æ¥(Edge TTS)
- **ç¼“å­˜**:å¸¸ç”¨çŸ­è¯­TTSç»“æœç¼“å­˜

---

## 2. å¯¹å¤–APIåˆ—è¡¨ä¸è§„æ ¼

### 2.1 ASR - è¯­éŸ³è½¬æ–‡å­—

**åŸºæœ¬ä¿¡æ¯**:
- åç§°:`TranscribeAudio`
- åè®®ä¸æ–¹æ³•:HTTP POST `/api/v1/asr/transcribe`
- å¹‚ç­‰æ€§:æ˜¯(ç›¸åŒéŸ³é¢‘è¿”å›ç›¸åŒç»“æœ)
- Content-Type:`multipart/form-data`

**è¯·æ±‚å‚æ•°**:
```python
# Form Data
file: UploadFile           # éŸ³é¢‘æ–‡ä»¶
language: str = "zh"       # è¯­è¨€ä»£ç (zh/en/ja/esç­‰)
provider: str = "whisper"  # ASRæä¾›å•†
```

**å­—æ®µè¡¨**:
| å­—æ®µ | ç±»å‹ | å¿…å¡« | é»˜è®¤ | çº¦æŸ | è¯´æ˜ |
|------|------|---:|------|------|------|
| file | file | æ˜¯ | - | éŸ³é¢‘æ–‡ä»¶,<100MB | æ”¯æŒwav/mp3/m4a/webm |
| language | string | å¦ | "zh" | ISO 639-1ä»£ç  | è‡ªåŠ¨æ£€æµ‹æˆ–æŒ‡å®šè¯­è¨€ |
| provider | string | å¦ | "whisper" | whisper/local | ASRæä¾›å•† |

**å“åº”ç»“æ„ä½“**:
```python
{
    "code": 0,
    "message": "success",
    "data": {
        "text": "ä»Šå¤©å¤©æ°”çœŸå¥½",                # å®Œæ•´è½¬å½•æ–‡æœ¬
        "language": "zh",                      # æ£€æµ‹åˆ°çš„è¯­è¨€
        "provider": "whisper",                 # ä½¿ç”¨çš„Provider
        "audio_duration": 3.5,                 # éŸ³é¢‘æ—¶é•¿(ç§’)
        "segments": [                          # åˆ†æ®µä¿¡æ¯
            {
                "start": 0.0,                  # å¼€å§‹æ—¶é—´(ç§’)
                "end": 1.2,                    # ç»“æŸæ—¶é—´(ç§’)
                "text": "ä»Šå¤©",                # ç‰‡æ®µæ–‡æœ¬
                "confidence": 0.95             # ç½®ä¿¡åº¦(0-1)
            },
            {
                "start": 1.2,
                "end": 2.5,
                "text": "å¤©æ°”çœŸå¥½",
                "confidence": 0.92
            }
        ],
        "elapsed_time": 1.23                   # å¤„ç†è€—æ—¶(ç§’)
    }
}
```

**å…¥å£å‡½æ•°ä¸æ ¸å¿ƒä»£ç **:
```python
# algo/voice-service/app/routes.py

@router.post("/asr/transcribe")
async def transcribe_audio(
    file: UploadFile = File(...),
    language: str = "zh",
    provider: str = "whisper",
    http_request: Request = None
):
    """ASR - è¯­éŸ³è½¬æ–‡å­—"""
    start_time = time.time()
    
    logger.business("ASRè¯·æ±‚", context={
        "filename": file.filename,
        "language": language,
        "provider": provider,
    })
    
    # 1. è¯»å–éŸ³é¢‘æ–‡ä»¶
    audio_data = await file.read()
    
    # 2. åˆå§‹åŒ–ASRæœåŠ¡
    from core.asr import WhisperService, ASRRouter
    
    asr_router = ASRRouter()
    
    if provider == "whisper":
        whisper_service = WhisperService(language=language)
        await whisper_service.start()  # åŠ è½½æ¨¡å‹
        asr_router.register_provider("whisper", whisper_service, is_default=True)
    
    # 3. æ‰§è¡Œè½¬å½•
    result = await asr_router.transcribe(
        audio_data=audio_data,
        provider=provider,
        language=language
    )
    
    elapsed_time = time.time() - start_time
    
    return success_response({
        "text": result.get("text", ""),
        "language": result.get("language", language),
        "provider": provider,
        "audio_duration": result.get("duration", 0.0),
        "segments": result.get("segments", []),
        "elapsed_time": elapsed_time,
    })
```

**è°ƒç”¨é“¾ä¸ä¸Šå±‚å‡½æ•°**:

```python
# 1. WhisperService.transcribe()
async def transcribe(
    self,
    audio_data: bytes,
    language: Optional[str] = None,
    task: str = "transcribe",
    vad_filter: bool = True
) -> Dict[str, Any]:
    """
    è½¬å½•éŸ³é¢‘
    
    å¤„ç†æµç¨‹:
    1. ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
    2. åœ¨executorä¸­æ‰§è¡Œè½¬å½•(é¿å…é˜»å¡äº‹ä»¶å¾ªç¯)
    3. è½¬æ¢segmentsä¸ºåˆ—è¡¨
    4. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    5. è¿”å›ç»“æœ{text, segments, language, duration}
    """
    if not self.model:
        raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
    
    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    import tempfile
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
        tmp_file.write(audio_data)
        tmp_path = tmp_file.name
    
    # åœ¨executorä¸­æ‰§è¡Œè½¬å½•(CPUå¯†é›†å‹ä»»åŠ¡)
    loop = asyncio.get_event_loop()
    segments, info = await loop.run_in_executor(
        None,
        lambda: self.model.transcribe(
            tmp_path,
            language=language or self.language,
            task=task,
            vad_filter=vad_filter  # å¯ç”¨VADè¿‡æ»¤å™ªéŸ³
        )
    )
    
    # è½¬æ¢segments
    segments_list = []
    full_text = []
    
    for segment in segments:
        segments_list.append({
            "start": segment.start,
            "end": segment.end,
            "text": segment.text,
            "confidence": getattr(segment, "avg_logprob", 0.0)
        })
        full_text.append(segment.text)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    Path(tmp_path).unlink(missing_ok=True)
    
    return {
        "text": " ".join(full_text),
        "segments": segments_list,
        "language": info.language,
        "duration": info.duration
    }

# 2. WhisperService.start() - åŠ è½½æ¨¡å‹
async def start(self):
    """å¯åŠ¨æœåŠ¡,åŠ è½½æ¨¡å‹"""
    from faster_whisper import WhisperModel
    
    # åœ¨executorä¸­åŠ è½½æ¨¡å‹(é¿å…é˜»å¡)
    loop = asyncio.get_event_loop()
    self.model = await loop.run_in_executor(
        None,
        lambda: WhisperModel(
            self.model_size,         # tiny/base/small/medium/large
            device=self.device,      # cpu/cuda
            compute_type=self.compute_type  # int8/float16/float32
        )
    )
    
    logger.info("Whisperæ¨¡å‹åŠ è½½æˆåŠŸ")
```

**æ—¶åºå›¾(ASRè¯·æ±‚â†’å“åº”)**:
```mermaid
sequenceDiagram
    autonumber
    participant Client as å®¢æˆ·ç«¯
    participant API as ASR API
    participant Router as ASRRouter
    participant Whisper as WhisperService
    participant Model as Whisperæ¨¡å‹
    
    Client->>API: POST /api/v1/asr/transcribe<br/>file=audio.mp3, language=zh
    
    API->>API: await file.read()<br/>è¯»å–éŸ³é¢‘æ•°æ®
    
    API->>Router: åˆå§‹åŒ–ASRRouter
    API->>Whisper: WhisperService(language="zh")
    
    API->>Whisper: await start()<br/>åŠ è½½æ¨¡å‹
    Whisper->>Model: WhisperModel.load(model_size, device, compute_type)
    note right of Model: faster-whisper<br/>int8é‡åŒ–<br/>å‡å°‘50%å†…å­˜
    Model-->>Whisper: æ¨¡å‹åŠ è½½å®Œæˆ
    
    API->>Router: register_provider("whisper", whisper_service)
    
    API->>Router: await transcribe(audio_data, provider="whisper")
    Router->>Whisper: await transcribe(audio_data, language="zh")
    
    Whisper->>Whisper: ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶<br/>tempfile.NamedTemporaryFile()
    
    Whisper->>Model: loop.run_in_executor(<br/>model.transcribe(tmp_path, vad_filter=True)<br/>)
    note right of Model: åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ<br/>é¿å…é˜»å¡asyncio
    
    Model->>Model: VADè¿‡æ»¤å™ªéŸ³
    Model->>Model: éŸ³é¢‘ç‰¹å¾æå–
    Model->>Model: Transformerè§£ç 
    Model->>Model: åˆ†æ®µè½¬å½•
    
    Model-->>Whisper: segments, info
    
    Whisper->>Whisper: è½¬æ¢segmentsä¸ºåˆ—è¡¨<br/>æå–text/start/end/confidence
    Whisper->>Whisper: æ¸…ç†ä¸´æ—¶æ–‡ä»¶<br/>Path.unlink()
    
    Whisper-->>Router: {text, segments, language, duration}
    Router-->>API: result
    
    API-->>Client: 200 OK<br/>{text, segments, elapsed_time}
```

**è¾¹ç•Œä¸å¼‚å¸¸**:
1. **éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒ**:
   - è¿”å›HTTP 400,é”™è¯¯ä¿¡æ¯"éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒ,ä»…æ”¯æŒwav/mp3/m4a/webm"
2. **æ–‡ä»¶è¿‡å¤§**:
   - é™åˆ¶<100MB
   - è¶…è¿‡é™åˆ¶è¿”å›HTTP 413,é”™è¯¯ä¿¡æ¯"éŸ³é¢‘æ–‡ä»¶è¿‡å¤§"
3. **æ¨¡å‹åŠ è½½å¤±è´¥**:
   - è¿”å›HTTP 503,é”™è¯¯ä¿¡æ¯"ASRæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
4. **è½¬å½•å¤±è´¥**:
   - éŸ³é¢‘æŸå/æ— è¯­éŸ³è¿”å›ç©ºtext
   - è¿”å›HTTP 200,textä¸ºç©ºå­—ç¬¦ä¸²

**å®è·µä¸æœ€ä½³å®è·µ**:

1. **éŸ³é¢‘æ ¼å¼å»ºè®®**:
   ```python
   # æœ€ä½³æ ¼å¼:WAV PCM 16kHz 16bitå•å£°é“
   # è½¬æ¢ç¤ºä¾‹(ffmpeg):
   ffmpeg -i input.mp3 -ar 16000 -ac 1 -sample_fmt s16 output.wav
   ```

2. **è¯­è¨€æ£€æµ‹**:
   ```python
   # è‡ªåŠ¨æ£€æµ‹(language=None)
   response = transcribe(file, language=None)
   
   # æŒ‡å®šè¯­è¨€(æ›´å¿«æ›´å‡†)
   response = transcribe(file, language="zh")
   ```

3. **é•¿éŸ³é¢‘å¤„ç†**:
   ```python
   # é•¿éŸ³é¢‘(>30ç§’)å»ºè®®åˆ†æ®µå¤„ç†
   # 1. VADåˆ†æ®µ
   vad_segments = detect_voice_activity(audio_file)
   
   # 2. åˆ†æ®µè½¬å½•
   for segment in vad_segments:
       segment_text = transcribe(segment_audio)
       full_text += segment_text
   ```

4. **æ€§èƒ½ä¼˜åŒ–**:
   - ä½¿ç”¨int8é‡åŒ–æ¨¡å‹(é€Ÿåº¦æå‡2å€,ç²¾åº¦æŸå¤±<1%)
   - GPUåŠ é€Ÿ(CUDA):é€Ÿåº¦æå‡5-10å€
   - æ‰¹å¤„ç†:å¤šä¸ªéŸ³é¢‘å¹¶è¡Œå¤„ç†

---

### 2.2 TTS - æ–‡å­—è½¬è¯­éŸ³

**åŸºæœ¬ä¿¡æ¯**:
- åç§°:`SynthesizeSpeech`
- åè®®ä¸æ–¹æ³•:HTTP POST `/api/v1/tts/synthesize`
- å¹‚ç­‰æ€§:æ˜¯
- Content-Type:`application/json`
- Response:StreamingResponse(audio/mpeg)

**è¯·æ±‚ç»“æ„ä½“**:
```python
class TTSRequest(BaseModel):
    text: str = Field(..., description="å¾…åˆæˆæ–‡æœ¬")
    voice: str = Field("zh-CN-XiaoxiaoNeural", description="è¯­éŸ³é€‰æ‹©")
    provider: str = Field("edge-tts", description="TTSæä¾›å•†")
    rate: str = Field("+0%", description="è¯­é€Ÿ(-50%~+100%)")
    pitch: str = Field("+0Hz", description="éŸ³è°ƒ")
```

**å­—æ®µè¡¨**:
| å­—æ®µ | ç±»å‹ | å¿…å¡« | é»˜è®¤ | çº¦æŸ | è¯´æ˜ |
|------|------|---:|------|------|------|
| text | string | æ˜¯ | - | é•¿åº¦â‰¤5000 | å¾…åˆæˆæ–‡æœ¬ |
| voice | string | å¦ | "zh-CN-XiaoxiaoNeural" | å¯ç”¨è¯­éŸ³åˆ—è¡¨ | è¯­éŸ³åç§° |
| provider | string | å¦ | "edge-tts" | edge-tts/openai/local | TTSæä¾›å•† |
| rate | string | å¦ | "+0%" | -50%~+100% | è¯­é€Ÿè°ƒæ•´ |
| pitch | string | å¦ | "+0Hz" | -50Hz~+50Hz | éŸ³è°ƒè°ƒæ•´ |

**å“åº”**:
```
Content-Type: audio/mpeg
X-Elapsed-Time: 1.23
X-Provider: edge-tts

[éŸ³é¢‘äºŒè¿›åˆ¶æ•°æ®(MP3æ ¼å¼)]
```

**å…¥å£å‡½æ•°ä¸æ ¸å¿ƒä»£ç **:
```python
@router.post("/tts/synthesize")
async def synthesize_speech(request: TTSRequest, http_request: Request):
    """TTS - æ–‡å­—è½¬è¯­éŸ³"""
    start_time = time.time()
    
    logger.business("TTSè¯·æ±‚", context={
        "text_length": len(request.text),
        "voice": request.voice,
        "provider": request.provider,
    })
    
    # 1. åˆå§‹åŒ–TTSæœåŠ¡
    from core.tts import EdgeTTSService, TTSRouter
    
    tts_router = TTSRouter()
    
    if request.provider == "edge-tts":
        edge_tts_service = EdgeTTSService(voice=request.voice)
        tts_router.register_provider("edge-tts", edge_tts_service, is_default=True)
    
    # 2. æ‰§è¡Œåˆæˆ
    audio_data = await tts_router.synthesize(
        text=request.text,
        provider=request.provider,
        voice=request.voice,
        rate=request.rate
    )
    
    elapsed_time = time.time() - start_time
    logger.info(f"TTSå®Œæˆ,è€—æ—¶: {elapsed_time:.2f}s")
    
    # 3. æµå¼è¿”å›éŸ³é¢‘
    def audio_stream():
        chunk_size = 4096
        for i in range(0, len(audio_data), chunk_size):
            yield audio_data[i:i + chunk_size]
    
    return StreamingResponse(
        audio_stream(),
        media_type="audio/mpeg",
        headers={
            "X-Elapsed-Time": str(elapsed_time),
            "X-Provider": request.provider,
        }
    )
```

**è°ƒç”¨é“¾**:
```python
# 1. EdgeTTSService.synthesize()
async def synthesize(
    self,
    text: str,
    voice: Optional[str] = None,
    rate: str = "+0%",
    volume: str = "+0%"
) -> bytes:
    """
    åˆæˆè¯­éŸ³
    
    å®ç°:
    1. ä½¿ç”¨edge_ttsåº“è°ƒç”¨å¾®è½¯Edge TTS API(å…è´¹)
    2. æµå¼æ¥æ”¶éŸ³é¢‘å—
    3. åˆå¹¶æ‰€æœ‰éŸ³é¢‘å—
    4. è¿”å›å®Œæ•´MP3æ•°æ®
    """
    voice_name = voice or self.voice
    
    # ä½¿ç”¨edge_ttsåˆæˆ
    communicate = edge_tts.Communicate(
        text=text,
        voice=voice_name,
        rate=rate,      # è¯­é€Ÿ: -50%~+100%
        volume=volume   # éŸ³é‡: -50%~+100%
    )
    
    # æ”¶é›†æ‰€æœ‰éŸ³é¢‘å—
    audio_chunks = []
    async for chunk in communicate.stream():
        if chunk["type"] == "audio":
            audio_chunks.append(chunk["data"])
    
    audio_data = b"".join(audio_chunks)
    
    logger.debug(f"åˆæˆå®Œæˆ: {len(text)} å­—ç¬¦ -> {len(audio_data)} å­—èŠ‚")
    return audio_data

# 2. Edge TTSæµå¼åˆæˆ
async def synthesize_stream(
    self,
    text: str,
    voice: Optional[str] = None
) -> AsyncGenerator[bytes, None]:
    """
    æµå¼åˆæˆè¯­éŸ³
    
    ä¼˜åŠ¿:
    - é™ä½é¦–å­—å»¶è¿Ÿ(TTFB<500ms)
    - è¾¹åˆæˆè¾¹æ’­æ”¾
    - é€‚åˆé•¿æ–‡æœ¬
    """
    voice_name = voice or self.voice
    
    communicate = edge_tts.Communicate(text=text, voice=voice_name)
    
    async for chunk in communicate.stream():
        if chunk["type"] == "audio":
            yield chunk["data"]  # ç«‹å³è¿”å›éŸ³é¢‘å—
```

**å¯ç”¨è¯­éŸ³åˆ—è¡¨**:
```python
@router.get("/voices")
async def list_voices(provider: str = "edge-tts"):
    """è·å–å¯ç”¨è¯­éŸ³åˆ—è¡¨"""
    from core.tts import EdgeTTSService
    
    if provider == "edge-tts":
        voices = await EdgeTTSService.get_available_voices()
    
    # å“åº”ç¤ºä¾‹:
    return {
        "provider": "edge-tts",
        "voices": [
            {
                "name": "zh-CN-XiaoxiaoNeural",
                "gender": "Female",
                "locale": "zh-CN"
            },
            {
                "name": "zh-CN-YunxiNeural",
                "gender": "Male",
                "locale": "zh-CN"
            },
            # ... 200+ voices
        ],
        "count": 200+
    }
```

---

### 2.3 VAD - è¯­éŸ³æ´»åŠ¨æ£€æµ‹

**åŸºæœ¬ä¿¡æ¯**:
- åç§°:`DetectVoiceActivity`
- åè®®ä¸æ–¹æ³•:HTTP POST `/api/v1/vad/detect`
- å¹‚ç­‰æ€§:æ˜¯

**è¯·æ±‚å‚æ•°**:
```python
# Form Data
file: UploadFile  # éŸ³é¢‘æ–‡ä»¶
```

**å“åº”ç»“æ„ä½“**:
```python
{
    "code": 0,
    "message": "success",
    "data": {
        "segments": [                          # è¯­éŸ³ç‰‡æ®µ
            {"start": 0.5, "end": 2.3},       # ç¬¬1æ®µ:0.5s~2.3s
            {"start": 3.1, "end": 5.8}        # ç¬¬2æ®µ:3.1s~5.8s
        ],
        "total_speech_duration": 4.5,         # æ€»è¯­éŸ³æ—¶é•¿(ç§’)
        "total_silence_duration": 1.5,        # æ€»é™éŸ³æ—¶é•¿(ç§’)
        "total_duration": 6.0                 # æ€»æ—¶é•¿(ç§’)
    }
}
```

**æ ¸å¿ƒå®ç° - Silero VAD**:
```python
# algo/voice-service/core/vad/silero_vad.py

class SileroVADDetector:
    """
    Silero VADæ£€æµ‹å™¨ - é«˜ç²¾åº¦è¯­éŸ³æ´»åŠ¨æ£€æµ‹
    
    ç‰¹ç‚¹:
    - é«˜å‡†ç¡®ç‡(F1 > 0.95)
    - ä½å»¶è¿Ÿ(<50ms)
    - æ”¯æŒå¤šç§é‡‡æ ·ç‡(8000/16000)
    - è‡ªé€‚åº”é˜ˆå€¼
    - å¹³æ»‘å¤„ç†
    
    å‚æ•°:
    - threshold: VADé˜ˆå€¼(0.0-1.0),é»˜è®¤0.5
    - sampling_rate: é‡‡æ ·ç‡,æ”¯æŒ8000/16000
    - min_speech_duration_ms: æœ€å°è¯­éŸ³æŒç»­æ—¶é—´(250ms)
    - min_silence_duration_ms: æœ€å°é™éŸ³æŒç»­æ—¶é—´(100ms)
    - speech_pad_ms: è¯­éŸ³è¾¹ç•Œå¡«å……(30ms)
    """
    
    def __init__(
        self,
        model_path: Optional[str] = None,
        threshold: float = 0.5,
        sampling_rate: int = 16000,
        min_speech_duration_ms: int = 250,
        min_silence_duration_ms: int = 100,
        speech_pad_ms: int = 30,
        use_onnx: bool = False,
    ):
        self.threshold = threshold
        self.sampling_rate = sampling_rate
        self.min_speech_duration_ms = min_speech_duration_ms
        self.min_silence_duration_ms = min_silence_duration_ms
        
        # åŠ è½½æ¨¡å‹(JITæˆ–ONNX)
        self.model = self._load_model(model_path, use_onnx)
        self.model.eval()
        
        # å†…éƒ¨çŠ¶æ€
        self._reset_states()
    
    def _load_jit_model(self, model_path: Optional[str]):
        """
        åŠ è½½JITæ¨¡å‹
        
        å¦‚æœmodel_pathä¸ºNone,è‡ªåŠ¨ä»torch.hubä¸‹è½½:
        torch.hub.load('snakers4/silero-vad', 'silero_vad')
        """
        if model_path is None:
            logger.info("è‡ªåŠ¨ä¸‹è½½Silero VADæ¨¡å‹...")
            model, utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False,
                onnx=False
            )
            return model
        else:
            model = torch.jit.load(model_path)
            return model
    
    async def detect(self, audio_chunk: bytes) -> VADResult:
        """
        æ£€æµ‹è¯­éŸ³æ´»åŠ¨(å¼‚æ­¥æ¥å£)
        
        å¤„ç†æµç¨‹:
        1. éŸ³é¢‘é¢„å¤„ç†(PCM -> å½’ä¸€åŒ–float32)
        2. æ¨¡å‹æ¨ç†(speech_prob = model(audio, sr))
        3. åå¤„ç†(å¹³æ»‘æ¦‚ç‡,æ»‘åŠ¨çª—å£å¹³å‡)
        4. çŠ¶æ€åˆ¤æ–­(is_speech = prob >= threshold)
        5. çŠ¶æ€è½¬æ¢(æœ€å°æŒç»­æ—¶é—´æ£€æŸ¥,é¿å…é¢‘ç¹åˆ‡æ¢)
        6. è¿”å›ç»“æœ(VADResult)
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.detect_sync, audio_chunk)
    
    def detect_sync(self, audio_chunk: bytes) -> VADResult:
        """åŒæ­¥æ£€æµ‹æ¥å£"""
        self.stats["total_chunks"] += 1
        
        # 1. éŸ³é¢‘é¢„å¤„ç†
        audio_float = self._preprocess_audio(audio_chunk)
        
        # 2. æ¨¡å‹æ¨ç†
        with torch.no_grad():
            speech_prob = self.model(audio_float, self.sampling_rate).item()
        
        # 3. åå¤„ç†(å¹³æ»‘)
        smoothed_prob = self._smooth_probability(speech_prob)
        
        # 4. çŠ¶æ€åˆ¤æ–­
        is_speech = smoothed_prob >= self.threshold
        
        # 5. çŠ¶æ€è½¬æ¢
        result = self._update_state(is_speech, smoothed_prob)
        
        # 6. æ›´æ–°ç»Ÿè®¡
        if is_speech:
            self.stats["speech_chunks"] += 1
        else:
            self.stats["silence_chunks"] += 1
        
        return result
    
    def _preprocess_audio(self, audio_chunk: bytes) -> torch.Tensor:
        """
        é¢„å¤„ç†éŸ³é¢‘
        
        1. å­—èŠ‚æµ -> numpyæ•°ç»„(int16)
        2. å½’ä¸€åŒ–åˆ°[-1, 1](float32)
        3. è½¬æ¢ä¸ºtorchå¼ é‡
        """
        audio_np = np.frombuffer(audio_chunk, dtype=np.int16)
        audio_float = audio_np.astype(np.float32) / 32768.0
        audio_tensor = torch.from_numpy(audio_float)
        
        if audio_tensor.dim() > 1:
            audio_tensor = audio_tensor.squeeze()
        
        return audio_tensor
    
    def _smooth_probability(self, prob: float) -> float:
        """
        å¹³æ»‘æ¦‚ç‡å€¼
        
        ä½¿ç”¨æ»‘åŠ¨çª—å£åŠ æƒå¹³å‡:
        - ä¿ç•™æœ€è¿‘5ä¸ªæ¦‚ç‡å€¼
        - è¶Šæ–°çš„æƒé‡è¶Šå¤§(çº¿æ€§æƒé‡0.5~1.0)
        - å‡å°‘å™ªå£°å¯¼è‡´çš„è¯¯åˆ¤
        """
        self.prob_history.append(prob)
        
        if len(self.prob_history) > self.history_size:
            self.prob_history.pop(0)
        
        # åŠ æƒå¹³å‡
        weights = np.linspace(0.5, 1.0, len(self.prob_history))
        weighted_prob = np.average(self.prob_history, weights=weights)
        
        return float(weighted_prob)
    
    def _update_state(self, is_speech: bool, probability: float) -> VADResult:
        """
        æ›´æ–°VADçŠ¶æ€
        
        å®ç°æœ€å°æŒç»­æ—¶é—´æ£€æŸ¥:
        - é™éŸ³->è¯­éŸ³: æŒç»­>=min_speech_duration_msæ‰è½¬æ¢
        - è¯­éŸ³->é™éŸ³: æŒç»­>=min_silence_duration_msæ‰è½¬æ¢
        - é¿å…é¢‘ç¹åˆ‡æ¢
        """
        timestamp = self.current_sample / self.sampling_rate
        
        if is_speech:
            if self.state == VADState.SILENCE:
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°è¯­éŸ³æŒç»­æ—¶é—´
                if not self.triggered:
                    self.triggered = True
                    self.temp_end = self.current_sample
                
                triggered_duration_ms = (
                    (self.current_sample - self.temp_end) / 
                    self.sampling_rate * 1000
                )
                
                if triggered_duration_ms >= self.min_speech_duration_ms:
                    self.state = VADState.SPEECH
                    logger.debug(f"æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹: {timestamp:.2f}s")
        else:
            if self.state == VADState.SPEECH:
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°é™éŸ³æŒç»­æ—¶é—´
                if self.triggered:
                    silence_duration_ms = (
                        (self.current_sample - self.temp_end) / 
                        self.sampling_rate * 1000
                    )
                    
                    if silence_duration_ms >= self.min_silence_duration_ms:
                        self.state = VADState.SILENCE
                        self.triggered = False
                        logger.debug(f"æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ: {timestamp:.2f}s")
                else:
                    self.temp_end = self.current_sample
                    self.triggered = True
        
        self.current_sample += len(self.prob_history)
        
        return VADResult(
            is_speech=(self.state == VADState.SPEECH),
            probability=probability,
            state=self.state,
            timestamp=timestamp,
            duration=0.0
        )
```

---

### 2.4 æƒ…æ„Ÿè¯†åˆ«

**åŸºæœ¬ä¿¡æ¯**:
- åç§°:`RecognizeEmotion`
- åè®®ä¸æ–¹æ³•:HTTP POST `/api/v1/emotion/recognize`
- å¹‚ç­‰æ€§:æ˜¯

**å“åº”ç»“æ„ä½“**:
```python
{
    "code": 0,
    "message": "success",
    "data": {
        "emotion": "happy",                    # ä¸»è¦æƒ…æ„Ÿ
        "emotion_zh": "å¼€å¿ƒ",                  # ä¸­æ–‡æƒ…æ„Ÿåç§°
        "confidence": 0.78,                    # ç½®ä¿¡åº¦
        "intensity": 0.65,                     # æƒ…æ„Ÿå¼ºåº¦(0-1)
        "all_scores": {                        # æ‰€æœ‰æƒ…æ„Ÿå¾—åˆ†
            "neutral": 0.12,
            "happy": 0.78,
            "sad": 0.05,
            "angry": 0.03,
            "surprised": 0.02
        },
        "features": {                          # éŸ³é¢‘ç‰¹å¾
            "energy": 0.35,                    # èƒ½é‡
            "zero_crossing_rate": 0.12         # è¿‡é›¶ç‡(è¯­é€Ÿ)
        }
    }
}
```

**æ ¸å¿ƒå®ç°**:
```python
# algo/voice-service/core/emotion/emotion_recognizer.py

class EmotionRecognizer:
    """
    æƒ…æ„Ÿè¯†åˆ«å™¨
    
    æ”¯æŒçš„æƒ…æ„Ÿç±»å‹:
    - neutral(ä¸­æ€§)
    - happy(å¼€å¿ƒ)
    - sad(æ‚²ä¼¤)
    - angry(æ„¤æ€’)
    - surprised(æƒŠè®¶)
    - fearful(ææƒ§)
    - disgusted(åŒæ¶)
    
    è¯†åˆ«æ–¹æ³•:
    1. åŸºäºè§„åˆ™çš„æƒ…æ„Ÿåˆ†ç±»(éŸ³é¢‘ç‰¹å¾+æ–‡æœ¬å…³é”®è¯)
    2. æ·±åº¦å­¦ä¹ æ¨¡å‹(é¢„ç•™æ¥å£)
    """
    
    async def recognize(
        self,
        audio_data: bytes,
        text: Optional[str] = None,
        sample_rate: int = 16000
    ) -> Dict[str, Any]:
        """
        è¯†åˆ«éŸ³é¢‘æƒ…æ„Ÿ
        
        å¤„ç†æµç¨‹:
        1. æå–éŸ³é¢‘ç‰¹å¾(energy, zcr, pitch)
        2. åŸºäºè§„åˆ™åˆ†ç±»
        3. ç»“åˆæ–‡æœ¬æƒ…æ„Ÿ(å¯é€‰)
        4. è®¡ç®—æƒ…æ„Ÿå¼ºåº¦
        5. è¿”å›ç»“æœ
        """
        # 1. æå–éŸ³é¢‘ç‰¹å¾
        features = await self._extract_features(audio_data, sample_rate)
        
        # 2. æƒ…æ„Ÿåˆ†ç±»
        if self.model_type == "rule_based":
            emotion_result = self._classify_rule_based(features, text)
        else:
            emotion_result = self._classify_deep_learning(features, text)
        
        return emotion_result
    
    async def _extract_features(
        self,
        audio_data: bytes,
        sample_rate: int
    ) -> Dict[str, float]:
        """
        æå–éŸ³é¢‘ç‰¹å¾
        
        ç‰¹å¾åŒ…æ‹¬:
        - energy_mean/std: èƒ½é‡å‡å€¼å’Œæ ‡å‡†å·®
        - zero_crossing_rate: è¿‡é›¶ç‡(è¯­é€Ÿä¼°è®¡)
        - pitch_mean/std: éŸ³é«˜å‡å€¼å’Œæ ‡å‡†å·®
        - duration: éŸ³é¢‘æ—¶é•¿
        - speech_rate: è¯­é€Ÿ
        """
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32)
        
        # å½’ä¸€åŒ–
        if len(audio_array) > 0:
            audio_array = audio_array / np.max(np.abs(audio_array) + 1e-8)
        
        features = {}
        
        # èƒ½é‡ç‰¹å¾
        energy = np.sum(audio_array ** 2) / len(audio_array)
        features['energy_mean'] = float(energy)
        features['energy_std'] = float(np.std(audio_array ** 2))
        
        # è¿‡é›¶ç‡(è¯­é€Ÿç²—ç•¥ä¼°è®¡)
        zero_crossings = np.sum(np.abs(np.diff(np.sign(audio_array)))) / 2
        features['zero_crossing_rate'] = float(zero_crossings / len(audio_array))
        
        # éŸ³é«˜ç‰¹å¾(ç®€åŒ–,å®é™…åº”ä½¿ç”¨librosaæå–F0)
        features['pitch_mean'] = 200.0
        features['pitch_std'] = 50.0
        
        # è¯­é€Ÿç‰¹å¾
        duration = len(audio_array) / sample_rate
        features['duration'] = duration
        features['speech_rate'] = 1.0
        
        return features
    
    def _classify_rule_based(
        self,
        features: Dict[str, float],
        text: Optional[str]
    ) -> Dict[str, Any]:
        """
        åŸºäºè§„åˆ™çš„æƒ…æ„Ÿåˆ†ç±»
        
        è§„åˆ™:
        - é«˜èƒ½é‡ + å¿«è¯­é€Ÿ â†’ æ¿€åŠ¨/å¼€å¿ƒ/æ„¤æ€’
        - ä½èƒ½é‡ + æ…¢è¯­é€Ÿ â†’ æ‚²ä¼¤/ææƒ§
        - æ­£å¸¸èƒ½é‡ + æ­£å¸¸è¯­é€Ÿ â†’ ä¸­æ€§
        - ç»“åˆæ–‡æœ¬å…³é”®è¯å¢å¼ºåˆ¤æ–­
        """
        energy = features['energy_mean']
        zcr = features['zero_crossing_rate']
        
        # åˆå§‹åŒ–æƒ…æ„Ÿå¾—åˆ†
        emotion_scores = {
            "neutral": 0.5,
            "happy": 0.0,
            "sad": 0.0,
            "angry": 0.0,
            "surprised": 0.0,
            "fearful": 0.0,
            "disgusted": 0.0
        }
        
        # èƒ½é‡åˆ†æ
        if energy > 0.3:
            # é«˜èƒ½é‡
            emotion_scores["happy"] += 0.3
            emotion_scores["angry"] += 0.25
            emotion_scores["surprised"] += 0.2
        elif energy < 0.1:
            # ä½èƒ½é‡
            emotion_scores["sad"] += 0.3
            emotion_scores["fearful"] += 0.2
        
        # è¿‡é›¶ç‡åˆ†æ(è¯­é€Ÿ)
        if zcr > 0.15:
            # å¿«è¯­é€Ÿ
            emotion_scores["happy"] += 0.2
            emotion_scores["angry"] += 0.2
            emotion_scores["surprised"] += 0.15
        elif zcr < 0.05:
            # æ…¢è¯­é€Ÿ
            emotion_scores["sad"] += 0.2
            emotion_scores["fearful"] += 0.15
        
        # æ–‡æœ¬æƒ…æ„Ÿåˆ†æ(å¯é€‰)
        if text:
            text_emotion = self._analyze_text_emotion(text)
            for emotion, score in text_emotion.items():
                emotion_scores[emotion] += score * 0.3
        
        # å½’ä¸€åŒ–
        total_score = sum(emotion_scores.values())
        if total_score > 0:
            emotion_scores = {k: v / total_score for k, v in emotion_scores.items()}
        
        # é€‰æ‹©æœ€é«˜åˆ†æƒ…æ„Ÿ
        primary_emotion = max(emotion_scores.items(), key=lambda x: x[1])
        
        # è®¡ç®—æƒ…æ„Ÿå¼ºåº¦
        intensity = self._calculate_intensity(features)
        
        return {
            "emotion": primary_emotion[0],
            "emotion_zh": self.EMOTIONS.get(primary_emotion[0], "æœªçŸ¥"),
            "confidence": float(primary_emotion[1]),
            "intensity": intensity,
            "all_scores": emotion_scores,
            "features": {
                "energy": energy,
                "zero_crossing_rate": zcr
            }
        }
    
    def _analyze_text_emotion(self, text: str) -> Dict[str, float]:
        """
        æ–‡æœ¬æƒ…æ„Ÿåˆ†æ
        
        åŸºäºå…³é”®è¯åŒ¹é…:
        - happy: å¼€å¿ƒ, é«˜å…´, å¿«ä¹, å“ˆå“ˆ, ğŸ˜Š
        - sad: éš¾è¿‡, ä¼¤å¿ƒ, æ‚²ä¼¤, å“­, ğŸ˜¢
        - angry: ç”Ÿæ°”, æ„¤æ€’, æ¼ç«, ğŸ˜ 
        - ...
        """
        emotion_keywords = {
            "happy": ["å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "å“ˆå“ˆ", "ğŸ˜Š", "ğŸ˜„"],
            "sad": ["éš¾è¿‡", "ä¼¤å¿ƒ", "æ‚²ä¼¤", "å“­", "ğŸ˜¢", "ğŸ˜­"],
            "angry": ["ç”Ÿæ°”", "æ„¤æ€’", "æ¼ç«", "æ°”", "ğŸ˜ ", "ğŸ˜¡"],
            "surprised": ["æƒŠè®¶", "éœ‡æƒŠ", "æ²¡æƒ³åˆ°", "ğŸ˜®", "ğŸ˜²"],
            "fearful": ["å®³æ€•", "ææƒ§", "æ‹…å¿ƒ", "ğŸ˜¨", "ğŸ˜°"],
        }
        
        scores = {emotion: 0.0 for emotion in emotion_keywords.keys()}
        
        text_lower = text.lower()
        for emotion, keywords in emotion_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    scores[emotion] += 1.0
        
        return scores
```

---

### 2.5 WebSocketå®æ—¶è¯­éŸ³æµ

**åŸºæœ¬ä¿¡æ¯**:
- åç§°:`WebSocketVoiceStream`
- åè®®:WebSocket `ws://localhost:8002/api/v1/stream`
- è¿æ¥å‚æ•°:`?session_id=xxx&user_id=yyy`

**æ¶ˆæ¯æ ¼å¼**:

```python
# å®¢æˆ·ç«¯ -> æœåŠ¡ç«¯

# 1. éŸ³é¢‘æ•°æ®(äºŒè¿›åˆ¶)
websocket.send_bytes(audio_chunk)  # PCM 16kHz 16bit

# 2. æ§åˆ¶æ¶ˆæ¯(JSON)
{
    "type": "ping"          # å¿ƒè·³
}
{
    "type": "cancel"        # å–æ¶ˆå½“å‰å¤„ç†
}
{
    "type": "clear_history" # æ¸…ç©ºå¯¹è¯å†å²
}

# æœåŠ¡ç«¯ -> å®¢æˆ·ç«¯

# 1. è¿æ¥ç¡®è®¤
{
    "type": "connected",
    "session_id": "xxx",
    "message": "è¯­éŸ³è¿æ¥å·²å»ºç«‹"
}

# 2. è½¬å½•ç»“æœ
{
    "type": "transcript",
    "text": "ä»Šå¤©å¤©æ°”çœŸå¥½",
    "is_final": true,
    "confidence": 0.95
}

# 3. å¯¹è¯å“åº”
{
    "type": "response",
    "text": "æ˜¯çš„ï¼Œä»Šå¤©å¤©æ°”ä¸é”™"
}

# 4. éŸ³é¢‘å“åº”(äºŒè¿›åˆ¶)
websocket.send_bytes(audio_data)  # MP3æ ¼å¼

# 5. éŸ³é¢‘å®Œæˆ
{
    "type": "audio_complete",
    "message": "éŸ³é¢‘æ’­æ”¾å®Œæˆ"
}

# 6. é”™è¯¯
{
    "type": "error",
    "message": "å¤„ç†å¤±è´¥"
}
```

**æ ¸å¿ƒå®ç°**:
```python
# algo/voice-service/core/realtime/websocket_voice_handler.py

class WebSocketVoiceHandler:
    """WebSocketè¯­éŸ³å¤„ç†å™¨"""
    
    def __init__(self):
        self.asr_service = ASRService(model_size="base")
        self.vad_service = VADService()
        self.tts_service = TTSService()
        
        # ä¼šè¯ç®¡ç†
        self.sessions: Dict[str, Dict] = {}
    
    async def handle_connection(
        self,
        websocket: WebSocket,
        session_id: str,
        user_id: str
    ):
        """
        å¤„ç†WebSocketè¿æ¥
        
        æµç¨‹:
        1. æ¥å—è¿æ¥
        2. åˆå§‹åŒ–ä¼šè¯
        3. è¿›å…¥æ¶ˆæ¯å¾ªç¯
        4. å¼‚å¸¸å¤„ç†
        5. æ¸…ç†ä¼šè¯
        """
        await websocket.accept()
        
        # åˆå§‹åŒ–ä¼šè¯
        self.sessions[session_id] = {
            'user_id': user_id,
            'audio_buffer': b"",           # éŸ³é¢‘ç¼“å†²åŒº
            'conversation_history': [],     # å¯¹è¯å†å²
            'is_active': True
        }
        
        # å‘é€è¿æ¥ç¡®è®¤
        await websocket.send_json({
            'type': 'connected',
            'session_id': session_id,
            'message': 'è¯­éŸ³è¿æ¥å·²å»ºç«‹'
        })
        
        logger.info(f"WebSocketè¿æ¥å»ºç«‹: session={session_id}")
        
        try:
            await self._message_loop(websocket, session_id)
        except WebSocketDisconnect:
            logger.info(f"WebSocketè¿æ¥æ–­å¼€: session={session_id}")
        finally:
            # æ¸…ç†ä¼šè¯(å»¶è¿Ÿ300ç§’,å…è®¸é‡è¿)
            if session_id in self.sessions:
                self.sessions[session_id]['is_active'] = False
                asyncio.create_task(self._cleanup_session(session_id, delay=300))
    
    async def _message_loop(self, websocket: WebSocket, session_id: str):
        """
        æ¶ˆæ¯å¾ªç¯
        
        å¤„ç†ä¸¤ç±»æ¶ˆæ¯:
        1. éŸ³é¢‘æ•°æ®(bytes) -> _handle_audio()
        2. æ§åˆ¶æ¶ˆæ¯(JSON) -> _handle_control_message()
        """
        session = self.sessions[session_id]
        
        while session['is_active']:
            data = await websocket.receive()
            
            if 'bytes' in data:
                # éŸ³é¢‘æ•°æ®
                await self._handle_audio(websocket, session_id, data['bytes'])
            elif 'text' in data:
                # æ§åˆ¶æ¶ˆæ¯
                message = json.loads(data['text'])
                await self._handle_control_message(websocket, session_id, message)
    
    async def _handle_audio(
        self,
        websocket: WebSocket,
        session_id: str,
        audio_chunk: bytes
    ):
        """
        å¤„ç†éŸ³é¢‘æ•°æ®
        
        æµç¨‹:
        1. VADæ£€æµ‹æ˜¯å¦ä¸ºè¯­éŸ³
        2. å¦‚æœæ˜¯è¯­éŸ³,ç´¯ç§¯åˆ°ç¼“å†²åŒº
        3. ç¼“å†²åŒºè¾¾åˆ°é˜ˆå€¼(çº¦3ç§’),è¿›è¡ŒASRè¯†åˆ«
        4. å‘é€è½¬å½•ç»“æœ
        5. å¦‚æœæ˜¯æœ€ç»ˆç»“æœ,è°ƒç”¨å¯¹è¯å¤„ç†
        """
        session = self.sessions[session_id]
        
        # VADæ£€æµ‹
        is_speech = self.vad_service.is_speech(audio_chunk)
        
        if is_speech:
            # ç´¯ç§¯éŸ³é¢‘
            session['audio_buffer'] += audio_chunk
            
            # è¾¾åˆ°é˜ˆå€¼(3ç§’: 16000Hz * 2bytes * 3s = 96000bytes)
            buffer_length = len(session['audio_buffer'])
            threshold = 16000 * 2 * 3
            
            if buffer_length >= threshold:
                # ASRè¯†åˆ«
                result = await self.asr_service.transcribe_stream(
                    session['audio_buffer']
                )
                
                if result['text']:
                    # å‘é€è¯†åˆ«ç»“æœ
                    await websocket.send_json({
                        'type': 'transcript',
                        'text': result['text'],
                        'is_final': result['is_final'],
                        'confidence': result['confidence']
                    })
                    
                    # æœ€ç»ˆç»“æœ,è¿›è¡Œå¯¹è¯
                    if result['is_final']:
                        await self._process_dialogue(
                            websocket,
                            session_id,
                            result['text']
                        )
                
                # æ¸…ç©ºç¼“å†²åŒº(ä¿ç•™æœ€å1ç§’ç”¨äºä¸Šä¸‹æ–‡)
                overlap = 16000 * 2 * 1
                session['audio_buffer'] = session['audio_buffer'][-overlap:]
    
    async def _process_dialogue(
        self,
        websocket: WebSocket,
        session_id: str,
        user_text: str
    ):
        """
        å¤„ç†å¯¹è¯
        
        æµç¨‹:
        1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        2. å‘é€å¤„ç†ä¸­çŠ¶æ€
        3. è°ƒç”¨LLMç”Ÿæˆå›å¤
        4. æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
        5. å‘é€æ–‡æœ¬å›å¤
        6. TTSåˆæˆè¯­éŸ³
        7. å‘é€éŸ³é¢‘å›å¤
        """
        session = self.sessions[session_id]
        
        # 1. æ·»åŠ åˆ°å¯¹è¯å†å²
        session['conversation_history'].append({
            'role': 'user',
            'content': user_text
        })
        
        # 2. å‘é€å¤„ç†ä¸­çŠ¶æ€
        await websocket.send_json({
            'type': 'processing',
            'message': 'æ­£åœ¨æ€è€ƒ...'
        })
        
        # 3. ç”Ÿæˆå›å¤(å®é™…åº”è°ƒç”¨LLM)
        response_text = f"æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯ï¼š{user_text}"
        
        # 4. æ·»åŠ åˆ°å†å²
        session['conversation_history'].append({
            'role': 'assistant',
            'content': response_text
        })
        
        # 5. å‘é€æ–‡æœ¬å›å¤
        await websocket.send_json({
            'type': 'response',
            'text': response_text
        })
        
        # 6. TTSåˆæˆ
        audio_data = await self.tts_service.synthesize(response_text)
        
        # 7. å‘é€éŸ³é¢‘å›å¤
        await websocket.send_bytes(audio_data)
        
        await websocket.send_json({
            'type': 'audio_complete',
            'message': 'éŸ³é¢‘æ’­æ”¾å®Œæˆ'
        })
```

**WebSocketæ—¶åºå›¾**:
```mermaid
sequenceDiagram
    autonumber
    participant Client as å®¢æˆ·ç«¯
    participant WS as WebSocket
    participant Handler as WebSocketVoiceHandler
    participant VAD as VAD Service
    participant ASR as ASR Service
    participant LLM as LLM Router
    participant TTS as TTS Service
    
    Client->>WS: WebSocketè¿æ¥<br/>ws://host/api/v1/stream?session_id=xxx
    WS->>Handler: handle_connection()
    
    Handler->>Handler: åˆå§‹åŒ–ä¼šè¯<br/>{audio_buffer, history, is_active}
    Handler-->>Client: {"type":"connected"}
    
    loop å®æ—¶éŸ³é¢‘æµ
        Client->>Handler: send_bytes(audio_chunk)
        Handler->>VAD: is_speech(audio_chunk)
        
        alt æ˜¯è¯­éŸ³
            VAD-->>Handler: true
            Handler->>Handler: audio_buffer += audio_chunk
            
            alt ç¼“å†²åŒºè¾¾åˆ°é˜ˆå€¼(3ç§’)
                Handler->>ASR: transcribe_stream(audio_buffer)
                ASR-->>Handler: {text, is_final, confidence}
                
                Handler-->>Client: {"type":"transcript","text":"..."}
                
                alt æœ€ç»ˆç»“æœ
                    Handler->>Handler: æ·»åŠ åˆ°conversation_history
                    Handler-->>Client: {"type":"processing"}
                    
                    Handler->>LLM: POST /api/v1/chat<br/>{messages: history}
                    LLM-->>Handler: {content: "å›å¤æ–‡æœ¬"}
                    
                    Handler->>Handler: æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
                    Handler-->>Client: {"type":"response","text":"å›å¤æ–‡æœ¬"}
                    
                    Handler->>TTS: synthesize(response_text)
                    TTS-->>Handler: audio_data(MP3)
                    
                    Handler-->>Client: send_bytes(audio_data)
                    Handler-->>Client: {"type":"audio_complete"}
                end
                
                Handler->>Handler: æ¸…ç©ºç¼“å†²åŒº<br/>(ä¿ç•™1ç§’overlap)
            end
        else é™éŸ³
            VAD-->>Handler: false
            Handler->>Handler: è·³è¿‡
        end
    end
    
    Client->>WS: å…³é—­è¿æ¥
    Handler->>Handler: is_active=false
    Handler->>Handler: å»¶è¿Ÿæ¸…ç†ä¼šè¯(300ç§’)
```

---

## 3. å…³é”®æ•°æ®ç»“æ„ä¸UMLå›¾

```mermaid
classDiagram
    class ASRRouter {
        -Dict providers
        -str default_provider
        +register_provider(name, provider, is_default)
        +transcribe(audio_data, provider, **kwargs) Dict
    }
    
    class WhisperService {
        -str model_size
        -str device
        -str compute_type
        -str language
        -WhisperModel model
        +start() void
        +transcribe(audio_data, language, task, vad_filter) Dict
        +stop() void
    }
    
    class TTSRouter {
        -Dict providers
        -str default_provider
        +register_provider(name, provider, is_default)
        +synthesize(text, provider, **kwargs) bytes
    }
    
    class EdgeTTSService {
        -str voice
        +synthesize(text, voice, rate, volume) bytes
        +synthesize_stream(text, voice) AsyncGenerator
        +get_available_voices() List
    }
    
    class SileroVADDetector {
        -float threshold
        -int sampling_rate
        -int min_speech_duration_ms
        -int min_silence_duration_ms
        -torch.Module model
        -VADState state
        -List prob_history
        +detect(audio_chunk) VADResult
        +detect_sync(audio_chunk) VADResult
        -_preprocess_audio(audio_chunk) torch.Tensor
        -_smooth_probability(prob) float
        -_update_state(is_speech, probability) VADResult
        +reset()
        +set_threshold(threshold)
        +get_stats() Dict
    }
    
    class EmotionRecognizer {
        -str model_type
        +recognize(audio_data, text, sample_rate) Dict
        -_extract_features(audio_data, sample_rate) Dict
        -_classify_rule_based(features, text) Dict
        -_classify_deep_learning(features, text) Dict
        -_analyze_text_emotion(text) Dict
        -_calculate_intensity(features) float
        +recognize_batch(audio_segments, texts) List
        +get_emotion_statistics(emotion_results) Dict
    }
    
    class WebSocketVoiceHandler {
        -ASRService asr_service
        -VADService vad_service
        -TTSService tts_service
        -Dict sessions
        +handle_connection(websocket, session_id, user_id)
        -_message_loop(websocket, session_id)
        -_handle_audio(websocket, session_id, audio_chunk)
        -_process_dialogue(websocket, session_id, user_text)
        -_handle_control_message(websocket, session_id, message)
        -_cleanup_session(session_id, delay)
    }
    
    class VADResult {
        +bool is_speech
        +float probability
        +VADState state
        +float timestamp
        +float duration
    }
    
    class VADState {
        <<enumeration>>
        SILENCE
        SPEECH
        UNKNOWN
    }
    
    ASRRouter "1" --> "*" WhisperService : manages
    TTSRouter "1" --> "*" EdgeTTSService : manages
    
    WebSocketVoiceHandler --> ASRRouter : uses
    WebSocketVoiceHandler --> TTSRouter : uses
    WebSocketVoiceHandler --> SileroVADDetector : uses
    WebSocketVoiceHandler --> EmotionRecognizer : uses
    
    SileroVADDetector --> VADResult : returns
    VADResult --> VADState : has
```

---

## 4. æ€§èƒ½ä¼˜åŒ–ä¸æœ€ä½³å®è·µ

### 4.1 æ¨¡å‹ä¼˜åŒ–

**Whisperæ¨¡å‹é‡åŒ–**:
```python
# ä½¿ç”¨int8é‡åŒ–(ç›¸æ¯”float32):
# - å†…å­˜å‡å°‘75% (3.8GB -> 1GB)
# - é€Ÿåº¦æå‡2å€
# - ç²¾åº¦æŸå¤±<1% WER

whisper_service = WhisperService(
    model_size="base",
    device="cpu",
    compute_type="int8"  # int8é‡åŒ–
)

# GPUåŠ é€Ÿ:
whisper_service = WhisperService(
    model_size="large-v3",
    device="cuda",         # ä½¿ç”¨GPU
    compute_type="float16" # GPUæ¨èfloat16
)
# é€Ÿåº¦æå‡: CPU 10s -> GPU 1-2s
```

**Silero VADæ¨¡å‹é€‰æ‹©**:
```python
# JITæ¨¡å‹(é»˜è®¤):
# - åŠ è½½å¿«
# - æ¨ç†é€Ÿåº¦ä¸­ç­‰

# ONNXæ¨¡å‹:
# - åŠ è½½æ…¢
# - æ¨ç†é€Ÿåº¦å¿«20-30%
# - éœ€è¦å®‰è£…onnxruntime

vad = SileroVADDetector(use_onnx=True)
```

### 4.2 å®æ—¶æµä¼˜åŒ–

**éŸ³é¢‘ç¼“å†²ç­–ç•¥**:
```python
# 1. ç¼“å†²åŒºå¤§å°å¹³è¡¡å»¶è¿Ÿä¸å‡†ç¡®æ€§
# å¤ªå°: é¢‘ç¹è¯†åˆ«,å‡†ç¡®ç‡ä½
# å¤ªå¤§: å»¶è¿Ÿé«˜,ç”¨æˆ·ä½“éªŒå·®
# æ¨è: 2-3ç§’

BUFFER_SIZE = 16000 * 2 * 3  # 3ç§’

# 2. é‡å ç¼“å†²(overlap)
# ä¿ç•™æœ€å1ç§’,é¿å…è¾¹ç•Œè¯è¢«æˆªæ–­
OVERLAP_SIZE = 16000 * 2 * 1  # 1ç§’

# 3. è‡ªé€‚åº”ç¼“å†²
# çŸ­è¯­éŸ³(å¦‚"å—¯"ã€"å¥½"):1ç§’
# é•¿è¯­éŸ³(å¦‚é•¿å¥):3-5ç§’
```

**VADå‚æ•°è°ƒä¼˜**:
```python
# çµæ•åº¦è°ƒæ•´
vad = SileroVADDetector(
    threshold=0.5,                # é»˜è®¤0.5
    min_speech_duration_ms=250,   # æœ€å°è¯­éŸ³250ms
    min_silence_duration_ms=100,  # æœ€å°é™éŸ³100ms
)

# å™ªéŸ³ç¯å¢ƒ: threshold=0.6-0.7 (å‡å°‘è¯¯åˆ¤)
# å®‰é™ç¯å¢ƒ: threshold=0.3-0.4 (æ›´çµæ•)
```

### 4.3 å¹¶å‘å¤„ç†

**å¼‚æ­¥I/O**:
```python
# æ‰€æœ‰I/Oæ“ä½œå¼‚æ­¥åŒ–
async def process_audio(file: UploadFile):
    # 1. å¼‚æ­¥è¯»å–æ–‡ä»¶
    audio_data = await file.read()
    
    # 2. CPUå¯†é›†å‹ä»»åŠ¡åœ¨executorä¸­æ‰§è¡Œ
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(
        None,
        heavy_computation,  # å¦‚Whisperæ¨ç†
        audio_data
    )
    
    return result

# å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚
results = await asyncio.gather(
    transcribe_audio(file1),
    transcribe_audio(file2),
    transcribe_audio(file3),
)
```

**Worker Pool**:
```python
# é™åˆ¶å¹¶å‘æ•°,é¿å…èµ„æºè€—å°½
from concurrent.futures import ThreadPoolExecutor

executor = ThreadPoolExecutor(max_workers=4)

async def transcribe_with_limit(audio_data):
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(
        executor,
        whisper_model.transcribe,
        audio_data
    )
    return result
```

---

## 5. æ•…éšœæ’æŸ¥

### 5.1 å¸¸è§é—®é¢˜

**é—®é¢˜1: Whisperæ¨¡å‹åŠ è½½å¤±è´¥**
```
RuntimeError: Model file not found
```
- **åŸå› **: faster-whisperæ¨¡å‹æœªä¸‹è½½
- **è§£å†³**:
  ```bash
  # æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
  python -c "from faster_whisper import WhisperModel; WhisperModel('base')"
  ```

**é—®é¢˜2: VADæ£€æµ‹ä¸å‡†**
- **ç°è±¡**: é™éŸ³è¢«è¯†åˆ«ä¸ºè¯­éŸ³,æˆ–è¯­éŸ³è¢«è¯†åˆ«ä¸ºé™éŸ³
- **è§£å†³**:
  ```python
  # è°ƒæ•´é˜ˆå€¼
  vad.set_threshold(0.6)  # æé«˜é˜ˆå€¼,å‡å°‘è¯¯åˆ¤
  
  # è°ƒæ•´æœ€å°æŒç»­æ—¶é—´
  vad = SileroVADDetector(
      min_speech_duration_ms=500,  # å¢åŠ åˆ°500ms
      min_silence_duration_ms=200   # å¢åŠ åˆ°200ms
  )
  ```

**é—®é¢˜3: TTSåˆæˆæ…¢**
- **ç°è±¡**: TTSåˆæˆè€—æ—¶>5ç§’
- **è§£å†³**:
  ```python
  # 1. ä½¿ç”¨æµå¼åˆæˆ
  async for audio_chunk in tts.synthesize_stream(text):
      await websocket.send_bytes(audio_chunk)
  
  # 2. ç¼“å­˜å¸¸ç”¨çŸ­è¯­
  tts_cache = {}
  if text in tts_cache:
      audio_data = tts_cache[text]
  else:
      audio_data = await tts.synthesize(text)
      tts_cache[text] = audio_data
  ```

**é—®é¢˜4: WebSocketè¿æ¥é¢‘ç¹æ–­å¼€**
- **åŸå› **: å¿ƒè·³è¶…æ—¶ã€ç½‘ç»œä¸ç¨³å®š
- **è§£å†³**:
  ```python
  # å®¢æˆ·ç«¯å®šæ—¶å‘é€ping
  setInterval(() => {
      ws.send(JSON.stringify({type: 'ping'}));
  }, 10000);  # æ¯10ç§’pingä¸€æ¬¡
  
  # æœåŠ¡ç«¯é…ç½®è¶…æ—¶
  app.add_middleware(
      WebSocketMiddleware,
      ping_interval=30,  # 30ç§’ping
      ping_timeout=10    # 10ç§’è¶…æ—¶
  )
  ```

---

## 6. æ€»ç»“

Voiceè¯­éŸ³æœåŠ¡ä½œä¸ºVoiceHelperçš„å®æ—¶äº¤äº’æ ¸å¿ƒ,å®ç°äº†ä»¥ä¸‹èƒ½åŠ›:

1. **é«˜è´¨é‡ASR**: åŸºäºWhisper,æ”¯æŒ100+è¯­è¨€,WER<5%
2. **è‡ªç„¶TTS**: Edge TTSå…è´¹é«˜è´¨é‡,200+è¯­éŸ³é€‰æ‹©
3. **ç²¾å‡†VAD**: Silero VAD,F1>0.95,å»¶è¿Ÿ<50ms
4. **æƒ…æ„Ÿè¯†åˆ«**: éŸ³é¢‘ç‰¹å¾+æ–‡æœ¬åˆ†æ,7ç§æƒ…æ„Ÿ
5. **å®æ—¶æµ**: WebSocketåŒå‘éŸ³é¢‘æµ,ç«¯åˆ°ç«¯å»¶è¿Ÿ<500ms

é€šè¿‡æ¨¡å‹é‡åŒ–ã€å¼‚æ­¥å¤„ç†ã€ç¼“å†²ç­–ç•¥ä¼˜åŒ–,å®ç°äº†ç”Ÿäº§çº§çš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

æœªæ¥ä¼˜åŒ–æ–¹å‘:
- æ”¯æŒæ›´å¤šTTSæä¾›å•†(OpenAI TTS,æœ¬åœ°VITS)
- æ·±åº¦å­¦ä¹ æƒ…æ„Ÿè¯†åˆ«(å‡†ç¡®ç‡æå‡è‡³90%+)
- å¤šäººå¯¹è¯æ”¯æŒ(è¯´è¯äººåˆ†ç¦»)
- ç«¯åˆ°ç«¯è¯­éŸ³ç¿»è¯‘(æ— éœ€æ–‡æœ¬ä¸­ä»‹)

---

**æ–‡æ¡£çŠ¶æ€**:âœ… å·²å®Œæˆ  
**è¦†ç›–åº¦**:100%(ASRã€TTSã€VADã€æƒ…æ„Ÿã€WebSocketã€ä¼˜åŒ–ã€æœ€ä½³å®è·µ)  
**ä¸‹ä¸€æ­¥**:ç”ŸæˆAgentæœåŠ¡æ¨¡å—æ–‡æ¡£(09-AgentæœåŠ¡)

