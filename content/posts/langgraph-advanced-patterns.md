---
title: "æ·±å…¥LangGraphé«˜çº§æ¨¡å¼ï¼šä¼ä¸šçº§åº”ç”¨ä¸æºç æ·±åº¦è§£æ"
date: 2025-07-17T15:00:00+08:00
draft: false
featured: true
series: "langgraph-architecture"
tags: ["LangGraph", "é«˜çº§æ¨¡å¼", "ä¼ä¸šåº”ç”¨", "æ·±åº¦ç ”ç©¶", "å¤šæ™ºèƒ½ä½“åä½œ", "åæ€æœºåˆ¶"]
categories: ["langgraph", "AIæ¡†æ¶"]
description: "æ·±åº¦è§£æLangGraphé«˜çº§æ¨¡å¼ä¸ä¼ä¸šçº§åº”ç”¨å®è·µï¼ŒåŒ…å«å¤šæ™ºèƒ½ä½“åä½œã€åæ€æœºåˆ¶ã€çŠ¶æ€ç®¡ç†ç­‰æ ¸å¿ƒæŠ€æœ¯çš„æºç åˆ†æä¸å®æˆ˜æŒ‡å—"
author: "tommie blog"
toc: true
tocOpen: true
showReadingTime: true
showWordCount: true
weight: 250
slug: "langgraph-advanced-patterns"
---

## æ¦‚è¿°

<!--more-->

## 1. æ·±åº¦ç ”ç©¶ç³»ç»Ÿæ¶æ„

### 1.1 å®Œæ•´çš„ç ”ç©¶å·¥ä½œæµå®ç°

åŸºäºå®é™…çš„æ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼Œå±•ç¤ºLangGraphåœ¨å¤æ‚å¤šé˜¶æ®µä»»åŠ¡ä¸­çš„åº”ç”¨ï¼š

```python
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
import operator
from datetime import datetime

class OverallState(TypedDict):
    """ç ”ç©¶ç³»ç»Ÿçš„æ•´ä½“çŠ¶æ€æ¨¡å¼
    
    è¿™ä¸ªçŠ¶æ€è®¾è®¡å±•ç¤ºäº†LangGraphåœ¨å¤æ‚å·¥ä½œæµä¸­çš„çŠ¶æ€ç®¡ç†èƒ½åŠ›ï¼š
    - ä½¿ç”¨Annotatedç±»å‹å®šä¹‰ç´¯ç§¯è¡Œä¸º
    - æ”¯æŒå¤šè½®è¿­ä»£çš„çŠ¶æ€è¿½è¸ª  
    - é›†æˆé…ç½®å‚æ•°å’Œè¿è¡Œæ—¶çŠ¶æ€
    """
    messages: Annotated[list, add_messages]                # å¯¹è¯æ¶ˆæ¯ç´¯ç§¯
    search_query: Annotated[list, operator.add]           # æœç´¢æŸ¥è¯¢ç´¯ç§¯
    web_research_result: Annotated[list, operator.add]    # ç ”ç©¶ç»“æœç´¯ç§¯
    sources_gathered: Annotated[list, operator.add]       # æ¥æºä¿¡æ¯ç´¯ç§¯
    initial_search_query_count: int                       # åˆå§‹æŸ¥è¯¢æ•°é‡
    max_research_loops: int                               # æœ€å¤§ç ”ç©¶å¾ªç¯æ¬¡æ•°
    research_loop_count: int                              # å½“å‰å¾ªç¯æ¬¡æ•°
    reasoning_model: str                                  # æ¨ç†æ¨¡å‹åç§°

class SearchQueryList(BaseModel):
    """æœç´¢æŸ¥è¯¢åˆ—è¡¨æ¨¡å‹ï¼šç¡®ä¿æŸ¥è¯¢ç”Ÿæˆçš„ç»“æ„åŒ–è¾“å‡º"""
    query: List[str] = Field(description="ä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯ä¸ªæŸ¥è¯¢å…³æ³¨ä¸åŒè§’åº¦")

class Reflection(BaseModel):
    """åæ€åˆ†æç»“æœæ¨¡å‹ï¼šæ”¯æŒçŸ¥è¯†ç¼ºå£åˆ†æå’Œè¿­ä»£å†³ç­–"""
    is_sufficient: bool = Field(description="å½“å‰ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜")
    knowledge_gap: List[str] = Field(description="è¯†åˆ«çš„çŸ¥è¯†ç¼ºå£åˆ—è¡¨")
    follow_up_queries: List[str] = Field(description="é’ˆå¯¹çŸ¥è¯†ç¼ºå£çš„åç»­æŸ¥è¯¢å»ºè®®")
    confidence_score: float = Field(description="ç­”æ¡ˆç½®ä¿¡åº¦è¯„åˆ†", ge=0.0, le=1.0)

async def run_agent_workflow_async(
    user_input: str,
    debug: bool = False,
    max_plan_iterations: int = 3,
    max_step_num: int = 5,
    enable_background_investigation: bool = True,
) -> Dict[str, Any]:
    """å¼‚æ­¥è¿è¡Œä»£ç†å·¥ä½œæµï¼Œå¤„ç†ç”¨æˆ·è¾“å…¥
    
    è¿™æ˜¯ä¸€ä¸ªç”Ÿäº§çº§çš„ç ”ç©¶å·¥ä½œæµå®ç°ï¼Œå±•ç¤ºäº†LangGraphåœ¨å¤æ‚
    å¤šé˜¶æ®µä»»åŠ¡ä¸­çš„åº”ç”¨èƒ½åŠ›ï¼ŒåŒ…æ‹¬ï¼š
    - æ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆ
    - å¹¶è¡Œä¿¡æ¯æ”¶é›†
    - åæ€å¼è´¨é‡æ§åˆ¶
    - è¿­ä»£ä¼˜åŒ–æœºåˆ¶
    
    Args:
        user_input: ç”¨æˆ·çš„æŸ¥è¯¢æˆ–è¯·æ±‚
        debug: æ˜¯å¦å¯ç”¨è°ƒè¯•çº§åˆ«æ—¥å¿—
        max_plan_iterations: æœ€å¤§è®¡åˆ’è¿­ä»£æ¬¡æ•°
        max_step_num: è®¡åˆ’ä¸­çš„æœ€å¤§æ­¥éª¤æ•°
        enable_background_investigation: æ˜¯å¦åœ¨è§„åˆ’å‰è¿›è¡Œç½‘ç»œæœç´¢
        
    Returns:
        Dict[str, Any]: å·¥ä½œæµå®Œæˆåçš„æœ€ç»ˆçŠ¶æ€ï¼ŒåŒ…å«ç ”ç©¶ç»“æœå’Œæ¥æº
    """
    
    # åˆå§‹åŒ–ç ”ç©¶çŠ¶æ€
    initial_state = OverallState(
        messages=[HumanMessage(content=user_input)],
        search_query=[],
        web_research_result=[],
        sources_gathered=[],
        initial_search_query_count=max_step_num,
        max_research_loops=max_plan_iterations,
        research_loop_count=0,
        reasoning_model="gemini-2.0-flash-exp",
    )
    
    # æ„å»ºç ”ç©¶å·¥ä½œæµå›¾
    research_graph = build_research_workflow()
    
    # é…ç½®è¿è¡Œç¯å¢ƒ
    config = RunnableConfig(
        configurable={
            "thread_id": f"research_{int(time.time())}",
            "user_id": "anonymous", 
            "query_generator_model": "gemini-2.0-flash-exp",
            "enable_tracing": debug,
        },
        callbacks=[
            LangfuseCallbackHandler() if debug else None,
            ConsoleCallbackHandler() if debug else None,
        ]
    )
    
    # å¼‚æ­¥æ‰§è¡Œå·¥ä½œæµ
    final_state = None
    execution_steps = []
    
    async for state_update in research_graph.astream(
        initial_state,
        config=config,
        stream_mode="updates"
    ):
        execution_steps.append({
            "timestamp": time.time(),
            "update": state_update,
        })
        
        if debug:
            for node_name, node_state in state_update.items():
                print(f"ğŸ“ èŠ‚ç‚¹ '{node_name}' æ‰§è¡Œå®Œæˆ")
                
                # æ˜¾ç¤ºå…³é”®çŠ¶æ€å˜åŒ–
                if "messages" in node_state and node_state["messages"]:
                    latest_message = node_state["messages"][-1]
                    preview = latest_message.content[:200] + "..." if len(latest_message.content) > 200 else latest_message.content
                    print(f"ğŸ’¬ è¾“å‡ºé¢„è§ˆ: {preview}")
                
                if "search_query" in node_state:
                    print(f"ğŸ” æ–°å¢æŸ¥è¯¢: {node_state['search_query']}")
                
                if "sources_gathered" in node_state:
                    print(f"ğŸ“š æ”¶é›†æ¥æº: {len(node_state['sources_gathered'])} ä¸ª")
        
        final_state = state_update
    
    # æ·»åŠ æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯
    if final_state:
        final_state["execution_stats"] = {
            "total_steps": len(execution_steps),
            "total_duration": execution_steps[-1]["timestamp"] - execution_steps[0]["timestamp"] if execution_steps else 0,
            "queries_executed": len(final_state.get("search_query", [])),
            "sources_collected": len(final_state.get("sources_gathered", [])),
        }
    
    return final_state

def build_research_workflow() -> CompiledStateGraph:
    """æ„å»ºæ·±åº¦ç ”ç©¶å·¥ä½œæµå›¾
    
    è¯¥å›¾å®ç°äº†ä¸€ä¸ªå®Œæ•´çš„ç ”ç©¶æµç¨‹ï¼š
    1. æŸ¥è¯¢ç”Ÿæˆï¼šå°†ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸ºå¤šä¸ªæœç´¢æŸ¥è¯¢
    2. å¹¶è¡Œæœç´¢ï¼šåŒæ—¶æ‰§è¡Œå¤šä¸ªæœç´¢ä»»åŠ¡
    3. ç»“æœæ”¶é›†ï¼šæ±‡æ€»æ‰€æœ‰æœç´¢ç»“æœ
    4. åæ€åˆ†æï¼šè¯„ä¼°ä¿¡æ¯å……åˆ†æ€§
    5. è¿­ä»£ä¼˜åŒ–ï¼šæ ¹æ®åæ€ç»“æœå†³å®šæ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯
    6. ç­”æ¡ˆç»¼åˆï¼šç”Ÿæˆæœ€ç»ˆçš„ç»¼åˆç­”æ¡ˆ
    """
    graph = StateGraph(OverallState)
    
    # æŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹
    def generate_query(state: OverallState, config: RunnableConfig) -> Dict[str, Any]:
        """æ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆï¼šå°†ç ”ç©¶ä¸»é¢˜åˆ†è§£ä¸ºå¤šä¸ªæœç´¢è§’åº¦"""
        configurable = config.get("configurable", {})
        query_model = configurable.get("query_generator_model", "gemini-2.0-flash-exp")
        
        # åˆå§‹åŒ–æŸ¥è¯¢ç”Ÿæˆæ¨¡å‹
        llm = ChatGoogleGenerativeAI(
            model=query_model,
            temperature=1.0,  # æé«˜æŸ¥è¯¢å¤šæ ·æ€§
            max_retries=2,
            api_key=os.getenv("GEMINI_API_KEY"),
        )
        structured_llm = llm.with_structured_output(SearchQueryList)
        
        # æ„å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æŸ¥è¯¢ç”Ÿæˆæç¤º
        research_topic = get_research_topic(state["messages"])
        current_date = datetime.now().strftime("%Y-%m-%d")
        previous_queries = state.get("search_query", [])
        
        query_instructions = f"""å½“å‰æ—¥æœŸ: {current_date}

ç ”ç©¶ä¸»é¢˜: {research_topic}

ä¹‹å‰å·²æ‰§è¡Œçš„æŸ¥è¯¢ï¼ˆé¿å…é‡å¤ï¼‰:
{chr(10).join(f"- {q}" for q in previous_queries[-5:]) if previous_queries else "æ— "}

è¯·ç”Ÿæˆ {state["initial_search_query_count"]} ä¸ªæ–°çš„ä¼˜åŒ–æœç´¢æŸ¥è¯¢ï¼Œè¦æ±‚ï¼š

1. **å¤šè§’åº¦è¦†ç›–**ï¼šä»ä¸åŒè§’åº¦å’Œå±‚é¢åˆ†æä¸»é¢˜
2. **æ—¶æ•ˆæ€§è€ƒè™‘**ï¼šåŒ…å«æœ€æ–°ä¿¡æ¯å’Œè¶‹åŠ¿åˆ†æ
3. **æ·±åº¦æŒ–æ˜**ï¼šä¸ä»…è·å–åŸºç¡€ä¿¡æ¯ï¼Œè¿˜è¦æ·±å…¥æŠ€æœ¯ç»†èŠ‚
4. **ä¸“ä¸šæœ¯è¯­**ï¼šä½¿ç”¨é¢†åŸŸä¸“ä¸šæœ¯è¯­æé«˜æœç´¢ç²¾åº¦
5. **é¿å…é‡å¤**ï¼šç¡®ä¿ä¸ä¹‹å‰æŸ¥è¯¢ä¸é‡å¤

æ¯ä¸ªæŸ¥è¯¢åº”è¯¥æ˜¯ç‹¬ç«‹ä¸”å…·ä½“çš„ï¼Œèƒ½å¤Ÿè·å¾—æœ‰ä»·å€¼çš„ä¿¡æ¯ç‰‡æ®µã€‚"""
        
        result = structured_llm.invoke(query_instructions)
        
        return {
            "query_list": result.query,
            "query_generation_completed": True,
            "query_generated_at": time.time(),
        }
    
    # å¹¶è¡Œæœç´¢åˆ†å‘èŠ‚ç‚¹
    def continue_to_web_research(state: OverallState) -> List[Send]:
        """å¯åŠ¨å¹¶è¡Œç½‘ç»œæœç´¢
        
        ä½¿ç”¨LangGraphçš„Sendæœºåˆ¶å®ç°çœŸæ­£çš„å¹¶è¡Œæœç´¢ï¼Œ
        æ¯ä¸ªæŸ¥è¯¢éƒ½ä¼šå¯åŠ¨ä¸€ä¸ªç‹¬ç«‹çš„æœç´¢ä»»åŠ¡
        """
        query_list = state.get("query_list", [])
        
        return [
            Send("web_research", {
                "search_query": search_query,
                "id": int(idx),
                "total_queries": len(query_list),
                "research_context": {
                    "main_topic": get_research_topic(state["messages"]),
                    "current_loop": state.get("research_loop_count", 0),
                }
            })
            for idx, search_query in enumerate(query_list)
        ]
    
    # ç½‘ç»œæœç´¢æ‰§è¡ŒèŠ‚ç‚¹
    def web_research(state: Dict[str, Any], config: RunnableConfig) -> OverallState:
        """æ‰§è¡Œå•ä¸ªæœç´¢æŸ¥è¯¢çš„ç½‘ç»œç ”ç©¶
        
        é›†æˆGoogle Search APIå’ŒGeminiæ¨¡å‹ï¼Œå®ç°ï¼š
        - æ™ºèƒ½æœç´¢æŸ¥è¯¢ä¼˜åŒ–
        - è‡ªåŠ¨å¼•ç”¨æå–å’Œæ ¼å¼åŒ–
        - URLä¼˜åŒ–å’ŒçŸ­é“¾æ¥ç”Ÿæˆ
        """
        search_query = state["search_query"]
        search_id = state["id"]
        research_context = state.get("research_context", {})
        
        # è·å–æ¨¡å‹é…ç½®
        configurable = config.get("configurable", {})
        model_name = configurable.get("query_generator_model", "gemini-2.0-flash-exp")
        
        # æ„å»ºæœç´¢ä¸Šä¸‹æ–‡
        search_prompt = f"""
ä½¿ç”¨Google Search APIæœç´¢ä»¥ä¸‹æŸ¥è¯¢å¹¶æä¾›è¯¦ç»†åˆ†æï¼š

æŸ¥è¯¢: {search_query}
ä¸»é¢˜èƒŒæ™¯: {research_context.get("main_topic", "æœªçŸ¥")}

æœç´¢è¦æ±‚:
1. ä½¿ç”¨å¤šä¸ªç›¸å…³å…³é”®è¯ç»„åˆè¿›è¡Œæœç´¢
2. ä¼˜å…ˆé€‰æ‹©æƒå¨å¯ä¿¡çš„ä¿¡æ¯æº
3. æå–å…³é”®äº‹å®ã€æ•°æ®å’Œè§‚ç‚¹
4. åˆ†æä¿¡æ¯çš„æ—¶æ•ˆæ€§å’Œç›¸å…³æ€§
5. æ€»ç»“æ ¸å¿ƒå‘ç°å’Œæ´å¯Ÿ

è¯·æä¾›ç»“æ„åŒ–çš„ç ”ç©¶ç»“æœï¼ŒåŒ…å«å¼•ç”¨æ¥æºã€‚
"""
        
        # æ‰§è¡Œæœç´¢
        genai_client = genai.GenerativeModel(model_name)
        response = genai_client.generate_content(
            search_prompt,
            tools=[{"google_search": {}}],
            config={"temperature": 0}  # ç¡®ä¿æœç´¢ç»“æœçš„ä¸€è‡´æ€§
        )
        
        # å¤„ç†æœç´¢ç»“æœå’Œå¼•ç”¨
        resolved_urls = resolve_urls(
            response.candidates[0].grounding_metadata.grounding_chunks, 
            search_id
        )
        citations = get_citations(response, resolved_urls)
        modified_text = insert_citation_markers(response.text, citations)
        
        # æ„å»ºç»“æ„åŒ–çš„æ¥æºä¿¡æ¯
        sources_gathered = [
            {
                "url": url_info["url"],
                "title": url_info.get("title", "æœªçŸ¥æ ‡é¢˜"),
                "short_url": url_info["short_url"],
                "value": url_info["value"],
                "search_id": search_id,
                "search_query": search_query,
                "collected_at": time.time(),
                "relevance_score": _calculate_relevance_score(
                    url_info, search_query, research_context
                ),
            }
            for url_info in resolved_urls
        ]
        
        return {
            "sources_gathered": sources_gathered,
            "search_query": [search_query],
            "web_research_result": [modified_text],
        }
    
    # åæ€åˆ†æèŠ‚ç‚¹
    def reflection(state: OverallState, config: RunnableConfig) -> Dict[str, Any]:
        """åæ€åˆ†æï¼šè¯„ä¼°ä¿¡æ¯å……åˆ†æ€§å¹¶è¯†åˆ«çŸ¥è¯†ç¼ºå£
        
        è¿™æ˜¯LangGraphåæ€æœºåˆ¶çš„æ ¸å¿ƒå®ç°ï¼Œæ”¯æŒï¼š
        - è‡ªåŠ¨è¯„ä¼°ç ”ç©¶ç»“æœçš„å®Œæ•´æ€§
        - è¯†åˆ«çŸ¥è¯†ç¼ºå£å’Œä¿¡æ¯ä¸è¶³çš„é¢†åŸŸ
        - ç”Ÿæˆé’ˆå¯¹æ€§çš„åç»­æŸ¥è¯¢å»ºè®®
        - è´¨é‡æ§åˆ¶å’Œè¿­ä»£å†³ç­–
        """
        reasoning_model = state.get("reasoning_model", "gemini-2.0-flash-exp")
        research_topic = get_research_topic(state["messages"])
        summaries = "\n\n---\n\n".join(state["web_research_result"])
        current_date = datetime.now().strftime("%Y-%m-%d")
        
        # æ„å»ºæ·±åº¦åæ€æç¤º
        reflection_instructions = f"""ä½œä¸ºç ”ç©¶åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹ç ”ç©¶ç»“æœè¿›è¡Œæ·±åº¦åæ€åˆ†æï¼š

å½“å‰æ—¥æœŸ: {current_date}
ç ”ç©¶ä¸»é¢˜: {research_topic}
å·²å®Œæˆæœç´¢æ¬¡æ•°: {len(state["search_query"])}
å½“å‰ç ”ç©¶å¾ªç¯: {state.get("research_loop_count", 0)}

ç ”ç©¶ç»“æœæ‘˜è¦:
{summaries}

è¯·è¿›è¡Œåæ€åˆ†æï¼š

1. **ä¿¡æ¯å®Œæ•´æ€§è¯„ä¼°**ï¼š
   - æ˜¯å¦è¦†ç›–äº†ä¸»é¢˜çš„æ ¸å¿ƒæ–¹é¢ï¼Ÿ
   - æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„ä¿¡æ¯ç©ºç™½ï¼Ÿ
   - ä¸åŒæ¥æºçš„ä¿¡æ¯æ˜¯å¦ä¸€è‡´ï¼Ÿ

2. **çŸ¥è¯†ç¼ºå£è¯†åˆ«**ï¼š
   - å“ªäº›é‡è¦é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†å›ç­”ï¼Ÿ
   - éœ€è¦å“ªäº›ç±»å‹çš„è¡¥å……ä¿¡æ¯ï¼Ÿ
   - æ˜¯å¦éœ€è¦æ›´ä¸“ä¸šæˆ–æ›´æ–°çš„ä¿¡æ¯ï¼Ÿ

3. **ä¿¡æ¯è´¨é‡è¯„ä¼°**ï¼š
   - æ¥æºçš„æƒå¨æ€§å’Œå¯ä¿¡åº¦å¦‚ä½•ï¼Ÿ
   - ä¿¡æ¯çš„æ—¶æ•ˆæ€§æ˜¯å¦æ»¡è¶³è¦æ±‚ï¼Ÿ
   - æ˜¯å¦å­˜åœ¨ç›¸äº’çŸ›ç›¾çš„ä¿¡æ¯ï¼Ÿ

4. **åç»­è¡ŒåŠ¨å»ºè®®**ï¼š
   - å¦‚æœä¿¡æ¯ä¸å……åˆ†ï¼Œå»ºè®®å…·ä½“çš„åç»­æŸ¥è¯¢
   - ä¼˜å…ˆçº§æ’åºå’Œæœç´¢ç­–ç•¥å»ºè®®

è¯·æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœã€‚"""
        
        # æ‰§è¡Œåæ€åˆ†æ
        llm = ChatGoogleGenerativeAI(
            model=reasoning_model,
            temperature=0.3,  # å¹³è¡¡åˆ›é€ æ€§å’Œä¸€è‡´æ€§
            max_retries=2,
            api_key=os.getenv("GEMINI_API_KEY"),
        )
        
        result = llm.with_structured_output(Reflection).invoke(reflection_instructions)
        
        return {
            "is_sufficient": result.is_sufficient,
            "knowledge_gap": result.knowledge_gap,
            "follow_up_queries": result.follow_up_queries,
            "confidence_score": result.confidence_score,
            "research_loop_count": state["research_loop_count"] + 1,
            "reflection_completed_at": time.time(),
        }
    
    # æœ€ç»ˆç­”æ¡ˆç»¼åˆèŠ‚ç‚¹
    def finalize_answer(state: OverallState, config: RunnableConfig) -> Dict[str, Any]:
        """ç»¼åˆæ‰€æœ‰ç ”ç©¶ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        
        å®ç°æ™ºèƒ½çš„ä¿¡æ¯ç»¼åˆå’Œç­”æ¡ˆç”Ÿæˆï¼š
        - æ•´åˆå¤šä¸ªæ¥æºçš„ä¿¡æ¯
        - ç”Ÿæˆç»“æ„åŒ–çš„ç»¼åˆç­”æ¡ˆ
        - è‡ªåŠ¨å¤„ç†å¼•ç”¨å’Œæ¥æºæ ‡æ³¨
        - è´¨é‡è¯„ä¼°å’Œç½®ä¿¡åº¦è®¡ç®—
        """
        reasoning_model = state.get("reasoning_model", "gemini-2.0-flash-exp") 
        research_topic = get_research_topic(state["messages"])
        summaries = "\n---\n\n".join(state["web_research_result"])
        current_date = datetime.now().strftime("%Y-%m-%d")
        
        # æ„å»ºç»¼åˆç­”æ¡ˆç”Ÿæˆæç¤º
        answer_instructions = f"""ä½œä¸ºç ”ç©¶åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹ç ”ç©¶ç»“æœç”Ÿæˆå…¨é¢ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š

å½“å‰æ—¥æœŸ: {current_date}
ç ”ç©¶ä¸»é¢˜: {research_topic}
ç ”ç©¶å¾ªç¯æ¬¡æ•°: {state.get("research_loop_count", 0)}
ä¿¡æ¯æ¥æºæ•°é‡: {len(state.get("sources_gathered", []))}

ç ”ç©¶ç»“æœè¯¦æƒ…:
{summaries}

ç­”æ¡ˆç”Ÿæˆè¦æ±‚:

1. **ç»“æ„åŒ–ç»„ç»‡**ï¼š
   - ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å’Œå­æ ‡é¢˜
   - é€»è¾‘æ€§å¼ºçš„ä¿¡æ¯ç»„ç»‡
   - é‡ç‚¹çªå‡ºå…³é”®å‘ç°

2. **å®¢è§‚æ€§å’Œå¹³è¡¡æ€§**ï¼š
   - å‘ˆç°å¤šç§è§‚ç‚¹å’Œè§’åº¦
   - é¿å…åè§å’Œä¸»è§‚åˆ¤æ–­
   - æ‰¿è®¤ä¸ç¡®å®šæ€§å’Œäº‰è®®

3. **å¼•ç”¨å’Œæ¥æº**ï¼š
   - æ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ¥æº
   - ä½¿ç”¨å†…è”å¼•ç”¨æ ¼å¼
   - æä¾›å®Œæ•´çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨

4. **å®ç”¨æ€§**ï¼š
   - ç›´æ¥å›ç­”ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜
   - æä¾›å¯æ“ä½œçš„å»ºè®®å’Œç»“è®º
   - çªå‡ºå…³é”®è¦ç‚¹å’Œå½±å“

è¯·ç”Ÿæˆä¸“ä¸šã€å…¨é¢çš„ç ”ç©¶æŠ¥å‘Šã€‚"""
        
        # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        llm = ChatGoogleGenerativeAI(
            model=reasoning_model,
            temperature=0,  # ç¡®ä¿ç­”æ¡ˆçš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§
            max_retries=2,
            api_key=os.getenv("GEMINI_API_KEY"),
        )
        
        result = llm.invoke(answer_instructions)
        
        # å¤„ç†å¼•ç”¨é“¾æ¥æ›¿æ¢
        unique_sources = []
        answer_content = result.content
        
        for source in state["sources_gathered"]:
            if source["short_url"] in answer_content:
                # å°†çŸ­é“¾æ¥æ›¿æ¢ä¸ºå®Œæ•´å¼•ç”¨
                answer_content = answer_content.replace(
                    source["short_url"], 
                    source["value"]
                )
                unique_sources.append(source)
        
        # è®¡ç®—ç­”æ¡ˆè´¨é‡æŒ‡æ ‡
        quality_metrics = {
            "sources_cited": len(unique_sources),
            "content_length": len(answer_content),
            "research_depth": state.get("research_loop_count", 0),
            "confidence_score": state.get("confidence_score", 0.8),
        }
        
        return {
            "messages": [AIMessage(content=answer_content)],
            "sources_gathered": unique_sources,
            "research_completed": True,
            "quality_metrics": quality_metrics,
            "final_answer_generated_at": time.time(),
        }
    
    # æ„å»ºå›¾ç»“æ„å’Œæµç¨‹æ§åˆ¶
    graph.add_node("generate_query", generate_query)
    graph.add_node("continue_to_web_research", continue_to_web_research)
    graph.add_node("web_research", web_research)
    graph.add_node("reflection", reflection)
    graph.add_node("finalize_answer", finalize_answer)
    
    # è®¾ç½®æµç¨‹è·¯å¾„
    graph.set_entry_point("generate_query")
    graph.add_edge("generate_query", "continue_to_web_research")
    graph.add_edge("continue_to_web_research", "web_research")
    graph.add_edge("web_research", "reflection")
    
    # æ™ºèƒ½æ¡ä»¶è·¯ç”±ï¼šåŸºäºåæ€ç»“æœå†³å®šä¸‹ä¸€æ­¥
    def should_continue_research(state: OverallState) -> str:
        """å†³å®šæ˜¯å¦ç»§ç»­ç ”ç©¶çš„æ™ºèƒ½æ¡ä»¶å‡½æ•°"""
        is_sufficient = state.get("is_sufficient", False)
        research_count = state.get("research_loop_count", 0)
        max_loops = state.get("max_research_loops", 3)
        confidence = state.get("confidence_score", 0)
        
        # å¤šé‡æ¡ä»¶åˆ¤æ–­
        if is_sufficient and confidence > 0.7:
            return "finalize_answer"
        elif research_count >= max_loops:
            # è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»“æŸ
            return "finalize_answer"
        elif len(state.get("knowledge_gap", [])) == 0:
            # æ²¡æœ‰è¯†åˆ«åˆ°çŸ¥è¯†ç¼ºå£
            return "finalize_answer"
        else:
            # ç»§ç»­ç ”ç©¶
            return "generate_query"
    
    graph.add_conditional_edges(
        "reflection",
        should_continue_research,
        {
            "generate_query": "generate_query",
            "finalize_answer": "finalize_answer",
        }
    )
    
    graph.set_finish_point("finalize_answer")
    
    return graph.compile(
        checkpointer=PostgresCheckpointSaver.from_conn_string(
            os.getenv("DATABASE_URL", "postgresql://localhost/langgraph")
        ),
        debug=True,
        name="DeepResearchWorkflow",
    )

# è¾…åŠ©å·¥å…·å‡½æ•°
def get_research_topic(messages: List[BaseMessage]) -> str:
    """ä»æ¶ˆæ¯å†å²ä¸­æ™ºèƒ½æå–ç ”ç©¶ä¸»é¢˜"""
    if not messages:
        return "æœªçŸ¥ç ”ç©¶ä¸»é¢˜"
    
    # æŸ¥æ‰¾æœ€åä¸€æ¡äººç±»æ¶ˆæ¯
    for message in reversed(messages):
        if isinstance(message, HumanMessage):
            content = message.content.strip()
            
            # ç®€å•çš„ä¸»é¢˜æå–é€»è¾‘
            if len(content) > 200:
                # é•¿æ¶ˆæ¯ï¼Œæå–å‰100ä¸ªå­—ç¬¦ä½œä¸ºä¸»é¢˜
                return content[:100] + "..."
            else:
                return content
    
    return "æœªçŸ¥ç ”ç©¶ä¸»é¢˜"

def resolve_urls(grounding_chunks, search_id: int) -> List[Dict[str, str]]:
    """è§£æå¹¶ä¼˜åŒ–URLå¼•ç”¨
    
    å¤„ç†Google Search APIè¿”å›çš„å¼•ç”¨ä¿¡æ¯ï¼š
    - æå–URLå’Œæ ‡é¢˜ä¿¡æ¯
    - ç”ŸæˆçŸ­é“¾æ¥æ ‡è®°
    - åˆ›å»ºæ ¼å¼åŒ–çš„å¼•ç”¨æ ¼å¼
    """
    resolved_urls = []
    
    for idx, chunk in enumerate(grounding_chunks):
        if hasattr(chunk, 'web') and chunk.web:
            url = chunk.web.uri
            title = getattr(chunk.web, 'title', f"æ¥æº {idx + 1}")
            
            # ç”Ÿæˆå”¯ä¸€çš„çŸ­é“¾æ¥æ ‡è®°
            short_url = f"[{search_id}-{idx}]"
            
            resolved_urls.append({
                "url": url,
                "title": title,
                "short_url": short_url,
                "value": f"[{title}]({url})",
                "chunk_index": idx,
            })
    
    return resolved_urls

def get_citations(response, resolved_urls: List[Dict]) -> List[Dict]:
    """æå–å’Œæ ¼å¼åŒ–å¼•ç”¨ä¿¡æ¯"""
    citations = []
    
    for url_info in resolved_urls:
        citations.append({
            "url": url_info["url"],
            "title": url_info["title"],
            "short_url": url_info["short_url"],
            "referenced_text": "",  # å¯ä»¥æ·»åŠ å¼•ç”¨çš„å…·ä½“æ–‡æœ¬ç‰‡æ®µ
        })
    
    return citations

def insert_citation_markers(text: str, citations: List[Dict]) -> str:
    """åœ¨æ–‡æœ¬ä¸­æ™ºèƒ½æ’å…¥å¼•ç”¨æ ‡è®°"""
    modified_text = text
    
    # ç®€å•çš„å¼•ç”¨æ’å…¥ç­–ç•¥
    # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯æ¥ç²¾ç¡®å®šä½å¼•ç”¨ä½ç½®
    for citation in citations:
        original_url = citation["url"]
        short_marker = citation["short_url"]
        
        if original_url in modified_text:
            modified_text = modified_text.replace(original_url, short_marker)
    
    return modified_text

def _calculate_relevance_score(
    url_info: Dict, 
    search_query: str, 
    research_context: Dict
) -> float:
    """è®¡ç®—æ¥æºçš„ç›¸å…³æ€§è¯„åˆ†"""
    score = 0.5  # åŸºç¡€åˆ†æ•°
    
    # åŸºäºæ ‡é¢˜ç›¸å…³æ€§
    title = url_info.get("title", "").lower()
    query_terms = search_query.lower().split()
    
    title_matches = sum(1 for term in query_terms if term in title)
    score += (title_matches / len(query_terms)) * 0.3
    
    # åŸºäºURLæƒå¨æ€§
    url = url_info.get("url", "")
    if any(domain in url for domain in [".edu", ".gov", ".org"]):
        score += 0.2
    
    # åŸºäºå†…å®¹é•¿åº¦ï¼ˆæ›´é•¿çš„å†…å®¹é€šå¸¸æ›´è¯¦ç»†ï¼‰
    content_length = len(url_info.get("value", ""))
    if content_length > 200:
        score += 0.1
    
    return min(score, 1.0)
```

## 2. å¤šæ™ºèƒ½ä½“åä½œæ¨¡å¼

### 2.1 åˆ†å±‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ

åŸºäºå®é™…åº”ç”¨æ¡ˆä¾‹ï¼Œå±•ç¤ºä¸“ä¸šçš„å¤šæ™ºèƒ½ä½“åä½œæ¶æ„ï¼š

```python
class MultiAgentResearchSystem:
    """å¤šæ™ºèƒ½ä½“ç ”ç©¶ç³»ç»Ÿï¼šå®ç°ä¸“ä¸šåˆ†å·¥å’Œåä½œ"""
    
    def __init__(self):
        self.coordinator = None
        self.specialists = {}
        self.coordination_graph = None
        
    def build_hierarchical_research_team(self) -> CompiledStateGraph:
        """æ„å»ºåˆ†å±‚ç ”ç©¶å›¢é˜Ÿ"""
        
        class TeamState(TypedDict):
            messages: Annotated[list, add_messages]
            current_task: Optional[str]
            task_queue: List[Dict[str, Any]]
            specialist_results: Dict[str, Any]
            coordination_history: List[Dict[str, Any]]
            research_plan: Optional[Dict[str, Any]]
        
        graph = StateGraph(TeamState)
        
        # åè°ƒè€…æ™ºèƒ½ä½“ï¼šè´Ÿè´£ä»»åŠ¡åˆ†è§£å’Œå›¢é˜Ÿåè°ƒ
        def coordinator_agent(state: TeamState) -> Dict[str, Any]:
            """åè°ƒè€…æ™ºèƒ½ä½“ï¼šä»»åŠ¡åˆ†è§£ã€åˆ†é…å’Œç»“æœæ•´åˆ"""
            
            messages = state["messages"]
            current_task = state.get("current_task")
            
            if not current_task:
                # åˆå§‹ä»»åŠ¡åˆ†è§£
                user_request = messages[-1].content if messages else ""
                
                # åˆ†æä»»åŠ¡å¤æ‚åº¦å’Œä¸“ä¸šéœ€æ±‚
                task_analysis = self._analyze_task_requirements(user_request)
                
                # ç”Ÿæˆç ”ç©¶è®¡åˆ’
                research_plan = self._create_research_plan(task_analysis)
                
                # åˆ†è§£ä¸ºå­ä»»åŠ¡
                subtasks = self._decompose_into_subtasks(research_plan)
                
                return {
                    "research_plan": research_plan,
                    "task_queue": subtasks,
                    "current_task": subtasks[0] if subtasks else None,
                    "coordination_history": [{
                        "action": "task_decomposition",
                        "plan": research_plan,
                        "subtasks_count": len(subtasks),
                        "timestamp": time.time(),
                    }],
                }
            else:
                # å¤„ç†ä¸“å®¶è¿”å›çš„ç»“æœ
                specialist_results = state.get("specialist_results", {})
                task_queue = state.get("task_queue", [])
                
                if specialist_results and task_queue:
                    # è®°å½•å½“å‰ä»»åŠ¡å®Œæˆ
                    completed_task = task_queue[0]
                    remaining_tasks = task_queue[1:]
                    
                    coordination_entry = {
                        "action": "task_completion",
                        "completed_task": completed_task,
                        "specialist": completed_task.get("assigned_specialist"),
                        "result_summary": specialist_results.get("summary", ""),
                        "timestamp": time.time(),
                    }
                    
                    if remaining_tasks:
                        # è¿˜æœ‰å¾…å¤„ç†ä»»åŠ¡
                        return {
                            "current_task": remaining_tasks[0],
                            "task_queue": remaining_tasks,
                            "coordination_history": state.get("coordination_history", []) + [coordination_entry],
                        }
                    else:
                        # æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæ•´åˆæœ€ç»ˆç»“æœ
                        final_result = self._integrate_specialist_results(
                            state["specialist_results"], 
                            state["research_plan"]
                        )
                        
                        return {
                            "messages": [AIMessage(content=final_result)],
                            "coordination_history": state.get("coordination_history", []) + [coordination_entry],
                            "research_completed": True,
                        }
                
                return {"current_task": None}  # å¼‚å¸¸æƒ…å†µå¤„ç†
        
        # æ•°æ®åˆ†æä¸“å®¶
        def data_analyst_agent(state: TeamState) -> Dict[str, Any]:
            """æ•°æ®åˆ†æä¸“å®¶ï¼šå¤„ç†æ•°æ®åˆ†æå’Œç»Ÿè®¡ä»»åŠ¡"""
            current_task = state.get("current_task", {})
            
            if current_task.get("type") != "data_analysis":
                return {}  # ä¸æ˜¯æ•°æ®åˆ†æä»»åŠ¡
            
            # æ‰§è¡Œæ•°æ®åˆ†æ
            analysis_result = self._perform_data_analysis(current_task)
            
            return {
                "specialist_results": {
                    "type": "data_analysis",
                    "summary": analysis_result["summary"],
                    "details": analysis_result["details"],
                    "visualizations": analysis_result.get("charts", []),
                    "confidence": analysis_result.get("confidence", 0.8),
                }
            }
        
        # ä»£ç ç”Ÿæˆä¸“å®¶
        def code_generator_agent(state: TeamState) -> Dict[str, Any]:
            """ä»£ç ç”Ÿæˆä¸“å®¶ï¼šå¤„ç†ç¼–ç¨‹å’ŒæŠ€æœ¯å®ç°ä»»åŠ¡"""
            current_task = state.get("current_task", {})
            
            if current_task.get("type") != "code_generation":
                return {}
            
            # æ‰§è¡Œä»£ç ç”Ÿæˆ
            code_result = self._generate_code_solution(current_task)
            
            return {
                "specialist_results": {
                    "type": "code_generation",
                    "summary": code_result["summary"],
                    "code": code_result["code"],
                    "tests": code_result.get("tests", []),
                    "documentation": code_result.get("docs", ""),
                }
            }
        
        # è´¨é‡ä¿è¯ä¸“å®¶
        def qa_specialist_agent(state: TeamState) -> Dict[str, Any]:
            """è´¨é‡ä¿è¯ä¸“å®¶ï¼šéªŒè¯ç»“æœè´¨é‡å’Œå‡†ç¡®æ€§"""
            current_task = state.get("current_task", {})
            
            if current_task.get("type") != "quality_assurance":
                return {}
            
            # æ‰§è¡Œè´¨é‡æ£€æŸ¥
            qa_result = self._perform_quality_assurance(
                current_task, 
                state.get("specialist_results", {})
            )
            
            return {
                "specialist_results": {
                    "type": "quality_assurance", 
                    "summary": qa_result["summary"],
                    "issues_found": qa_result["issues"],
                    "recommendations": qa_result["recommendations"],
                    "quality_score": qa_result["score"],
                }
            }
        
        # æ·»åŠ æ‰€æœ‰æ™ºèƒ½ä½“èŠ‚ç‚¹
        graph.add_node("coordinator", coordinator_agent)
        graph.add_node("data_analyst", data_analyst_agent)
        graph.add_node("code_generator", code_generator_agent)
        graph.add_node("qa_specialist", qa_specialist_agent)
        
        # è®¾ç½®åä½œæµç¨‹
        graph.set_entry_point("coordinator")
        
        # æ™ºèƒ½è·¯ç”±ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹åˆ†é…ç»™ç›¸åº”ä¸“å®¶
        def route_to_specialist(state: TeamState) -> str:
            """æ™ºèƒ½è·¯ç”±åˆ°ä¸“ä¸šæ™ºèƒ½ä½“"""
            current_task = state.get("current_task", {})
            task_type = current_task.get("type", "unknown")
            
            routing_map = {
                "data_analysis": "data_analyst",
                "code_generation": "code_generator", 
                "quality_assurance": "qa_specialist",
            }
            
            return routing_map.get(task_type, END)
        
        graph.add_conditional_edges(
            "coordinator",
            route_to_specialist,
            {
                "data_analyst": "data_analyst",
                "code_generator": "code_generator",
                "qa_specialist": "qa_specialist",
                END: END,
            }
        )
        
        # ä¸“å®¶å®Œæˆåè¿”å›åè°ƒè€…
        for specialist in ["data_analyst", "code_generator", "qa_specialist"]:
            graph.add_edge(specialist, "coordinator")
        
        return graph.compile(
            checkpointer=PostgresCheckpointSaver.from_conn_string(
                os.getenv("DATABASE_URL")
            ),
            name="MultiAgentResearchTeam",
        )
    
    def _analyze_task_requirements(self, user_request: str) -> Dict[str, Any]:
        """åˆ†æä»»åŠ¡éœ€æ±‚å’Œå¤æ‚åº¦"""
        # ä½¿ç”¨NLPæŠ€æœ¯åˆ†æä»»åŠ¡ç‰¹å¾
        task_features = {
            "contains_data": any(term in user_request.lower() for term in ["data", "statistics", "numbers", "chart"]),
            "requires_code": any(term in user_request.lower() for term in ["code", "programming", "implementation", "algorithm"]),
            "needs_qa": any(term in user_request.lower() for term in ["test", "verify", "validate", "check"]),
            "complexity_level": self._assess_complexity_level(user_request),
        }
        
        return task_features
    
    def _create_research_plan(self, task_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºç ”ç©¶è®¡åˆ’"""
        plan = {
            "phases": [],
            "estimated_duration": 0,
            "required_specialists": [],
        }
        
        if task_analysis["contains_data"]:
            plan["phases"].append("data_analysis")
            plan["required_specialists"].append("data_analyst")
            plan["estimated_duration"] += 30  # åˆ†é’Ÿ
        
        if task_analysis["requires_code"]:
            plan["phases"].append("code_generation")
            plan["required_specialists"].append("code_generator")
            plan["estimated_duration"] += 45
        
        if task_analysis["needs_qa"]:
            plan["phases"].append("quality_assurance")
            plan["required_specialists"].append("qa_specialist")
            plan["estimated_duration"] += 20
        
        return plan
```

## 3. ä¼ä¸šçº§æ¨¡å¼å’Œæœ€ä½³å®è·µ

### 3.1 æ•…éšœæ¢å¤å’Œå®¹é”™æœºåˆ¶

```python
class ResilientWorkflowManager:
    """å¼¹æ€§å·¥ä½œæµç®¡ç†å™¨ï¼šä¼ä¸šçº§æ•…éšœæ¢å¤å’Œå®¹é”™"""
    
    def __init__(self, graph: CompiledStateGraph):
        self.graph = graph
        self.failure_tracker = defaultdict(int)
        self.recovery_strategies = self._setup_recovery_strategies()
        
    async def execute_with_resilience(
        self,
        input_data: Dict[str, Any],
        config: RunnableConfig,
        max_retries: int = 3,
    ) -> Dict[str, Any]:
        """å¼¹æ€§æ‰§è¡Œï¼šæ”¯æŒè‡ªåŠ¨æ•…éšœæ¢å¤å’Œé‡è¯•"""
        
        retry_count = 0
        last_checkpoint = None
        
        while retry_count < max_retries:
            try:
                # å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤
                if last_checkpoint:
                    config = {
                        **config,
                        "configurable": {
                            **config.get("configurable", {}),
                            "checkpoint_id": last_checkpoint,
                        }
                    }
                    input_data = None  # ä»æ£€æŸ¥ç‚¹æ¢å¤æ—¶ä¸éœ€è¦æ–°è¾“å…¥
                
                # æ‰§è¡Œå·¥ä½œæµ
                final_state = None
                async for state in self.graph.astream(input_data, config):
                    final_state = state
                    last_checkpoint = state.get("checkpoint_id")
                
                return final_state
                
            except Exception as e:
                retry_count += 1
                error_type = type(e).__name__
                self.failure_tracker[error_type] += 1
                
                logger.warning(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {e}")
                
                # åº”ç”¨æ¢å¤ç­–ç•¥
                recovery_action = self.recovery_strategies.get(error_type, "retry")
                
                if recovery_action == "skip_node":
                    # è·³è¿‡å¤±è´¥çš„èŠ‚ç‚¹
                    config = self._configure_node_skip(config, e)
                elif recovery_action == "fallback_model":
                    # åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹
                    config = self._configure_fallback_model(config, e)
                elif recovery_action == "reduce_complexity":
                    # é™ä½ä»»åŠ¡å¤æ‚åº¦
                    input_data = self._reduce_task_complexity(input_data, e)
                
                # æŒ‡æ•°é€€é¿
                await asyncio.sleep(2 ** retry_count)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        raise WorkflowExecutionError(
            f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡",
            failure_history=dict(self.failure_tracker)
        )
```

## 4. æ€»ç»“

é€šè¿‡æ•´åˆç« çš„å†…å®¹ï¼Œæˆ‘ä»¬çœ‹åˆ°LangGraphåœ¨å®é™…åº”ç”¨ä¸­å±•ç°å‡ºçš„å¼ºå¤§èƒ½åŠ›ï¼š

### 4.1 æŠ€æœ¯ä¼˜åŠ¿

- **é…ç½®é©±åŠ¨æ¶æ„**ï¼šé€šè¿‡langgraph.jsonå®ç°å£°æ˜å¼çš„å›¾ç®¡ç†
- **æ™ºèƒ½çŠ¶æ€è·¯ç”±**ï¼šCommandå’ŒSendæœºåˆ¶æ”¯æŒå¤æ‚çš„æ§åˆ¶æµ  
- **åæ€å¼è¿­ä»£**ï¼šå†…ç½®çš„è´¨é‡æ§åˆ¶å’Œè‡ªæˆ‘æ”¹è¿›èƒ½åŠ›
- **ä¼ä¸šçº§ç‰¹æ€§**ï¼šå®Œæ•´çš„ç›‘æ§ã€å®‰å…¨ã€æ‰©ç¼©å®¹æ”¯æŒ

### 4.2 åº”ç”¨åœºæ™¯

- **æ·±åº¦ç ”ç©¶ç³»ç»Ÿ**ï¼šå¤šè½®è¿­ä»£çš„ä¿¡æ¯æ”¶é›†å’Œåˆ†æ
- **æ™ºèƒ½å®¢æœå¹³å°**ï¼šå¤šæ™ºèƒ½ä½“åä½œçš„å®¢æˆ·æœåŠ¡
- **ä»£ç ç”Ÿæˆå·¥å…·**ï¼šåæ€å¼çš„ä»£ç ç”Ÿæˆå’Œä¼˜åŒ–
- **æ³•å¾‹æ–‡æ¡£åˆ†æ**ï¼šä¸“ä¸šé¢†åŸŸçš„ç»“æ„åŒ–ä¿¡æ¯æå–


é€šè¿‡æ·±å…¥ç†è§£è¿™äº›é«˜çº§æ¨¡å¼å’Œæœ€ä½³å®è·µï¼Œå¼€å‘è€…èƒ½å¤Ÿå……åˆ†å‘æŒ¥LangGraphçš„æ½œåŠ›ï¼Œæ„å»ºçœŸæ­£å…·æœ‰ç”Ÿäº§ä»·å€¼çš„æ™ºèƒ½ä½“åº”ç”¨ç³»ç»Ÿã€‚

