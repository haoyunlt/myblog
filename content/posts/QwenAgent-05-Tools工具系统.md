---
title: "QwenAgent-05 - Toolså·¥å…·ç³»ç»Ÿæ·±åº¦è§£æ"
date: 2025-09-28T00:47:16+08:00
draft: false
tags: ['æºç åˆ†æ', 'æŠ€æœ¯æ–‡æ¡£']
categories: ['æŠ€æœ¯åˆ†æ']
description: "QwenAgent-05 - Toolså·¥å…·ç³»ç»Ÿæ·±åº¦è§£æçš„æ·±å…¥æŠ€æœ¯åˆ†ææ–‡æ¡£"
keywords: ['æºç åˆ†æ', 'æŠ€æœ¯æ–‡æ¡£']
author: "æŠ€æœ¯åˆ†æå¸ˆ"
weight: 1
---

## ğŸ“ æ¦‚è¿°

Toolså·¥å…·ç³»ç»Ÿæ˜¯Qwen-Agentæ¡†æ¶çš„æ ¸å¿ƒèƒ½åŠ›ä¹‹ä¸€ï¼Œä¸ºAgentæä¾›äº†ä¸å¤–éƒ¨ä¸–ç•Œäº¤äº’çš„èƒ½åŠ›ã€‚é€šè¿‡ç»Ÿä¸€çš„å·¥å…·æ¥å£ï¼ŒAgentå¯ä»¥æ‰§è¡Œä»£ç ã€æœç´¢ç½‘ç»œã€è§£ææ–‡æ¡£ã€ç”Ÿæˆå›¾ç‰‡ç­‰å„ç§å¤æ‚ä»»åŠ¡ã€‚æœ¬æ–‡æ¡£æ·±å…¥åˆ†æå·¥å…·ç³»ç»Ÿçš„è®¾è®¡åŸç†ã€æ ¸å¿ƒç»„ä»¶å’Œå…·ä½“å®ç°ã€‚

## ğŸ—ï¸ Toolsæ¨¡å—æ¶æ„è®¾è®¡

### å·¥å…·ç³»ç»Ÿæ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "å·¥å…·æ³¨å†Œä¸ç®¡ç†"
        A[TOOL_REGISTRY] --> B[å…¨å±€å·¥å…·æ³¨å†Œè¡¨]
        C[register_toolè£…é¥°å™¨] --> D[å·¥å…·è‡ªåŠ¨æ³¨å†Œ]
        E[å·¥å…·å‘ç°æœºåˆ¶] --> F[åŠ¨æ€å·¥å…·åŠ è½½]
    end
    
    subgraph "å·¥å…·åŸºç±»å±‚"
        G[BaseTool] --> H[åŸºç¡€å·¥å…·æ¥å£]
        I[BaseToolWithFileAccess] --> J[æ–‡ä»¶è®¿é—®å·¥å…·]
        K[å·¥å…·é…ç½®ç®¡ç†] --> L[å‚æ•°éªŒè¯]
    end
    
    subgraph "å†…ç½®æ ¸å¿ƒå·¥å…·"
        M[CodeInterpreter] --> N[ä»£ç æ‰§è¡Œç¯å¢ƒ]
        O[DocParser] --> P[æ–‡æ¡£è§£æå¼•æ“]
        Q[WebSearch] --> R[ç½‘ç»œæœç´¢æœåŠ¡]
        S[Retrieval] --> T[RAGæ£€ç´¢ç³»ç»Ÿ]
        U[ImageGen] --> V[å›¾åƒç”ŸæˆæœåŠ¡]
        W[WebExtractor] --> X[ç½‘é¡µå†…å®¹æå–]
    end
    
    subgraph "æœç´¢å·¥å…·é›†åˆ"
        Y[KeywordSearch] --> Z[å…³é”®è¯æœç´¢]
        AA[VectorSearch] --> BB[å‘é‡è¯­ä¹‰æœç´¢]
        CC[HybridSearch] --> DD[æ··åˆæœç´¢ç­–ç•¥]
        EE[FrontPageSearch] --> FF[é¦–é¡µæœç´¢]
    end
    
    subgraph "æ‰©å±•å·¥å…·"
        GG[AmapWeather] --> HH[å¤©æ°”æŸ¥è¯¢]
        II[ImageZoomIn] --> JJ[å›¾åƒç¼©æ”¾]
        KK[ImageSearch] --> LL[å›¾æœåŠŸèƒ½]
        MM[MCPManager] --> NN[MCPåè®®å·¥å…·]
        OO[Storage] --> PP[æ•°æ®å­˜å‚¨]
    end
    
    A --> G
    C --> G
    G --> I
    
    I --> M
    I --> O
    G --> Q
    I --> S
    G --> U
    I --> W
    
    S --> Y
    S --> AA
    S --> CC
    S --> EE
    
    G --> GG
    G --> II
    G --> KK
    G --> MM
    I --> OO
    
    style G fill:#e1f5fe
    style I fill:#f3e5f5
    style M fill:#e8f5e8
    style S fill:#fff3e0
    style CC fill:#fce4ec
```

### æ ¸å¿ƒç±»ç»§æ‰¿å…³ç³»å›¾

```mermaid
classDiagram
    class BaseTool {
        <<abstract>>
        +name: str
        +description: str
        +parameters: Union[List[dict], dict]
        +cfg: dict
        +function: dict
        +name_for_human: str
        +args_format: str
        +file_access: bool
        +call(params, **kwargs)*
        +_verify_json_format_args(params, strict_json)
    }
    
    class BaseToolWithFileAccess {
        +work_dir: str
        +file_access: bool = True
        +call(params, files, **kwargs)
    }
    
    class CodeInterpreter {
        +description: str = "Pythonä»£ç æ²™ç®±"
        +parameters: dict
        +instance_id: str
        +work_dir: str
        +call(params, **kwargs)
        +_get_kernel_client()
        +_fix_matplotlib_cjk_font()
        +_execute_code(code)
    }
    
    class DocParser {
        +description: str = "æ–‡ä»¶å†…å®¹æå–å’Œåˆ†å—"
        +parameters: dict
        +max_ref_token: int
        +parser_page_size: int
        +db: Storage
        +doc_extractor: SimpleDocParser
        +call(params, **kwargs)
        +_check_exists(url)
        +_parse_file(url, **kwargs)
    }
    
    class WebSearch {
        +name: str = "web_search"
        +description: str = "ç½‘ç»œä¿¡æ¯æœç´¢"
        +parameters: dict
        +call(params, **kwargs)
        +search(query)
        +_format_results(results)
    }
    
    class Retrieval {
        +description: str = "RAGæ£€ç´¢å·¥å…·"
        +parameters: dict
        +max_ref_token: int
        +doc_parse: DocParser
        +search: SearchTool
        +call(params, **kwargs)
    }
    
    class ImageGen {
        +description: str = "å›¾åƒç”Ÿæˆå·¥å…·"
        +parameters: dict
        +call(params, **kwargs)
        +_generate_image(prompt)
    }
    
    BaseTool <|-- BaseToolWithFileAccess
    BaseToolWithFileAccess <|-- CodeInterpreter
    BaseTool <|-- DocParser
    BaseTool <|-- WebSearch
    BaseTool <|-- Retrieval
    BaseTool <|-- ImageGen
    
    note for BaseTool "å·¥å…·åŸºç±»ï¼Œå®šä¹‰ç»Ÿä¸€æ¥å£"
    note for BaseToolWithFileAccess "æ”¯æŒæ–‡ä»¶è®¿é—®çš„å·¥å…·åŸºç±»"
    note for CodeInterpreter "Jupyterå†…æ ¸çš„ä»£ç æ‰§è¡Œå™¨"
    note for DocParser "å¤šæ ¼å¼æ–‡æ¡£è§£æå™¨"
    note for WebSearch "Serper APIç½‘ç»œæœç´¢"
    note for Retrieval "RAGæ£€ç´¢ä¸æ–‡æ¡£é—®ç­”"
```

## ğŸ”§ BaseToolåŸºç±»è¯¦ç»†åˆ†æ

### BaseToolæ ¸å¿ƒè®¾è®¡

```python
class BaseTool(ABC):
    """å·¥å…·åŸºç±» - å®šä¹‰æ‰€æœ‰å·¥å…·çš„ç»Ÿä¸€æ¥å£
    
    è®¾è®¡åŸåˆ™:
        1. ç»Ÿä¸€æ¥å£ï¼šæ‰€æœ‰å·¥å…·éƒ½éµå¾ªç›¸åŒçš„è°ƒç”¨è§„èŒƒ
        2. å‚æ•°éªŒè¯ï¼šå†…ç½®å‚æ•°æ ¼å¼éªŒè¯å’Œç±»å‹æ£€æŸ¥
        3. é”™è¯¯å¤„ç†ï¼šç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
        4. å¯æ‰©å±•æ€§ï¼šæ”¯æŒçµæ´»çš„å‚æ•°é…ç½®å’ŒåŠŸèƒ½æ‰©å±•
        5. æ–‡æ¡£åŒ–ï¼šå¼ºåˆ¶è¦æ±‚å·¥å…·æè¿°å’Œå‚æ•°è¯´æ˜
    
    æ ¸å¿ƒå±æ€§:
        name: å·¥å…·åç§°ï¼Œå…¨å±€å”¯ä¸€æ ‡è¯†ç¬¦
        description: å·¥å…·åŠŸèƒ½æè¿°ï¼Œç”¨äºAgentç†è§£å·¥å…·ç”¨é€”
        parameters: å‚æ•°å®šä¹‰ï¼Œæ”¯æŒåˆ—è¡¨å’ŒOpenAI JSON Schemaæ ¼å¼
        cfg: å·¥å…·é…ç½®ï¼ŒåŒ…å«ä¸ªæ€§åŒ–è®¾ç½®
    
    å…³é”®æ–¹æ³•:
        call(): å·¥å…·æ‰§è¡Œå…¥å£ï¼Œå­ç±»å¿…é¡»å®ç°
        _verify_json_format_args(): å‚æ•°éªŒè¯æ–¹æ³•
        function: å·¥å…·ä¿¡æ¯å±æ€§ï¼Œç”¨äºAgentå‡½æ•°è°ƒç”¨
    """
    
    # ç±»å±æ€§å®šä¹‰
    name: str = ''                    # å·¥å…·åç§°
    description: str = ''             # åŠŸèƒ½æè¿°
    parameters: Union[List[dict], dict] = []  # å‚æ•°å®šä¹‰
    
    def __init__(self, cfg: Optional[dict] = None):
        """BaseToolåˆå§‹åŒ–
        
        åˆå§‹åŒ–æµç¨‹:
            1. éªŒè¯å·¥å…·åç§°çš„æœ‰æ•ˆæ€§
            2. å‚æ•°æ ¼å¼è§„èŒƒæ€§æ£€æŸ¥
            3. é…ç½®ä¿¡æ¯åŠ è½½å’ŒéªŒè¯
        
        å¼‚å¸¸å¤„ç†:
            - ValueError: å·¥å…·åç§°ä¸ºç©ºæˆ–å‚æ•°æ ¼å¼é”™è¯¯
            - jsonschema.ValidationError: JSON SchemaéªŒè¯å¤±è´¥
        """
        self.cfg = cfg or {}
        
        # 1. å·¥å…·åç§°éªŒè¯
        if not self.name:
            raise ValueError(
                f'You must set {self.__class__.__name__}.name, '
                f'either by @register_tool(name=...) or explicitly setting {self.__class__.__name__}.name'
            )
        
        # 2. å‚æ•°æ ¼å¼éªŒè¯ï¼ˆé’ˆå¯¹JSON Schemaæ ¼å¼ï¼‰
        if isinstance(self.parameters, dict):
            if not is_tool_schema({
                'name': self.name, 
                'description': self.description, 
                'parameters': self.parameters
            }):
                raise ValueError(
                    'The parameters, when provided as a dict, '
                    'must conform to a valid openai-compatible JSON schema.'
                )
    
    @abstractmethod
    def call(self, params: Union[str, dict], **kwargs) -> Union[str, list, dict, List[ContentItem]]:
        """å·¥å…·è°ƒç”¨çš„æ ¸å¿ƒæ¥å£
        
        è¿™æ˜¯æ¯ä¸ªå·¥å…·å¿…é¡»å®ç°çš„æ ¸å¿ƒæ–¹æ³•ï¼Œå®šä¹‰äº†å·¥å…·çš„å…·ä½“æ‰§è¡Œé€»è¾‘
        
        å‚æ•°è¯´æ˜:
            params: å·¥å…·å‚æ•°ï¼Œå¯ä»¥æ˜¯JSONå­—ç¬¦ä¸²æˆ–å­—å…¸
                   - str: JSONæ ¼å¼çš„å‚æ•°å­—ç¬¦ä¸²ï¼ˆå¸¸è§æƒ…å†µï¼‰
                   - dict: ç›´æ¥ä¼ é€’çš„å‚æ•°å­—å…¸
            **kwargs: é¢å¤–çš„ä¸Šä¸‹æ–‡å‚æ•°
                     - messages: å½“å‰å¯¹è¯æ¶ˆæ¯å†å²
                     - files: ç›¸å…³æ–‡ä»¶åˆ—è¡¨
                     - å…¶ä»–Agentä¼ é€’çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        è¿”å›å€¼ç±»å‹:
            - str: æ–‡æœ¬ç»“æœï¼ˆæœ€å¸¸è§ï¼‰
            - list: ç»“æ„åŒ–æ•°æ®åˆ—è¡¨
            - dict: ç»“æ„åŒ–æ•°æ®å­—å…¸
            - List[ContentItem]: å¤šæ¨¡æ€å†…å®¹åˆ—è¡¨
        
        å®ç°è¦æ±‚:
            1. ä½¿ç”¨_verify_json_format_args()éªŒè¯å‚æ•°
            2. æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå¼‚å¸¸å¤„ç†
            3. è¿”å›æ ¼å¼åŒ–çš„ã€å¯¹Agentå‹å¥½çš„ç»“æœ
            4. è®°å½•å¿…è¦çš„æ—¥å¿—ä¿¡æ¯
        """
        raise NotImplementedError
    
    def _verify_json_format_args(self, params: Union[str, dict], strict_json: bool = False) -> dict:
        """å‚æ•°éªŒè¯æ–¹æ³• - ç¡®ä¿å·¥å…·å‚æ•°æ ¼å¼æ­£ç¡®
        
        éªŒè¯æµç¨‹:
            1. JSONæ ¼å¼è§£æå’ŒéªŒè¯
            2. å¿…éœ€å‚æ•°å­˜åœ¨æ€§æ£€æŸ¥
            3. å‚æ•°ç±»å‹å’Œå€¼åŸŸéªŒè¯
            4. è¿”å›æ ‡å‡†åŒ–çš„å‚æ•°å­—å…¸
        
        å‚æ•°è¯´æ˜:
            params: å¾…éªŒè¯çš„å‚æ•°
            strict_json: æ˜¯å¦ä½¿ç”¨ä¸¥æ ¼çš„JSONè§£æ
        
        è¿”å›å€¼:
            dict: éªŒè¯é€šè¿‡çš„å‚æ•°å­—å…¸
        
        å¼‚å¸¸:
            ValueError: å‚æ•°æ ¼å¼é”™è¯¯æˆ–ç¼ºå°‘å¿…éœ€å‚æ•°
            json.JSONDecodeError: JSONè§£æå¤±è´¥
        """
        # 1. å‚æ•°ç±»å‹è½¬æ¢
        if isinstance(params, str):
            try:
                if strict_json:
                    params_json: dict = json.loads(params)
                else:
                    # ä½¿ç”¨json5æ”¯æŒæ›´å®½æ¾çš„JSONæ ¼å¼
                    params_json: dict = json_loads(params)
            except json.decoder.JSONDecodeError as e:
                raise ValueError(f'Parameters must be formatted as valid JSON: {e}')
        else:
            params_json: dict = params
        
        # 2. å‚æ•°éªŒè¯ï¼ˆåŸºäºparameterså®šä¹‰ï¼‰
        if isinstance(self.parameters, list):
            # åˆ—è¡¨æ ¼å¼å‚æ•°éªŒè¯
            for param in self.parameters:
                if param.get('required', False):
                    if param['name'] not in params_json:
                        raise ValueError(f'Parameters {param["name"]} is required!')
        elif isinstance(self.parameters, dict):
            # JSON Schemaæ ¼å¼éªŒè¯
            import jsonschema
            try:
                jsonschema.validate(instance=params_json, schema=self.parameters)
            except jsonschema.ValidationError as e:
                raise ValueError(f'Parameter validation failed: {e.message}')
        else:
            raise ValueError('Invalid parameters definition format')
        
        return params_json
    
    @property
    def function(self) -> dict:
        """å·¥å…·å‡½æ•°ä¿¡æ¯ - ç”¨äºAgentå‡½æ•°è°ƒç”¨
        
        è¿”å›OpenAIå‡½æ•°è°ƒç”¨æ ¼å¼çš„å·¥å…·æè¿°ï¼ŒåŒ…å«ï¼š
        - name: å·¥å…·åç§°
        - description: åŠŸèƒ½æè¿°
        - parameters: å‚æ•°å®šä¹‰
        
        è¿™ä¸ªå±æ€§è¢«Agentç”¨äºæ„å»ºfunctionsåˆ—è¡¨ï¼Œä¼ é€’ç»™LLMè¿›è¡Œå‡½æ•°è°ƒç”¨
        """
        return {
            'name': self.name,
            'description': self.description,
            'parameters': self.parameters,
        }
    
    @property
    def name_for_human(self) -> str:
        """äººç±»å¯è¯»çš„å·¥å…·åç§°
        
        ä¼˜å…ˆçº§ï¼šé…ç½®ä¸­çš„name_for_human > å·¥å…·name
        ç”¨äºGUIç•Œé¢æ˜¾ç¤ºå’Œç”¨æˆ·äº¤äº’
        """
        return self.cfg.get('name_for_human', self.name)
    
    @property
    def args_format(self) -> str:
        """å‚æ•°æ ¼å¼è¯´æ˜ - å¸®åŠ©Agentç†è§£å¦‚ä½•ä½¿ç”¨å·¥å…·
        
        è‡ªåŠ¨æ ¹æ®å·¥å…·çš„è¯­è¨€ç¯å¢ƒç”Ÿæˆé€‚å½“çš„æ ¼å¼è¯´æ˜ï¼š
        - ä¸­æ–‡ç¯å¢ƒï¼šè¿”å›ä¸­æ–‡è¯´æ˜
        - è‹±æ–‡ç¯å¢ƒï¼šè¿”å›è‹±æ–‡è¯´æ˜
        """
        fmt = self.cfg.get('args_format')
        if fmt is None:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
            if has_chinese_chars([self.name_for_human, self.name, self.description, self.parameters]):
                fmt = 'æ­¤å·¥å…·çš„è¾“å…¥åº”ä¸ºJSONå¯¹è±¡ã€‚'
            else:
                fmt = 'Format the arguments as a JSON object.'
        return fmt
    
    @property
    def file_access(self) -> bool:
        """å·¥å…·æ˜¯å¦éœ€è¦æ–‡ä»¶è®¿é—®æƒé™
        
        è¿”å›å€¼:
            False: åŸºç¡€å·¥å…·ä¸éœ€è¦æ–‡ä»¶è®¿é—®
        
        å­ç±»å¯ä»¥é‡å†™æ­¤å±æ€§ä»¥å£°æ˜æ–‡ä»¶è®¿é—®éœ€æ±‚
        """
        return False
```

### å·¥å…·æ³¨å†Œæœºåˆ¶è¯¦è§£

```python
# å…¨å±€å·¥å…·æ³¨å†Œè¡¨
TOOL_REGISTRY: Dict[str, Type[BaseTool]] = {}

def register_tool(name: str, allow_overwrite: bool = False):
    """å·¥å…·æ³¨å†Œè£…é¥°å™¨ - å®ç°å·¥å…·çš„è‡ªåŠ¨å‘ç°å’Œæ³¨å†Œ
    
    è®¾è®¡ç›®æ ‡:
        1. è‡ªåŠ¨åŒ–æ³¨å†Œï¼šé€šè¿‡è£…é¥°å™¨è‡ªåŠ¨å°†å·¥å…·æ³¨å†Œåˆ°å…¨å±€æ³¨å†Œè¡¨
        2. åç§°ç®¡ç†ï¼šç¡®ä¿å·¥å…·åç§°çš„å”¯ä¸€æ€§å’Œä¸€è‡´æ€§
        3. è¦†ç›–æ§åˆ¶ï¼šæä¾›å®‰å…¨çš„å·¥å…·è¦†ç›–æœºåˆ¶
        4. è¿è¡Œæ—¶å‘ç°ï¼šæ”¯æŒåŠ¨æ€å·¥å…·åŠ è½½å’Œç®¡ç†
    
    ä½¿ç”¨æ–¹å¼:
        @register_tool('tool_name')
        class MyTool(BaseTool):
            pass
    
    å‚æ•°è¯´æ˜:
        name: å·¥å…·åç§°ï¼Œå¿…é¡»å…¨å±€å”¯ä¸€
        allow_overwrite: æ˜¯å¦å…è®¸è¦†ç›–å·²å­˜åœ¨çš„å·¥å…·
    
    æ³¨å†Œæµç¨‹:
        1. æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦å·²å­˜åœ¨
        2. éªŒè¯åç§°ä¸€è‡´æ€§
        3. è®¾ç½®å·¥å…·åç§°å±æ€§
        4. æ³¨å†Œåˆ°å…¨å±€æ³¨å†Œè¡¨
    """
    def decorator(cls: Type[BaseTool]):
        # 1. é‡å¤æ³¨å†Œæ£€æŸ¥
        if name in TOOL_REGISTRY:
            if allow_overwrite:
                logger.warning(f'Tool `{name}` already exists! Overwriting with class {cls}.')
            else:
                raise ValueError(
                    f'Tool `{name}` already exists! '
                    f'Please ensure that the tool name is unique.'
                )
        
        # 2. åç§°ä¸€è‡´æ€§éªŒè¯
        if hasattr(cls, 'name') and cls.name and (cls.name != name):
            raise ValueError(
                f'{cls.__name__}.name="{cls.name}" conflicts with @register_tool(name="{name}").'
            )
        
        # 3. è®¾ç½®å·¥å…·åç§°
        cls.name = name
        
        # 4. æ³¨å†Œåˆ°å…¨å±€æ³¨å†Œè¡¨
        TOOL_REGISTRY[name] = cls
        
        return cls
    
    return decorator

# å·¥å…·å®ä¾‹åŒ–å‡½æ•°
def get_tool_instance(tool_identifier: Union[str, dict, BaseTool]) -> BaseTool:
    """è·å–å·¥å…·å®ä¾‹ - ç»Ÿä¸€çš„å·¥å…·å®ä¾‹åŒ–æ¥å£
    
    æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼:
        1. å­—ç¬¦ä¸²ï¼šå·¥å…·åç§°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        2. å­—å…¸ï¼šåŒ…å«nameå’Œé…ç½®çš„å­—å…¸
        3. å®ä¾‹ï¼šç›´æ¥è¿”å›å·¥å…·å®ä¾‹
    
    å‚æ•°è¯´æ˜:
        tool_identifier: å·¥å…·æ ‡è¯†ç¬¦
    
    è¿”å›å€¼:
        BaseTool: å·¥å…·å®ä¾‹
    
    å¼‚å¸¸:
        ValueError: å·¥å…·åç§°ä¸å­˜åœ¨æˆ–é…ç½®æ ¼å¼é”™è¯¯
    """
    if isinstance(tool_identifier, BaseTool):
        # å·²ç»æ˜¯å·¥å…·å®ä¾‹ï¼Œç›´æ¥è¿”å›
        return tool_identifier
    elif isinstance(tool_identifier, str):
        # å­—ç¬¦ä¸²æ ¼å¼ï¼šå·¥å…·åç§°
        if tool_identifier not in TOOL_REGISTRY:
            raise ValueError(f'Tool {tool_identifier} is not registered.')
        tool_class = TOOL_REGISTRY[tool_identifier]
        return tool_class()
    elif isinstance(tool_identifier, dict):
        # å­—å…¸æ ¼å¼ï¼šåŒ…å«nameå’Œé…ç½®
        tool_name = tool_identifier.get('name')
        if not tool_name:
            raise ValueError('Tool configuration must contain "name" field.')
        if tool_name not in TOOL_REGISTRY:
            raise ValueError(f'Tool {tool_name} is not registered.')
        
        tool_class = TOOL_REGISTRY[tool_name]
        tool_config = {k: v for k, v in tool_identifier.items() if k != 'name'}
        return tool_class(cfg=tool_config)
    else:
        raise ValueError(f'Invalid tool identifier type: {type(tool_identifier)}')
```

## ğŸ› ï¸ æ ¸å¿ƒå†…ç½®å·¥å…·è¯¦è§£

### 1. CodeInterpreter - ä»£ç æ‰§è¡Œå™¨

```python
@register_tool('code_interpreter')
class CodeInterpreter(BaseToolWithFileAccess):
    """Pythonä»£ç æ²™ç®±æ‰§è¡Œå™¨
    
    æ ¸å¿ƒåŠŸèƒ½:
        1. å®‰å…¨çš„Pythonä»£ç æ‰§è¡Œç¯å¢ƒ
        2. Jupyterå†…æ ¸é›†æˆï¼Œæ”¯æŒçŠ¶æ€ä¿æŒ
        3. å›¾è¡¨ç”Ÿæˆå’Œå¯è§†åŒ–æ”¯æŒ
        4. æ–‡ä»¶ç³»ç»Ÿè®¿é—®å’Œç®¡ç†
        5. å¤šå®ä¾‹éš”ç¦»æ‰§è¡Œ
    
    æŠ€æœ¯ç‰¹ç‚¹:
        - åŸºäºJupyterå†…æ ¸çš„ä»£ç æ‰§è¡Œ
        - è‡ªåŠ¨ä¸­æ–‡å­—ä½“é…ç½®ï¼ˆmatplotlibï¼‰
        - å®ä¾‹çº§åˆ«çš„å·¥ä½œç›®å½•éš”ç¦»
        - å®Œæ•´çš„é”™è¯¯æ•è·å’Œæ—¥å¿—è®°å½•
        - æ”¯æŒå¼‚æ­¥ä»£ç æ‰§è¡Œ
    
    å®‰å…¨è€ƒè™‘:
        - å·¥ä½œç›®å½•éš”ç¦»
        - å­è¿›ç¨‹ç®¡ç†å’Œæ¸…ç†
        - èµ„æºä½¿ç”¨ç›‘æ§
        - å±é™©æ“ä½œé™åˆ¶
    """
    
    description = 'Python code sandbox, which can be used to execute Python code.'
    parameters = {
        'type': 'object',
        'properties': {
            'code': {
                'description': 'The python code.',
                'type': 'string',
            }
        },
        'required': ['code'],
    }
    
    def __init__(self, cfg: Optional[Dict] = None):
        """CodeInterpreteråˆå§‹åŒ–
        
        åˆå§‹åŒ–è¿‡ç¨‹:
            1. è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œè®¾ç½®å·¥ä½œç›®å½•
            2. ç”Ÿæˆå”¯ä¸€å®ä¾‹IDï¼Œç¡®ä¿å¤šå®ä¾‹éš”ç¦»
            3. æ£€æŸ¥ä¾èµ–é¡¹ï¼ˆJupyterã€matplotlibç­‰ï¼‰
            4. é…ç½®ä»£ç æ‰§è¡Œç¯å¢ƒ
        """
        super().__init__(cfg)
        
        # 1. å·¥ä½œç›®å½•é…ç½®ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
        self.work_dir: str = os.getenv('M6_CODE_INTERPRETER_WORK_DIR', self.work_dir)
        self.work_dir: str = self.cfg.get('work_dir', self.work_dir)
        
        # 2. å®ä¾‹IDç”Ÿæˆï¼ˆç¡®ä¿å¤šå®ä¾‹éš”ç¦»ï¼‰
        self.instance_id: str = str(uuid.uuid4())
        
        # 3. ä¾èµ–æ£€æŸ¥
        _check_deps_for_code_interpreter()
    
    def call(self, params: Union[str, dict], **kwargs) -> str:
        """æ‰§è¡ŒPythonä»£ç 
        
        æ‰§è¡Œæµç¨‹:
            1. å‚æ•°è§£æå’ŒéªŒè¯
            2. ä»£ç é¢„å¤„ç†ï¼ˆæå–ä»£ç å—ï¼‰
            3. Jupyterå†…æ ¸è·å–å’Œåˆå§‹åŒ–
            4. ä»£ç æ‰§è¡Œå’Œç»“æœæ•è·
            5. é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
            6. ç»“æœæ ¼å¼åŒ–å’Œè¿”å›
        
        å‚æ•°è¯´æ˜:
            params: åŒ…å«codeå­—æ®µçš„å‚æ•°
            **kwargs: é¢å¤–ä¸Šä¸‹æ–‡å‚æ•°
        
        è¿”å›å€¼:
            str: ä»£ç æ‰§è¡Œç»“æœï¼ŒåŒ…å«è¾“å‡ºå’Œé”™è¯¯ä¿¡æ¯
        """
        # 1. å‚æ•°éªŒè¯
        params = self._verify_json_format_args(params)
        code_input = params['code']
        
        # 2. ä»£ç æå–ï¼ˆä»markdownä»£ç å—ä¸­æå–ï¼‰
        code_blocks = extract_code(code_input)
        
        if not code_blocks:
            return 'No Python code found in the input.'
        
        # 3. åˆå§‹åŒ–æ‰§è¡Œç¯å¢ƒ
        os.makedirs(self.work_dir, exist_ok=True)
        
        result_messages = []
        
        # 4. é€ä¸ªæ‰§è¡Œä»£ç å—
        for i, code_block in enumerate(code_blocks):
            try:
                # è·å–æˆ–åˆ›å»ºJupyterå†…æ ¸å®¢æˆ·ç«¯
                kernel_client = self._get_kernel_client()
                
                # æ‰§è¡Œä»£ç 
                execution_result = self._execute_code(code_block, kernel_client)
                
                result_messages.append(f"Code block {i+1} executed successfully:")
                result_messages.append(execution_result)
                
            except Exception as e:
                error_msg = f"Error executing code block {i+1}: {str(e)}"
                logger.error(error_msg)
                result_messages.append(error_msg)
        
        return '\n'.join(result_messages)
    
    def _get_kernel_client(self):
        """è·å–æˆ–åˆ›å»ºJupyterå†…æ ¸å®¢æˆ·ç«¯
        
        å†…æ ¸ç®¡ç†ç­–ç•¥:
            1. å®ä¾‹çº§åˆ«çš„å†…æ ¸å¤ç”¨
            2. è‡ªåŠ¨å†…æ ¸å¯åŠ¨å’Œè¿æ¥
            3. å¼‚å¸¸æ—¶çš„å†…æ ¸é‡å¯æœºåˆ¶
            4. èµ„æºæ¸…ç†å’Œç”Ÿå‘½å‘¨æœŸç®¡ç†
        """
        if self.instance_id in _KERNEL_CLIENTS:
            return _KERNEL_CLIENTS[self.instance_id]
        
        # åˆ›å»ºæ–°çš„å†…æ ¸å®¢æˆ·ç«¯
        kernel_client = self._create_kernel_client()
        _KERNEL_CLIENTS[self.instance_id] = kernel_client
        
        # åˆå§‹åŒ–ä»£ç æ‰§è¡Œï¼ˆå­—ä½“é…ç½®ç­‰ï¼‰
        self._initialize_kernel(kernel_client)
        
        return kernel_client
    
    def _create_kernel_client(self):
        """åˆ›å»ºJupyterå†…æ ¸å®¢æˆ·ç«¯
        
        åˆ›å»ºè¿‡ç¨‹:
            1. å¯åŠ¨ç‹¬ç«‹çš„Jupyterå†…æ ¸è¿›ç¨‹
            2. å»ºç«‹å®¢æˆ·ç«¯è¿æ¥
            3. é…ç½®å†…æ ¸å‚æ•°
            4. æ³¨å†Œæ¸…ç†å¤„ç†å™¨
        """
        try:
            import jupyter_client
        except ImportError:
            raise ImportError('jupyter_client is required for code execution.')
        
        # å†…æ ¸ç®¡ç†å™¨é…ç½®
        km = jupyter_client.KernelManager()
        km.start_kernel(
            cwd=self.work_dir,
            extra_arguments=[
                '--IPKernelApp.parent_appname=qwen_agent',
            ]
        )
        
        # åˆ›å»ºå®¢æˆ·ç«¯è¿æ¥
        kc = km.client()
        kc.start_channels()
        
        return kc
    
    def _initialize_kernel(self, kernel_client):
        """åˆå§‹åŒ–å†…æ ¸ç¯å¢ƒ
        
        åˆå§‹åŒ–å†…å®¹:
            1. matplotlibä¸­æ–‡å­—ä½“é…ç½®
            2. å·¥ä½œç›®å½•è®¾ç½®
            3. å¸¸ç”¨åº“å¯¼å…¥
            4. ç¯å¢ƒå˜é‡é…ç½®
        """
        # ä¸­æ–‡å­—ä½“é…ç½®ä»£ç 
        init_code = f"""
import os
import sys
os.chdir('{self.work_dir}')

# Configure matplotlib for Chinese font support
try:
    import matplotlib.pyplot as plt
    import matplotlib
    font_path = '{ALIB_FONT_FILE}'
    if os.path.exists(font_path):
        matplotlib.font_manager.fontManager.addfont(font_path)
        plt.rcParams['font.sans-serif'] = ['AlibabaPuHuiTi-3-45-Light']
        plt.rcParams['axes.unicode_minus'] = False
except ImportError:
    pass

print("Code interpreter initialized successfully.")
"""
        
        # æ‰§è¡Œåˆå§‹åŒ–ä»£ç 
        self._execute_code(init_code, kernel_client)
    
    def _execute_code(self, code: str, kernel_client) -> str:
        """æ‰§è¡Œå•æ®µä»£ç 
        
        æ‰§è¡Œæµç¨‹:
            1. æäº¤ä»£ç åˆ°å†…æ ¸
            2. ç›‘å¬æ‰§è¡Œæ¶ˆæ¯
            3. æ”¶é›†è¾“å‡ºç»“æœ
            4. å¤„ç†é”™è¯¯å’Œå¼‚å¸¸
            5. æ ¼å¼åŒ–è¿”å›ç»“æœ
        
        å‚æ•°è¯´æ˜:
            code: è¦æ‰§è¡Œçš„Pythonä»£ç 
            kernel_client: Jupyterå†…æ ¸å®¢æˆ·ç«¯
        
        è¿”å›å€¼:
            str: æ‰§è¡Œç»“æœï¼ŒåŒ…å«æ ‡å‡†è¾“å‡ºã€é”™è¯¯è¾“å‡ºç­‰
        """
        # æäº¤ä»£ç æ‰§è¡Œ
        msg_id = kernel_client.execute(code, silent=False, store_history=True)
        
        outputs = []
        errors = []
        
        # ç›‘å¬æ‰§è¡Œç»“æœ
        while True:
            try:
                # è·å–æ‰§è¡Œæ¶ˆæ¯ï¼ˆè¶…æ—¶å¤„ç†ï¼‰
                msg = kernel_client.get_iopub_msg(timeout=10)
                
                if msg['parent_header'].get('msg_id') == msg_id:
                    msg_type = msg['header']['msg_type']
                    content = msg['content']
                    
                    if msg_type == 'stream':
                        # æ ‡å‡†è¾“å‡º/é”™è¯¯è¾“å‡º
                        stream_content = content['text']
                        if content['name'] == 'stdout':
                            outputs.append(stream_content)
                        elif content['name'] == 'stderr':
                            errors.append(stream_content)
                    
                    elif msg_type == 'execute_result':
                        # æ‰§è¡Œç»“æœ
                        if 'text/plain' in content['data']:
                            outputs.append(content['data']['text/plain'])
                    
                    elif msg_type == 'display_data':
                        # æ˜¾ç¤ºæ•°æ®ï¼ˆå›¾åƒç­‰ï¼‰
                        if 'image/png' in content['data']:
                            # ä¿å­˜å›¾åƒåˆ°å·¥ä½œç›®å½•
                            image_filename = self._save_image(content['data']['image/png'])
                            outputs.append(f"Generated image saved as: {image_filename}")
                    
                    elif msg_type == 'error':
                        # æ‰§è¡Œé”™è¯¯
                        error_name = content['ename']
                        error_value = content['evalue']
                        traceback = '\n'.join(content['traceback'])
                        errors.append(f"{error_name}: {error_value}\n{traceback}")
                    
                    elif msg_type == 'status' and content['execution_state'] == 'idle':
                        # æ‰§è¡Œå®Œæˆ
                        break
            
            except queue.Empty:
                # æ‰§è¡Œè¶…æ—¶
                errors.append("Code execution timeout")
                break
            except Exception as e:
                errors.append(f"Execution error: {str(e)}")
                break
        
        # æ ¼å¼åŒ–ç»“æœ
        result_parts = []
        
        if outputs:
            result_parts.append("Output:")
            result_parts.extend(outputs)
        
        if errors:
            result_parts.append("Errors:")
            result_parts.extend(errors)
        
        if not outputs and not errors:
            result_parts.append("Code executed successfully (no output)")
        
        return '\n'.join(result_parts)
    
    def _save_image(self, image_data: str) -> str:
        """ä¿å­˜ç”Ÿæˆçš„å›¾åƒ
        
        å‚æ•°è¯´æ˜:
            image_data: base64ç¼–ç çš„å›¾åƒæ•°æ®
        
        è¿”å›å€¼:
            str: ä¿å­˜çš„å›¾åƒæ–‡ä»¶å
        """
        import base64
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        timestamp = int(time.time())
        filename = f"generated_image_{timestamp}.png"
        filepath = os.path.join(self.work_dir, filename)
        
        # ä¿å­˜å›¾åƒ
        with open(filepath, 'wb') as f:
            f.write(base64.b64decode(image_data))
        
        return filename
```

### 2. WebSearch - ç½‘ç»œæœç´¢å·¥å…·

```python
@register_tool('web_search', allow_overwrite=True)
class WebSearch(BaseTool):
    """ç½‘ç»œæœç´¢å·¥å…· - åŸºäºSerper APIçš„æœç´¢æœåŠ¡
    
    æ ¸å¿ƒåŠŸèƒ½:
        1. Googleæœç´¢ç»“æœè·å–
        2. æœç´¢ç»“æœç»“æ„åŒ–å¤„ç†
        3. å¤šè¯­è¨€æœç´¢æ”¯æŒ
        4. ç»“æœæ ¼å¼åŒ–å’Œæ‘˜è¦
    
    æŠ€æœ¯ç‰¹ç‚¹:
        - é›†æˆSerper.dev APIæœåŠ¡
        - ç»“æ„åŒ–æœç´¢ç»“æœè¿”å›
        - è‡ªåŠ¨ç»“æœæ ¼å¼åŒ–
        - æ”¯æŒæœç´¢å‚æ•°è‡ªå®šä¹‰
    
    ä½¿ç”¨åœºæ™¯:
        - å®æ—¶ä¿¡æ¯æŸ¥è¯¢
        - æ–°é—»æœç´¢å’Œè·Ÿè¸ª
        - ç ”ç©¶èµ„æ–™æ”¶é›†
        - äº‹å®éªŒè¯å’Œæ ¸å®
    """
    
    name = 'web_search'
    description = 'Search for information from the internet.'
    parameters = {
        'type': 'object',
        'properties': {
            'query': {
                'type': 'string',
                'description': 'The search query string'
            }
        },
        'required': ['query'],
    }
    
    def call(self, params: Union[str, dict], **kwargs) -> str:
        """æ‰§è¡Œç½‘ç»œæœç´¢
        
        æœç´¢æµç¨‹:
            1. å‚æ•°éªŒè¯å’Œè§£æ
            2. è°ƒç”¨Serper APIè¿›è¡Œæœç´¢
            3. ç»“æœå¤„ç†å’Œæ ¼å¼åŒ–
            4. è¿”å›ç»“æ„åŒ–æœç´¢ç»“æœ
        
        å‚æ•°è¯´æ˜:
            params: åŒ…å«queryå­—æ®µçš„æœç´¢å‚æ•°
        
        è¿”å›å€¼:
            str: æ ¼å¼åŒ–çš„æœç´¢ç»“æœ
        """
        # 1. å‚æ•°éªŒè¯
        params = self._verify_json_format_args(params)
        query = params['query']
        
        try:
            # 2. æ‰§è¡Œæœç´¢
            search_results = self.search(query)
            
            # 3. æ ¼å¼åŒ–ç»“æœ
            formatted_results = self._format_results(search_results)
            
            return formatted_results
            
        except Exception as e:
            error_msg = f"æœç´¢å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    @staticmethod
    def search(query: str) -> List[Any]:
        """è°ƒç”¨Serper APIæ‰§è¡Œæœç´¢
        
        APIé…ç½®:
            - SERPER_API_KEY: APIå¯†é’¥ï¼ˆç¯å¢ƒå˜é‡ï¼‰
            - SERPER_URL: APIç«¯ç‚¹URL
        
        å‚æ•°è¯´æ˜:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        
        è¿”å›å€¼:
            List[Any]: åŸå§‹æœç´¢ç»“æœåˆ—è¡¨
        
        å¼‚å¸¸:
            ValueError: APIå¯†é’¥æœªé…ç½®
            requests.RequestException: APIè°ƒç”¨å¤±è´¥
        """
        # 1. APIå¯†é’¥æ£€æŸ¥
        if not SERPER_API_KEY:
            raise ValueError(
                'SERPER_API_KEY is None! Please apply for an API key from https://serper.dev '
                'and set it as an environment variable: export SERPER_API_KEY=xxxxxx'
            )
        
        # 2. æ„å»ºè¯·æ±‚
        headers = {
            'Content-Type': 'application/json',
            'X-API-KEY': SERPER_API_KEY
        }
        payload = {'q': query}
        
        # 3. è°ƒç”¨API
        response = requests.post(SERPER_URL, json=payload, headers=headers)
        response.raise_for_status()
        
        # 4. è§£æç»“æœ
        response_data = response.json()
        return response_data.get('organic', [])
    
    @staticmethod
    def _format_results(search_results: List[Any]) -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœ
        
        æ ¼å¼åŒ–ç­–ç•¥:
            1. æå–å…³é”®ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€æ‘˜è¦ã€æ—¥æœŸï¼‰
            2. ç»“æ„åŒ–å±•ç¤º
            3. æ·»åŠ ç´¢å¼•ç¼–å·
            4. Markdownæ ¼å¼è¾“å‡º
        
        å‚æ•°è¯´æ˜:
            search_results: åŸå§‹æœç´¢ç»“æœåˆ—è¡¨
        
        è¿”å›å€¼:
            str: æ ¼å¼åŒ–çš„æœç´¢ç»“æœå­—ç¬¦ä¸²
        """
        if not search_results:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœã€‚"
        
        # æ ¼å¼åŒ–æ¯ä¸ªæœç´¢ç»“æœ
        formatted_items = []
        for i, result in enumerate(search_results, 1):
            title = result.get('title', 'No Title')
            snippet = result.get('snippet', 'No Description')
            date = result.get('date', '')
            url = result.get('link', '')
            
            # æ„å»ºå•ä¸ªç»“æœçš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
            formatted_item = f"[{i}] **{title}**\n"
            formatted_item += f"   {snippet}\n"
            if date:
                formatted_item += f"   å‘å¸ƒæ—¶é—´: {date}\n"
            formatted_item += f"   é“¾æ¥: {url}"
            
            formatted_items.append(formatted_item)
        
        # ç»„åˆæ‰€æœ‰ç»“æœ
        result_content = '\n\n'.join(formatted_items)
        
        # æ·»åŠ æœç´¢ç»“æœå¤´éƒ¨
        header = f"æœç´¢ç»“æœ (å…±æ‰¾åˆ° {len(search_results)} æ¡ç›¸å…³ä¿¡æ¯):\n\n"
        
        return header + result_content
```

### 3. DocParser - æ–‡æ¡£è§£æå·¥å…·

```python
@register_tool('doc_parser')
class DocParser(BaseTool):
    """æ–‡æ¡£è§£æå·¥å…· - å¤šæ ¼å¼æ–‡æ¡£å†…å®¹æå–å’Œåˆ†å—å¤„ç†
    
    æ ¸å¿ƒåŠŸèƒ½:
        1. å¤šæ ¼å¼æ–‡æ¡£è§£æï¼ˆPDFã€Wordã€PPTã€HTMLç­‰ï¼‰
        2. æ™ºèƒ½åˆ†å—å’Œtokenç®¡ç†
        3. æ–‡æ¡£å†…å®¹ç¼“å­˜å’Œç´¢å¼•
        4. å…ƒæ•°æ®æå–å’Œç®¡ç†
        5. ç»“æ„åŒ–å†…å®¹è¾“å‡º
    
    æŠ€æœ¯ç‰¹ç‚¹:
        - æ”¯æŒ10+ç§æ–‡æ¡£æ ¼å¼
        - æ™ºèƒ½åˆ†å—ç®—æ³•ï¼Œä¿æŒå†…å®¹å®Œæ•´æ€§
        - åŸºäºtokençš„é•¿åº¦æ§åˆ¶
        - æŒä¹…åŒ–å­˜å‚¨å’Œç¼“å­˜æœºåˆ¶
        - å¢é‡è§£æå’Œæ›´æ–°
    
    æ”¯æŒæ ¼å¼:
        - PDFæ–‡æ¡£ (.pdf)
        - Wordæ–‡æ¡£ (.docx, .doc)
        - PowerPoint (.pptx, .ppt)
        - HTMLç½‘é¡µ (.html, .htm)
        - Markdownæ–‡æ¡£ (.md)
        - çº¯æ–‡æœ¬æ–‡ä»¶ (.txt)
        - CSVæ•°æ®æ–‡ä»¶ (.csv)
        - JSONæ•°æ®æ–‡ä»¶ (.json)
    """
    
    description = 'å¯¹ä¸€ä¸ªæ–‡ä»¶è¿›è¡Œå†…å®¹æå–å’Œåˆ†å—ã€è¿”å›åˆ†å—åçš„æ–‡ä»¶å†…å®¹'
    parameters = {
        'type': 'object',
        'properties': {
            'url': {
                'description': 'å¾…è§£æçš„æ–‡ä»¶çš„è·¯å¾„ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªæœ¬åœ°è·¯å¾„æˆ–å¯ä¸‹è½½çš„http(s)é“¾æ¥',
                'type': 'string',
            }
        },
        'required': ['url'],
    }
    
    def __init__(self, cfg: Optional[Dict] = None):
        """DocParseråˆå§‹åŒ–
        
        åˆå§‹åŒ–ç»„ä»¶:
            1. é…ç½®å‚æ•°åŠ è½½
            2. å­˜å‚¨ç³»ç»Ÿåˆå§‹åŒ–
            3. æ–‡æ¡£æå–å™¨åˆ›å»º
            4. ç¼“å­˜æœºåˆ¶è®¾ç½®
        """
        super().__init__(cfg)
        
        # 1. é…ç½®å‚æ•°
        self.max_ref_token: int = self.cfg.get('max_ref_token', DEFAULT_MAX_REF_TOKEN)
        self.parser_page_size: int = self.cfg.get('parser_page_size', DEFAULT_PARSER_PAGE_SIZE)
        
        # 2. å­˜å‚¨ç³»ç»Ÿåˆå§‹åŒ–
        self.data_root = self.cfg.get('path', os.path.join(DEFAULT_WORKSPACE, 'tools', self.name))
        self.db = Storage({'storage_root_path': self.data_root})
        
        # 3. æ–‡æ¡£æå–å™¨
        self.doc_extractor = SimpleDocParser({'structured_doc': True})
    
    def call(self, params: Union[str, dict], **kwargs) -> dict:
        """æ–‡æ¡£è§£æä¸»å…¥å£
        
        è§£ææµç¨‹:
            1. å‚æ•°éªŒè¯å’ŒURLå¤„ç†
            2. ç¼“å­˜æ£€æŸ¥ï¼ˆé¿å…é‡å¤è§£æï¼‰
            3. æ–‡æ¡£ä¸‹è½½å’Œé¢„å¤„ç†
            4. å†…å®¹æå–å’Œåˆ†å—
            5. ç»“æœå­˜å‚¨å’Œè¿”å›
        
        è¿”å›æ ¼å¼:
            {
                'url': 'æ–‡ä»¶URL',
                'title': 'æå–çš„æ ‡é¢˜',
                'raw': [
                    {
                        'content': 'åˆ†å—å†…å®¹',
                        'token': 'tokenæ•°é‡',
                        'metadata': {}  # å…ƒæ•°æ®ä¿¡æ¯
                    },
                    ...
                ]
            }
        """
        # 1. å‚æ•°éªŒè¯
        params = self._verify_json_format_args(params)
        url = params.get('url', params.get('file_path', ''))  # å…¼å®¹æ—§ç‰ˆæœ¬
        
        if not url:
            raise ValueError('URL parameter is required')
        
        # 2. ç¼“å­˜æ£€æŸ¥
        if self._check_exists(url):
            logger.info(f"Document {url} already parsed, loading from cache")
            return self._load_from_cache(url)
        
        # 3. æ‰§è¡Œè§£æ
        try:
            result = self._parse_file(url, **kwargs)
            
            # 4. å­˜å‚¨ç»“æœ
            self._save_to_cache(url, result)
            
            return result
            
        except Exception as e:
            error_msg = f"Document parsing failed for {url}: {str(e)}"
            logger.error(error_msg)
            raise DocParserError(error_msg)
    
    def _parse_file(self, url: str, **kwargs) -> dict:
        """æ‰§è¡Œæ–‡ä»¶è§£æ
        
        è§£ææ­¥éª¤:
            1. æ–‡æ¡£å†…å®¹æå–
            2. æ ‡é¢˜å’Œå…ƒæ•°æ®æå–
            3. å†…å®¹æ¸…ç†å’Œé¢„å¤„ç†
            4. æ™ºèƒ½åˆ†å—å¤„ç†
            5. Tokenè®¡ç®—å’ŒéªŒè¯
        """
        # 1. å†…å®¹æå–
        raw_content = self.doc_extractor.parse(url)
        
        if not raw_content:
            raise DocParserError(f"No content extracted from {url}")
        
        # 2. æå–æ ‡é¢˜
        title = self._extract_title(raw_content, url)
        
        # 3. å†…å®¹åˆ†å—
        chunks = self._chunk_content(raw_content, url)
        
        # 4. æ„å»ºç»“æœ
        result = {
            'url': url,
            'title': title,
            'raw': [chunk.to_dict() for chunk in chunks]
        }
        
        return result
    
    def _chunk_content(self, content: str, url: str) -> List[Chunk]:
        """æ™ºèƒ½å†…å®¹åˆ†å—
        
        åˆ†å—ç­–ç•¥:
            1. æŒ‰æ®µè½åˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
            2. æ§åˆ¶æ¯ä¸ªå—çš„tokenæ•°é‡
            3. å¤„ç†é‡å å†…å®¹ï¼Œæä¾›ä¸Šä¸‹æ–‡
            4. ä¿ç•™ç»“æ„åŒ–ä¿¡æ¯
        
        å‚æ•°è¯´æ˜:
            content: åŸå§‹æ–‡æ¡£å†…å®¹
            url: æ–‡æ¡£URLï¼ˆç”¨äºå…ƒæ•°æ®ï¼‰
        
        è¿”å›å€¼:
            List[Chunk]: åˆ†å—ç»“æœåˆ—è¡¨
        """
        chunks = []
        
        # 1. æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split(PARAGRAPH_SPLIT_SYMBOL)
        
        current_chunk = ""
        current_tokens = 0
        chunk_index = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # è®¡ç®—æ®µè½tokenæ•°
            para_tokens = count_tokens(paragraph)
            
            # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦æ–°å»ºå—
            if current_tokens + para_tokens > self.parser_page_size and current_chunk:
                # åˆ›å»ºå½“å‰å—
                chunk = Chunk(
                    content=current_chunk.strip(),
                    metadata={
                        'source': get_basename_from_url(url),
                        'chunk_index': chunk_index,
                        'url': url
                    },
                    token=current_tokens
                )
                chunks.append(chunk)
                
                # é‡ç½®è®¡æ•°å™¨
                current_chunk = ""
                current_tokens = 0
                chunk_index += 1
            
            # 3. æ·»åŠ æ®µè½åˆ°å½“å‰å—
            if current_chunk:
                current_chunk += "\n\n" + paragraph
            else:
                current_chunk = paragraph
            current_tokens += para_tokens
        
        # 4. å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunk = Chunk(
                content=current_chunk.strip(),
                metadata={
                    'source': get_basename_from_url(url),
                    'chunk_index': chunk_index,
                    'url': url
                },
                token=current_tokens
            )
            chunks.append(chunk)
        
        return chunks
    
    def _extract_title(self, content: str, url: str) -> str:
        """æå–æ–‡æ¡£æ ‡é¢˜
        
        æå–ç­–ç•¥:
            1. æŸ¥æ‰¾æ˜æ˜¾çš„æ ‡é¢˜æ ‡è®°
            2. ä½¿ç”¨æ–‡ä»¶åä½œä¸ºåå¤‡æ ‡é¢˜
            3. æå–é¦–æ®µä½œä¸ºæ ‡é¢˜
        """
        # 1. æŸ¥æ‰¾æ ‡é¢˜æ ‡è®°
        title_patterns = [
            r'^#\s+(.+)$',           # Markdownæ ‡é¢˜
            r'^(.+)\n=+$',           # ä¸‹åˆ’çº¿æ ‡é¢˜
            r'<title>(.+)</title>',  # HTMLæ ‡é¢˜
            r'<h1>(.+)</h1>'         # HTML H1æ ‡é¢˜
        ]
        
        for pattern in title_patterns:
            match = re.search(pattern, content, re.MULTILINE | re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # 2. ä½¿ç”¨æ–‡ä»¶å
        filename = get_basename_from_url(url)
        if filename:
            return os.path.splitext(filename)[0]
        
        # 3. ä½¿ç”¨é¦–æ®µ
        first_line = content.split('\n')[0].strip()
        if first_line:
            return first_line[:100] + ('...' if len(first_line) > 100 else '')
        
        return 'Untitled Document'
    
    def _check_exists(self, url: str) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²è§£æ
        
        æ£€æŸ¥ç­–ç•¥:
            1. åŸºäºURLå“ˆå¸Œå€¼æŸ¥æ‰¾ç¼“å­˜
            2. æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            3. éªŒè¯ç¼“å­˜å®Œæ•´æ€§
        """
        try:
            url_hash = hash_sha256(url)
            return self.db.get(url_hash) is not None
        except KeyNotExistsError:
            return False
        except Exception:
            return False
    
    def _save_to_cache(self, url: str, result: dict):
        """ä¿å­˜è§£æç»“æœåˆ°ç¼“å­˜"""
        try:
            url_hash = hash_sha256(url)
            self.db.put(url_hash, json.dumps(result, ensure_ascii=False))
        except Exception as e:
            logger.warning(f"Failed to cache result for {url}: {e}")
    
    def _load_from_cache(self, url: str) -> dict:
        """ä»ç¼“å­˜åŠ è½½è§£æç»“æœ"""
        try:
            url_hash = hash_sha256(url)
            cached_data = self.db.get(url_hash)
            return json.loads(cached_data)
        except Exception as e:
            logger.error(f"Failed to load from cache for {url}: {e}")
            raise DocParserError(f"Cache loading failed: {e}")
```

### 4. Retrieval - RAGæ£€ç´¢å·¥å…·

```python
@register_tool('retrieval')
class Retrieval(BaseTool):
    """RAGæ£€ç´¢å·¥å…· - æ–‡æ¡£é—®ç­”å’ŒçŸ¥è¯†æ£€ç´¢ç³»ç»Ÿ
    
    æ ¸å¿ƒåŠŸèƒ½:
        1. å¤šæ–‡æ¡£å¹¶è¡Œè§£æå’Œç´¢å¼•
        2. æ··åˆæœç´¢ç­–ç•¥ï¼ˆå…³é”®è¯+è¯­ä¹‰+BM25ï¼‰
        3. æ™ºèƒ½ç›¸å…³æ€§æ’åº
        4. ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç»“æœèšåˆ
        5. å¤šè¯­è¨€æ£€ç´¢æ”¯æŒ
    
    æŠ€æœ¯æ¶æ„:
        - æ–‡æ¡£è§£æ: DocParseré›†æˆ
        - æ£€ç´¢å¼•æ“: å¯é…ç½®çš„æœç´¢ç­–ç•¥
        - æ’åºç®—æ³•: å¤šå› ç´ ç»¼åˆæ’åº
        - ç»“æœèšåˆ: æ™ºèƒ½å»é‡å’Œåˆå¹¶
    
    æ£€ç´¢ç­–ç•¥:
        - KeywordSearch: åŸºäºTF-IDFçš„å…³é”®è¯æœç´¢
        - VectorSearch: åŸºäºembeddingçš„è¯­ä¹‰æœç´¢
        - HybridSearch: æ··åˆæœç´¢ç­–ç•¥
        - FrontPageSearch: é¦–é¡µå†…å®¹æœç´¢
    """
    
    description = f"ä»ç»™å®šæ–‡ä»¶åˆ—è¡¨ä¸­æ£€ç´¢å‡ºå’Œé—®é¢˜ç›¸å…³çš„å†…å®¹ï¼Œæ”¯æŒæ–‡ä»¶ç±»å‹åŒ…æ‹¬ï¼š{' / '.join(PARSER_SUPPORTED_FILE_TYPES)}"
    parameters = {
        'type': 'object',
        'properties': {
            'query': {
                'description': 'åœ¨è¿™é‡Œåˆ—å‡ºå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œç›®çš„æ˜¯æ–¹ä¾¿åœ¨æ–‡æ¡£ä¸­åŒ¹é…åˆ°ç›¸å…³çš„å†…å®¹ï¼Œç”±äºæ–‡æ¡£å¯èƒ½å¤šè¯­è¨€ï¼Œå…³é”®è¯æœ€å¥½ä¸­è‹±æ–‡éƒ½æœ‰ã€‚',
                'type': 'string',
            },
            'files': {
                'description': 'å¾…è§£æçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œæ”¯æŒæœ¬åœ°æ–‡ä»¶è·¯å¾„æˆ–å¯ä¸‹è½½çš„http(s)é“¾æ¥ã€‚',
                'type': 'array',
                'items': {
                    'type': 'string'
                }
            },
        },
        'required': ['query', 'files'],
    }
    
    def __init__(self, cfg: Optional[Dict] = None):
        """Retrievalåˆå§‹åŒ–
        
        åˆå§‹åŒ–ç»„ä»¶:
            1. RAGä¾èµ–æ£€æŸ¥
            2. æ–‡æ¡£è§£æå™¨åˆå§‹åŒ–
            3. æœç´¢å¼•æ“é…ç½®
            4. å‚æ•°é…ç½®åŠ è½½
        """
        super().__init__(cfg)
        
        # 1. ä¾èµ–æ£€æŸ¥
        _check_deps_for_rag()
        
        # 2. é…ç½®å‚æ•°
        self.max_ref_token: int = self.cfg.get('max_ref_token', DEFAULT_MAX_REF_TOKEN)
        self.parser_page_size: int = self.cfg.get('parser_page_size', DEFAULT_PARSER_PAGE_SIZE)
        
        # 3. æ–‡æ¡£è§£æå™¨
        self.doc_parse = DocParser({
            'max_ref_token': self.max_ref_token,
            'parser_page_size': self.parser_page_size
        })
        
        # 4. æœç´¢å¼•æ“é…ç½®
        self.rag_searchers = self.cfg.get('rag_searchers', DEFAULT_RAG_SEARCHERS)
        
        if len(self.rag_searchers) == 1:
            # å•ä¸€æœç´¢ç­–ç•¥
            searcher_name = self.rag_searchers[0]
            self.search = TOOL_REGISTRY[searcher_name]({'max_ref_token': self.max_ref_token})
        else:
            # æ··åˆæœç´¢ç­–ç•¥
            from qwen_agent.tools.search_tools.hybrid_search import HybridSearch
            self.search = HybridSearch({
                'max_ref_token': self.max_ref_token,
                'rag_searchers': self.rag_searchers
            })
    
    def call(self, params: Union[str, dict], **kwargs) -> list:
        """RAGæ£€ç´¢ä¸»å…¥å£
        
        æ£€ç´¢æµç¨‹:
            1. å‚æ•°éªŒè¯å’Œé¢„å¤„ç†
            2. æ–‡æ¡£å¹¶è¡Œè§£æå’Œç´¢å¼•
            3. æŸ¥è¯¢é¢„å¤„ç†å’Œæ‰©å±•
            4. å¤šç­–ç•¥æ£€ç´¢æ‰§è¡Œ
            5. ç»“æœæ’åºå’Œèšåˆ
            6. ä¸Šä¸‹æ–‡ç›¸å…³æ€§ä¼˜åŒ–
        
        å‚æ•°è¯´æ˜:
            params: åŒ…å«queryå’Œfilesçš„æ£€ç´¢å‚æ•°
        
        è¿”å›å€¼:
            list: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
        """
        # 1. å‚æ•°éªŒè¯
        params = self._verify_json_format_args(params)
        query = params['query']
        files = params.get('files', [])
        
        if isinstance(files, str):
            files = json5.loads(files)
        
        if not files:
            return []
        
        # 2. æ–‡æ¡£è§£æé˜¶æ®µ
        logger.info(f"å¼€å§‹è§£æ {len(files)} ä¸ªæ–‡æ¡£")
        records = []
        
        for file_path in files:
            try:
                # è§£æå•ä¸ªæ–‡æ¡£
                parsed_record = self.doc_parse.call(params={'url': file_path}, **kwargs)
                
                if parsed_record and parsed_record.get('raw'):
                    # è½¬æ¢ä¸ºRecordå¯¹è±¡
                    chunks = []
                    for chunk_data in parsed_record['raw']:
                        chunk = Chunk(
                            content=chunk_data['content'],
                            metadata=chunk_data['metadata'],
                            token=chunk_data['token']
                        )
                        chunks.append(chunk)
                    
                    record = Record(
                        url=parsed_record['url'],
                        raw=chunks,
                        title=parsed_record['title']
                    )
                    records.append(record)
                    
                logger.info(f"æ–‡æ¡£è§£æå®Œæˆ: {file_path} ({len(chunks)} ä¸ªåˆ†å—)")
                
            except Exception as e:
                logger.error(f"æ–‡æ¡£è§£æå¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
                continue
        
        if not records:
            return []
        
        # 3. æ£€ç´¢é˜¶æ®µ
        logger.info(f"å¼€å§‹æ£€ç´¢ï¼ŒæŸ¥è¯¢: {query}")
        
        try:
            # æ‰§è¡Œæ£€ç´¢
            search_results = self.search.call(
                params={'query': query, 'files': records},
                **kwargs
            )
            
            logger.info(f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³ç»“æœ")
            return search_results
            
        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
```

## ğŸ” æœç´¢å·¥å…·å­ç³»ç»Ÿ

### æœç´¢ç­–ç•¥æ¶æ„å›¾

```mermaid
graph TB
    subgraph "æœç´¢ç­–ç•¥å±‚"
        A[HybridSearch] --> B[æ··åˆæœç´¢ç­–ç•¥]
        C[KeywordSearch] --> D[å…³é”®è¯æœç´¢]
        E[VectorSearch] --> F[è¯­ä¹‰å‘é‡æœç´¢]
        G[FrontPageSearch] --> H[é¦–é¡µå†…å®¹æœç´¢]
    end
    
    subgraph "ç®—æ³•å®ç°å±‚"
        D --> I[TF-IDFç®—æ³•]
        D --> J[BM25ç®—æ³•]
        F --> K[Embeddingæ¨¡å‹]
        F --> L[ç›¸ä¼¼åº¦è®¡ç®—]
        B --> M[å¤šç­–ç•¥èåˆ]
        B --> N[ç»“æœæ’åº]
    end
    
    subgraph "æ•°æ®å¤„ç†å±‚"
        I --> O[æ–‡æœ¬é¢„å¤„ç†]
        J --> O
        K --> P[å‘é‡åŒ–å¤„ç†]
        L --> P
        O --> Q[åˆ†è¯å’Œæ¸…ç†]
        P --> R[å‘é‡å­˜å‚¨]
    end
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style M fill:#e8f5e8
    style N fill:#fff3e0
```

### HybridSearch - æ··åˆæœç´¢å®ç°

```python
class HybridSearch(BaseTool):
    """æ··åˆæœç´¢ç­–ç•¥ - ç»“åˆå¤šç§æœç´¢ç®—æ³•çš„ç»¼åˆæ£€ç´¢
    
    æ ¸å¿ƒæ€æƒ³:
        é€šè¿‡ç»„åˆä¸åŒçš„æœç´¢ç­–ç•¥ï¼Œå¼¥è¡¥å•ä¸€ç®—æ³•çš„ä¸è¶³ï¼Œ
        æä¾›æ›´å‡†ç¡®ã€æ›´å…¨é¢çš„æ£€ç´¢ç»“æœ
    
    æœç´¢ç­–ç•¥ç»„åˆ:
        1. KeywordSearch: ç²¾ç¡®åŒ¹é…å’Œå…³é”®è¯é¢‘ç‡
        2. VectorSearch: è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§
        3. BM25Search: æ”¹è¿›çš„TF-IDFç®—æ³•
        4. è‡ªå®šä¹‰åŠ æƒå’Œæ’åºæœºåˆ¶
    
    èåˆç®—æ³•:
        - å¤šç­–ç•¥å¹¶è¡Œæ‰§è¡Œ
        - ç»“æœå½’ä¸€åŒ–å’Œæƒé‡åˆ†é…
        - ç»¼åˆæ’åºå’Œå»é‡
        - å¤šæ ·æ€§ä¿è¯æœºåˆ¶
    """
    
    def __init__(self, cfg: Optional[Dict] = None):
        """æ··åˆæœç´¢åˆå§‹åŒ–"""
        super().__init__(cfg)
        self.rag_searchers = self.cfg.get('rag_searchers', ['keyword_search', 'vector_search'])
        self.search_weights = self.cfg.get('search_weights', {})  # æœç´¢ç­–ç•¥æƒé‡
        
        # åˆå§‹åŒ–å„ä¸ªæœç´¢å™¨
        self.searchers = {}
        for searcher_name in self.rag_searchers:
            if searcher_name in TOOL_REGISTRY:
                self.searchers[searcher_name] = TOOL_REGISTRY[searcher_name](self.cfg)
    
    def call(self, params: Union[str, dict], **kwargs) -> list:
        """æ‰§è¡Œæ··åˆæœç´¢
        
        æœç´¢æµç¨‹:
            1. å¹¶è¡Œæ‰§è¡Œå„ä¸ªæœç´¢ç­–ç•¥
            2. ç»“æœæ”¶é›†å’Œé¢„å¤„ç†
            3. ç›¸å…³æ€§å¾—åˆ†è®¡ç®—
            4. å¤šç­–ç•¥ç»“æœèåˆ
            5. ç»¼åˆæ’åºå’Œç­›é€‰
        """
        params = self._verify_json_format_args(params)
        query = params['query']
        files = params['files']
        
        # 1. å¹¶è¡Œæ‰§è¡Œæœç´¢ç­–ç•¥
        all_results = []
        strategy_results = {}
        
        for strategy_name, searcher in self.searchers.items():
            try:
                strategy_result = searcher.call(params, **kwargs)
                strategy_results[strategy_name] = strategy_result
                
                # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ ç­–ç•¥æ¥æºæ ‡è®°
                for result in strategy_result:
                    result['search_strategy'] = strategy_name
                    all_results.append(result)
                    
            except Exception as e:
                logger.warning(f"æœç´¢ç­–ç•¥ {strategy_name} æ‰§è¡Œå¤±è´¥: {str(e)}")
                continue
        
        # 2. ç»“æœå»é‡å’Œå½’ä¸€åŒ–
        unique_results = self._deduplicate_results(all_results)
        
        # 3. å¤šç­–ç•¥èåˆæ’åº
        final_results = self._hybrid_ranking(unique_results, query, strategy_results)
        
        # 4. ç»“æœç­›é€‰å’Œæˆªæ–­
        max_results = self.cfg.get('max_results', 10)
        return final_results[:max_results]
    
    def _hybrid_ranking(self, results: list, query: str, strategy_results: dict) -> list:
        """æ··åˆæ’åºç®—æ³•
        
        æ’åºå› ç´ :
            1. å¤šç­–ç•¥ä¸€è‡´æ€§å¾—åˆ†
            2. å„ç­–ç•¥çš„ç½®ä¿¡åº¦æƒé‡
            3. å†…å®¹è´¨é‡å’Œå®Œæ•´æ€§
            4. æŸ¥è¯¢ç›¸å…³æ€§å¾—åˆ†
        """
        # è®¡ç®—æ¯ä¸ªç»“æœçš„ç»¼åˆå¾—åˆ†
        for result in results:
            scores = []
            
            # å„ç­–ç•¥å¾—åˆ†æ”¶é›†
            for strategy_name in self.searchers.keys():
                if strategy_name in strategy_results:
                    strategy_score = self._get_strategy_score(result, strategy_results[strategy_name])
                    weight = self.search_weights.get(strategy_name, 1.0)
                    scores.append(strategy_score * weight)
            
            # ç»¼åˆå¾—åˆ†è®¡ç®—
            if scores:
                result['hybrid_score'] = sum(scores) / len(scores)
                result['strategy_consensus'] = len([s for s in scores if s > 0.5]) / len(scores)
            else:
                result['hybrid_score'] = 0.0
                result['strategy_consensus'] = 0.0
        
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        return sorted(results, key=lambda x: x['hybrid_score'], reverse=True)
```

## ğŸ“Š å·¥å…·ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–

### 1. ç¼“å­˜å’ŒæŒä¹…åŒ–ç­–ç•¥

```python
class ToolCache:
    """å·¥å…·ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = None):
        self.cache_dir = cache_dir or os.path.join(DEFAULT_WORKSPACE, 'cache')
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # ä¸åŒçº§åˆ«çš„ç¼“å­˜
        self.memory_cache = {}      # å†…å­˜ç¼“å­˜
        self.disk_cache = None      # ç£ç›˜ç¼“å­˜
        self._init_disk_cache()
    
    def _init_disk_cache(self):
        """åˆå§‹åŒ–ç£ç›˜ç¼“å­˜"""
        try:
            import diskcache
            self.disk_cache = diskcache.Cache(self.cache_dir)
        except ImportError:
            logger.warning("diskcache not available, using memory cache only")
    
    def get(self, key: str) -> Any:
        """è·å–ç¼“å­˜å€¼"""
        # 1. å†…å­˜ç¼“å­˜æŸ¥æ‰¾
        if key in self.memory_cache:
            return self.memory_cache[key]
        
        # 2. ç£ç›˜ç¼“å­˜æŸ¥æ‰¾
        if self.disk_cache:
            value = self.disk_cache.get(key)
            if value is not None:
                # æå‡åˆ°å†…å­˜ç¼“å­˜
                self.memory_cache[key] = value
                return value
        
        return None
    
    def set(self, key: str, value: Any, expire: int = 3600):
        """è®¾ç½®ç¼“å­˜å€¼"""
        # å†…å­˜ç¼“å­˜
        self.memory_cache[key] = value
        
        # ç£ç›˜ç¼“å­˜
        if self.disk_cache:
            self.disk_cache.set(key, value, expire=expire)
```

### 2. å¹¶å‘å¤„ç†ä¼˜åŒ–

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

class ParallelToolExecutor:
    """å¹¶è¡Œå·¥å…·æ‰§è¡Œå™¨"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    def execute_parallel(self, tool_calls: List[Tuple[BaseTool, dict]]) -> List[Any]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå·¥å…·è°ƒç”¨
        
        ä¼˜åŠ¿:
            1. æé«˜æ‰§è¡Œæ•ˆç‡
            2. å‡å°‘ç­‰å¾…æ—¶é—´
            3. èµ„æºåˆ©ç”¨æœ€å¤§åŒ–
        """
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = []
        for tool, params in tool_calls:
            future = self.executor.submit(tool.call, params)
            futures.append(future)
        
        # æ”¶é›†ç»“æœ
        results = []
        for future in as_completed(futures):
            try:
                result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                results.append(result)
            except Exception as e:
                logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}")
                results.append(f"æ‰§è¡Œé”™è¯¯: {str(e)}")
        
        return results
    
    async def execute_async(self, tool_calls: List[Tuple[BaseTool, dict]]) -> List[Any]:
        """å¼‚æ­¥æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        loop = asyncio.get_event_loop()
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = []
        for tool, params in tool_calls:
            task = loop.run_in_executor(self.executor, tool.call, params)
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                processed_results.append(f"æ‰§è¡Œé”™è¯¯: {str(result)}")
            else:
                processed_results.append(result)
        
        return processed_results
```

### 3. èµ„æºç®¡ç†å’Œæ¸…ç†

```python
class ResourceManager:
    """å·¥å…·èµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.active_resources = {}
        self.cleanup_handlers = []
        
        # æ³¨å†Œæ¸…ç†å¤„ç†å™¨
        atexit.register(self.cleanup_all)
    
    def register_resource(self, resource_id: str, resource: Any, cleanup_func: callable = None):
        """æ³¨å†Œèµ„æº"""
        self.active_resources[resource_id] = {
            'resource': resource,
            'cleanup_func': cleanup_func,
            'created_at': time.time()
        }
    
    def cleanup_resource(self, resource_id: str):
        """æ¸…ç†æŒ‡å®šèµ„æº"""
        if resource_id in self.active_resources:
            resource_info = self.active_resources[resource_id]
            
            if resource_info['cleanup_func']:
                try:
                    resource_info['cleanup_func'](resource_info['resource'])
                except Exception as e:
                    logger.warning(f"èµ„æºæ¸…ç†å¤±è´¥ {resource_id}: {str(e)}")
            
            del self.active_resources[resource_id]
    
    def cleanup_all(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        for resource_id in list(self.active_resources.keys()):
            self.cleanup_resource(resource_id)
    
    def cleanup_expired(self, max_age: int = 3600):
        """æ¸…ç†è¿‡æœŸèµ„æº"""
        current_time = time.time()
        expired_resources = []
        
        for resource_id, resource_info in self.active_resources.items():
            if current_time - resource_info['created_at'] > max_age:
                expired_resources.append(resource_id)
        
        for resource_id in expired_resources:
            self.cleanup_resource(resource_id)
```

## ğŸ¯ Toolsæ¨¡å—æ€»ç»“

### è®¾è®¡ä¼˜åŠ¿

1. **ç»Ÿä¸€æ¥å£**: BaseToolæä¾›ç»Ÿä¸€çš„å·¥å…·æŠ½è±¡ï¼Œç®€åŒ–å·¥å…·å¼€å‘å’Œä½¿ç”¨
2. **æ’ä»¶åŒ–æ¶æ„**: é€šè¿‡æ³¨å†Œæœºåˆ¶æ”¯æŒåŠ¨æ€å·¥å…·åŠ è½½å’Œç®¡ç†
3. **ä¸°å¯Œç”Ÿæ€**: å†…ç½®å¤šç§å®ç”¨å·¥å…·ï¼Œè¦†ç›–å¸¸è§ä½¿ç”¨åœºæ™¯
4. **å‚æ•°éªŒè¯**: å®Œå–„çš„å‚æ•°éªŒè¯å’Œé”™è¯¯å¤„ç†æœºåˆ¶
5. **ç¼“å­˜ä¼˜åŒ–**: æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼Œæå‡å·¥å…·æ‰§è¡Œæ•ˆç‡
6. **å¹¶å‘æ”¯æŒ**: æ”¯æŒå¹¶è¡Œå·¥å…·è°ƒç”¨ï¼Œæé«˜æ•´ä½“æ€§èƒ½

### æ ¸å¿ƒç‰¹æ€§

1. **å¤šæ ¼å¼æ”¯æŒ**: æ”¯æŒ10+ç§æ–‡æ¡£æ ¼å¼è§£æ
2. **æ™ºèƒ½æ£€ç´¢**: RAGç³»ç»Ÿæä¾›ç²¾å‡†çš„æ–‡æ¡£é—®ç­”èƒ½åŠ›
3. **ä»£ç æ‰§è¡Œ**: å®‰å…¨çš„Pythonä»£ç æ‰§è¡Œç¯å¢ƒ
4. **ç½‘ç»œæœç´¢**: å®æ—¶ç½‘ç»œä¿¡æ¯è·å–å’Œå¤„ç†
5. **å›¾åƒå¤„ç†**: å›¾åƒç”Ÿæˆã€æœç´¢ã€ç¼©æ”¾ç­‰åŠŸèƒ½
6. **æ‰©å±•æœºåˆ¶**: æ”¯æŒMCPåè®®å’Œè‡ªå®šä¹‰å·¥å…·å¼€å‘

### æ‰©å±•å»ºè®®

1. **å®‰å…¨åŠ å›º**: å¢å¼ºä»£ç æ‰§è¡Œæ²™ç®±çš„å®‰å…¨æ€§
2. **æ€§èƒ½ä¼˜åŒ–**: æ”¯æŒæ›´å¤šçš„å¹¶è¡Œå¤„ç†å’Œç¼“å­˜ç­–ç•¥
3. **å·¥å…·å¸‚åœº**: å»ºç«‹å·¥å…·æ’ä»¶å¸‚åœºå’Œåˆ†äº«æœºåˆ¶
4. **ç›‘æ§å‘Šè­¦**: å¢åŠ å·¥å…·æ‰§è¡Œçš„ç›‘æ§å’Œå‘Šè­¦åŠŸèƒ½
5. **ç‰ˆæœ¬ç®¡ç†**: æ”¯æŒå·¥å…·ç‰ˆæœ¬ç®¡ç†å’Œå…¼å®¹æ€§æ£€æŸ¥

---

*æœ¬Toolså·¥å…·ç³»ç»Ÿåˆ†ææ–‡æ¡£åŸºäºQwen-Agent v0.0.30ç‰ˆæœ¬ï¼Œè¯¦ç»†æè¿°äº†å·¥å…·ç³»ç»Ÿçš„æ¶æ„è®¾è®¡å’Œå®ç°åŸç†ã€‚*
