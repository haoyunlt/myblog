---
title: "LangChain-10-TextSplitters"
date: 2025-10-04T21:26:31+08:00
draft: false
tags:
  - LangChain
  - æ¶æ„è®¾è®¡
  - æ¦‚è§ˆ
  - æºç åˆ†æ
categories:
  - LangChain
  - AIæ¡†æ¶
  - Python
series: "langchain-source-analysis"
description: "LangChain æºç å‰–æ - 10-TextSplitters"
author: "æºç åˆ†æ"
weight: 500
ShowToc: true
TocOpen: true

---

# LangChain-10-TextSplitters

## æ¨¡å—æ¦‚è§ˆ

## æ¨¡å—åŸºæœ¬ä¿¡æ¯

**æ¨¡å—åç§°**: langchain-text-splitters
**æ¨¡å—è·¯å¾„**: `libs/text-splitters/langchain_text_splitters/`
**æ ¸å¿ƒèŒè´£**: å°†é•¿æ–‡æ¡£åˆ†å‰²æˆé€‚åˆ LLM å¤„ç†çš„å°å—ï¼ˆchunksï¼‰ï¼Œæ˜¯ RAG ç³»ç»Ÿçš„å…³é”®é¢„å¤„ç†æ­¥éª¤

## 1. æ¨¡å—èŒè´£

### 1.1 æ ¸å¿ƒèŒè´£

Text Splitters æ¨¡å—è´Ÿè´£æ™ºèƒ½åˆ†å‰²æ–‡æ¡£ï¼Œæä¾›ä»¥ä¸‹èƒ½åŠ›ï¼š

1. **æ–‡æ¡£åˆ†å—**: å°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå°å—
2. **ä¸Šä¸‹æ–‡ä¿ç•™**: é€šè¿‡é‡å ä¿ç•™å—é—´ä¸Šä¸‹æ–‡
3. **è¯­ä¹‰å®Œæ•´æ€§**: å°½é‡ä¿æŒå¥å­/æ®µè½å®Œæ•´
4. **å¤šç§ç­–ç•¥**: å­—ç¬¦ã€Tokenã€é€’å½’ã€è¯­ä¹‰ç­‰åˆ†å‰²æ–¹å¼
5. **å…ƒæ•°æ®ç®¡ç†**: ä¿ç•™å¹¶ä¼ é€’æ–‡æ¡£å…ƒæ•°æ®
6. **æ ¼å¼æ„ŸçŸ¥**: é’ˆå¯¹ç‰¹å®šæ ¼å¼ï¼ˆä»£ç ã€Markdownï¼‰ä¼˜åŒ–åˆ†å‰²

### 1.2 æ ¸å¿ƒæ¦‚å¿µ

```
é•¿æ–‡æ¡£ (10,000+ å­—ç¬¦)
  â†“
Text Splitter (åˆ†å‰²ç­–ç•¥)
  â†“
æ–‡æ¡£å—åˆ—è¡¨ (æ¯å— 500-1500 å­—ç¬¦)
  â†“
åµŒå…¥ â†’ å‘é‡å­˜å‚¨ â†’ æ£€ç´¢
```

**å…³é”®å‚æ•°**:

- **chunk_size**: æ¯ä¸ªå—çš„ç›®æ ‡å¤§å°ï¼ˆå­—ç¬¦æ•°æˆ– Token æ•°ï¼‰
- **chunk_overlap**: å—ä¹‹é—´çš„é‡å éƒ¨åˆ†ï¼ˆä¿ç•™ä¸Šä¸‹æ–‡ï¼‰
- **separators**: åˆ†éš”ç¬¦åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼‰
- **length_function**: è®¡ç®—é•¿åº¦çš„å‡½æ•°ï¼ˆå­—ç¬¦æ•°æˆ– Token æ•°ï¼‰

### 1.3 Text Splitter ç±»å‹å¯¹æ¯”

| Splitter ç±»å‹ | åˆ†å‰²å•ä½ | é€‚ç”¨åœºæ™¯ | ä¿æŒå®Œæ•´æ€§ | æ€§èƒ½ |
|--------------|---------|---------|----------|------|
| **CharacterTextSplitter** | å­—ç¬¦ | ç®€å•æ–‡æœ¬ | ä½ | âš¡ æœ€å¿« |
| **RecursiveCharacterTextSplitter** | å­—ç¬¦ï¼ˆé€’å½’ï¼‰ | é€šç”¨åœºæ™¯ | é«˜ | âš¡ å¿« |
| **TokenTextSplitter** | Token | Tokené™åˆ¶åœºæ™¯ | ä¸­ | ğŸ¢ è¾ƒæ…¢ |
| **SentenceTextSplitter** | å¥å­ | ä¿æŒå¥å­å®Œæ•´ | é«˜ | ğŸŒ æ…¢ |
| **MarkdownHeaderTextSplitter** | Markdownæ ‡é¢˜ | Markdownæ–‡æ¡£ | é«˜ | âš¡ å¿« |
| **CodeTextSplitter** | ä»£ç è¯­æ³• | æºä»£ç  | é«˜ | ğŸ¢ è¾ƒæ…¢ |
| **SemanticChunker** | è¯­ä¹‰ç›¸ä¼¼åº¦ | é«˜è´¨é‡å— | æœ€é«˜ | ğŸŒ æœ€æ…¢ |

### 1.4 è¾“å…¥/è¾“å‡º

**è¾“å…¥**:

- **texts**: `list[str]` - æ–‡æœ¬åˆ—è¡¨
- **documents**: `list[Document]` - æ–‡æ¡£åˆ—è¡¨

**è¾“å‡º**:

- `list[Document]` - åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨ï¼ˆåŒ…å« `page_content` å’Œ `metadata`ï¼‰

### 1.5 ä¸Šä¸‹æ¸¸ä¾èµ–

**ä¸Šæ¸¸è°ƒç”¨è€…**:

- Document Loadersï¼ˆåŠ è½½ååˆ†å‰²ï¼‰
- RAG åº”ç”¨ï¼ˆæ–‡æ¡£å…¥åº“å‰ï¼‰

**ä¸‹æ¸¸ä¾èµ–**:

- `langchain_core.documents`: Document ç±»
- `tiktoken` æˆ– `transformers`: Token è®¡æ•°

## 2. æ¨¡å—çº§æ¶æ„å›¾

```mermaid
flowchart TB
    subgraph Base["åŸºç¡€æŠ½è±¡å±‚"]
        BTS[TextSplitter<br/>æ–‡æœ¬åˆ†å‰²å™¨åŸºç±»]
    end

    subgraph Character["å­—ç¬¦çº§åˆ†å‰²"]
        CTS[CharacterTextSplitter<br/>ç®€å•å­—ç¬¦åˆ†å‰²]
        RCTS[RecursiveCharacterTextSplitter<br/>é€’å½’å­—ç¬¦åˆ†å‰²]
    end

    subgraph Token["Tokençº§åˆ†å‰²"]
        TTS[TokenTextSplitter<br/>Tokenåˆ†å‰²]
        SPACY[SpacyTextSplitter<br/>Spacyåˆ†å‰²]
    end

    subgraph Semantic["è¯­ä¹‰åˆ†å‰²"]
        SENT[SentenceTextSplitter<br/>å¥å­åˆ†å‰²]
        SEM[SemanticChunker<br/>è¯­ä¹‰å—åˆ†å‰²]
    end

    subgraph Format["æ ¼å¼æ„ŸçŸ¥åˆ†å‰²"]
        MD[MarkdownHeaderTextSplitter<br/>Markdownåˆ†å‰²]
        CODE[CodeTextSplitter<br/>ä»£ç åˆ†å‰²]
        HTML[HTMLHeaderTextSplitter<br/>HTMLåˆ†å‰²]
        LATEX[LatexTextSplitter<br/>LaTeXåˆ†å‰²]
    end

    subgraph Process["åˆ†å‰²æµç¨‹"]
        SPLIT[split_text<br/>åˆ†å‰²æ–‡æœ¬]
        CREATE[create_documents<br/>åˆ›å»ºæ–‡æ¡£]
        MERGE[merge_splits<br/>åˆå¹¶å—]
    end

    BTS --> CTS
    BTS --> RCTS
    BTS --> TTS
    BTS --> SPACY
    BTS --> SENT
    BTS --> SEM
    BTS --> MD
    BTS --> CODE
    BTS --> HTML
    BTS --> LATEX

    BTS --> SPLIT
    BTS --> CREATE
    BTS --> MERGE

    style Base fill:#e1f5ff
    style Character fill:#fff4e1
    style Token fill:#e8f5e9
    style Semantic fill:#f3e5f5
    style Format fill:#fff3e0
    style Process fill:#ffe4e1
```

### æ¶æ„å›¾è¯¦ç»†è¯´æ˜

**1. åŸºç¡€æŠ½è±¡å±‚**

- **TextSplitter**: æ‰€æœ‰åˆ†å‰²å™¨çš„åŸºç±»

  ```python
  class TextSplitter(ABC):
      chunk_size: int = 4000  # å—å¤§å°
      chunk_overlap: int = 200  # é‡å å¤§å°
      length_function: Callable[[str], int] = len  # é•¿åº¦å‡½æ•°
      keep_separator: bool = False  # æ˜¯å¦ä¿ç•™åˆ†éš”ç¬¦
      add_start_index: bool = False  # æ˜¯å¦æ·»åŠ èµ·å§‹ç´¢å¼•

      @abstractmethod
      def split_text(self, text: str) -> list[str]:
          """åˆ†å‰²æ–‡æœ¬ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨"""

      def create_documents(
          self,
          texts: list[str],
          metadatas: Optional[list[dict]] = None
      ) -> list[Document]:
          """åˆ›å»ºæ–‡æ¡£å¯¹è±¡"""

      def split_documents(self, documents: list[Document]) -> list[Document]:
          """åˆ†å‰²æ–‡æ¡£åˆ—è¡¨"""
```

**2. å­—ç¬¦çº§åˆ†å‰²**

- **CharacterTextSplitter**: ç®€å•å­—ç¬¦åˆ†å‰²
  - æŒ‰å•ä¸ªåˆ†éš”ç¬¦åˆ†å‰²
  - æœ€ç®€å•ä½†å¯èƒ½ç ´åè¯­ä¹‰

  ```python
  splitter = CharacterTextSplitter(
      separator="\n\n",  # æŒ‰æ®µè½åˆ†å‰²
      chunk_size=1000,
      chunk_overlap=200
  )
```

- **RecursiveCharacterTextSplitter**: é€’å½’åˆ†å‰²ï¼ˆæ¨èï¼‰
  - æŒ‰ä¼˜å…ˆçº§å°è¯•å¤šä¸ªåˆ†éš”ç¬¦
  - é»˜è®¤é¡ºåºï¼š`\n\n` â†’ `\n` â†’ ` ` â†’ ``
  - æœ€å¤§åŒ–ä¿æŒæ®µè½å’Œå¥å­å®Œæ•´

  ```python
  splitter = RecursiveCharacterTextSplitter(
      chunk_size=1000,
      chunk_overlap=200,
      separators=["\n\n", "\n", " ", ""]
  )
```

**3. Token çº§åˆ†å‰²**

- **TokenTextSplitter**: åŸºäº Token åˆ†å‰²
  - ä½¿ç”¨ `tiktoken` è®¡ç®— Token
  - ç²¾ç¡®æ§åˆ¶ LLM Token é™åˆ¶

  ```python
  from langchain.text_splitter import TokenTextSplitter

  splitter = TokenTextSplitter(
      chunk_size=512,  # Token æ•°é‡
      chunk_overlap=50,
      encoding_name="cl100k_base"  # GPT-4 ç¼–ç 
  )
```

- **SpacyTextSplitter**: ä½¿ç”¨ Spacy NLP
  - åŸºäº Spacy çš„å¥å­åˆ†å‰²
  - ä¿æŒå¥å­å®Œæ•´æ€§

**4. è¯­ä¹‰åˆ†å‰²**

- **SentenceTextSplitter**: å¥å­çº§åˆ†å‰²
  - ä¸ä¼šåœ¨å¥å­ä¸­é—´åˆ‡æ–­
  - ä¿æŒè¯­ä¹‰å®Œæ•´

- **SemanticChunker**: è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å‰²
  - ä½¿ç”¨åµŒå…¥æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦
  - æ ¹æ®è¯­ä¹‰è¾¹ç•Œåˆ†å‰²
  - æœ€é«˜è´¨é‡ä½†æœ€æ…¢

**5. æ ¼å¼æ„ŸçŸ¥åˆ†å‰²**

- **MarkdownHeaderTextSplitter**: Markdown åˆ†å‰²
  - æŒ‰æ ‡é¢˜å±‚çº§åˆ†å‰²
  - ä¿ç•™æ ‡é¢˜å±‚çº§ä¿¡æ¯

  ```python
  from langchain.text_splitter import MarkdownHeaderTextSplitter

  headers_to_split_on = [
      ("#", "Header 1"),
      ("##", "Header 2"),
      ("###", "Header 3"),
  ]

  splitter = MarkdownHeaderTextSplitter(
      headers_to_split_on=headers_to_split_on
  )
```

- **CodeTextSplitter**: ä»£ç åˆ†å‰²
  - æŒ‰ç¼–ç¨‹è¯­è¨€è¯­æ³•åˆ†å‰²
  - æ”¯æŒ Pythonã€JavaScriptã€Java ç­‰
  - ä¿æŒå‡½æ•°/ç±»å®Œæ•´æ€§

  ```python
  from langchain.text_splitter import RecursiveCharacterTextSplitter

  python_splitter = RecursiveCharacterTextSplitter.from_language(
      language=Language.PYTHON,
      chunk_size=500,
      chunk_overlap=50
  )
```

- **HTMLHeaderTextSplitter**: HTML åˆ†å‰²
  - æŒ‰ HTML æ ‡ç­¾åˆ†å‰²
  - ä¿ç•™ç»“æ„ä¿¡æ¯

- **LatexTextSplitter**: LaTeX åˆ†å‰²
  - è¯†åˆ« LaTeX ç»“æ„
  - ä¿æŒå…¬å¼å®Œæ•´

**6. åˆ†å‰²æµç¨‹**

- **split_text**: æ ¸å¿ƒåˆ†å‰²é€»è¾‘
  - é€’å½’å°è¯•åˆ†éš”ç¬¦
  - åˆå¹¶å°å—
  - æ§åˆ¶å—å¤§å°

- **create_documents**: åˆ›å»º Document å¯¹è±¡
  - æ·»åŠ å…ƒæ•°æ®
  - æ·»åŠ èµ·å§‹ç´¢å¼•ï¼ˆå¯é€‰ï¼‰

- **merge_splits**: åˆå¹¶å—
  - åˆå¹¶è¿‡å°çš„å—
  - ä¿æŒé‡å 

## 3. æ ¸å¿ƒ API è¯¦è§£

### 3.1 RecursiveCharacterTextSplitter - æ¨èä½¿ç”¨

**æ ¸å¿ƒä»£ç **:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

class RecursiveCharacterTextSplitter(TextSplitter):
    separators: list[str] = ["\n\n", "\n", " ", ""]

    def split_text(self, text: str) -> list[str]:
        """
        é€’å½’åˆ†å‰²æ–‡æœ¬

        1. å°è¯•ç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦åˆ†å‰²
        2. æ£€æŸ¥æ¯ä¸ªå—å¤§å°
        3. å¦‚æœå—å¤ªå¤§ï¼Œé€’å½’ä½¿ç”¨ä¸‹ä¸€ä¸ªåˆ†éš”ç¬¦
        4. åˆå¹¶å°å—
        """
        final_chunks = []
        separator = self.separators[-1]
        new_separators = []

        # æ‰¾åˆ°æœ‰æ•ˆçš„åˆ†éš”ç¬¦
        for i, _s in enumerate(self.separators):
            if _s == "":
                separator = _s
                break
            if _s in text:
                separator = _s
                new_separators = self.separators[i + 1:]
                break

        # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²
        splits = text.split(separator)

        # å¤„ç†æ¯ä¸ªåˆ†å‰²
        good_splits = []
        for s in splits:
            if self._length_function(s) < self.chunk_size:
                good_splits.append(s)
            else:
                # é€’å½’åˆ†å‰²å¤§å—
                if good_splits:
                    merged = self._merge_splits(good_splits, separator)
                    final_chunks.extend(merged)
                    good_splits = []

                # ä½¿ç”¨ä¸‹ä¸€ä¸ªåˆ†éš”ç¬¦
                if new_separators:
                    other_splits = self._split_text(s, new_separators)
                    final_chunks.extend(other_splits)
                else:
                    # å¼ºåˆ¶åˆ†å‰²
                    final_chunks.append(s)

        # åˆå¹¶å‰©ä½™çš„å°å—
        if good_splits:
            merged = self._merge_splits(good_splits, separator)
            final_chunks.extend(merged)

        return final_chunks

    def _merge_splits(
        self,
        splits: list[str],
        separator: str
    ) -> list[str]:
        """
        åˆå¹¶å°å—å¹¶ä¿æŒé‡å 
        """
        docs = []
        current_doc = []
        total = 0

        for d in splits:
            _len = self._length_function(d)
            if total + _len >= self.chunk_size:
                if total > self.chunk_size:
                    # è­¦å‘Šï¼šå—è¿‡å¤§
                    pass
                if len(current_doc) > 0:
                    doc = separator.join(current_doc)
                    docs.append(doc)

                    # ä¿æŒé‡å 
                    while total > self.chunk_overlap or (
                        total + _len > self.chunk_size and total > 0
                    ):
                        total -= self._length_function(current_doc[0])
                        current_doc = current_doc[1:]

            current_doc.append(d)
            total += _len

        # æ·»åŠ æœ€åä¸€ä¸ªæ–‡æ¡£
        if current_doc:
            doc = separator.join(current_doc)
            docs.append(doc)

        return docs

```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åŸºç¡€ç”¨æ³•
text = """
# Introduction

LangChain is a framework for building applications with large language models.

## Features

- LCEL (LangChain Expression Language)
- Agents and Tools
- Memory Management
- RAG (Retrieval Augmented Generation)

## Getting Started

First, install LangChain:
```bash

pip install langchain

```

Then import and use:
```python

from langchain import OpenAI
llm = OpenAI()

```
"""

splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50,
    length_function=len,
    is_separator_regex=False
)

chunks = splitter.split_text(text)
print(f"åˆ†å‰²æˆ {len(chunks)} ä¸ªå—")

for i, chunk in enumerate(chunks):
    print(f"\nå— {i+1} ({len(chunk)} å­—ç¬¦):")
    print(chunk[:100] + "...")

# ä»æ–‡æ¡£åˆ†å‰²
from langchain_core.documents import Document

documents = [
    Document(
        page_content=text,
        metadata={"source": "langchain_intro.md"}
    )
]

split_docs = splitter.split_documents(documents)
print(f"\nåˆ†å‰²æˆ {len(split_docs)} ä¸ªæ–‡æ¡£")

for doc in split_docs:
    print(f"Metadata: {doc.metadata}")
    print(f"Content: {doc.page_content[:100]}...")
```

### 3.2 TokenTextSplitter - Token ç²¾ç¡®æ§åˆ¶

```python
from langchain.text_splitter import TokenTextSplitter

# åŸºäº Token åˆ†å‰²
splitter = TokenTextSplitter(
    chunk_size=512,  # Token æ•°é‡
    chunk_overlap=50,
    encoding_name="cl100k_base"  # GPT-4 ç¼–ç 
)

text = "..." * 10000  # é•¿æ–‡æœ¬

chunks = splitter.split_text(text)

# éªŒè¯ Token æ•°é‡
import tiktoken

enc = tiktoken.get_encoding("cl100k_base")
for i, chunk in enumerate(chunks):
    token_count = len(enc.encode(chunk))
    print(f"å— {i+1}: {token_count} tokens")
    assert token_count <= 512  # ç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
```

### 3.3 MarkdownHeaderTextSplitter - Markdown ç»“æ„åŒ–åˆ†å‰²

```python
from langchain.text_splitter import MarkdownHeaderTextSplitter

markdown_text = """
# Main Title

This is the introduction.

## Section 1

Content of section 1.

### Subsection 1.1

Details of subsection 1.1.

### Subsection 1.2

Details of subsection 1.2.

## Section 2

Content of section 2.
"""

# å®šä¹‰è¦åˆ†å‰²çš„æ ‡é¢˜å±‚çº§
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)

docs = splitter.split_text(markdown_text)

for doc in docs:
    print(f"Content: {doc.page_content}")
    print(f"Metadata: {doc.metadata}\n")
    # Metadata åŒ…å«æ ‡é¢˜å±‚çº§:
    # {"Header 1": "Main Title", "Header 2": "Section 1", "Header 3": "Subsection 1.1"}
```

### 3.4 CodeTextSplitter - ä»£ç åˆ†å‰²

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter, Language

# Python ä»£ç åˆ†å‰²
python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=500,
    chunk_overlap=50
)

python_code = """
def hello_world():
    \"\"\"æ‰“å° Hello World\"\"\"
    print("Hello World")

class Calculator:
    \"\"\"ç®€å•è®¡ç®—å™¨\"\"\"

    def add(self, a, b):
        return a + b

    def subtract(self, a, b):
        return a - b

if __name__ == "__main__":
    calc = Calculator()
    print(calc.add(1, 2))
"""

chunks = python_splitter.split_text(python_code)

# ä»£ç åˆ†å‰²å™¨ä¼šå°è¯•ä¿æŒå‡½æ•°/ç±»å®Œæ•´

# JavaScript ä»£ç åˆ†å‰²
js_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.JS,
    chunk_size=500,
    chunk_overlap=50
)

# æ”¯æŒçš„è¯­è¨€ï¼š
# Language.PYTHON, Language.JS, Language.JAVA, Language.CPP,
# Language.GO, Language.RUST, Language.MARKDOWN, Language.HTML, etc.
```

### 3.5 SemanticChunker - è¯­ä¹‰åˆ†å‰²ï¼ˆé«˜è´¨é‡ï¼‰

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å‰²
embeddings = OpenAIEmbeddings()

semantic_chunker = SemanticChunker(
    embeddings=embeddings,
    breakpoint_threshold_type="percentile",  # æˆ– "standard_deviation", "interquartile"
    breakpoint_threshold_amount=95  # ç™¾åˆ†ä½æ•°
)

text = """
LangChain is a framework for building LLM applications.
It provides tools for prompts, chains, and agents.

RAG is a technique that combines retrieval and generation.
It retrieves relevant documents and uses them to generate answers.

Vector stores are used to store document embeddings.
Popular options include FAISS, Chroma, and Pinecone.
"""

chunks = semantic_chunker.create_documents([text])

# SemanticChunker ä¼šåœ¨è¯­ä¹‰è¾¹ç•Œå¤„åˆ†å‰²
# ä¾‹å¦‚ï¼Œå°† LangChain ç›¸å…³å†…å®¹åˆ†ä¸ºä¸€å—ï¼ŒRAG ç›¸å…³å†…å®¹åˆ†ä¸ºå¦ä¸€å—
```

### 3.6 è‡ªå®šä¹‰ Text Splitter

```python
from langchain.text_splitter import TextSplitter

class CustomSentenceSplitter(TextSplitter):
    """è‡ªå®šä¹‰å¥å­åˆ†å‰²å™¨"""

    def split_text(self, text: str) -> list[str]:
        """
        æŒ‰å¥å­åˆ†å‰²ï¼Œä¿æŒ chunk_size é™åˆ¶
        """
        import re

        # ç®€å•çš„å¥å­åˆ†å‰²ï¼ˆå®é™…åº”ç”¨å¯ä½¿ç”¨ nltk æˆ– spacyï¼‰
        sentences = re.split(r'(?<=[.!?])\s+', text)

        chunks = []
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            sentence_length = len(sentence)

            if current_length + sentence_length > self.chunk_size:
                # å¼€å§‹æ–°å—
                if current_chunk:
                    chunks.append(" ".join(current_chunk))

                    # ä¿æŒé‡å 
                    overlap_sentences = []
                    overlap_length = 0
                    for s in reversed(current_chunk):
                        overlap_length += len(s)
                        if overlap_length > self.chunk_overlap:
                            break
                        overlap_sentences.insert(0, s)

                    current_chunk = overlap_sentences
                    current_length = overlap_length

            current_chunk.append(sentence)
            current_length += sentence_length

        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(" ".join(current_chunk))

        return chunks

# ä½¿ç”¨
splitter = CustomSentenceSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_text(long_text)
```

## 4. æœ€ä½³å®è·µ

### 4.1 é€‰æ‹©åˆé€‚çš„ chunk_size

```python
# ä¸€èˆ¬å»ºè®®
# å°å— (200-500): ç²¾ç¡®æ£€ç´¢ï¼Œä½†å¯èƒ½ç¼ºä¹ä¸Šä¸‹æ–‡
# ä¸­å— (500-1500): å¹³è¡¡ç²¾åº¦å’Œä¸Šä¸‹æ–‡ï¼ˆæ¨èï¼‰
# å¤§å— (1500-3000): æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†æ£€ç´¢ç²¾åº¦é™ä½

# æ ¹æ®ç”¨ä¾‹è°ƒæ•´
qa_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,  # é—®ç­”ï¼šä¸­ç­‰å—
    chunk_overlap=200
)

summarization_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,  # æ‘˜è¦ï¼šå¤§å—
    chunk_overlap=500
)

search_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,  # æœç´¢ï¼šå°å—
    chunk_overlap=100
)
```

### 4.2 åˆç†è®¾ç½® chunk_overlap

```python
# chunk_overlap = 10-20% of chunk_sizeï¼ˆæ¨èï¼‰

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200  # 20%
)

# ä¸ºä»€ä¹ˆéœ€è¦é‡å ï¼Ÿ
# 1. é¿å…åœ¨å…³é”®ä¿¡æ¯å¤„åˆ‡æ–­
# 2. æä¾›è·¨å—çš„ä¸Šä¸‹æ–‡è¿ç»­æ€§
# 3. æé«˜æ£€ç´¢å¬å›ç‡
```

### 4.3 æ·»åŠ å…ƒæ•°æ®

```python
from langchain_core.documents import Document

documents = [
    Document(
        page_content=chunk,
        metadata={
            "source": "langchain_docs.pdf",
            "page": 5,
            "chunk_id": i,
            "total_chunks": len(chunks),
            "language": "en"
        }
    )
    for i, chunk in enumerate(chunks)
]

# å…ƒæ•°æ®å¯ç”¨äºè¿‡æ»¤å’Œè¿½æº¯
```

### 4.4 æ·»åŠ èµ·å§‹ç´¢å¼•

```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    add_start_index=True  # æ·»åŠ èµ·å§‹ç´¢å¼•
)

docs = splitter.create_documents([text])

for doc in docs:
    print(doc.metadata)
    # {"start_index": 0}  # å—åœ¨åŸå§‹æ–‡æ¡£ä¸­çš„èµ·å§‹ä½ç½®
```

### 4.5 ç»„åˆå¤šä¸ªåˆ†å‰²å™¨

```python
# å…ˆæŒ‰ Markdown æ ‡é¢˜åˆ†å‰²ï¼Œå†æŒ‰å­—ç¬¦åˆ†å‰²
md_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[("#", "Header 1"), ("##", "Header 2")]
)

char_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

# ä¸¤é˜¶æ®µåˆ†å‰²
md_docs = md_splitter.split_text(markdown_text)
final_docs = char_splitter.split_documents(md_docs)
```

### 4.6 æ€§èƒ½ä¼˜åŒ–

```python
# æ‰¹é‡å¤„ç†
texts = [doc.page_content for doc in documents]
chunks = splitter.create_documents(
    texts,
    metadatas=[doc.metadata for doc in documents]
)

# å¹¶è¡Œå¤„ç†ï¼ˆå¤§é‡æ–‡æ¡£ï¼‰
from multiprocessing import Pool

def split_batch(args):
    splitter, texts, metadatas = args
    return splitter.create_documents(texts, metadatas)

with Pool(4) as pool:
    results = pool.map(split_batch, batches)
```

## 5. å¸¸è§åœºæ™¯å’Œè§£å†³æ–¹æ¡ˆ

### 5.1 ä»£ç æ–‡æ¡£åˆ†å‰²

```python
# ç»“åˆä»£ç å’Œ Markdown
splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.MARKDOWN,
    chunk_size=1000,
    chunk_overlap=200
)

# Markdown ä¸­çš„ä»£ç å—ä¼šè¢«è¯†åˆ«å¹¶ä¿æŒå®Œæ•´
```

### 5.2 å¤šè¯­è¨€æ–‡æ¡£

```python
# ä¸­æ–‡æ–‡æ¡£
chinese_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # ä¸­æ–‡å­—ç¬¦æ•°è¾ƒå°‘
    chunk_overlap=100,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
)

# å¤šè¯­è¨€æ··åˆ
multilingual_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150,
    separators=["\n\n", "\n", ". ", "ã€‚", " ", ""]
)
```

### 5.3 PDF æ–‡æ¡£åˆ†å‰²

```python
from langchain_community.document_loaders import PyPDFLoader

# åŠ è½½ PDF
loader = PyPDFLoader("document.pdf")
pages = loader.load()

# åˆ†å‰²ï¼ˆä¿ç•™é¡µç ä¿¡æ¯ï¼‰
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

chunks = splitter.split_documents(pages)

# æ¯ä¸ªå—éƒ½åŒ…å«åŸå§‹é¡µç 
for chunk in chunks:
    print(f"Page {chunk.metadata['page']}: {chunk.page_content[:100]}...")
```

### 5.4 é•¿ä»£ç æ–‡ä»¶

```python
# æŒ‰å‡½æ•°/ç±»åˆ†å‰²
python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=1500,  # ä»£ç éœ€è¦æ›´å¤§çš„å—
    chunk_overlap=200
)

# ä»£ç å—ä¼šå°½é‡ä¿æŒå‡½æ•°/ç±»å®Œæ•´
```

## 6. ä¸å…¶ä»–æ¨¡å—çš„åä½œ

- **Document Loaders**: åŠ è½½æ–‡æ¡£ååˆ†å‰²
- **VectorStores**: åˆ†å‰²åçš„å—å­˜å…¥å‘é‡å­˜å‚¨
- **Embeddings**: æ¯ä¸ªå—ç”ŸæˆåµŒå…¥
- **Retrievers**: æ£€ç´¢åˆ†å‰²åçš„å—

## 7. å®Œæ•´å·¥ä½œæµç¤ºä¾‹

```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# 1. åŠ è½½æ–‡æ¡£
loader = TextLoader("article.txt")
documents = loader.load()

# 2. åˆ†å‰²æ–‡æ¡£
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    add_start_index=True
)

texts = text_splitter.split_documents(documents)
print(f"åˆ†å‰²æˆ {len(texts)} ä¸ªå—")

# 3. åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(texts, embeddings)

# 4. æ£€ç´¢
query = "What is the main topic?"
results = vectorstore.similarity_search(query, k=3)

for result in results:
    print(f"Source: {result.metadata['source']}")
    print(f"Start index: {result.metadata['start_index']}")
    print(f"Content: {result.page_content}\n")
```

## 8. æ€»ç»“

Text Splitters æ˜¯ RAG ç³»ç»Ÿçš„å…³é”®é¢„å¤„ç†ç»„ä»¶ï¼Œæä¾›æ™ºèƒ½æ–‡æ¡£åˆ†å‰²èƒ½åŠ›ã€‚å…³é”®ç‰¹æ€§ï¼š

1. **å¤šç§åˆ†å‰²ç­–ç•¥**: å­—ç¬¦ã€Tokenã€è¯­ä¹‰ã€æ ¼å¼æ„ŸçŸ¥
2. **ä¸Šä¸‹æ–‡ä¿ç•™**: é€šè¿‡é‡å ä¿æŒè¿ç»­æ€§
3. **è¯­ä¹‰å®Œæ•´æ€§**: å°½é‡ä¿æŒå¥å­/æ®µè½å®Œæ•´
4. **å…ƒæ•°æ®ç®¡ç†**: ä¿ç•™æ¥æºå’Œä½ç½®ä¿¡æ¯
5. **æ ¼å¼æ„ŸçŸ¥**: é’ˆå¯¹ Markdownã€ä»£ç ç­‰ä¼˜åŒ–

**å…³é”®åŸåˆ™**:

- ä¼˜å…ˆä½¿ç”¨ **RecursiveCharacterTextSplitter**ï¼ˆé€šç”¨åœºæ™¯ï¼‰
- **chunk_size**: 500-1500 å­—ç¬¦ï¼ˆæ¨èï¼‰
- **chunk_overlap**: chunk_size çš„ 10-20%
- ç‰¹æ®Šæ ¼å¼ä½¿ç”¨ä¸“ç”¨åˆ†å‰²å™¨ï¼ˆMarkdownã€Codeï¼‰
- æ·»åŠ èµ·å§‹ç´¢å¼•å’Œä¸°å¯Œå…ƒæ•°æ®
- å…ˆæŒ‰ç»“æ„åˆ†å‰²ï¼Œå†æŒ‰å¤§å°åˆ†å‰²

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-03
**ç›¸å…³æ–‡æ¡£**:

- LangChain-00-æ€»è§ˆ.md
- LangChain-08-VectorStores-Retrievers-æ¦‚è§ˆ.md
- LangChain-11-DocumentLoaders-æ¦‚è§ˆ.mdï¼ˆå¾…ç”Ÿæˆï¼‰

---

## APIæ¥å£

## æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£è¯¦ç»†æè¿° **Text Splitters æ¨¡å—**çš„å¯¹å¤– APIï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†å‰²ã€å—ç®¡ç†ã€é‡å å¤„ç†ã€åˆ†éš”ç¬¦ç­–ç•¥ç­‰æ ¸å¿ƒæ¥å£çš„æ‰€æœ‰å…¬å¼€æ–¹æ³•å’Œå‚æ•°è§„æ ¼ã€‚

---

## 1. TextSplitter åŸºç¡€ API

### 1.1 åŸºç¡€æ¥å£

#### åŸºæœ¬ä¿¡æ¯
- **ç±»å**ï¼š`TextSplitter`
- **åŠŸèƒ½**ï¼šæ–‡æœ¬åˆ†å‰²çš„æŠ½è±¡åŸºç±»
- **æ ¸å¿ƒèŒè´£**ï¼šå°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºé€‚åˆå¤„ç†çš„å°å—

#### æ ¸å¿ƒæ–¹æ³•

```python
class TextSplitter(ABC):
    """æ–‡æœ¬åˆ†å‰²å™¨åŸºç±»ã€‚"""

    def __init__(
        self,
        chunk_size: int = 4000,
        chunk_overlap: int = 200,
        length_function: Callable[[str], int] = len,
        keep_separator: bool = False,
        add_start_index: bool = False,
        strip_whitespace: bool = True,
    ):
        """åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨ã€‚"""

    @abstractmethod
    def split_text(self, text: str) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚"""

    def create_documents(
        self,
        texts: List[str],
        metadatas: Optional[List[dict]] = None,
    ) -> List[Document]:
        """åˆ›å»ºæ–‡æ¡£å¯¹è±¡åˆ—è¡¨ã€‚"""

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£åˆ—è¡¨ã€‚"""
```

**æ–¹æ³•è¯¦è§£**ï¼š

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|-----|------|---------|------|
| split_text | `text: str` | `List[str]` | å°†æ–‡æœ¬åˆ†å‰²ä¸ºå­—ç¬¦ä¸²å—åˆ—è¡¨ |
| create_documents | `texts: List[str]`, `metadatas: List[dict]` | `List[Document]` | åˆ›å»ºå¸¦å…ƒæ•°æ®çš„æ–‡æ¡£å¯¹è±¡ |
| split_documents | `documents: List[Document]` | `List[Document]` | åˆ†å‰²ç°æœ‰æ–‡æ¡£åˆ—è¡¨ |

#### æ„é€ å‚æ•°è¯¦è§£

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|------|--------|------|
| chunk_size | `int` | `4000` | æ¯ä¸ªå—çš„æœ€å¤§å¤§å° |
| chunk_overlap | `int` | `200` | å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•° |
| length_function | `Callable` | `len` | è®¡ç®—æ–‡æœ¬é•¿åº¦çš„å‡½æ•° |
| keep_separator | `bool` | `False` | æ˜¯å¦ä¿ç•™åˆ†éš”ç¬¦ |
| add_start_index | `bool` | `False` | æ˜¯å¦æ·»åŠ èµ·å§‹ç´¢å¼•åˆ°å…ƒæ•°æ® |
| strip_whitespace | `bool` | `True` | æ˜¯å¦å»é™¤ç©ºç™½å­—ç¬¦ |

---

## 2. CharacterTextSplitter API

### 2.1 å­—ç¬¦åˆ†å‰²å™¨

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šåŸºäºæŒ‡å®šåˆ†éš”ç¬¦åˆ†å‰²æ–‡æœ¬
- **ç‰¹ç‚¹**ï¼šç®€å•ç›´æ¥ï¼Œé€‚ç”¨äºç»“æ„åŒ–æ–‡æœ¬
- **é€‚ç”¨åœºæ™¯**ï¼šæœ‰æ˜ç¡®åˆ†éš”ç¬¦çš„æ–‡æœ¬ï¼ˆå¦‚æ®µè½ã€å¥å­ï¼‰

#### æ„é€ å‚æ•°

```python
class CharacterTextSplitter(TextSplitter):
    def __init__(
        self,
        separator: str = "\n\n",
        chunk_size: int = 4000,
        chunk_overlap: int = 200,
        length_function: Callable[[str], int] = len,
        is_separator_regex: bool = False,
        **kwargs: Any,
    ):
        """å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨æ„é€ å‡½æ•°ã€‚"""
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain_text_splitters import CharacterTextSplitter

# åŸºç¡€ç”¨æ³• - æŒ‰æ®µè½åˆ†å‰²
text = """
ç¬¬ä¸€æ®µå†…å®¹ã€‚è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ®µè½ï¼ŒåŒ…å«äº†ç›¸å…³çš„ä¿¡æ¯ã€‚

ç¬¬äºŒæ®µå†…å®¹ã€‚è¿™æ˜¯å¦ä¸€ä¸ªæ®µè½ï¼Œè®¨è®ºä¸åŒçš„ä¸»é¢˜ã€‚

ç¬¬ä¸‰æ®µå†…å®¹ã€‚æœ€åä¸€ä¸ªæ®µè½ï¼Œæ€»ç»“å‰é¢çš„å†…å®¹ã€‚
"""

splitter = CharacterTextSplitter(
    separator="\n\n",  # æŒ‰åŒæ¢è¡Œåˆ†å‰²
    chunk_size=100,    # æ¯å—æœ€å¤§100å­—ç¬¦
    chunk_overlap=20,  # é‡å 20å­—ç¬¦
    length_function=len,
    is_separator_regex=False
)

chunks = splitter.split_text(text)
print(f"åˆ†å‰²åçš„å—æ•°: {len(chunks)}")
for i, chunk in enumerate(chunks):
    print(f"å— {i+1}: {repr(chunk)}")

# è¾“å‡º:
# å— 1: 'ç¬¬ä¸€æ®µå†…å®¹ã€‚è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ®µè½ï¼ŒåŒ…å«äº†ç›¸å…³çš„ä¿¡æ¯ã€‚'
# å— 2: 'ç¬¬äºŒæ®µå†…å®¹ã€‚è¿™æ˜¯å¦ä¸€ä¸ªæ®µè½ï¼Œè®¨è®ºä¸åŒçš„ä¸»é¢˜ã€‚'
# å— 3: 'ç¬¬ä¸‰æ®µå†…å®¹ã€‚æœ€åä¸€ä¸ªæ®µè½ï¼Œæ€»ç»“å‰é¢çš„å†…å®¹ã€‚'
```

#### æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²

```python
import re

# ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²
text = "å¥å­1ã€‚å¥å­2ï¼å¥å­3ï¼Ÿå¥å­4ã€‚"

regex_splitter = CharacterTextSplitter(
    separator=r'[ã€‚ï¼ï¼Ÿ]',  # æŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ†å‰²
    chunk_size=50,
    chunk_overlap=5,
    is_separator_regex=True,
    keep_separator=True  # ä¿ç•™åˆ†éš”ç¬¦
)

chunks = regex_splitter.split_text(text)
print("æ­£åˆ™åˆ†å‰²ç»“æœ:")
for chunk in chunks:
    print(f"- {repr(chunk)}")

# è¾“å‡º:
# - 'å¥å­1ã€‚'
# - 'å¥å­2ï¼'
# - 'å¥å­3ï¼Ÿ'
# - 'å¥å­4ã€‚'
```

#### æ ¸å¿ƒå®ç°

```python
def split_text(self, text: str) -> List[str]:
    """åˆ†å‰²æ–‡æœ¬å®ç°ã€‚"""
    # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²
    if self.is_separator_regex:
        splits = re.split(self.separator, text)
    else:
        splits = text.split(self.separator)

    # å¤„ç†åˆ†éš”ç¬¦ä¿ç•™
    if self.keep_separator and not self.is_separator_regex:
        # é‡æ–°æ·»åŠ åˆ†éš”ç¬¦
        result = []
        for i, split in enumerate(splits[:-1]):
            result.append(split + self.separator)
        if splits:
            result.append(splits[-1])
        splits = result

    # åˆå¹¶å°å—å¹¶å¤„ç†é‡å 
    return self._merge_splits(splits, self.separator)
```

---

## 3. RecursiveCharacterTextSplitter API

### 3.1 é€’å½’å­—ç¬¦åˆ†å‰²å™¨

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šé€’å½’ä½¿ç”¨å¤šä¸ªåˆ†éš”ç¬¦åˆ†å‰²æ–‡æœ¬
- **ç‰¹ç‚¹**ï¼šæ™ºèƒ½é€‰æ‹©æœ€ä½³åˆ†éš”ç¬¦ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
- **é€‚ç”¨åœºæ™¯**ï¼šé€šç”¨æ–‡æœ¬åˆ†å‰²ï¼Œç‰¹åˆ«æ˜¯ä»£ç å’Œç»“æ„åŒ–æ–‡æ¡£

#### æ„é€ å‚æ•°

```python
class RecursiveCharacterTextSplitter(TextSplitter):
    def __init__(
        self,
        separators: Optional[List[str]] = None,
        keep_separator: bool = True,
        is_separator_regex: bool = False,
        **kwargs: Any,
    ):
        """é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨æ„é€ å‡½æ•°ã€‚"""
        super().__init__(keep_separator=keep_separator, **kwargs)
        self._separators = separators or self._get_default_separators()
        self._is_separator_regex = is_separator_regex
```

#### é»˜è®¤åˆ†éš”ç¬¦ä¼˜å…ˆçº§

```python
def _get_default_separators(self) -> List[str]:
    """è·å–é»˜è®¤åˆ†éš”ç¬¦åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰ã€‚"""
    return [
        "\n\n",    # æ®µè½åˆ†éš”
        "\n",      # è¡Œåˆ†éš”
        " ",       # è¯åˆ†éš”
        "",        # å­—ç¬¦åˆ†éš”
    ]
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# é•¿æ–‡æœ¬ç¤ºä¾‹
long_text = """
# æ ‡é¢˜

è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹ã€‚å®ƒåŒ…å«äº†ä¸€äº›é‡è¦çš„ä¿¡æ¯ï¼Œéœ€è¦è¢«æ­£ç¡®åœ°åˆ†å‰²ã€‚

è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ã€‚å®ƒç»§ç»­å‰é¢çš„è®¨è®ºï¼Œå¹¶æ·»åŠ äº†æ–°çš„è§‚ç‚¹ã€‚

## å­æ ‡é¢˜

è¿™é‡Œæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼š

- é¡¹ç›®1ï¼šæè¿°å†…å®¹
- é¡¹ç›®2ï¼šæ›´å¤šæè¿°
- é¡¹ç›®3ï¼šæœ€åçš„æè¿°

ç»“è®ºæ®µè½åŒ…å«äº†æ€»ç»“æ€§çš„å†…å®¹ã€‚
"""

# åˆ›å»ºé€’å½’åˆ†å‰²å™¨
splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50,
    length_function=len,
    separators=["\n\n", "\n", " ", ""]
)

chunks = splitter.split_text(long_text)
print(f"é€’å½’åˆ†å‰²ç»“æœ - å—æ•°: {len(chunks)}")

for i, chunk in enumerate(chunks):
    print(f"\nå— {i+1} ({len(chunk)} å­—ç¬¦):")
    print("-" * 40)
    print(chunk)
    print("-" * 40)
```

#### è‡ªå®šä¹‰åˆ†éš”ç¬¦ç­–ç•¥

```python
# ä»£ç åˆ†å‰²å™¨
code_splitter = RecursiveCharacterTextSplitter(
    separators=[
        "\n\nclass ",    # ç±»å®šä¹‰
        "\n\ndef ",     # å‡½æ•°å®šä¹‰
        "\n\n",         # æ®µè½
        "\n",           # è¡Œ
        " ",            # ç©ºæ ¼
        "",             # å­—ç¬¦
    ],
    chunk_size=500,
    chunk_overlap=50,
    keep_separator=True
)

# Markdownåˆ†å‰²å™¨
markdown_splitter = RecursiveCharacterTextSplitter(
    separators=[
        "\n## ",        # äºŒçº§æ ‡é¢˜
        "\n### ",       # ä¸‰çº§æ ‡é¢˜
        "\n\n",         # æ®µè½
        "\n",           # è¡Œ
        " ",            # ç©ºæ ¼
        "",             # å­—ç¬¦
    ],
    chunk_size=300,
    chunk_overlap=30
)
```

#### æ ¸å¿ƒé€’å½’ç®—æ³•

```python
def split_text(self, text: str) -> List[str]:
    """é€’å½’åˆ†å‰²æ–‡æœ¬ã€‚"""
    final_chunks = []

    # é€‰æ‹©åˆé€‚çš„åˆ†éš”ç¬¦
    separator = self._separators[-1]  # é»˜è®¤ä½¿ç”¨æœ€åä¸€ä¸ª
    new_separators = []

    for i, _s in enumerate(self._separators):
        if _s == "":
            separator = _s
            break
        if re.search(_s, text) if self._is_separator_regex else _s in text:
            separator = _s
            new_separators = self._separators[i + 1:]
            break

    # ä½¿ç”¨é€‰å®šçš„åˆ†éš”ç¬¦åˆ†å‰²
    splits = self._split_text_with_regex(text, separator) if self._is_separator_regex else text.split(separator)

    # å¤„ç†æ¯ä¸ªåˆ†å‰²å—
    good_splits = []
    for s in splits:
        if self._length_function(s) < self._chunk_size:
            good_splits.append(s)
        else:
            if good_splits:
                merged_text = self._merge_splits(good_splits, separator)
                final_chunks.extend(merged_text)
                good_splits = []

            # é€’å½’å¤„ç†è¿‡å¤§çš„å—
            if not new_separators:
                final_chunks.append(s)
            else:
                other_info = self.split_text(s)
                final_chunks.extend(other_info)

    # å¤„ç†å‰©ä½™çš„å—
    if good_splits:
        merged_text = self._merge_splits(good_splits, separator)
        final_chunks.extend(merged_text)

    return final_chunks
```

---

## 4. TokenTextSplitter API

### 4.1 ä»¤ç‰Œåˆ†å‰²å™¨

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šåŸºäºä»¤ç‰Œæ•°é‡åˆ†å‰²æ–‡æœ¬
- **ç‰¹ç‚¹**ï¼šç²¾ç¡®æ§åˆ¶ä»¤ç‰Œæ•°é‡ï¼Œé€‚ç”¨äºLLMè¾“å…¥
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦ä¸¥æ ¼æ§åˆ¶ä»¤ç‰Œæ•°çš„åº”ç”¨

#### æ„é€ å‚æ•°

```python
class TokenTextSplitter(TextSplitter):
    def __init__(
        self,
        encoding_name: str = "gpt2",
        model_name: Optional[str] = None,
        allowed_special: Union[Literal["all"], AbstractSet[str]] = set(),
        disallowed_special: Union[Literal["all"], Collection[str]] = "all",
        **kwargs: Any,
    ):
        """ä»¤ç‰Œæ–‡æœ¬åˆ†å‰²å™¨æ„é€ å‡½æ•°ã€‚"""
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain_text_splitters import TokenTextSplitter

# åˆ›å»ºä»¤ç‰Œåˆ†å‰²å™¨
token_splitter = TokenTextSplitter(
    encoding_name="cl100k_base",  # GPT-4 ç¼–ç 
    chunk_size=100,               # 100ä¸ªä»¤ç‰Œ
    chunk_overlap=20              # 20ä¸ªä»¤ç‰Œé‡å 
)

text = """
äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
å®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚
"""

chunks = token_splitter.split_text(text)
print(f"ä»¤ç‰Œåˆ†å‰²ç»“æœ - å—æ•°: {len(chunks)}")

for i, chunk in enumerate(chunks):
    token_count = token_splitter.count_tokens(chunk)
    print(f"å— {i+1} ({token_count} ä»¤ç‰Œ): {chunk}")
```

#### ä¸åŒç¼–ç å™¨çš„ä½¿ç”¨

```python
# GPT-3.5/GPT-4 ç¼–ç å™¨
gpt4_splitter = TokenTextSplitter(
    encoding_name="cl100k_base",
    chunk_size=2000,
    chunk_overlap=200
)

# GPT-3 ç¼–ç å™¨
gpt3_splitter = TokenTextSplitter(
    encoding_name="p50k_base",
    chunk_size=1500,
    chunk_overlap=150
)

# Claude ç¼–ç å™¨
claude_splitter = TokenTextSplitter(
    encoding_name="gpt2",  # è¿‘ä¼¼
    chunk_size=1800,
    chunk_overlap=180
)

# æ¯”è¾ƒä¸åŒç¼–ç å™¨çš„ä»¤ç‰Œè®¡æ•°
test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨æ¥æ¯”è¾ƒä¸åŒç¼–ç å™¨çš„ä»¤ç‰Œè®¡æ•°å·®å¼‚ã€‚"

print("ä»¤ç‰Œè®¡æ•°æ¯”è¾ƒ:")
print(f"GPT-4: {gpt4_splitter.count_tokens(test_text)} ä»¤ç‰Œ")
print(f"GPT-3: {gpt3_splitter.count_tokens(test_text)} ä»¤ç‰Œ")
print(f"è¿‘ä¼¼: {claude_splitter.count_tokens(test_text)} ä»¤ç‰Œ")
```

#### ä»¤ç‰Œè®¡æ•°å®ç°

```python
def count_tokens(self, text: str) -> int:
    """è®¡ç®—æ–‡æœ¬çš„ä»¤ç‰Œæ•°é‡ã€‚"""
    return len(self._tokenizer.encode(text))

def split_text(self, text: str) -> List[str]:
    """åŸºäºä»¤ç‰Œåˆ†å‰²æ–‡æœ¬ã€‚"""
    splits = []
    input_ids = self._tokenizer.encode(text)

    start_idx = 0
    cur_idx = min(start_idx + self._chunk_size, len(input_ids))
    chunk_ids = input_ids[start_idx:cur_idx]

    while start_idx < len(input_ids):
        chunk_text = self._tokenizer.decode(chunk_ids)
        splits.append(chunk_text)

        # è®¡ç®—ä¸‹ä¸€ä¸ªå—çš„èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
        start_idx += self._chunk_size - self._chunk_overlap
        cur_idx = min(start_idx + self._chunk_size, len(input_ids))
        chunk_ids = input_ids[start_idx:cur_idx]

    return splits
```

---

## 5. ä¸“ç”¨åˆ†å‰²å™¨ API

### 5.1 MarkdownHeaderTextSplitter

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šåŸºäºMarkdownæ ‡é¢˜å±‚æ¬¡åˆ†å‰²æ–‡æœ¬
- **ç‰¹ç‚¹**ï¼šä¿æŒæ–‡æ¡£ç»“æ„ï¼Œæ”¯æŒæ ‡é¢˜å±‚æ¬¡
- **é€‚ç”¨åœºæ™¯**ï¼šMarkdownæ–‡æ¡£ã€æŠ€æœ¯æ–‡æ¡£

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain_text_splitters import MarkdownHeaderTextSplitter

markdown_text = """
# ä¸»æ ‡é¢˜

è¿™æ˜¯ä¸»æ ‡é¢˜ä¸‹çš„å†…å®¹ã€‚

## äºŒçº§æ ‡é¢˜1

è¿™æ˜¯äºŒçº§æ ‡é¢˜1çš„å†…å®¹ã€‚

### ä¸‰çº§æ ‡é¢˜1.1

è¿™æ˜¯ä¸‰çº§æ ‡é¢˜1.1çš„å†…å®¹ã€‚

### ä¸‰çº§æ ‡é¢˜1.2

è¿™æ˜¯ä¸‰çº§æ ‡é¢˜1.2çš„å†…å®¹ã€‚

## äºŒçº§æ ‡é¢˜2

è¿™æ˜¯äºŒçº§æ ‡é¢˜2çš„å†…å®¹ã€‚
"""

# å®šä¹‰æ ‡é¢˜åˆ†å‰²è§„åˆ™
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)

md_header_splits = markdown_splitter.split_text(markdown_text)

for split in md_header_splits:
    print(f"å†…å®¹: {split.page_content}")
    print(f"å…ƒæ•°æ®: {split.metadata}")
    print("-" * 50)
```

#### ä¸å…¶ä»–åˆ†å‰²å™¨ç»„åˆ

```python
# å…ˆæŒ‰æ ‡é¢˜åˆ†å‰²ï¼Œå†æŒ‰å­—ç¬¦æ•°åˆ†å‰²
chunk_size = 200
chunk_overlap = 30

# ç¬¬ä¸€æ­¥ï¼šæŒ‰æ ‡é¢˜åˆ†å‰²
md_header_splits = markdown_splitter.split_text(markdown_text)

# ç¬¬äºŒæ­¥ï¼šå¯¹é•¿æ®µè½è¿›è¡Œå­—ç¬¦åˆ†å‰²
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap
)

final_splits = text_splitter.split_documents(md_header_splits)

print(f"æœ€ç»ˆåˆ†å‰²ç»“æœ: {len(final_splits)} ä¸ªå—")
for i, split in enumerate(final_splits):
    print(f"\nå— {i+1}:")
    print(f"å†…å®¹: {split.page_content[:100]}...")
    print(f"å…ƒæ•°æ®: {split.metadata}")
```

---

### 5.2 PythonCodeTextSplitter

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šä¸“é—¨ç”¨äºPythonä»£ç çš„åˆ†å‰²
- **ç‰¹ç‚¹**ï¼šç†è§£Pythonè¯­æ³•ç»“æ„
- **é€‚ç”¨åœºæ™¯**ï¼šä»£ç æ–‡æ¡£ã€ä»£ç åˆ†æ

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain_text_splitters import PythonCodeTextSplitter

python_code = '''
class DataProcessor:
    """æ•°æ®å¤„ç†å™¨ç±»ã€‚"""

    def __init__(self, config):
        self.config = config
        self.data = []

    def load_data(self, file_path):
        """åŠ è½½æ•°æ®æ–‡ä»¶ã€‚"""
        with open(file_path, 'r') as f:
            self.data = f.readlines()
        return len(self.data)

    def process_data(self):
        """å¤„ç†æ•°æ®ã€‚"""
        processed = []
        for line in self.data:
            # æ¸…ç†æ•°æ®
            cleaned = line.strip()
            if cleaned:
                processed.append(cleaned.upper())
        return processed

def main():
    """ä¸»å‡½æ•°ã€‚"""
    processor = DataProcessor({"debug": True})
    processor.load_data("data.txt")
    result = processor.process_data()
    print(f"å¤„ç†äº† {len(result)} æ¡æ•°æ®")

if __name__ == "__main__":
    main()
'''

python_splitter = PythonCodeTextSplitter(
    chunk_size=300,
    chunk_overlap=50
)

python_chunks = python_splitter.split_text(python_code)

print(f"Pythonä»£ç åˆ†å‰²ç»“æœ: {len(python_chunks)} ä¸ªå—")
for i, chunk in enumerate(python_chunks):
    print(f"\nä»£ç å— {i+1}:")
    print("-" * 40)
    print(chunk)
    print("-" * 40)
```

---

### 5.3 HTMLHeaderTextSplitter

#### åŸºæœ¬ä¿¡æ¯
- **åŠŸèƒ½**ï¼šåŸºäºHTMLæ ‡é¢˜æ ‡ç­¾åˆ†å‰²æ–‡æœ¬
- **ç‰¹ç‚¹**ï¼šä¿æŒHTMLæ–‡æ¡£ç»“æ„
- **é€‚ç”¨åœºæ™¯**ï¼šç½‘é¡µå†…å®¹ã€HTMLæ–‡æ¡£

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain_text_splitters import HTMLHeaderTextSplitter

html_text = """
<html>
<body>
    <h1>ç½‘ç«™ä¸»æ ‡é¢˜</h1>
    <p>è¿™æ˜¯ç½‘ç«™çš„ä¸»è¦ä»‹ç»å†…å®¹ã€‚</p>

    <h2>äº§å“ä»‹ç»</h2>
    <p>æˆ‘ä»¬çš„äº§å“å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š</p>
    <ul>
        <li>é«˜æ€§èƒ½</li>
        <li>æ˜“ä½¿ç”¨</li>
        <li>å¯æ‰©å±•</li>
    </ul>

    <h3>æŠ€æœ¯è§„æ ¼</h3>
    <p>è¯¦ç»†çš„æŠ€æœ¯è§„æ ¼ä¿¡æ¯ã€‚</p>

    <h2>è”ç³»æˆ‘ä»¬</h2>
    <p>å¦‚æœ‰ç–‘é—®ï¼Œè¯·è”ç³»æˆ‘ä»¬ã€‚</p>
</body>
</html>
"""

headers_to_split_on = [
    ("h1", "Header 1"),
    ("h2", "Header 2"),
    ("h3", "Header 3"),
]

html_splitter = HTMLHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)

html_header_splits = html_splitter.split_text(html_text)

for split in html_header_splits:
    print(f"å†…å®¹: {split.page_content}")
    print(f"å…ƒæ•°æ®: {split.metadata}")
    print("-" * 50)
```

---

## 6. æ–‡æ¡£å¤„ç† API

### 6.1 æ–‡æ¡£åˆ›å»ºå’Œåˆ†å‰²

#### create_documents æ–¹æ³•

```python
def create_documents(
    self,
    texts: List[str],
    metadatas: Optional[List[dict]] = None,
) -> List[Document]:
    """ä»æ–‡æœ¬åˆ—è¡¨åˆ›å»ºæ–‡æ¡£å¯¹è±¡ã€‚"""

    # ä½¿ç”¨ç¤ºä¾‹
    texts = [
        "ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„å†…å®¹...",
        "ç¬¬äºŒä¸ªæ–‡æ¡£çš„å†…å®¹...",
        "ç¬¬ä¸‰ä¸ªæ–‡æ¡£çš„å†…å®¹..."
    ]

    metadatas = [
        {"source": "doc1.txt", "author": "å¼ ä¸‰"},
        {"source": "doc2.txt", "author": "æå››"},
        {"source": "doc3.txt", "author": "ç‹äº”"}
    ]

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=50
    )

    documents = splitter.create_documents(texts, metadatas)

    print(f"åˆ›å»ºäº† {len(documents)} ä¸ªæ–‡æ¡£")
    for doc in documents:
        print(f"å†…å®¹: {doc.page_content[:50]}...")
        print(f"å…ƒæ•°æ®: {doc.metadata}")
```

#### split_documents æ–¹æ³•

```python
def split_documents(self, documents: List[Document]) -> List[Document]:
    """åˆ†å‰²ç°æœ‰æ–‡æ¡£åˆ—è¡¨ã€‚"""

    # ä½¿ç”¨ç¤ºä¾‹
    from langchain_core.documents import Document

    # åˆ›å»ºåŸå§‹æ–‡æ¡£
    original_docs = [
        Document(
            page_content="è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æ¡£å†…å®¹..." * 100,
            metadata={"source": "long_doc.txt", "type": "article"}
        ),
        Document(
            page_content="å¦ä¸€ä¸ªé•¿æ–‡æ¡£çš„å†…å®¹..." * 80,
            metadata={"source": "another_doc.txt", "type": "report"}
        )
    ]

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100,
        add_start_index=True  # æ·»åŠ èµ·å§‹ç´¢å¼•
    )

    split_docs = splitter.split_documents(original_docs)

    print(f"åŸå§‹æ–‡æ¡£æ•°: {len(original_docs)}")
    print(f"åˆ†å‰²åæ–‡æ¡£æ•°: {len(split_docs)}")

    for i, doc in enumerate(split_docs[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
        print(f"\nåˆ†å‰²æ–‡æ¡£ {i+1}:")
        print(f"å†…å®¹: {doc.page_content[:100]}...")
        print(f"å…ƒæ•°æ®: {doc.metadata}")
```

---

## 7. é«˜çº§é…ç½®å’Œä¼˜åŒ–

### 7.1 è‡ªå®šä¹‰é•¿åº¦å‡½æ•°

```python
import tiktoken

def token_length_function(text: str) -> int:
    """åŸºäºGPTä»¤ç‰Œçš„é•¿åº¦å‡½æ•°ã€‚"""
    encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

def chinese_char_length(text: str) -> int:
    """ä¸­æ–‡å­—ç¬¦è®¡æ•°å‡½æ•°ã€‚"""
    chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
    english_chars = sum(1 for char in text if char.isalpha() and not ('\u4e00' <= char <= '\u9fff'))
    return chinese_chars * 2 + english_chars  # ä¸­æ–‡å­—ç¬¦æƒé‡æ›´é«˜

# ä½¿ç”¨è‡ªå®šä¹‰é•¿åº¦å‡½æ•°
custom_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100,
    length_function=token_length_function  # ä½¿ç”¨ä»¤ç‰Œè®¡æ•°
)

chinese_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=chinese_char_length  # ä¸­æ–‡ä¼˜åŒ–
)
```

### 7.2 åˆ†å‰²ç­–ç•¥ä¼˜åŒ–

```python
class SmartTextSplitter:
    """æ™ºèƒ½æ–‡æœ¬åˆ†å‰²å™¨ã€‚"""

    def __init__(self, target_chunk_size: int = 1000):
        self.target_chunk_size = target_chunk_size
        self.splitters = {
            'markdown': MarkdownHeaderTextSplitter([
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
            ]),
            'code': PythonCodeTextSplitter(
                chunk_size=target_chunk_size,
                chunk_overlap=100
            ),
            'general': RecursiveCharacterTextSplitter(
                chunk_size=target_chunk_size,
                chunk_overlap=100
            )
        }

    def detect_text_type(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬ç±»å‹ã€‚"""
        if text.count('#') > 3 and '##' in text:
            return 'markdown'
        elif 'def ' in text and 'class ' in text and ':' in text:
            return 'code'
        else:
            return 'general'

    def smart_split(self, text: str) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ã€‚"""
        text_type = self.detect_text_type(text)
        splitter = self.splitters[text_type]

        if text_type == 'markdown':
            # Markdownéœ€è¦ä¸¤æ­¥åˆ†å‰²
            header_splits = splitter.split_text(text)
            final_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.target_chunk_size,
                chunk_overlap=100
            )
            return final_splitter.split_documents(header_splits)
        else:
            return splitter.split_text(text)

# ä½¿ç”¨æ™ºèƒ½åˆ†å‰²å™¨
smart_splitter = SmartTextSplitter(target_chunk_size=800)

# æµ‹è¯•ä¸åŒç±»å‹çš„æ–‡æœ¬
markdown_text = "# æ ‡é¢˜\n\nå†…å®¹..."
code_text = "def function():\n    pass"
general_text = "è¿™æ˜¯ä¸€èˆ¬çš„æ–‡æœ¬å†…å®¹ã€‚"

for text_type, text in [('Markdown', markdown_text), ('Code', code_text), ('General', general_text)]:
    chunks = smart_splitter.smart_split(text)
    print(f"{text_type} æ–‡æœ¬åˆ†å‰²ç»“æœ: {len(chunks)} ä¸ªå—")
```

---

## 8. æ€§èƒ½ç›‘æ§å’Œè°ƒä¼˜

### 8.1 åˆ†å‰²æ€§èƒ½åˆ†æ

```python
import time
from typing import Dict, Any

class SplitterPerformanceAnalyzer:
    """åˆ†å‰²å™¨æ€§èƒ½åˆ†æå™¨ã€‚"""

    def __init__(self):
        self.metrics = {
            'split_times': [],
            'chunk_counts': [],
            'chunk_sizes': [],
            'overlap_ratios': []
        }

    def analyze_splitter(
        self,
        splitter: TextSplitter,
        texts: List[str],
        test_name: str = "default"
    ) -> Dict[str, Any]:
        """åˆ†æåˆ†å‰²å™¨æ€§èƒ½ã€‚"""
        start_time = time.time()

        all_chunks = []
        for text in texts:
            chunks = splitter.split_text(text)
            all_chunks.extend(chunks)

        end_time = time.time()

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        split_time = end_time - start_time
        chunk_count = len(all_chunks)
        avg_chunk_size = sum(len(chunk) for chunk in all_chunks) / chunk_count if chunk_count > 0 else 0

        # è®¡ç®—é‡å æ¯”ç‡
        total_original_length = sum(len(text) for text in texts)
        total_chunk_length = sum(len(chunk) for chunk in all_chunks)
        overlap_ratio = (total_chunk_length - total_original_length) / total_original_length if total_original_length > 0 else 0

        metrics = {
            'test_name': test_name,
            'split_time': split_time,
            'chunk_count': chunk_count,
            'avg_chunk_size': avg_chunk_size,
            'overlap_ratio': overlap_ratio,
            'throughput': len(texts) / split_time if split_time > 0 else 0,
            'chunks_per_text': chunk_count / len(texts) if texts else 0
        }

        return metrics

    def compare_splitters(self, splitters: Dict[str, TextSplitter], test_texts: List[str]):
        """æ¯”è¾ƒå¤šä¸ªåˆ†å‰²å™¨çš„æ€§èƒ½ã€‚"""
        results = {}

        for name, splitter in splitters.items():
            results[name] = self.analyze_splitter(splitter, test_texts, name)

        # æ‰“å°æ¯”è¾ƒç»“æœ
        print("åˆ†å‰²å™¨æ€§èƒ½æ¯”è¾ƒ:")
        print("-" * 80)
        print(f"{'åˆ†å‰²å™¨':<20} {'æ—¶é—´(s)':<10} {'å—æ•°':<8} {'å¹³å‡å¤§å°':<10} {'é‡å ç‡':<10} {'ååé‡':<10}")
        print("-" * 80)

        for name, metrics in results.items():
            print(f"{name:<20} {metrics['split_time']:<10.3f} {metrics['chunk_count']:<8} "
                  f"{metrics['avg_chunk_size']:<10.1f} {metrics['overlap_ratio']:<10.2%} "
                  f"{metrics['throughput']:<10.1f}")

        return results

# ä½¿ç”¨æ€§èƒ½åˆ†æå™¨
analyzer = SplitterPerformanceAnalyzer()

# å‡†å¤‡æµ‹è¯•æ•°æ®
test_texts = [
    "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬..." * 100,
    "å¦ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬..." * 150,
    "ç¬¬ä¸‰ä¸ªæµ‹è¯•æ–‡æœ¬..." * 80
]

# å‡†å¤‡ä¸åŒçš„åˆ†å‰²å™¨
splitters_to_test = {
    "Character": CharacterTextSplitter(chunk_size=500, chunk_overlap=50),
    "Recursive": RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50),
    "Token": TokenTextSplitter(chunk_size=100, chunk_overlap=10)
}

# æ‰§è¡Œæ€§èƒ½æ¯”è¾ƒ
performance_results = analyzer.compare_splitters(splitters_to_test, test_texts)
```

---

## 9. æœ€ä½³å®è·µå’Œé…ç½®æŒ‡å—

### 9.1 åˆ†å‰²å™¨é€‰æ‹©æŒ‡å—

| æ–‡æœ¬ç±»å‹ | æ¨èåˆ†å‰²å™¨ | é…ç½®å»ºè®® | ä½¿ç”¨åœºæ™¯ |
|---------|-----------|---------|---------|
| é€šç”¨æ–‡æœ¬ | `RecursiveCharacterTextSplitter` | chunk_size=1000, overlap=100 | å¤§å¤šæ•°æ–‡æœ¬å¤„ç† |
| Markdown | `MarkdownHeaderTextSplitter` + `RecursiveCharacterTextSplitter` | å…ˆæŒ‰æ ‡é¢˜ï¼Œå†æŒ‰å¤§å° | æŠ€æœ¯æ–‡æ¡£ã€åšå®¢ |
| ä»£ç  | `PythonCodeTextSplitter` | chunk_size=800, overlap=100 | ä»£ç åˆ†æã€æ–‡æ¡£ |
| HTML | `HTMLHeaderTextSplitter` | æŒ‰æ ‡ç­¾å±‚æ¬¡åˆ†å‰² | ç½‘é¡µå†…å®¹æå– |
| ä»¤ç‰Œæ•æ„Ÿ | `TokenTextSplitter` | æ ¹æ®æ¨¡å‹é™åˆ¶è®¾ç½® | LLMè¾“å…¥å‡†å¤‡ |

### 9.2 å‚æ•°è°ƒä¼˜å»ºè®®

```python
def get_optimal_splitter_config(
    text_type: str,
    target_model: str,
    use_case: str
) -> Dict[str, Any]:
    """è·å–æœ€ä¼˜åˆ†å‰²å™¨é…ç½®ã€‚"""

    configs = {
        # RAGåº”ç”¨é…ç½®
        "rag": {
            "general": {
                "splitter": RecursiveCharacterTextSplitter,
                "chunk_size": 1000,
                "chunk_overlap": 200,
                "separators": ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
            },
            "technical": {
                "splitter": RecursiveCharacterTextSplitter,
                "chunk_size": 1500,
                "chunk_overlap": 300,
                "separators": ["\n\n", "\n", ". ", "! ", "? ", " ", ""]
            }
        },

        # æ‘˜è¦åº”ç”¨é…ç½®
        "summarization": {
            "general": {
                "splitter": RecursiveCharacterTextSplitter,
                "chunk_size": 2000,
                "chunk_overlap": 100,
                "separators": ["\n\n", "\n", " ", ""]
            }
        },

        # é—®ç­”åº”ç”¨é…ç½®
        "qa": {
            "general": {
                "splitter": RecursiveCharacterTextSplitter,
                "chunk_size": 500,
                "chunk_overlap": 100,
                "separators": ["\n\n", "\n", "ã€‚", " ", ""]
            }
        }
    }

    return configs.get(use_case, {}).get(text_type, configs["rag"]["general"])

# ä½¿ç”¨é…ç½®ç”Ÿæˆå™¨
config = get_optimal_splitter_config(
    text_type="general",
    target_model="gpt-3.5-turbo",
    use_case="rag"
)

optimal_splitter = config["splitter"](
    chunk_size=config["chunk_size"],
    chunk_overlap=config["chunk_overlap"],
    separators=config.get("separators")
)
```

---

## 10. æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº† **Text Splitters æ¨¡å—**çš„æ ¸å¿ƒ APIï¼š

### ä¸»è¦åˆ†å‰²å™¨ç±»å‹
1. **TextSplitter**ï¼šæŠ½è±¡åŸºç±»ï¼Œå®šä¹‰é€šç”¨æ¥å£
2. **CharacterTextSplitter**ï¼šåŸºäºå­—ç¬¦åˆ†éš”ç¬¦çš„ç®€å•åˆ†å‰²
3. **RecursiveCharacterTextSplitter**ï¼šé€’å½’å¤šåˆ†éš”ç¬¦æ™ºèƒ½åˆ†å‰²
4. **TokenTextSplitter**ï¼šåŸºäºä»¤ç‰Œæ•°é‡çš„ç²¾ç¡®åˆ†å‰²
5. **ä¸“ç”¨åˆ†å‰²å™¨**ï¼šMarkdownã€Pythonä»£ç ã€HTMLç­‰ä¸“é—¨åˆ†å‰²å™¨

### æ ¸å¿ƒåŠŸèƒ½
1. **æ–‡æœ¬åˆ†å‰²**ï¼šsplit_textæ–¹æ³•å°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºå°å—
2. **æ–‡æ¡£å¤„ç†**ï¼šcreate_documentså’Œsplit_documentså¤„ç†æ–‡æ¡£å¯¹è±¡
3. **é‡å æ§åˆ¶**ï¼šchunk_overlapå‚æ•°æ§åˆ¶å—é—´é‡å 
4. **é•¿åº¦æ§åˆ¶**ï¼šè‡ªå®šä¹‰length_functionç²¾ç¡®æ§åˆ¶å—å¤§å°

### é…ç½®å‚æ•°
1. **chunk_size**ï¼šæ§åˆ¶æ¯ä¸ªå—çš„æœ€å¤§å¤§å°
2. **chunk_overlap**ï¼šæ§åˆ¶å—é—´é‡å ç¨‹åº¦
3. **separators**ï¼šå®šä¹‰åˆ†å‰²ä¼˜å…ˆçº§ç­–ç•¥
4. **length_function**ï¼šè‡ªå®šä¹‰é•¿åº¦è®¡ç®—æ–¹æ³•

æ¯ä¸ª API å‡åŒ…å«ï¼š

- å®Œæ•´çš„æ„é€ å‚æ•°å’Œé…ç½®é€‰é¡¹
- è¯¦ç»†çš„ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µ
- æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®
- ä¸åŒåœºæ™¯çš„é…ç½®æŒ‡å—

Text Splitters æ¨¡å—æ˜¯æ–‡æ¡£å¤„ç†å’ŒRAGç³»ç»Ÿçš„åŸºç¡€ç»„ä»¶ï¼Œæ­£ç¡®é€‰æ‹©å’Œé…ç½®åˆ†å‰²å™¨å¯¹æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ•ˆæœè‡³å…³é‡è¦ã€‚

---

## æ•°æ®ç»“æ„

## æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£è¯¦ç»†æè¿° **Text Splitters æ¨¡å—**çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ŒåŒ…æ‹¬åˆ†å‰²å™¨ç±»å±‚æ¬¡ã€å—ç®¡ç†ã€åˆ†éš”ç¬¦ç­–ç•¥ã€ä»¤ç‰Œå¤„ç†ã€æ–‡æ¡£ç»“æ„ç­‰ã€‚æ‰€æœ‰ç»“æ„å‡é…å¤‡ UML ç±»å›¾å’Œè¯¦ç»†çš„å­—æ®µè¯´æ˜ã€‚

---

## 1. TextSplitter ç±»å±‚æ¬¡ç»“æ„

### 1.1 æ ¸å¿ƒåˆ†å‰²å™¨ç»§æ‰¿ä½“ç³»

```mermaid
classDiagram
    class TextSplitter {
        <<abstract>>
        +chunk_size: int
        +chunk_overlap: int
        +length_function: Callable[[str], int]
        +keep_separator: bool
        +add_start_index: bool
        +strip_whitespace: bool
        +split_text(text: str) List[str]
        +create_documents(texts: List[str], metadatas: List[dict]) List[Document]
        +split_documents(documents: List[Document]) List[Document]
        +_merge_splits(splits: List[str], separator: str) List[str]
        +_join_docs(docs: List[str], separator: str) str
        +_split_text_with_regex(text: str, separator: str) List[str]
    }

    class CharacterTextSplitter {
        +separator: str
        +is_separator_regex: bool
        +split_text(text: str) List[str]
        +_split_text_with_regex(text: str, separator: str) List[str]
    }

    class RecursiveCharacterTextSplitter {
        +separators: List[str]
        +is_separator_regex: bool
        +split_text(text: str) List[str]
        +_get_default_separators() List[str]
        +_split_text_with_regex(text: str, separator: str) List[str]
    }

    class TokenTextSplitter {
        +encoding_name: str
        +model_name: Optional[str]
        +allowed_special: Union[Literal["all"], AbstractSet[str]]
        +disallowed_special: Union[Literal["all"], Collection[str]]
        +tokenizer: Encoding
        +split_text(text: str) List[str]
        +count_tokens(text: str) int
    }

    class MarkdownHeaderTextSplitter {
        +headers_to_split_on: List[Tuple[str, str]]
        +return_each_line: bool
        +strip_headers: bool
        +split_text(text: str) List[Document]
        +_split_on_headers(text: str) List[Dict]
        +_aggregate_lines_to_chunks(lines: List[Dict]) List[Document]
    }

    class PythonCodeTextSplitter {
        +language: str
        +split_text(text: str) List[str]
        +_get_separators_for_language(language: str) List[str]
    }

    class HTMLHeaderTextSplitter {
        +headers_to_split_on: List[Tuple[str, str]]
        +return_each_element: bool
        +split_text(html: str) List[Document]
        +_split_html_by_headers(soup: BeautifulSoup) List[Dict]
    }

    TextSplitter <|-- CharacterTextSplitter
    TextSplitter <|-- RecursiveCharacterTextSplitter
    TextSplitter <|-- TokenTextSplitter
    TextSplitter <|-- MarkdownHeaderTextSplitter
    TextSplitter <|-- PythonCodeTextSplitter
    TextSplitter <|-- HTMLHeaderTextSplitter
```

**å›¾è§£è¯´æ˜**ï¼š

1. **æŠ½è±¡åŸºç±»**ï¼š
   - `TextSplitter`ï¼šå®šä¹‰æ‰€æœ‰åˆ†å‰²å™¨çš„é€šç”¨æ¥å£å’Œè¡Œä¸º

2. **åŸºç¡€åˆ†å‰²å™¨**ï¼š
   - `CharacterTextSplitter`ï¼šåŸºäºå­—ç¬¦åˆ†éš”ç¬¦çš„ç®€å•åˆ†å‰²
   - `RecursiveCharacterTextSplitter`ï¼šé€’å½’å¤šåˆ†éš”ç¬¦æ™ºèƒ½åˆ†å‰²
   - `TokenTextSplitter`ï¼šåŸºäºä»¤ç‰Œæ•°é‡çš„ç²¾ç¡®åˆ†å‰²

3. **ä¸“ç”¨åˆ†å‰²å™¨**ï¼š
   - `MarkdownHeaderTextSplitter`ï¼šMarkdownæ ‡é¢˜å±‚æ¬¡åˆ†å‰²
   - `PythonCodeTextSplitter`ï¼šPythonä»£ç ç»“æ„åˆ†å‰²
   - `HTMLHeaderTextSplitter`ï¼šHTMLæ ‡ç­¾å±‚æ¬¡åˆ†å‰²

4. **æ ¸å¿ƒèƒ½åŠ›**ï¼š
   - æ–‡æœ¬åˆ†å‰²å’Œå—ç®¡ç†
   - é‡å å¤„ç†å’Œé•¿åº¦æ§åˆ¶
   - å…ƒæ•°æ®ä¿æŒå’Œç´¢å¼•æ·»åŠ 
   - è‡ªå®šä¹‰åˆ†éš”ç¬¦å’Œæ­£åˆ™è¡¨è¾¾å¼æ”¯æŒ

---

## 2. åˆ†å‰²é…ç½®æ•°æ®ç»“æ„

### 2.1 TextSplitter åŸºç¡€é…ç½®

```python
class TextSplitter(ABC):
    """æ–‡æœ¬åˆ†å‰²å™¨åŸºç±»æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        chunk_size: int = 4000,
        chunk_overlap: int = 200,
        length_function: Callable[[str], int] = len,
        keep_separator: bool = False,
        add_start_index: bool = False,
        strip_whitespace: bool = True,
    ):
        # æ ¸å¿ƒé…ç½®
        self._chunk_size = chunk_size
        self._chunk_overlap = chunk_overlap
        self._length_function = length_function
        self._keep_separator = keep_separator
        self._add_start_index = add_start_index
        self._strip_whitespace = strip_whitespace

        # éªŒè¯é…ç½®
        if chunk_overlap >= chunk_size:
            raise ValueError("chunk_overlapä¸èƒ½å¤§äºæˆ–ç­‰äºchunk_size")

        # ç»Ÿè®¡ä¿¡æ¯
        self._split_count = 0
        self._total_input_length = 0
        self._total_output_chunks = 0
        self._created_at = time.time()
```

**å­—æ®µè¯¦è§£**ï¼š

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|------|--------|------|
| chunk_size | `int` | `4000` | æ¯ä¸ªå—çš„æœ€å¤§é•¿åº¦ |
| chunk_overlap | `int` | `200` | å—ä¹‹é—´çš„é‡å é•¿åº¦ |
| length_function | `Callable` | `len` | è®¡ç®—æ–‡æœ¬é•¿åº¦çš„å‡½æ•° |
| keep_separator | `bool` | `False` | æ˜¯å¦ä¿ç•™åˆ†éš”ç¬¦åœ¨ç»“æœä¸­ |
| add_start_index | `bool` | `False` | æ˜¯å¦åœ¨å…ƒæ•°æ®ä¸­æ·»åŠ èµ·å§‹ç´¢å¼• |
| strip_whitespace | `bool` | `True` | æ˜¯å¦å»é™¤å—é¦–å°¾çš„ç©ºç™½å­—ç¬¦ |

---

### 2.2 åˆ†å‰²ç»“æœæ•°æ®ç»“æ„

```python
class SplitResult:
    """åˆ†å‰²ç»“æœæ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        chunks: List[str],
        original_text: str,
        splitter_config: Dict[str, Any]
    ):
        self.chunks = chunks                    # åˆ†å‰²åçš„æ–‡æœ¬å—
        self.original_text = original_text      # åŸå§‹æ–‡æœ¬
        self.splitter_config = splitter_config # åˆ†å‰²å™¨é…ç½®

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.chunk_count = len(chunks)
        self.original_length = len(original_text)
        self.total_chunks_length = sum(len(chunk) for chunk in chunks)
        self.average_chunk_size = (
            self.total_chunks_length / self.chunk_count
            if self.chunk_count > 0 else 0
        )
        self.overlap_ratio = (
            (self.total_chunks_length - self.original_length) / self.original_length
            if self.original_length > 0 else 0
        )

        # å—å¤§å°åˆ†å¸ƒ
        self.chunk_sizes = [len(chunk) for chunk in chunks]
        self.min_chunk_size = min(self.chunk_sizes) if self.chunk_sizes else 0
        self.max_chunk_size = max(self.chunk_sizes) if self.chunk_sizes else 0

        # æ—¶é—´æˆ³
        self.created_at = time.time()

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–åˆ†å‰²ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        return {
            "chunk_count": self.chunk_count,
            "original_length": self.original_length,
            "total_chunks_length": self.total_chunks_length,
            "average_chunk_size": self.average_chunk_size,
            "min_chunk_size": self.min_chunk_size,
            "max_chunk_size": self.max_chunk_size,
            "overlap_ratio": self.overlap_ratio,
            "compression_ratio": self.original_length / self.total_chunks_length if self.total_chunks_length > 0 else 0,
            "splitter_config": self.splitter_config
        }

    def get_chunk_size_distribution(self) -> Dict[str, int]:
        """è·å–å—å¤§å°åˆ†å¸ƒã€‚"""
        distribution = {}
        ranges = [(0, 100), (100, 500), (500, 1000), (1000, 2000), (2000, float('inf'))]

        for start, end in ranges:
            range_key = f"{start}-{end if end != float('inf') else 'âˆ'}"
            count = sum(1 for size in self.chunk_sizes if start <= size < end)
            distribution[range_key] = count

        return distribution
```

---

## 3. åˆ†éš”ç¬¦ç­–ç•¥æ•°æ®ç»“æ„

### 3.1 SeparatorStrategy ç»“æ„

```python
class SeparatorStrategy:
    """åˆ†éš”ç¬¦ç­–ç•¥æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        separators: List[str],
        priorities: Optional[List[int]] = None,
        is_regex: bool = False,
        keep_separator: bool = False
    ):
        self.separators = separators            # åˆ†éš”ç¬¦åˆ—è¡¨
        self.priorities = priorities or list(range(len(separators)))  # ä¼˜å…ˆçº§
        self.is_regex = is_regex               # æ˜¯å¦ä¸ºæ­£åˆ™è¡¨è¾¾å¼
        self.keep_separator = keep_separator   # æ˜¯å¦ä¿ç•™åˆ†éš”ç¬¦

        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self.compiled_patterns = []
        if is_regex:
            import re
            for separator in separators:
                try:
                    pattern = re.compile(separator)
                    self.compiled_patterns.append(pattern)
                except re.error as e:
                    raise ValueError(f"æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼ '{separator}': {e}")

        # ä½¿ç”¨ç»Ÿè®¡
        self.usage_stats = {sep: 0 for sep in separators}
        self.total_splits = 0

    def find_best_separator(self, text: str) -> Optional[str]:
        """æ‰¾åˆ°æœ€é€‚åˆçš„åˆ†éš”ç¬¦ã€‚"""
        for i, separator in enumerate(self.separators):
            if self.is_regex:
                if self.compiled_patterns[i].search(text):
                    self.usage_stats[separator] += 1
                    return separator
            else:
                if separator in text:
                    self.usage_stats[separator] += 1
                    return separator
        return None

    def split_with_separator(self, text: str, separator: str) -> List[str]:
        """ä½¿ç”¨æŒ‡å®šåˆ†éš”ç¬¦åˆ†å‰²æ–‡æœ¬ã€‚"""
        if self.is_regex:
            import re
            pattern = next((p for p, s in zip(self.compiled_patterns, self.separators) if s == separator), None)
            if pattern:
                splits = pattern.split(text)
            else:
                splits = [text]
        else:
            splits = text.split(separator)

        # å¤„ç†åˆ†éš”ç¬¦ä¿ç•™
        if self.keep_separator and not self.is_regex:
            result = []
            for i, split in enumerate(splits[:-1]):
                result.append(split + separator)
            if splits:
                result.append(splits[-1])
            splits = result

        self.total_splits += len(splits) - 1
        return splits

    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ã€‚"""
        return {
            "separator_usage": self.usage_stats,
            "total_splits": self.total_splits,
            "most_used_separator": max(self.usage_stats.items(), key=lambda x: x[1])[0] if self.usage_stats else None
        }

# é¢„å®šä¹‰åˆ†éš”ç¬¦ç­–ç•¥
class DefaultSeparatorStrategies:
    """é»˜è®¤åˆ†éš”ç¬¦ç­–ç•¥é›†åˆã€‚"""

    GENERAL_TEXT = SeparatorStrategy([
        "\n\n",    # æ®µè½
        "\n",      # è¡Œ
        " ",       # ç©ºæ ¼
        "",        # å­—ç¬¦
    ])

    CHINESE_TEXT = SeparatorStrategy([
        "\n\n",    # æ®µè½
        "\n",      # è¡Œ
        "ã€‚",      # å¥å·
        "ï¼",      # æ„Ÿå¹å·
        "ï¼Ÿ",      # é—®å·
        "ï¼›",      # åˆ†å·
        " ",       # ç©ºæ ¼
        "",        # å­—ç¬¦
    ])

    CODE_TEXT = SeparatorStrategy([
        "\n\nclass ",     # ç±»å®šä¹‰
        "\n\ndef ",      # å‡½æ•°å®šä¹‰
        "\n\n",          # æ®µè½
        "\n",            # è¡Œ
        " ",             # ç©ºæ ¼
        "",              # å­—ç¬¦
    ])

    MARKDOWN_TEXT = SeparatorStrategy([
        "\n## ",         # äºŒçº§æ ‡é¢˜
        "\n### ",        # ä¸‰çº§æ ‡é¢˜
        "\n\n",          # æ®µè½
        "\n",            # è¡Œ
        " ",             # ç©ºæ ¼
        "",              # å­—ç¬¦
    ])
```

---

## 4. ä»¤ç‰Œå¤„ç†æ•°æ®ç»“æ„

### 4.1 TokenProcessor ç»“æ„

```python
class TokenProcessor:
    """ä»¤ç‰Œå¤„ç†å™¨æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        encoding_name: str = "gpt2",
        model_name: Optional[str] = None,
        allowed_special: Union[str, Set[str]] = set(),
        disallowed_special: Union[str, Set[str]] = "all"
    ):
        self.encoding_name = encoding_name
        self.model_name = model_name
        self.allowed_special = allowed_special
        self.disallowed_special = disallowed_special

        # åˆå§‹åŒ–tokenizer
        try:
            import tiktoken
            if model_name:
                self.tokenizer = tiktoken.encoding_for_model(model_name)
            else:
                self.tokenizer = tiktoken.get_encoding(encoding_name)
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£…tiktokenåº“: pip install tiktoken")
        except Exception as e:
            raise ValueError(f"æ— æ³•åŠ è½½tokenizer: {e}")

        # ä»¤ç‰Œç»Ÿè®¡
        self.token_stats = {
            "total_texts_processed": 0,
            "total_tokens_counted": 0,
            "total_chunks_created": 0,
            "average_tokens_per_text": 0.0,
            "average_tokens_per_chunk": 0.0
        }

        # ç¼“å­˜æœºåˆ¶
        self._token_cache: Dict[str, int] = {}
        self._cache_max_size = 1000

    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„ä»¤ç‰Œæ•°é‡ã€‚"""
        # æ£€æŸ¥ç¼“å­˜
        if text in self._token_cache:
            return self._token_cache[text]

        # è®¡ç®—ä»¤ç‰Œæ•°
        try:
            tokens = self.tokenizer.encode(
                text,
                allowed_special=self.allowed_special,
                disallowed_special=self.disallowed_special
            )
            token_count = len(tokens)
        except Exception as e:
            # å›é€€åˆ°å­—ç¬¦è®¡æ•°
            token_count = len(text) // 4  # ç²—ç•¥ä¼°è®¡

        # æ›´æ–°ç¼“å­˜
        if len(self._token_cache) < self._cache_max_size:
            self._token_cache[text] = token_count

        # æ›´æ–°ç»Ÿè®¡
        self.token_stats["total_tokens_counted"] += token_count

        return token_count

    def encode_text(self, text: str) -> List[int]:
        """ç¼–ç æ–‡æœ¬ä¸ºä»¤ç‰ŒIDåˆ—è¡¨ã€‚"""
        try:
            return self.tokenizer.encode(
                text,
                allowed_special=self.allowed_special,
                disallowed_special=self.disallowed_special
            )
        except Exception:
            return []

    def decode_tokens(self, tokens: List[int]) -> str:
        """è§£ç ä»¤ç‰ŒIDåˆ—è¡¨ä¸ºæ–‡æœ¬ã€‚"""
        try:
            return self.tokenizer.decode(tokens)
        except Exception:
            return ""

    def split_by_tokens(
        self,
        text: str,
        chunk_size: int,
        chunk_overlap: int = 0
    ) -> List[str]:
        """åŸºäºä»¤ç‰Œæ•°é‡åˆ†å‰²æ–‡æœ¬ã€‚"""
        # ç¼–ç æ•´ä¸ªæ–‡æœ¬
        tokens = self.encode_text(text)
        if not tokens:
            return [text]

        chunks = []
        start_idx = 0

        while start_idx < len(tokens):
            # ç¡®å®šå½“å‰å—çš„ç»“æŸä½ç½®
            end_idx = min(start_idx + chunk_size, len(tokens))

            # æå–å½“å‰å—çš„ä»¤ç‰Œ
            chunk_tokens = tokens[start_idx:end_idx]

            # è§£ç ä¸ºæ–‡æœ¬
            chunk_text = self.decode_tokens(chunk_tokens)
            if chunk_text.strip():  # åªæ·»åŠ éç©ºå—
                chunks.append(chunk_text)

            # è®¡ç®—ä¸‹ä¸€ä¸ªå—çš„èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            if end_idx >= len(tokens):
                break

            start_idx = end_idx - chunk_overlap
            if start_idx < 0:
                start_idx = end_idx

        # æ›´æ–°ç»Ÿè®¡
        self.token_stats["total_chunks_created"] += len(chunks)
        self.token_stats["total_texts_processed"] += 1

        if self.token_stats["total_texts_processed"] > 0:
            self.token_stats["average_tokens_per_text"] = (
                self.token_stats["total_tokens_counted"] /
                self.token_stats["total_texts_processed"]
            )

        if self.token_stats["total_chunks_created"] > 0:
            self.token_stats["average_tokens_per_chunk"] = (
                self.token_stats["total_tokens_counted"] /
                self.token_stats["total_chunks_created"]
            )

        return chunks

    def get_token_statistics(self) -> Dict[str, Any]:
        """è·å–ä»¤ç‰Œå¤„ç†ç»Ÿè®¡ã€‚"""
        return {
            **self.token_stats,
            "encoding_name": self.encoding_name,
            "model_name": self.model_name,
            "cache_size": len(self._token_cache),
            "cache_hit_ratio": len(self._token_cache) / max(self.token_stats["total_texts_processed"], 1)
        }
```

---

## 5. æ–‡æ¡£ç»“æ„æ•°æ®

### 5.1 DocumentChunk ç»“æ„

```python
class DocumentChunk:
    """æ–‡æ¡£å—æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        content: str,
        metadata: Dict[str, Any],
        chunk_index: int,
        start_index: Optional[int] = None,
        end_index: Optional[int] = None,
        parent_doc_id: Optional[str] = None
    ):
        self.content = content                  # å—å†…å®¹
        self.metadata = metadata.copy()        # å…ƒæ•°æ®å‰¯æœ¬
        self.chunk_index = chunk_index          # å—ç´¢å¼•
        self.start_index = start_index          # åœ¨åŸæ–‡ä¸­çš„èµ·å§‹ä½ç½®
        self.end_index = end_index              # åœ¨åŸæ–‡ä¸­çš„ç»“æŸä½ç½®
        self.parent_doc_id = parent_doc_id      # çˆ¶æ–‡æ¡£ID

        # è®¡ç®—å±æ€§
        self.length = len(content)
        self.word_count = len(content.split())
        self.line_count = content.count('\n') + 1

        # æ·»åŠ å—ç‰¹å®šçš„å…ƒæ•°æ®
        self.metadata.update({
            "chunk_index": chunk_index,
            "chunk_length": self.length,
            "word_count": self.word_count,
            "line_count": self.line_count
        })

        if start_index is not None:
            self.metadata["start_index"] = start_index
        if end_index is not None:
            self.metadata["end_index"] = end_index
        if parent_doc_id is not None:
            self.metadata["parent_doc_id"] = parent_doc_id

        # æ—¶é—´æˆ³
        self.created_at = time.time()

    def to_document(self) -> 'Document':
        """è½¬æ¢ä¸ºDocumentå¯¹è±¡ã€‚"""
        from langchain_core.documents import Document
        return Document(page_content=self.content, metadata=self.metadata)

    def get_preview(self, max_length: int = 100) -> str:
        """è·å–å†…å®¹é¢„è§ˆã€‚"""
        if len(self.content) <= max_length:
            return self.content
        return self.content[:max_length] + "..."

    def calculate_overlap_with(self, other: 'DocumentChunk') -> float:
        """è®¡ç®—ä¸å¦ä¸€ä¸ªå—çš„é‡å åº¦ã€‚"""
        if not self.start_index or not self.end_index or not other.start_index or not other.end_index:
            return 0.0

        # è®¡ç®—é‡å åŒºé—´
        overlap_start = max(self.start_index, other.start_index)
        overlap_end = min(self.end_index, other.end_index)

        if overlap_start >= overlap_end:
            return 0.0

        overlap_length = overlap_end - overlap_start
        min_length = min(self.length, other.length)

        return overlap_length / min_length if min_length > 0 else 0.0

class DocumentChunkCollection:
    """æ–‡æ¡£å—é›†åˆç®¡ç†å™¨ã€‚"""

    def __init__(self, source_document: Optional['Document'] = None):
        self.source_document = source_document
        self.chunks: List[DocumentChunk] = []
        self.chunk_index_map: Dict[int, DocumentChunk] = {}

        # é›†åˆç»Ÿè®¡
        self.total_length = 0
        self.total_word_count = 0
        self.average_chunk_size = 0.0
        self.overlap_statistics = []

    def add_chunk(self, chunk: DocumentChunk) -> None:
        """æ·»åŠ æ–‡æ¡£å—ã€‚"""
        self.chunks.append(chunk)
        self.chunk_index_map[chunk.chunk_index] = chunk

        # æ›´æ–°ç»Ÿè®¡
        self.total_length += chunk.length
        self.total_word_count += chunk.word_count
        self.average_chunk_size = self.total_length / len(self.chunks)

        # è®¡ç®—ä¸å‰ä¸€ä¸ªå—çš„é‡å 
        if len(self.chunks) > 1:
            previous_chunk = self.chunks[-2]
            overlap = chunk.calculate_overlap_with(previous_chunk)
            self.overlap_statistics.append(overlap)

    def get_chunk_by_index(self, index: int) -> Optional[DocumentChunk]:
        """æ ¹æ®ç´¢å¼•è·å–å—ã€‚"""
        return self.chunk_index_map.get(index)

    def get_chunks_in_range(self, start_index: int, end_index: int) -> List[DocumentChunk]:
        """è·å–æŒ‡å®šèŒƒå›´å†…çš„å—ã€‚"""
        return [chunk for chunk in self.chunks if start_index <= chunk.chunk_index <= end_index]

    def to_documents(self) -> List['Document']:
        """è½¬æ¢ä¸ºDocumentå¯¹è±¡åˆ—è¡¨ã€‚"""
        return [chunk.to_document() for chunk in self.chunks]

    def get_collection_statistics(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯ã€‚"""
        return {
            "chunk_count": len(self.chunks),
            "total_length": self.total_length,
            "total_word_count": self.total_word_count,
            "average_chunk_size": self.average_chunk_size,
            "min_chunk_size": min(chunk.length for chunk in self.chunks) if self.chunks else 0,
            "max_chunk_size": max(chunk.length for chunk in self.chunks) if self.chunks else 0,
            "average_overlap": sum(self.overlap_statistics) / len(self.overlap_statistics) if self.overlap_statistics else 0.0,
            "source_document_length": len(self.source_document.page_content) if self.source_document else 0
        }
```

---

## 6. ä¸“ç”¨åˆ†å‰²å™¨æ•°æ®ç»“æ„

### 6.1 MarkdownStructure ç»“æ„

```python
class MarkdownStructure:
    """Markdownæ–‡æ¡£ç»“æ„æ•°æ®ã€‚"""

    def __init__(self):
        self.headers: List[MarkdownHeader] = []
        self.sections: List[MarkdownSection] = []
        self.toc: Dict[str, Any] = {}  # ç›®å½•ç»“æ„

    def add_header(
        self,
        level: int,
        text: str,
        line_number: int,
        start_index: int
    ) -> 'MarkdownHeader':
        """æ·»åŠ æ ‡é¢˜ã€‚"""
        header = MarkdownHeader(level, text, line_number, start_index)
        self.headers.append(header)
        return header

    def build_toc(self) -> Dict[str, Any]:
        """æ„å»ºç›®å½•ç»“æ„ã€‚"""
        toc = {}
        stack = [toc]

        for header in self.headers:
            # è°ƒæ•´æ ˆæ·±åº¦åˆ°å½“å‰æ ‡é¢˜çº§åˆ«
            while len(stack) > header.level:
                stack.pop()

            # åˆ›å»ºå½“å‰æ ‡é¢˜èŠ‚ç‚¹
            header_node = {
                "text": header.text,
                "level": header.level,
                "line_number": header.line_number,
                "start_index": header.start_index,
                "children": {}
            }

            # æ·»åŠ åˆ°å½“å‰å±‚çº§
            current_level = stack[-1]
            current_level[header.text] = header_node

            # å°†childrenä½œä¸ºä¸‹ä¸€å±‚çº§
            stack.append(header_node["children"])

        self.toc = toc
        return toc

class MarkdownHeader:
    """Markdownæ ‡é¢˜æ•°æ®ç»“æ„ã€‚"""

    def __init__(self, level: int, text: str, line_number: int, start_index: int):
        self.level = level              # æ ‡é¢˜çº§åˆ« (1-6)
        self.text = text               # æ ‡é¢˜æ–‡æœ¬
        self.line_number = line_number # è¡Œå·
        self.start_index = start_index # å­—ç¬¦ç´¢å¼•
        self.children: List['MarkdownHeader'] = []  # å­æ ‡é¢˜
        self.content_start = start_index + len(f"{'#' * level} {text}")
        self.content_end: Optional[int] = None

    def add_child(self, child: 'MarkdownHeader') -> None:
        """æ·»åŠ å­æ ‡é¢˜ã€‚"""
        self.children.append(child)

    def get_hierarchy_path(self) -> List[str]:
        """è·å–å±‚æ¬¡è·¯å¾„ã€‚"""
        # è¿™éœ€è¦åœ¨æ„å»ºæ—¶è®¾ç½®çˆ¶å­å…³ç³»
        path = [self.text]
        return path

class MarkdownSection:
    """Markdownæ®µè½æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        content: str,
        header: Optional[MarkdownHeader],
        start_index: int,
        end_index: int
    ):
        self.content = content          # æ®µè½å†…å®¹
        self.header = header           # æ‰€å±æ ‡é¢˜
        self.start_index = start_index # èµ·å§‹ä½ç½®
        self.end_index = end_index     # ç»“æŸä½ç½®
        self.metadata = self._build_metadata()

    def _build_metadata(self) -> Dict[str, Any]:
        """æ„å»ºå…ƒæ•°æ®ã€‚"""
        metadata = {
            "start_index": self.start_index,
            "end_index": self.end_index,
            "length": self.end_index - self.start_index
        }

        if self.header:
            metadata.update({
                f"Header {self.header.level}": self.header.text,
                "header_level": self.header.level,
                "header_text": self.header.text
            })

            # æ·»åŠ å±‚æ¬¡è·¯å¾„
            hierarchy_path = self.header.get_hierarchy_path()
            for i, header_text in enumerate(hierarchy_path):
                metadata[f"Header {i+1}"] = header_text

        return metadata
```

---

### 6.2 CodeStructure ç»“æ„

```python
class CodeStructure:
    """ä»£ç ç»“æ„æ•°æ®ã€‚"""

    def __init__(self, language: str = "python"):
        self.language = language
        self.classes: List[CodeClass] = []
        self.functions: List[CodeFunction] = []
        self.imports: List[CodeImport] = []
        self.comments: List[CodeComment] = []
        self.separators = self._get_language_separators()

    def _get_language_separators(self) -> List[str]:
        """è·å–è¯­è¨€ç‰¹å®šçš„åˆ†éš”ç¬¦ã€‚"""
        separators_map = {
            "python": [
                "\n\nclass ",
                "\n\ndef ",
                "\n\nasync def ",
                "\n\n",
                "\n",
                " ",
                ""
            ],
            "javascript": [
                "\n\nclass ",
                "\n\nfunction ",
                "\n\nconst ",
                "\n\nlet ",
                "\n\n",
                "\n",
                " ",
                ""
            ],
            "java": [
                "\n\npublic class ",
                "\n\nprivate class ",
                "\n\npublic ",
                "\n\nprivate ",
                "\n\n",
                "\n",
                " ",
                ""
            ]
        }
        return separators_map.get(self.language, separators_map["python"])

class CodeClass:
    """ä»£ç ç±»æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        name: str,
        start_line: int,
        end_line: int,
        docstring: Optional[str] = None
    ):
        self.name = name
        self.start_line = start_line
        self.end_line = end_line
        self.docstring = docstring
        self.methods: List[CodeFunction] = []
        self.attributes: List[str] = []

    def add_method(self, method: 'CodeFunction') -> None:
        """æ·»åŠ æ–¹æ³•ã€‚"""
        self.methods.append(method)

    def get_metadata(self) -> Dict[str, Any]:
        """è·å–ç±»å…ƒæ•°æ®ã€‚"""
        return {
            "type": "class",
            "name": self.name,
            "start_line": self.start_line,
            "end_line": self.end_line,
            "method_count": len(self.methods),
            "has_docstring": self.docstring is not None
        }

class CodeFunction:
    """ä»£ç å‡½æ•°æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        name: str,
        start_line: int,
        end_line: int,
        parameters: List[str],
        docstring: Optional[str] = None,
        is_async: bool = False
    ):
        self.name = name
        self.start_line = start_line
        self.end_line = end_line
        self.parameters = parameters
        self.docstring = docstring
        self.is_async = is_async

    def get_metadata(self) -> Dict[str, Any]:
        """è·å–å‡½æ•°å…ƒæ•°æ®ã€‚"""
        return {
            "type": "function",
            "name": self.name,
            "start_line": self.start_line,
            "end_line": self.end_line,
            "parameter_count": len(self.parameters),
            "parameters": self.parameters,
            "has_docstring": self.docstring is not None,
            "is_async": self.is_async
        }

class CodeImport:
    """ä»£ç å¯¼å…¥æ•°æ®ç»“æ„ã€‚"""

    def __init__(self, module: str, items: List[str], line_number: int):
        self.module = module
        self.items = items
        self.line_number = line_number

    def get_metadata(self) -> Dict[str, Any]:
        """è·å–å¯¼å…¥å…ƒæ•°æ®ã€‚"""
        return {
            "type": "import",
            "module": self.module,
            "items": self.items,
            "line_number": self.line_number
        }
```

---

## 7. æ€§èƒ½ç›‘æ§æ•°æ®ç»“æ„

### 7.1 SplitterMetrics ç»“æ„

```python
class SplitterMetrics:
    """åˆ†å‰²å™¨æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„ã€‚"""

    def __init__(self, splitter_type: str):
        self.splitter_type = splitter_type

        # åŸºç¡€æŒ‡æ ‡
        self.split_count = 0
        self.total_input_length = 0
        self.total_output_chunks = 0
        self.total_processing_time = 0.0

        # æ€§èƒ½æŒ‡æ ‡
        self.processing_times: List[float] = []
        self.chunk_counts: List[int] = []
        self.input_lengths: List[int] = []
        self.chunk_sizes: List[List[int]] = []

        # é”™è¯¯ç»Ÿè®¡
        self.error_count = 0
        self.error_types: Dict[str, int] = defaultdict(int)

        # ç¼“å­˜ç»Ÿè®¡
        self.cache_hits = 0
        self.cache_misses = 0

    def record_split_operation(
        self,
        input_length: int,
        output_chunks: int,
        processing_time: float,
        chunk_sizes: List[int],
        error: Optional[Exception] = None
    ) -> None:
        """è®°å½•åˆ†å‰²æ“ä½œã€‚"""
        self.split_count += 1
        self.total_input_length += input_length
        self.total_processing_time += processing_time

        if error:
            self.error_count += 1
            self.error_types[type(error).__name__] += 1
        else:
            self.total_output_chunks += output_chunks
            self.processing_times.append(processing_time)
            self.chunk_counts.append(output_chunks)
            self.input_lengths.append(input_length)
            self.chunk_sizes.append(chunk_sizes)

    def record_cache_operation(self, hit: bool) -> None:
        """è®°å½•ç¼“å­˜æ“ä½œã€‚"""
        if hit:
            self.cache_hits += 1
        else:
            self.cache_misses += 1

    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦ã€‚"""
        if self.split_count == 0:
            return {"no_data": True}

        successful_splits = self.split_count - self.error_count

        return {
            "splitter_type": self.splitter_type,
            "total_operations": self.split_count,
            "successful_operations": successful_splits,
            "error_rate": self.error_count / self.split_count,
            "average_processing_time": (
                sum(self.processing_times) / len(self.processing_times)
                if self.processing_times else 0
            ),
            "throughput_chars_per_second": (
                self.total_input_length / self.total_processing_time
                if self.total_processing_time > 0 else 0
            ),
            "average_chunks_per_text": (
                sum(self.chunk_counts) / len(self.chunk_counts)
                if self.chunk_counts else 0
            ),
            "average_chunk_size": (
                sum(sum(sizes) for sizes in self.chunk_sizes) /
                sum(len(sizes) for sizes in self.chunk_sizes)
                if self.chunk_sizes else 0
            ),
            "cache_hit_rate": (
                self.cache_hits / (self.cache_hits + self.cache_misses)
                if (self.cache_hits + self.cache_misses) > 0 else 0
            ),
            "error_breakdown": dict(self.error_types)
        }

    def get_percentile_stats(self) -> Dict[str, float]:
        """è·å–ç™¾åˆ†ä½ç»Ÿè®¡ã€‚"""
        if not self.processing_times:
            return {}

        sorted_times = sorted(self.processing_times)
        n = len(sorted_times)

        return {
            "p50_processing_time": sorted_times[n // 2],
            "p90_processing_time": sorted_times[int(0.9 * n)],
            "p95_processing_time": sorted_times[int(0.95 * n)],
            "p99_processing_time": sorted_times[int(0.99 * n)]
        }
```

---

## 8. é…ç½®ç®¡ç†æ•°æ®ç»“æ„

### 8.1 SplitterConfig ç»“æ„

```python
class SplitterConfig:
    """åˆ†å‰²å™¨é…ç½®ç®¡ç†æ•°æ®ç»“æ„ã€‚"""

    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        separators: Optional[List[str]] = None,
        length_function: str = "len",
        keep_separator: bool = False,
        add_start_index: bool = False,
        strip_whitespace: bool = True,
        custom_params: Optional[Dict[str, Any]] = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ["\n\n", "\n", " ", ""]
        self.length_function = length_function
        self.keep_separator = keep_separator
        self.add_start_index = add_start_index
        self.strip_whitespace = strip_whitespace
        self.custom_params = custom_params or {}

        # é…ç½®éªŒè¯
        self.validate()

        # é…ç½®å…ƒæ•°æ®
        self.created_at = time.time()
        self.config_id = self._generate_config_id()
        self.version = "1.0"

    def validate(self) -> None:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§ã€‚"""
        if self.chunk_size <= 0:
            raise ValueError("chunk_sizeå¿…é¡»å¤§äº0")

        if self.chunk_overlap < 0:
            raise ValueError("chunk_overlapä¸èƒ½å°äº0")

        if self.chunk_overlap >= self.chunk_size:
            raise ValueError("chunk_overlapä¸èƒ½å¤§äºç­‰äºchunk_size")

        if not self.separators:
            raise ValueError("separatorsä¸èƒ½ä¸ºç©º")

    def _generate_config_id(self) -> str:
        """ç”Ÿæˆé…ç½®IDã€‚"""
        import hashlib
        config_str = f"{self.chunk_size}_{self.chunk_overlap}_{self.separators}_{self.length_function}"
        return hashlib.md5(config_str.encode()).hexdigest()[:8]

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸ã€‚"""
        return {
            "chunk_size": self.chunk_size,
            "chunk_overlap": self.chunk_overlap,
            "separators": self.separators,
            "length_function": self.length_function,
            "keep_separator": self.keep_separator,
            "add_start_index": self.add_start_index,
            "strip_whitespace": self.strip_whitespace,
            "custom_params": self.custom_params,
            "config_id": self.config_id,
            "version": self.version,
            "created_at": self.created_at
        }

    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'SplitterConfig':
        """ä»å­—å…¸åˆ›å»ºé…ç½®ã€‚"""
        return cls(
            chunk_size=config_dict.get("chunk_size", 1000),
            chunk_overlap=config_dict.get("chunk_overlap", 200),
            separators=config_dict.get("separators"),
            length_function=config_dict.get("length_function", "len"),
            keep_separator=config_dict.get("keep_separator", False),
            add_start_index=config_dict.get("add_start_index", False),
            strip_whitespace=config_dict.get("strip_whitespace", True),
            custom_params=config_dict.get("custom_params", {})
        )

    def copy(self, **overrides) -> 'SplitterConfig':
        """åˆ›å»ºé…ç½®å‰¯æœ¬å¹¶åº”ç”¨è¦†ç›–ã€‚"""
        config_dict = self.to_dict()
        config_dict.update(overrides)
        return self.from_dict(config_dict)

class ConfigRegistry:
    """é…ç½®æ³¨å†Œè¡¨ã€‚"""

    def __init__(self):
        self._configs: Dict[str, SplitterConfig] = {}
        self._default_configs = self._load_default_configs()

    def _load_default_configs(self) -> Dict[str, SplitterConfig]:
        """åŠ è½½é»˜è®¤é…ç½®ã€‚"""
        return {
            "general": SplitterConfig(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", " ", ""]
            ),
            "code": SplitterConfig(
                chunk_size=800,
                chunk_overlap=100,
                separators=["\n\nclass ", "\n\ndef ", "\n\n", "\n", " ", ""]
            ),
            "markdown": SplitterConfig(
                chunk_size=1200,
                chunk_overlap=200,
                separators=["\n## ", "\n### ", "\n\n", "\n", " ", ""]
            ),
            "chinese": SplitterConfig(
                chunk_size=800,
                chunk_overlap=150,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
            )
        }

    def register_config(self, name: str, config: SplitterConfig) -> None:
        """æ³¨å†Œé…ç½®ã€‚"""
        self._configs[name] = config

    def get_config(self, name: str) -> Optional[SplitterConfig]:
        """è·å–é…ç½®ã€‚"""
        return self._configs.get(name) or self._default_configs.get(name)

    def list_configs(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é…ç½®åç§°ã€‚"""
        all_configs = set(self._configs.keys()) | set(self._default_configs.keys())
        return sorted(all_configs)
```

---

## 9. æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº† **Text Splitters æ¨¡å—**çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼š

1. **ç±»å±‚æ¬¡ç»“æ„**ï¼šä»TextSplitteråŸºç±»åˆ°å„ç§ä¸“ç”¨åˆ†å‰²å™¨çš„å®Œæ•´ç»§æ‰¿å…³ç³»
2. **åˆ†å‰²é…ç½®**ï¼šåˆ†å‰²å™¨çš„å‚æ•°é…ç½®å’Œç»“æœæ•°æ®ç»“æ„
3. **åˆ†éš”ç¬¦ç­–ç•¥**ï¼šå¤šç§åˆ†éš”ç¬¦çš„ä¼˜å…ˆçº§å’Œä½¿ç”¨ç­–ç•¥
4. **ä»¤ç‰Œå¤„ç†**ï¼šåŸºäºä»¤ç‰Œçš„ç²¾ç¡®åˆ†å‰²å’Œç»Ÿè®¡æœºåˆ¶
5. **æ–‡æ¡£ç»“æ„**ï¼šæ–‡æ¡£å—çš„ç®¡ç†å’Œå…ƒæ•°æ®å¤„ç†
6. **ä¸“ç”¨ç»“æ„**ï¼šMarkdownã€ä»£ç ç­‰ç‰¹æ®Šæ ¼å¼çš„ç»“æ„åŒ–æ•°æ®
7. **æ€§èƒ½ç›‘æ§**ï¼šåˆ†å‰²å™¨æ‰§è¡Œçš„æŒ‡æ ‡æ”¶é›†å’Œåˆ†æ
8. **é…ç½®ç®¡ç†**ï¼šé…ç½®çš„æ³¨å†Œã€éªŒè¯å’Œç‰ˆæœ¬ç®¡ç†

æ‰€æœ‰æ•°æ®ç»“æ„å‡åŒ…å«ï¼š

- å®Œæ•´çš„å­—æ®µå®šä¹‰å’Œç±»å‹è¯´æ˜
- è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯å’Œç›‘æ§æœºåˆ¶
- æ€§èƒ½ä¼˜åŒ–ç­–ç•¥å’Œç¼“å­˜æœºåˆ¶
- é…ç½®éªŒè¯å’Œé”™è¯¯å¤„ç†
- å¯æ‰©å±•çš„æ¶æ„è®¾è®¡

è¿™äº›ç»“æ„ä¸ºæ„å»ºé«˜æ•ˆã€å¯é çš„æ–‡æœ¬åˆ†å‰²ç³»ç»Ÿæä¾›äº†å®Œæ•´çš„æ•°æ®æ¨¡å‹åŸºç¡€ï¼Œæ”¯æŒä»ç®€å•å­—ç¬¦åˆ†å‰²åˆ°å¤æ‚ç»“æ„åŒ–æ–‡æ¡£å¤„ç†çš„å„ç§éœ€æ±‚ã€‚

---

## æ—¶åºå›¾

## æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£é€šè¿‡è¯¦ç»†çš„æ—¶åºå›¾å±•ç¤º **Text Splitters æ¨¡å—**åœ¨å„ç§åœºæ™¯ä¸‹çš„æ‰§è¡Œæµç¨‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†å‰²ã€å—ç®¡ç†ã€åˆ†éš”ç¬¦é€‰æ‹©ã€ä»¤ç‰Œå¤„ç†ã€æ–‡æ¡£ç»“æ„åŒ–ç­‰å¤æ‚äº¤äº’è¿‡ç¨‹ã€‚

---

## 1. åŸºç¡€åˆ†å‰²åœºæ™¯

### 1.1 CharacterTextSplitter åŸºç¡€åˆ†å‰²æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Splitter as CharacterTextSplitter
    participant Validator as ConfigValidator
    participant Processor as TextProcessor
    participant Merger as ChunkMerger

    User->>Splitter: split_text("æ®µè½1\n\næ®µè½2\n\næ®µè½3")

    Splitter->>Validator: éªŒè¯é…ç½®å‚æ•°<br/>chunk_size=200, chunk_overlap=50, separator="\n\n"

    Validator->>Validator: æ£€æŸ¥å‚æ•°æœ‰æ•ˆæ€§<br/>chunk_overlap < chunk_size âœ“<br/>separatorä¸ä¸ºç©º âœ“

    Validator-->>Splitter: é…ç½®éªŒè¯é€šè¿‡

    Splitter->>Processor: æŒ‰åˆ†éš”ç¬¦åˆ†å‰²æ–‡æœ¬<br/>text.split("\n\n")

    Processor->>Processor: æ‰§è¡Œæ–‡æœ¬åˆ†å‰²<br/>splits = ["æ®µè½1", "æ®µè½2", "æ®µè½3"]

    alt keep_separator = True
        Processor->>Processor: é‡æ–°æ·»åŠ åˆ†éš”ç¬¦<br/>["æ®µè½1\n\n", "æ®µè½2\n\n", "æ®µè½3"]
    end

    Processor-->>Splitter: åˆå§‹åˆ†å‰²ç»“æœ<br/>splits = ["æ®µè½1", "æ®µè½2", "æ®µè½3"]

    Splitter->>Merger: åˆå¹¶å°å—å¹¶å¤„ç†é‡å <br/>_merge_splits(splits, separator)

    Merger->>Merger: æ£€æŸ¥æ¯ä¸ªåˆ†å‰²å—çš„é•¿åº¦<br/>length_function(split) for split in splits

    loop å¤„ç†æ¯ä¸ªåˆ†å‰²å—
        Merger->>Merger: å½“å‰å—é•¿åº¦ = len("æ®µè½1") = 3

        alt å—é•¿åº¦ < chunk_size
            Merger->>Merger: å—å¤§å°åˆé€‚ï¼Œä¿ç•™
        else å—é•¿åº¦ >= chunk_size
            Merger->>Merger: å—è¿‡å¤§ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²<br/>ï¼ˆCharacterTextSplitteræ— é€’å½’å¤„ç†ï¼‰
        end
    end

    Merger->>Merger: åˆå¹¶ç›¸é‚»å°å—<br/>ç¡®ä¿å—å¤§å°æ¥è¿‘chunk_size<br/>åŒæ—¶å¤„ç†chunk_overlapé‡å 

    Merger-->>Splitter: æœ€ç»ˆåˆå¹¶ç»“æœ<br/>["æ®µè½1", "æ®µè½2", "æ®µè½3"]

    Splitter->>Splitter: åº”ç”¨åå¤„ç†<br/>strip_whitespace, add_start_index

    Splitter-->>User: åˆ†å‰²ç»“æœ<br/>["æ®µè½1", "æ®µè½2", "æ®µè½3"]
```

**å…³é”®æ­¥éª¤è¯´æ˜**ï¼š

1. **é…ç½®éªŒè¯**ï¼ˆæ­¥éª¤ 2-4ï¼‰ï¼š
   - æ£€æŸ¥chunk_sizeå’Œchunk_overlapçš„åˆç†æ€§
   - éªŒè¯åˆ†éš”ç¬¦çš„æœ‰æ•ˆæ€§
   - ç¡®ä¿length_functionå¯è°ƒç”¨

2. **æ–‡æœ¬åˆ†å‰²**ï¼ˆæ­¥éª¤ 5-8ï¼‰ï¼š
   - ä½¿ç”¨æŒ‡å®šåˆ†éš”ç¬¦åˆ†å‰²æ–‡æœ¬
   - å¯é€‰æ‹©ä¿ç•™åˆ†éš”ç¬¦åœ¨ç»“æœä¸­
   - å¤„ç†æ­£åˆ™è¡¨è¾¾å¼åˆ†éš”ç¬¦

3. **å—åˆå¹¶**ï¼ˆæ­¥éª¤ 9-16ï¼‰ï¼š
   - æ£€æŸ¥æ¯ä¸ªå—çš„é•¿åº¦
   - åˆå¹¶è¿‡å°çš„ç›¸é‚»å—
   - å¤„ç†å—é—´é‡å 

---

### 1.2 RecursiveCharacterTextSplitter é€’å½’åˆ†å‰²æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Splitter as RecursiveCharacterTextSplitter
    participant Strategy as SeparatorStrategy
    participant Processor as RecursiveProcessor
    participant Merger as ChunkMerger

    User->>Splitter: split_text("# æ ‡é¢˜\n\næ®µè½å†…å®¹...\n\nå¦ä¸€ä¸ªå¾ˆé•¿çš„æ®µè½...")

    Splitter->>Strategy: è·å–åˆ†éš”ç¬¦ä¼˜å…ˆçº§åˆ—è¡¨<br/>separators = ["\n\n", "\n", " ", ""]

    Strategy-->>Splitter: åˆ†éš”ç¬¦ç­–ç•¥å·²å‡†å¤‡

    Splitter->>Processor: å¯åŠ¨é€’å½’åˆ†å‰²<br/>_recursive_split(text, separators)

    Processor->>Processor: é€‰æ‹©æœ€ä½³åˆ†éš”ç¬¦<br/>éå†separatorsæ‰¾åˆ°ç¬¬ä¸€ä¸ªå­˜åœ¨äºæ–‡æœ¬ä¸­çš„

    Processor->>Processor: æ‰¾åˆ°åˆ†éš”ç¬¦: "\n\n"<br/>å› ä¸ºæ–‡æœ¬åŒ…å«æ®µè½åˆ†éš”ç¬¦

    Processor->>Processor: ä½¿ç”¨"\n\n"åˆ†å‰²æ–‡æœ¬<br/>splits = ["# æ ‡é¢˜", "æ®µè½å†…å®¹...", "å¦ä¸€ä¸ªå¾ˆé•¿çš„æ®µè½..."]

    loop å¤„ç†æ¯ä¸ªåˆ†å‰²å—
        Processor->>Processor: æ£€æŸ¥å—å¤§å°<br/>current_split = "å¦ä¸€ä¸ªå¾ˆé•¿çš„æ®µè½..."

        alt å—å¤§å° <= chunk_size
            Processor->>Processor: å—å¤§å°åˆé€‚<br/>æ·»åŠ åˆ°good_splits
        else å—å¤§å° > chunk_size
            Processor->>Processor: å—è¿‡å¤§ï¼Œéœ€è¦é€’å½’å¤„ç†<br/>remaining_separators = ["\n", " ", ""]

            alt è¿˜æœ‰å‰©ä½™åˆ†éš”ç¬¦
                Processor->>Processor: é€’å½’è°ƒç”¨<br/>_recursive_split(oversized_split, remaining_separators)

                Processor->>Processor: ä½¿ç”¨ä¸‹ä¸€ä¸ªåˆ†éš”ç¬¦"\n"<br/>è¿›ä¸€æ­¥åˆ†å‰²è¿‡å¤§çš„å—

                Processor->>Processor: å¦‚æœä»ç„¶è¿‡å¤§ï¼Œç»§ç»­ä½¿ç”¨" "åˆ†éš”ç¬¦<br/>æœ€ç»ˆä½¿ç”¨""ï¼ˆå­—ç¬¦çº§åˆ†å‰²ï¼‰

            else æ— å‰©ä½™åˆ†éš”ç¬¦
                Processor->>Processor: å¼ºåˆ¶æ·»åŠ è¿‡å¤§å—<br/>ï¼ˆæ— æ³•è¿›ä¸€æ­¥åˆ†å‰²ï¼‰
            end
        end
    end

    Processor->>Merger: åˆå¹¶good_splits<br/>_merge_splits(good_splits, current_separator)

    Merger->>Merger: æ™ºèƒ½åˆå¹¶ç›¸é‚»å—<br/>è€ƒè™‘chunk_sizeå’Œchunk_overlap

    Merger-->>Processor: å½“å‰å±‚çº§åˆå¹¶ç»“æœ

    Processor->>Processor: æ”¶é›†æ‰€æœ‰å±‚çº§çš„ç»“æœ<br/>final_chunks.extend(merged_chunks)

    Processor-->>Splitter: é€’å½’åˆ†å‰²å®Œæˆ<br/>æ‰€æœ‰å—éƒ½æ»¡è¶³å¤§å°è¦æ±‚

    Splitter-->>User: æœ€ç»ˆåˆ†å‰²ç»“æœ<br/>["# æ ‡é¢˜", "æ®µè½å†…å®¹...", "åˆ†å‰²åçš„å—1", "åˆ†å‰²åçš„å—2", ...]
```

**é€’å½’ç®—æ³•æ ¸å¿ƒ**ï¼š

```python
def _recursive_split_logic(text: str, separators: List[str]) -> List[str]:
    """é€’å½’åˆ†å‰²é€»è¾‘ä¼ªä»£ç ã€‚"""

    # 1. é€‰æ‹©åˆ†éš”ç¬¦
    separator = None
    for sep in separators:
        if sep in text:
            separator = sep
            remaining_separators = separators[separators.index(sep) + 1:]
            break

    if separator is None:
        return [text]  # æ— æ³•åˆ†å‰²

    # 2. åˆ†å‰²æ–‡æœ¬
    splits = text.split(separator)

    # 3. å¤„ç†æ¯ä¸ªåˆ†å‰²å—
    final_chunks = []
    good_splits = []

    for split in splits:
        if len(split) <= chunk_size:
            good_splits.append(split)
        else:
            # å…ˆå¤„ç†å·²æ”¶é›†çš„å¥½å—
            if good_splits:
                merged = merge_splits(good_splits, separator)
                final_chunks.extend(merged)
                good_splits = []

            # é€’å½’å¤„ç†è¿‡å¤§çš„å—
            if remaining_separators:
                recursive_result = _recursive_split_logic(split, remaining_separators)
                final_chunks.extend(recursive_result)
            else:
                final_chunks.append(split)  # æ— æ³•è¿›ä¸€æ­¥åˆ†å‰²

    # å¤„ç†å‰©ä½™çš„å¥½å—
    if good_splits:
        merged = merge_splits(good_splits, separator)
        final_chunks.extend(merged)

    return final_chunks
```

---

## 2. ä»¤ç‰Œåˆ†å‰²åœºæ™¯

### 2.1 TokenTextSplitter ä»¤ç‰Œå¤„ç†æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Splitter as TokenTextSplitter
    participant Tokenizer as TikTokenEncoder
    participant Calculator as TokenCalculator
    participant Decoder as TokenDecoder
    participant Cache as TokenCache

    User->>Splitter: split_text("è¿™æ˜¯ä¸€ä¸ªéœ€è¦åˆ†å‰²çš„é•¿æ–‡æœ¬...")

    Splitter->>Cache: æ£€æŸ¥æ–‡æœ¬æ˜¯å¦å·²ç¼“å­˜<br/>cache_key = hash(text)

    alt ç¼“å­˜å‘½ä¸­
        Cache-->>Splitter: è¿”å›ç¼“å­˜çš„ä»¤ç‰Œæ•°æ®<br/>tokens = [cached_tokens]
    else ç¼“å­˜æœªå‘½ä¸­
        Splitter->>Tokenizer: ç¼–ç æ–‡æœ¬ä¸ºä»¤ç‰Œ<br/>encode(text, allowed_special, disallowed_special)

        Tokenizer->>Tokenizer: ä½¿ç”¨cl100k_baseç¼–ç å™¨<br/>å¤„ç†ä¸­æ–‡å’Œè‹±æ–‡æ··åˆæ–‡æœ¬

        Tokenizer-->>Splitter: ä»¤ç‰ŒIDåˆ—è¡¨<br/>tokens = [123, 456, 789, ...]

        Splitter->>Cache: ç¼“å­˜ä»¤ç‰Œç»“æœ<br/>put(cache_key, tokens)
    end

    Splitter->>Calculator: è®¡ç®—åˆ†å‰²å‚æ•°<br/>total_tokens = len(tokens)<br/>chunk_size = 100, chunk_overlap = 20

    Calculator->>Calculator: è®¡ç®—åˆ†å‰²ç‚¹<br/>start_idx = 0<br/>end_idx = min(100, total_tokens)

    loop åˆ†å‰²ä»¤ç‰Œåºåˆ—
        Calculator->>Calculator: æå–å½“å‰å—çš„ä»¤ç‰Œ<br/>chunk_tokens = tokens[start_idx:end_idx]

        Calculator->>Decoder: è§£ç ä»¤ç‰Œä¸ºæ–‡æœ¬<br/>decode(chunk_tokens)

        Decoder->>Decoder: å°†ä»¤ç‰ŒIDè½¬æ¢å›æ–‡æœ¬<br/>å¤„ç†ç‰¹æ®Šä»¤ç‰Œå’Œç¼–ç é—®é¢˜

        Decoder-->>Calculator: è§£ç åçš„æ–‡æœ¬å—<br/>chunk_text = "è¿™æ˜¯ä¸€ä¸ªéœ€è¦..."

        Calculator->>Calculator: éªŒè¯æ–‡æœ¬å—è´¨é‡<br/>æ£€æŸ¥æ˜¯å¦ä¸ºç©ºæˆ–åªæœ‰ç©ºç™½

        alt æ–‡æœ¬å—æœ‰æ•ˆ
            Calculator->>Calculator: æ·»åŠ åˆ°ç»“æœåˆ—è¡¨<br/>chunks.append(chunk_text)
        else æ–‡æœ¬å—æ— æ•ˆ
            Calculator->>Calculator: è·³è¿‡æ— æ•ˆå—
        end

        Calculator->>Calculator: è®¡ç®—ä¸‹ä¸€ä¸ªå—çš„èµ·å§‹ä½ç½®<br/>start_idx = end_idx - chunk_overlap<br/>end_idx = min(start_idx + chunk_size, total_tokens)

        alt è¿˜æœ‰å‰©ä½™ä»¤ç‰Œ
            Calculator->>Calculator: ç»§ç»­ä¸‹ä¸€è½®åˆ†å‰²
        else æ‰€æœ‰ä»¤ç‰Œå·²å¤„ç†
            Calculator->>Calculator: é€€å‡ºå¾ªç¯
        end
    end

    Calculator-->>Splitter: åˆ†å‰²å®Œæˆ<br/>chunks = ["å—1", "å—2", "å—3", ...]

    Splitter->>Splitter: æ›´æ–°ç»Ÿè®¡ä¿¡æ¯<br/>total_texts_processed++<br/>total_tokens_counted += len(tokens)

    Splitter-->>User: ä»¤ç‰Œåˆ†å‰²ç»“æœ<br/>æ¯ä¸ªå—éƒ½ç²¾ç¡®æ§åˆ¶åœ¨ä»¤ç‰Œé™åˆ¶å†…
```

**ä»¤ç‰Œå¤„ç†ç‰¹ç‚¹**ï¼š

1. **ç²¾ç¡®æ§åˆ¶**ï¼šæ¯ä¸ªå—çš„ä»¤ç‰Œæ•°é‡ä¸¥æ ¼æ§åˆ¶åœ¨æŒ‡å®šèŒƒå›´å†…
2. **ç¼–ç æ„ŸçŸ¥**ï¼šç†è§£ä¸åŒæ¨¡å‹çš„ä»¤ç‰Œç¼–ç å·®å¼‚
3. **ç¼“å­˜ä¼˜åŒ–**ï¼šç¼“å­˜ä»¤ç‰ŒåŒ–ç»“æœï¼Œæé«˜é‡å¤å¤„ç†æ•ˆç‡
4. **é‡å å¤„ç†**ï¼šåœ¨ä»¤ç‰Œçº§åˆ«å¤„ç†å—é—´é‡å 

---

## 3. ä¸“ç”¨åˆ†å‰²åœºæ™¯

### 3.1 MarkdownHeaderTextSplitter ç»“æ„åŒ–åˆ†å‰²æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Splitter as MarkdownHeaderTextSplitter
    participant Parser as MarkdownParser
    participant Structure as DocumentStructure
    participant Builder as DocumentBuilder

    User->>Splitter: split_text("# ä¸»æ ‡é¢˜\n\nå†…å®¹1\n\n## äºŒçº§æ ‡é¢˜\n\nå†…å®¹2\n\n### ä¸‰çº§æ ‡é¢˜\n\nå†…å®¹3")

    Splitter->>Parser: è§£æMarkdownç»“æ„<br/>headers_to_split_on = [("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")]

    Parser->>Parser: é€è¡Œæ‰«ææ–‡æœ¬<br/>è¯†åˆ«æ ‡é¢˜è¡Œå’Œå†…å®¹è¡Œ

    loop å¤„ç†æ¯ä¸€è¡Œ
        Parser->>Parser: æ£€æŸ¥è¡Œæ˜¯å¦ä¸ºæ ‡é¢˜<br/>line.startswith("#")

        alt æ˜¯æ ‡é¢˜è¡Œ
            Parser->>Structure: åˆ›å»ºæ ‡é¢˜å¯¹è±¡<br/>header = Header(level=1, text="ä¸»æ ‡é¢˜", line_num=1)

            Structure->>Structure: æ·»åŠ åˆ°æ ‡é¢˜å±‚æ¬¡ç»“æ„<br/>headers.append(header)

            Structure->>Structure: æ›´æ–°å½“å‰ä¸Šä¸‹æ–‡<br/>current_header = header

        else æ˜¯å†…å®¹è¡Œ
            Parser->>Structure: æ·»åŠ å†…å®¹åˆ°å½“å‰æ®µè½<br/>current_section.content += line
        end
    end

    Parser-->>Splitter: è§£æå®Œæˆ<br/>ç»“æ„åŒ–æ•°æ® = {headers: [...], sections: [...]}

    Splitter->>Builder: æ„å»ºæ–‡æ¡£å—<br/>_aggregate_lines_to_chunks(structured_data)

    Builder->>Builder: ä¸ºæ¯ä¸ªæ®µè½åˆ›å»ºDocumentå¯¹è±¡

    loop å¤„ç†æ¯ä¸ªæ®µè½
        Builder->>Builder: è·å–æ®µè½å†…å®¹å’Œæ‰€å±æ ‡é¢˜<br/>section = {content: "å†…å®¹1", header: "ä¸»æ ‡é¢˜"}

        Builder->>Builder: æ„å»ºå…ƒæ•°æ®<br/>metadata = {<br/>  "Header 1": "ä¸»æ ‡é¢˜",<br/>  "start_index": 0,<br/>  "section_level": 1<br/>}

        Builder->>Builder: åˆ›å»ºDocumentå¯¹è±¡<br/>doc = Document(page_content=content, metadata=metadata)

        Builder->>Builder: æ·»åŠ åˆ°ç»“æœåˆ—è¡¨<br/>documents.append(doc)
    end

    Builder-->>Splitter: æ–‡æ¡£æ„å»ºå®Œæˆ<br/>documents = [doc1, doc2, doc3, ...]

    Splitter->>Splitter: åº”ç”¨å±‚æ¬¡ä¿¡æ¯<br/>ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ å®Œæ•´çš„æ ‡é¢˜å±‚æ¬¡è·¯å¾„

    loop å¤„ç†æ ‡é¢˜å±‚æ¬¡
        Splitter->>Splitter: è®¡ç®—æ ‡é¢˜è·¯å¾„<br/>"ä¸»æ ‡é¢˜ > äºŒçº§æ ‡é¢˜ > ä¸‰çº§æ ‡é¢˜"

        Splitter->>Splitter: æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®<br/>æ·»åŠ å®Œæ•´çš„å±‚æ¬¡ä¿¡æ¯
    end

    Splitter-->>User: ç»“æ„åŒ–åˆ†å‰²ç»“æœ<br/>[<br/>  Document(content="å†…å®¹1", metadata={"Header 1": "ä¸»æ ‡é¢˜"}),<br/>  Document(content="å†…å®¹2", metadata={"Header 1": "ä¸»æ ‡é¢˜", "Header 2": "äºŒçº§æ ‡é¢˜"}),<br/>  Document(content="å†…å®¹3", metadata={"Header 1": "ä¸»æ ‡é¢˜", "Header 2": "äºŒçº§æ ‡é¢˜", "Header 3": "ä¸‰çº§æ ‡é¢˜"})<br/>]
```

**Markdownç»“æ„åŒ–å¤„ç†**ï¼š

```python
class MarkdownStructureProcessor:
    def process_headers(self, text: str) -> List[HeaderInfo]:
        """å¤„ç†æ ‡é¢˜ç»“æ„ã€‚"""
        headers = []
        lines = text.split('\n')

        for line_num, line in enumerate(lines):
            # æ£€æµ‹æ ‡é¢˜
            if line.strip().startswith('#'):
                level = 0
                for char in line:
                    if char == '#':
                        level += 1
                    else:
                        break

                title = line[level:].strip()
                headers.append(HeaderInfo(
                    level=level,
                    title=title,
                    line_number=line_num,
                    start_index=sum(len(l) + 1 for l in lines[:line_num])
                ))

        return headers

    def build_hierarchy(self, headers: List[HeaderInfo]) -> Dict[str, Any]:
        """æ„å»ºæ ‡é¢˜å±‚æ¬¡ç»“æ„ã€‚"""
        hierarchy = {}
        stack = [hierarchy]

        for header in headers:
            # è°ƒæ•´æ ˆæ·±åº¦
            while len(stack) > header.level:
                stack.pop()

            # æ·»åŠ å½“å‰æ ‡é¢˜
            current_level = stack[-1]
            current_level[header.title] = {
                "level": header.level,
                "line_number": header.line_number,
                "children": {}
            }

            # æ¨å…¥ä¸‹ä¸€å±‚
            stack.append(current_level[header.title]["children"])

        return hierarchy
```

---

### 3.2 PythonCodeTextSplitter ä»£ç ç»“æ„åˆ†å‰²æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Splitter as PythonCodeTextSplitter
    participant AST as ASTParser
    participant Analyzer as CodeAnalyzer
    participant Segmenter as CodeSegmenter

    User->>Splitter: split_text("class MyClass:\n    def method1(self):\n        pass\n\ndef function1():\n    return True")

    Splitter->>AST: è§£æPythonä»£ç ç»“æ„<br/>ast.parse(code_text)

    AST->>AST: æ„å»ºæŠ½è±¡è¯­æ³•æ ‘<br/>è¯†åˆ«ç±»ã€å‡½æ•°ã€å¯¼å…¥ç­‰ç»“æ„

    AST-->>Splitter: ASTèŠ‚ç‚¹æ ‘<br/>nodes = [ClassDef, FunctionDef, ...]

    Splitter->>Analyzer: åˆ†æä»£ç ç»“æ„<br/>extract_code_elements(ast_nodes)

    Analyzer->>Analyzer: éå†ASTèŠ‚ç‚¹<br/>æå–ç»“æ„åŒ–ä¿¡æ¯

    loop å¤„ç†æ¯ä¸ªASTèŠ‚ç‚¹
        Analyzer->>Analyzer: è¯†åˆ«èŠ‚ç‚¹ç±»å‹<br/>node_type = ClassDef | FunctionDef | Import

        alt ç±»å®šä¹‰èŠ‚ç‚¹
            Analyzer->>Analyzer: æå–ç±»ä¿¡æ¯<br/>class_info = {<br/>  name: "MyClass",<br/>  start_line: 1,<br/>  end_line: 3,<br/>  methods: ["method1"]<br/>}

        else å‡½æ•°å®šä¹‰èŠ‚ç‚¹
            Analyzer->>Analyzer: æå–å‡½æ•°ä¿¡æ¯<br/>function_info = {<br/>  name: "function1",<br/>  start_line: 5,<br/>  end_line: 6,<br/>  parameters: [],<br/>  is_method: False<br/>}

        else å¯¼å…¥èŠ‚ç‚¹
            Analyzer->>Analyzer: æå–å¯¼å…¥ä¿¡æ¯<br/>import_info = {<br/>  module: "os",<br/>  items: ["path"],<br/>  line: 1<br/>}
        end
    end

    Analyzer-->>Splitter: ç»“æ„åŒ–ä»£ç ä¿¡æ¯<br/>code_structure = {<br/>  classes: [...],<br/>  functions: [...],<br/>  imports: [...]<br/>}

    Splitter->>Segmenter: åŸºäºç»“æ„åˆ†å‰²ä»£ç <br/>segment_by_structure(code_text, code_structure)

    Segmenter->>Segmenter: ä½¿ç”¨Pythonç‰¹å®šåˆ†éš”ç¬¦<br/>separators = ["\n\nclass ", "\n\ndef ", "\n\n", "\n", " ", ""]

    Segmenter->>Segmenter: åº”ç”¨é€’å½’åˆ†å‰²ç­–ç•¥<br/>ä¼˜å…ˆä¿æŒä»£ç ç»“æ„å®Œæ•´æ€§

    loop å¤„ç†ä»£ç æ®µ
        Segmenter->>Segmenter: æ£€æŸ¥ä»£ç æ®µå®Œæ•´æ€§<br/>ç¡®ä¿ç±»å’Œå‡½æ•°ä¸è¢«æˆªæ–­

        Segmenter->>Segmenter: è®¡ç®—ä»£ç æ®µå¤§å°<br/>è€ƒè™‘ç¼©è¿›å’Œè¯­æ³•ç»“æ„

        alt ä»£ç æ®µè¿‡å¤§
            Segmenter->>Segmenter: åœ¨åˆé€‚çš„ä½ç½®åˆ†å‰²<br/>ä¼˜å…ˆåœ¨å‡½æ•°è¾¹ç•Œåˆ†å‰²
        else ä»£ç æ®µåˆé€‚
            Segmenter->>Segmenter: ä¿æŒä»£ç æ®µå®Œæ•´
        end
    end

    Segmenter->>Segmenter: æ·»åŠ ä»£ç å…ƒæ•°æ®<br/>ä¸ºæ¯ä¸ªæ®µæ·»åŠ ç»“æ„ä¿¡æ¯

    Segmenter-->>Splitter: åˆ†å‰²å®Œæˆ<br/>code_chunks = [<br/>  "class MyClass:\n    def method1(self):\n        pass",<br/>  "def function1():\n    return True"<br/>]

    Splitter-->>User: ä»£ç åˆ†å‰²ç»“æœ<br/>ä¿æŒè¯­æ³•å®Œæ•´æ€§çš„ä»£ç å—
```

**Pythonä»£ç åˆ†å‰²ç‰¹ç‚¹**ï¼š

1. **è¯­æ³•æ„ŸçŸ¥**ï¼šç†è§£Pythonè¯­æ³•ç»“æ„ï¼Œé¿å…ç ´åä»£ç å®Œæ•´æ€§
2. **ç»“æ„ä¼˜å…ˆ**ï¼šä¼˜å…ˆåœ¨ç±»ã€å‡½æ•°è¾¹ç•Œè¿›è¡Œåˆ†å‰²
3. **ç¼©è¿›ä¿æŒ**ï¼šä¿æŒPythonä»£ç çš„ç¼©è¿›ç»“æ„
4. **å…ƒæ•°æ®ä¸°å¯Œ**ï¼šä¸ºæ¯ä¸ªä»£ç å—æ·»åŠ ç»“æ„åŒ–å…ƒæ•°æ®

---

## 4. æ–‡æ¡£å¤„ç†åœºæ™¯

### 4.1 create_documents æ–‡æ¡£åˆ›å»ºæµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Splitter as TextSplitter
    participant Processor as DocumentProcessor
    participant MetadataManager
    participant IndexManager

    User->>Splitter: create_documents(<br/>  texts=["æ–‡æ¡£1å†…å®¹", "æ–‡æ¡£2å†…å®¹"],<br/>  metadatas=[{"source": "doc1.txt"}, {"source": "doc2.txt"}]<br/>)

    Splitter->>Splitter: éªŒè¯è¾“å…¥å‚æ•°<br/>æ£€æŸ¥textså’Œmetadatasé•¿åº¦åŒ¹é…

    alt å‚æ•°éªŒè¯å¤±è´¥
        Splitter-->>User: raise ValueError("å‚æ•°ä¸åŒ¹é…")
    end

    Splitter->>Processor: å¼€å§‹æ‰¹é‡æ–‡æ¡£å¤„ç†<br/>process_documents(texts, metadatas)

    loop å¤„ç†æ¯ä¸ªæ–‡æ¡£
        Processor->>Processor: è·å–å½“å‰æ–‡æ¡£<br/>text = texts[i], metadata = metadatas[i]

        Processor->>Splitter: åˆ†å‰²å½“å‰æ–‡æ¡£<br/>split_text(text)

        Splitter->>Splitter: æ‰§è¡Œæ–‡æœ¬åˆ†å‰²é€»è¾‘<br/>chunks = ["å—1", "å—2", "å—3"]

        Splitter-->>Processor: è¿”å›æ–‡æ¡£å—<br/>text_chunks

        loop å¤„ç†æ¯ä¸ªæ–‡æ¡£å—
            Processor->>MetadataManager: æ„å»ºå—å…ƒæ•°æ®<br/>build_chunk_metadata(original_metadata, chunk_index)

            MetadataManager->>MetadataManager: å¤åˆ¶åŸå§‹å…ƒæ•°æ®<br/>chunk_metadata = metadata.copy()

            alt add_start_index = True
                MetadataManager->>IndexManager: è®¡ç®—èµ·å§‹ç´¢å¼•<br/>start_index = calculate_start_index(chunk, original_text)

                IndexManager-->>MetadataManager: start_index = 150

                MetadataManager->>MetadataManager: æ·»åŠ ç´¢å¼•ä¿¡æ¯<br/>chunk_metadata["start_index"] = 150
            end

            MetadataManager->>MetadataManager: æ·»åŠ å—ç‰¹å®šä¿¡æ¯<br/>chunk_metadata.update({<br/>  "chunk_index": chunk_idx,<br/>  "total_chunks": len(chunks),<br/>  "chunk_length": len(chunk)<br/>})

            MetadataManager-->>Processor: å®Œæ•´çš„å—å…ƒæ•°æ®

            Processor->>Processor: åˆ›å»ºDocumentå¯¹è±¡<br/>doc = Document(<br/>  page_content=chunk,<br/>  metadata=chunk_metadata<br/>)

            Processor->>Processor: æ·»åŠ åˆ°ç»“æœåˆ—è¡¨<br/>all_documents.append(doc)
        end
    end

    Processor-->>Splitter: æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆ<br/>documents = [doc1, doc2, doc3, ...]

    Splitter->>Splitter: æ›´æ–°åˆ†å‰²ç»Ÿè®¡<br/>total_documents_created += len(documents)<br/>total_original_texts += len(texts)

    Splitter-->>User: æ–‡æ¡£åˆ›å»ºå®Œæˆ<br/>List[Document] åŒ…å«æ‰€æœ‰åˆ†å‰²åçš„æ–‡æ¡£å—
```

**æ–‡æ¡£åˆ›å»ºä¼˜åŒ–**ï¼š

```python
def create_documents_optimized(
    self,
    texts: List[str],
    metadatas: Optional[List[dict]] = None
) -> List[Document]:
    """ä¼˜åŒ–çš„æ–‡æ¡£åˆ›å»ºæµç¨‹ã€‚"""

    # 1. å‚æ•°é¢„å¤„ç†
    if metadatas is None:
        metadatas = [{}] * len(texts)

    if len(texts) != len(metadatas):
        raise ValueError("textså’Œmetadatasé•¿åº¦å¿…é¡»ç›¸åŒ")

    # 2. æ‰¹é‡å¤„ç†ä¼˜åŒ–
    all_documents = []

    # å¹¶è¡Œå¤„ç†å¤§æ‰¹é‡æ–‡æ¡£
    if len(texts) > 100:
        from concurrent.futures import ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for text, metadata in zip(texts, metadatas):
                future = executor.submit(self._process_single_document, text, metadata)
                futures.append(future)

            for future in futures:
                documents = future.result()
                all_documents.extend(documents)
    else:
        # ä¸²è¡Œå¤„ç†å°æ‰¹é‡
        for text, metadata in zip(texts, metadatas):
            documents = self._process_single_document(text, metadata)
            all_documents.extend(documents)

    return all_documents

def _process_single_document(self, text: str, metadata: dict) -> List[Document]:
    """å¤„ç†å•ä¸ªæ–‡æ¡£ã€‚"""
    chunks = self.split_text(text)
    documents = []

    for i, chunk in enumerate(chunks):
        chunk_metadata = metadata.copy()

        # æ·»åŠ å—ä¿¡æ¯
        chunk_metadata.update({
            "chunk_index": i,
            "total_chunks": len(chunks),
            "chunk_length": len(chunk)
        })

        # æ·»åŠ èµ·å§‹ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self._add_start_index:
            start_index = self._calculate_start_index(text, chunk, i)
            chunk_metadata["start_index"] = start_index

        documents.append(Document(
            page_content=chunk,
            metadata=chunk_metadata
        ))

    return documents
```

---

### 4.2 split_documents æ–‡æ¡£åˆ†å‰²æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant Splitter
    participant DocumentProcessor
    participant MetadataPreserver
    participant ResultAggregator

    User->>Splitter: split_documents([<br/>  Document(content="é•¿æ–‡æ¡£1", metadata={"source": "file1.txt"}),<br/>  Document(content="é•¿æ–‡æ¡£2", metadata={"source": "file2.txt"})<br/>])

    Splitter->>DocumentProcessor: å¼€å§‹æ‰¹é‡æ–‡æ¡£åˆ†å‰²<br/>process_document_list(documents)

    loop å¤„ç†æ¯ä¸ªæ–‡æ¡£
        DocumentProcessor->>DocumentProcessor: æå–æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®<br/>content = doc.page_content<br/>original_metadata = doc.metadata

        DocumentProcessor->>Splitter: åˆ†å‰²æ–‡æ¡£å†…å®¹<br/>split_text(content)

        Splitter->>Splitter: æ‰§è¡Œåˆ†å‰²é€»è¾‘<br/>chunks = self._split_implementation(content)

        Splitter-->>DocumentProcessor: è¿”å›æ–‡æœ¬å—<br/>text_chunks = ["å—1", "å—2", ...]

        DocumentProcessor->>MetadataPreserver: å‡†å¤‡å…ƒæ•°æ®ä¿æŒ<br/>prepare_metadata_preservation(original_metadata, len(chunks))

        loop ä¸ºæ¯ä¸ªå—åˆ›å»ºæ–‡æ¡£
            MetadataPreserver->>MetadataPreserver: åˆ›å»ºå—å…ƒæ•°æ®<br/>chunk_metadata = original_metadata.copy()

            MetadataPreserver->>MetadataPreserver: æ·»åŠ åˆ†å‰²ä¿¡æ¯<br/>chunk_metadata.update({<br/>  "chunk_index": idx,<br/>  "parent_document": original_metadata.get("source"),<br/>  "split_timestamp": time.now()<br/>})

            alt éœ€è¦ä¿æŒæ–‡æ¡£å…³ç³»
                MetadataPreserver->>MetadataPreserver: æ·»åŠ å…³ç³»ä¿¡æ¯<br/>chunk_metadata.update({<br/>  "parent_doc_id": generate_doc_id(original_doc),<br/>  "sibling_chunks": len(chunks)<br/>})
            end

            MetadataPreserver->>DocumentProcessor: åˆ›å»ºæ–°æ–‡æ¡£<br/>new_doc = Document(<br/>  page_content=chunk,<br/>  metadata=chunk_metadata<br/>)

            DocumentProcessor->>ResultAggregator: æ·»åŠ åˆ°ç»“æœé›†<br/>add_document(new_doc)
        end
    end

    ResultAggregator->>ResultAggregator: æ•´ç†æœ€ç»ˆç»“æœ<br/>æŒ‰åŸå§‹æ–‡æ¡£é¡ºåºæ’åˆ—åˆ†å‰²åçš„æ–‡æ¡£

    ResultAggregator->>ResultAggregator: éªŒè¯ç»“æœå®Œæ•´æ€§<br/>æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼æˆ–é‡å¤

    ResultAggregator-->>Splitter: åˆ†å‰²å®Œæˆ<br/>split_documents = [doc1_chunk1, doc1_chunk2, doc2_chunk1, ...]

    Splitter->>Splitter: æ›´æ–°å¤„ç†ç»Ÿè®¡<br/>documents_processed += len(original_docs)<br/>chunks_created += len(split_documents)

    Splitter-->>User: æ–‡æ¡£åˆ†å‰²ç»“æœ<br/>List[Document] ä¿æŒåŸæœ‰å…ƒæ•°æ®å¹¶æ·»åŠ åˆ†å‰²ä¿¡æ¯
```

---

## 5. æ€§èƒ½ä¼˜åŒ–åœºæ™¯

### 5.1 æ‰¹é‡åˆ†å‰²ä¼˜åŒ–æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User
    participant BatchProcessor
    participant Splitter
    participant Cache as SplitterCache
    participant Pool as ThreadPool
    participant Monitor as PerformanceMonitor

    User->>BatchProcessor: batch_split_texts([text1, text2, ..., text1000])

    BatchProcessor->>Monitor: å¼€å§‹æ€§èƒ½ç›‘æ§<br/>start_batch_operation()

    BatchProcessor->>BatchProcessor: åˆ†ææ‰¹é‡å¤§å°<br/>batch_size = 1000, å†³å®šå¤„ç†ç­–ç•¥

    alt å¤§æ‰¹é‡å¤„ç† (>100)
        BatchProcessor->>Pool: åˆ›å»ºçº¿ç¨‹æ± <br/>ThreadPoolExecutor(max_workers=4)

        BatchProcessor->>BatchProcessor: å°†æ–‡æœ¬åˆ†ç»„<br/>groups = [texts[0:250], texts[250:500], ...]

        par å¹¶è¡Œå¤„ç†å„ç»„
            BatchProcessor->>Pool: æäº¤ä»»åŠ¡ç»„1<br/>submit(process_group, group1)
            Pool->>Splitter: å¤„ç†group1æ–‡æœ¬

            loop å¤„ç†ç»„å†…æ–‡æœ¬
                Splitter->>Cache: æ£€æŸ¥ç¼“å­˜<br/>cache_key = hash(text + config)

                alt ç¼“å­˜å‘½ä¸­
                    Cache-->>Splitter: è¿”å›ç¼“å­˜ç»“æœ<br/>cached_chunks
                    Splitter->>Monitor: è®°å½•ç¼“å­˜å‘½ä¸­<br/>cache_hit++
                else ç¼“å­˜æœªå‘½ä¸­
                    Splitter->>Splitter: æ‰§è¡Œå®é™…åˆ†å‰²<br/>split_text(text)
                    Splitter->>Cache: ç¼“å­˜ç»“æœ<br/>put(cache_key, chunks)
                    Splitter->>Monitor: è®°å½•ç¼“å­˜æœªå‘½ä¸­<br/>cache_miss++
                end
            end

            Pool-->>BatchProcessor: ç»„1å¤„ç†å®Œæˆ<br/>group1_results
        and
            BatchProcessor->>Pool: æäº¤ä»»åŠ¡ç»„2<br/>submit(process_group, group2)
            Pool-->>BatchProcessor: ç»„2å¤„ç†å®Œæˆ<br/>group2_results
        and
            BatchProcessor->>Pool: æäº¤ä»»åŠ¡ç»„3<br/>submit(process_group, group3)
            Pool-->>BatchProcessor: ç»„3å¤„ç†å®Œæˆ<br/>group3_results
        and
            BatchProcessor->>Pool: æäº¤ä»»åŠ¡ç»„4<br/>submit(process_group, group4)
            Pool-->>BatchProcessor: ç»„4å¤„ç†å®Œæˆ<br/>group4_results
        end

        BatchProcessor->>BatchProcessor: åˆå¹¶æ‰€æœ‰ç»“æœ<br/>all_results = group1 + group2 + group3 + group4

    else å°æ‰¹é‡å¤„ç† (<=100)
        BatchProcessor->>Splitter: ä¸²è¡Œå¤„ç†<br/>sequential_processing(texts)

        loop é€ä¸ªå¤„ç†æ–‡æœ¬
            Splitter->>Cache: æ£€æŸ¥ç¼“å­˜
            Splitter->>Splitter: åˆ†å‰²æ–‡æœ¬
            Splitter->>Monitor: è®°å½•å¤„ç†æ—¶é—´
        end

        Splitter-->>BatchProcessor: ä¸²è¡Œç»“æœ<br/>sequential_results
    end

    BatchProcessor->>Monitor: ç»“æŸæ€§èƒ½ç›‘æ§<br/>end_batch_operation()

    Monitor->>Monitor: è®¡ç®—æ€§èƒ½æŒ‡æ ‡<br/>throughput = total_texts / total_time<br/>cache_hit_rate = hits / (hits + misses)<br/>average_chunk_size = total_chunks / total_texts

    Monitor-->>BatchProcessor: æ€§èƒ½æŠ¥å‘Š<br/>{<br/>  "throughput": "500 texts/sec",<br/>  "cache_hit_rate": "75%",<br/>  "total_chunks": 2500<br/>}

    BatchProcessor-->>User: æ‰¹é‡å¤„ç†å®Œæˆ<br/>results + performance_stats
```

**æ‰¹é‡ä¼˜åŒ–ç­–ç•¥**ï¼š

| æ‰¹é‡å¤§å° | å¤„ç†ç­–ç•¥ | å¹¶å‘æ•° | ç¼“å­˜ç­–ç•¥ | é¢„æœŸæ€§èƒ½ |
|---------|---------|--------|---------|---------|
| 1-10 | ä¸²è¡Œå¤„ç† | 1 | åŸºç¡€ç¼“å­˜ | 100% |
| 11-100 | ä¸²è¡Œ+ç¼“å­˜ | 1 | æ™ºèƒ½ç¼“å­˜ | 300% |
| 101-1000 | å¹¶è¡Œå¤„ç† | 4 | åˆ†å¸ƒå¼ç¼“å­˜ | 800% |
| 1000+ | åˆ†æ‰¹+å¹¶è¡Œ | 8 | é¢„åŠ è½½ç¼“å­˜ | 1500% |

---

### 5.2 æ™ºèƒ½ç¼“å­˜ç®¡ç†æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant Splitter
    participant CacheManager
    participant LRUCache
    participant HashCalculator
    participant StatsCollector

    Splitter->>CacheManager: è¯·æ±‚åˆ†å‰²ç»“æœ<br/>get_or_split(text, config)

    CacheManager->>HashCalculator: ç”Ÿæˆç¼“å­˜é”®<br/>calculate_cache_key(text, config)

    HashCalculator->>HashCalculator: è®¡ç®—æ–‡æœ¬å“ˆå¸Œ<br/>text_hash = md5(text)<br/>config_hash = md5(str(config))

    HashCalculator-->>CacheManager: cache_key = f"{text_hash}_{config_hash}"

    CacheManager->>LRUCache: æŸ¥æ‰¾ç¼“å­˜<br/>get(cache_key)

    alt ç¼“å­˜å‘½ä¸­
        LRUCache->>LRUCache: æ›´æ–°è®¿é—®æ—¶é—´<br/>move_to_end(cache_key)

        LRUCache-->>CacheManager: ç¼“å­˜ç»“æœ<br/>cached_chunks

        CacheManager->>StatsCollector: è®°å½•å‘½ä¸­<br/>record_cache_hit(cache_key, hit_time)

        CacheManager-->>Splitter: è¿”å›ç¼“å­˜ç»“æœ<br/>chunks (< 1ms)

    else ç¼“å­˜æœªå‘½ä¸­
        LRUCache-->>CacheManager: None

        CacheManager->>Splitter: æ‰§è¡Œå®é™…åˆ†å‰²<br/>perform_split(text, config)

        Splitter->>Splitter: åˆ†å‰²å¤„ç†<br/>split_text_implementation()

        Splitter-->>CacheManager: åˆ†å‰²ç»“æœ<br/>chunks (10-100ms)

        CacheManager->>CacheManager: è¯„ä¼°ç¼“å­˜ä»·å€¼<br/>should_cache(text_length, split_time, chunk_count)

        alt å€¼å¾—ç¼“å­˜
            CacheManager->>LRUCache: å­˜å‚¨ç»“æœ<br/>put(cache_key, chunks, ttl=3600)

            LRUCache->>LRUCache: æ£€æŸ¥å®¹é‡é™åˆ¶<br/>if len(cache) > max_size: evict_lru()

            alt éœ€è¦æ·˜æ±°
                LRUCache->>LRUCache: æ·˜æ±°æœ€ä¹…æœªä½¿ç”¨é¡¹<br/>evicted_key = popitem(last=False)

                LRUCache->>StatsCollector: è®°å½•æ·˜æ±°<br/>record_eviction(evicted_key)
            end

        else ä¸å€¼å¾—ç¼“å­˜
            CacheManager->>StatsCollector: è®°å½•è·³è¿‡<br/>record_cache_skip(reason="too_small")
        end

        CacheManager->>StatsCollector: è®°å½•æœªå‘½ä¸­<br/>record_cache_miss(cache_key, split_time)

        CacheManager-->>Splitter: è¿”å›åˆ†å‰²ç»“æœ<br/>chunks
    end

    StatsCollector->>StatsCollector: æ›´æ–°ç»Ÿè®¡ä¿¡æ¯<br/>hit_rate = hits / (hits + misses)<br/>average_split_time = total_time / operations<br/>cache_efficiency = saved_time / total_time
```

**ç¼“å­˜ç­–ç•¥ä¼˜åŒ–**ï¼š

```python
class IntelligentCacheManager:
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "total_saved_time": 0.0
        }

    def should_cache(self, text_length: int, split_time: float, chunk_count: int) -> bool:
        """æ™ºèƒ½ç¼“å­˜å†³ç­–ã€‚"""
        # ç¼“å­˜ç­–ç•¥ï¼š
        # 1. é•¿æ–‡æœ¬ä¼˜å…ˆç¼“å­˜ï¼ˆå¤„ç†æ—¶é—´é•¿ï¼‰
        # 2. å¤æ‚åˆ†å‰²ä¼˜å…ˆç¼“å­˜ï¼ˆå—æ•°å¤šï¼‰
        # 3. é¿å…ç¼“å­˜ä¸€æ¬¡æ€§æ–‡æœ¬

        if text_length < 100:  # å¤ªçŸ­ï¼Œä¸å€¼å¾—ç¼“å­˜
            return False

        if split_time < 0.01:  # å¤„ç†å¤ªå¿«ï¼Œç¼“å­˜æ”¶ç›Šå°
            return False

        if chunk_count < 2:  # åˆ†å‰²ç»“æœç®€å•
            return False

        # é¢„ä¼°ç¼“å­˜ä»·å€¼
        cache_value_score = (
            text_length * 0.001 +  # æ–‡æœ¬é•¿åº¦æƒé‡
            split_time * 100 +     # å¤„ç†æ—¶é—´æƒé‡
            chunk_count * 10       # å¤æ‚åº¦æƒé‡
        )

        return cache_value_score > 50  # é˜ˆå€¼

    def evict_intelligently(self) -> None:
        """æ™ºèƒ½æ·˜æ±°ç­–ç•¥ã€‚"""
        if len(self.cache) <= self.max_size:
            return

        # æŒ‰è®¿é—®é¢‘ç‡å’Œæ—¶é—´ç»¼åˆè¯„åˆ†
        candidates = []
        for key, entry in self.cache.items():
            score = (
                entry.access_count * 0.3 +  # è®¿é—®é¢‘ç‡
                (time.time() - entry.last_access) * -0.001 +  # æœ€è¿‘è®¿é—®æ—¶é—´
                entry.cache_value * 0.7  # ç¼“å­˜ä»·å€¼
            )
            candidates.append((key, score))

        # æ·˜æ±°è¯„åˆ†æœ€ä½çš„é¡¹
        candidates.sort(key=lambda x: x[1])
        to_evict = candidates[:len(self.cache) - self.max_size + 1]

        for key, _ in to_evict:
            del self.cache[key]
            self.stats["evictions"] += 1
```

---

## 6. æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†å±•ç¤ºäº† **Text Splitters æ¨¡å—**çš„å…³é”®æ‰§è¡Œæ—¶åºï¼š

1. **åŸºç¡€åˆ†å‰²**ï¼šCharacterTextSplitterå’ŒRecursiveCharacterTextSplitterçš„åˆ†å‰²ç­–ç•¥å’Œé€’å½’å¤„ç†
2. **ä»¤ç‰Œå¤„ç†**ï¼šTokenTextSplitterçš„ç²¾ç¡®ä»¤ç‰Œæ§åˆ¶å’Œç¼–ç å¤„ç†æœºåˆ¶
3. **ä¸“ç”¨åˆ†å‰²**ï¼šMarkdownHeaderTextSplitterå’ŒPythonCodeTextSplitterçš„ç»“æ„åŒ–å¤„ç†
4. **æ–‡æ¡£å¤„ç†**ï¼šcreate_documentså’Œsplit_documentsçš„æ‰¹é‡å¤„ç†æµç¨‹
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ‰¹é‡å¤„ç†ã€æ™ºèƒ½ç¼“å­˜å’Œå¹¶å‘ä¼˜åŒ–ç­–ç•¥

æ¯å¼ æ—¶åºå›¾åŒ…å«ï¼š

- è¯¦ç»†çš„å‚ä¸è€…äº¤äº’è¿‡ç¨‹
- å…³é”®ç®—æ³•å’Œå¤„ç†é€»è¾‘
- æ€§èƒ½ä¼˜åŒ–ç‚¹å’Œç¼“å­˜ç­–ç•¥
- é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ
- ç»Ÿè®¡ä¿¡æ¯æ”¶é›†å’Œç›‘æ§

è¿™äº›æ—¶åºå›¾å¸®åŠ©å¼€å‘è€…æ·±å…¥ç†è§£æ–‡æœ¬åˆ†å‰²ç³»ç»Ÿçš„å†…éƒ¨å·¥ä½œæœºåˆ¶ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€å¯é çš„æ–‡æ¡£å¤„ç†ç®¡é“æä¾›æŒ‡å¯¼ã€‚Text Splittersæ˜¯RAGç³»ç»Ÿå’Œæ–‡æ¡£å¤„ç†åº”ç”¨çš„åŸºç¡€ç»„ä»¶ï¼Œæ­£ç¡®ç†è§£å…¶æ‰§è¡Œæµç¨‹å¯¹æé«˜æ–‡æ¡£å¤„ç†è´¨é‡å’Œç³»ç»Ÿæ€§èƒ½è‡³å…³é‡è¦ã€‚

é€šè¿‡é€’å½’åˆ†å‰²ã€æ™ºèƒ½ç¼“å­˜ã€æ‰¹é‡ä¼˜åŒ–ç­‰æŠ€æœ¯ï¼ŒText Splittersæ¨¡å—èƒ½å¤Ÿé«˜æ•ˆå¤„ç†å„ç§ç±»å‹å’Œè§„æ¨¡çš„æ–‡æœ¬æ•°æ®ï¼Œä¸ºä¸‹æ¸¸çš„å‘é‡åŒ–ã€æ£€ç´¢å’Œç”Ÿæˆä»»åŠ¡æä¾›ä¼˜è´¨çš„è¾“å…¥ã€‚

---
