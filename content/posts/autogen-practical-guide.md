---
title: "AutoGenå®æˆ˜æŒ‡å—ï¼šä»å…¥é—¨åˆ°ç”Ÿäº§éƒ¨ç½²"
date: 2025-05-05T16:00:00+08:00
draft: false
featured: true
series: "autogen-architecture"
tags: ["AutoGen", "å®æˆ˜æŒ‡å—", "æœ€ä½³å®è·µ", "ç”Ÿäº§éƒ¨ç½²", "æ€§èƒ½ä¼˜åŒ–"]
categories: ["autogen", "å®æˆ˜æŒ‡å—"]
author: "Architecture Analysis"
description: "AutoGenæ¡†æ¶çš„å®Œæ•´å®æˆ˜æŒ‡å—ï¼ŒåŒ…å«å¼€å‘å®è·µã€æ€§èƒ½ä¼˜åŒ–ã€éƒ¨ç½²ç­–ç•¥å’Œæ•…éšœæ’æŸ¥"
image: "/images/articles/autogen-practical-guide.svg"
toc: true
tocOpen: true
showReadingTime: true
showWordCount: true

weight: 160
slug: "autogen-practical-guide"
---

## æ¦‚è¿°

æœ¬æŒ‡å—åŸºäºå¤§é‡ç”Ÿäº§ç¯å¢ƒå®è·µç»éªŒï¼Œæä¾›AutoGenæ¡†æ¶ä»å¼€å‘åˆ°éƒ¨ç½²çš„å®Œæ•´å®æˆ˜æŒ‡å¯¼ï¼ŒåŒ…å«æ€§èƒ½ä¼˜åŒ–ã€æ•…éšœæ’æŸ¥ã€ç›‘æ§å‘Šè­¦ç­‰å…³é”®å®è·µã€‚

## 1. å¿«é€Ÿå¼€å§‹æŒ‡å—

### 1.1 ç¯å¢ƒå‡†å¤‡

#### Pythonç¯å¢ƒè®¾ç½®

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv autogen-env
source autogen-env/bin/activate  # Linux/Mac
# autogen-env\Scripts\activate  # Windows

# å®‰è£…æ ¸å¿ƒåŒ…
pip install autogen-core autogen-agentchat

# å®‰è£…æ‰©å±•åŒ…
pip install autogen-ext[openai,azure,anthropic]

# å¼€å‘å·¥å…·
pip install pytest pytest-asyncio black isort mypy
```

#### .NETç¯å¢ƒè®¾ç½®

```bash
# å®‰è£….NET SDK 8.0+
dotnet --version

# åˆ›å»ºæ–°é¡¹ç›®
dotnet new console -n AutoGenApp
cd AutoGenApp

# æ·»åŠ AutoGenåŒ…
dotnet add package Microsoft.AutoGen.Core
dotnet add package Microsoft.AutoGen.Agents
dotnet add package Microsoft.Extensions.Hosting
```

### 1.2 ç¬¬ä¸€ä¸ªä»£ç†åº”ç”¨

#### Pythonç‰ˆæœ¬

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main():
    """ç¬¬ä¸€ä¸ªAutoGenåº”ç”¨"""
    
    # 1. åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        api_key="your-openai-api-key"
    )
    
    # 2. åˆ›å»ºåŠ©æ‰‹ä»£ç†
    assistant = AssistantAgent(
        name="assistant",
        model_client=model_client,
        description="ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹"
    )
    
    # 3. è¿è¡Œå¯¹è¯
    await Console(assistant.run_stream(
        task="è¯·ä»‹ç»ä¸€ä¸‹AutoGenæ¡†æ¶çš„ä¸»è¦ç‰¹ç‚¹"
    ))

if __name__ == "__main__":
    asyncio.run(main())
```

#### .NETç‰ˆæœ¬

```csharp
using Microsoft.AutoGen.Core;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Hosting;
using Microsoft.Extensions.Logging;

// 1. å®šä¹‰ä»£ç†
public class GreetingAgent : Agent
{
    public GreetingAgent(ILogger<GreetingAgent> logger) : base(logger)
    {
    }
    
    [MessageHandler]
    public async Task<string> HandleGreeting(string message, MessageContext context)
    {
        return $"ä½ å¥½ï¼ä½ è¯´ï¼š{message}";
    }
}

// 2. é…ç½®æœåŠ¡
var builder = Host.CreateApplicationBuilder(args);

builder.Services.AddAutoGenCore();
builder.Services.AddAgent<GreetingAgent>();

var host = builder.Build();

// 3. è¿è¡Œåº”ç”¨
var runtime = host.Services.GetRequiredService<IAgentRuntime>();
var agentId = new AgentId("GreetingAgent", "default");

var response = await runtime.SendMessageAsync<string>(
    "Hello AutoGen!", 
    agentId
);

Console.WriteLine(response);
```

## 2. æ ¸å¿ƒæ¦‚å¿µå®æˆ˜

### 2.1 ä»£ç†è®¾è®¡æ¨¡å¼

#### å•ä¸€èŒè´£ä»£ç†

```python
class WeatherAgent(AssistantAgent):
    """ä¸“é—¨å¤„ç†å¤©æ°”æŸ¥è¯¢çš„ä»£ç†"""
    
    def __init__(self, weather_api_key: str):
        super().__init__(
            name="weather_agent",
            model_client=OpenAIChatCompletionClient(model="gpt-4o-mini"),
            tools=[self.get_weather],
            description="ä¸“ä¸šçš„å¤©æ°”æŸ¥è¯¢åŠ©æ‰‹",
            system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¤©æ°”æŸ¥è¯¢åŠ©æ‰‹ï¼Œåªå›ç­”å¤©æ°”ç›¸å…³çš„é—®é¢˜ã€‚"
        )
        self.weather_api_key = weather_api_key
    
    async def get_weather(self, city: str, date: str = None) -> str:
        """
        è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯
        
        Args:
            city: åŸå¸‚åç§°
            date: æ—¥æœŸï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»Šå¤©ï¼‰
            
        Returns:
            å¤©æ°”ä¿¡æ¯å­—ç¬¦ä¸²
        """
        # å®é™…å®ç°ä¸­è°ƒç”¨å¤©æ°”API
        return f"{city}ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦25Â°C"

# ä½¿ç”¨ç¤ºä¾‹
weather_agent = WeatherAgent("your-weather-api-key")
result = await weather_agent.run(task="åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
```

#### ç»„åˆä»£ç†æ¨¡å¼

```python
class TravelPlannerAgent(AssistantAgent):
    """æ—…è¡Œè§„åˆ’ä»£ç† - ç»„åˆå¤šä¸ªä¸“ä¸šä»£ç†"""
    
    def __init__(self):
        super().__init__(
            name="travel_planner",
            model_client=OpenAIChatCompletionClient(model="gpt-4o"),
            tools=[
                AgentTool(WeatherAgent("weather-key")),
                AgentTool(HotelAgent("hotel-key")),
                AgentTool(FlightAgent("flight-key"))
            ],
            description="ä¸“ä¸šçš„æ—…è¡Œè§„åˆ’åŠ©æ‰‹",
            system_message="""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡Œè§„åˆ’åŠ©æ‰‹ã€‚ä½ å¯ä»¥ï¼š
            1. æŸ¥è¯¢å¤©æ°”ä¿¡æ¯
            2. æœç´¢é…’åº—
            3. æŸ¥æ‰¾èˆªç­
            4. åˆ¶å®šè¯¦ç»†çš„æ—…è¡Œè®¡åˆ’
            
            è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œåˆç†ä½¿ç”¨å„ç§å·¥å…·æ¥åˆ¶å®šæœ€ä½³çš„æ—…è¡Œæ–¹æ¡ˆã€‚
            """
        )

# ä½¿ç”¨ç¤ºä¾‹
planner = TravelPlannerAgent()
result = await planner.run(
    task="æˆ‘æƒ³ä¸‹å‘¨å»ä¸Šæµ·æ—…è¡Œ3å¤©ï¼Œè¯·å¸®æˆ‘åˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„è®¡åˆ’"
)
```

### 2.2 å›¢é˜Ÿåä½œæ¨¡å¼

#### ä¸“å®¶å›¢é˜Ÿæ¨¡å¼

```python
async def create_expert_team():
    """åˆ›å»ºä¸“å®¶å›¢é˜Ÿè¿›è¡Œåä½œ"""
    
    # 1. åˆ›å»ºå„é¢†åŸŸä¸“å®¶
    researcher = AssistantAgent(
        name="researcher",
        model_client=OpenAIChatCompletionClient(model="gpt-4o"),
        description="ä¸“ä¸šçš„ç ”ç©¶å‘˜ï¼Œè´Ÿè´£ä¿¡æ¯æ”¶é›†å’Œåˆ†æ",
        system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶å‘˜ï¼Œæ“…é•¿æ”¶é›†å’Œåˆ†æå„ç§ä¿¡æ¯ã€‚"
    )
    
    writer = AssistantAgent(
        name="writer",
        model_client=OpenAIChatCompletionClient(model="gpt-4o"),
        description="ä¸“ä¸šçš„å†™ä½œè€…ï¼Œè´Ÿè´£å†…å®¹åˆ›ä½œ",
        system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†™ä½œè€…ï¼Œæ“…é•¿åˆ›ä½œå„ç§ç±»å‹çš„æ–‡æ¡£ã€‚"
    )
    
    reviewer = AssistantAgent(
        name="reviewer",
        model_client=OpenAIChatCompletionClient(model="gpt-4o"),
        description="ä¸“ä¸šçš„å®¡æ ¸è€…ï¼Œè´Ÿè´£è´¨é‡æ§åˆ¶",
        system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¡æ ¸è€…ï¼Œè´Ÿè´£æ£€æŸ¥å†…å®¹è´¨é‡å¹¶æå‡ºæ”¹è¿›å»ºè®®ã€‚è¯·åœ¨å®Œæˆåè¯´'TERMINATE'ã€‚"
    )
    
    # 2. åˆ›å»ºå›¢é˜Ÿ
    team = RoundRobinGroupChat(
        name="expert_team",
        description="ä¸“å®¶åä½œå›¢é˜Ÿ",
        participants=[researcher, writer, reviewer],
        termination_condition=MaxMessageTermination(15)
    )
    
    return team

# ä½¿ç”¨ç¤ºä¾‹
team = await create_expert_team()
result = await team.run(
    task="å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨çš„æŠ€æœ¯æŠ¥å‘Šï¼Œè¦æ±‚å†…å®¹å‡†ç¡®ã€ç»“æ„æ¸…æ™°ã€è¯­è¨€ä¸“ä¸šã€‚"
)
```

#### å±‚æ¬¡åŒ–å†³ç­–æ¨¡å¼

```python
class HierarchicalTeam:
    """å±‚æ¬¡åŒ–å†³ç­–å›¢é˜Ÿ"""
    
    def __init__(self):
        # æ“ä½œå±‚ä»£ç†
        self.operational_agents = [
            self.create_data_analyst(),
            self.create_market_researcher(),
            self.create_technical_expert()
        ]
        
        # ç®¡ç†å±‚ä»£ç†
        self.manager = self.create_manager()
        
        # å†³ç­–å±‚ä»£ç†
        self.executive = self.create_executive()
    
    def create_data_analyst(self):
        return AssistantAgent(
            name="data_analyst",
            model_client=OpenAIChatCompletionClient(model="gpt-4o-mini"),
            description="æ•°æ®åˆ†æä¸“å®¶",
            system_message="ä½ æ˜¯æ•°æ®åˆ†æä¸“å®¶ï¼Œè´Ÿè´£åˆ†ææ•°æ®å¹¶æä¾›æ´å¯Ÿã€‚"
        )
    
    def create_manager(self):
        return AssistantAgent(
            name="manager",
            model_client=OpenAIChatCompletionClient(model="gpt-4o"),
            tools=[AgentTool(agent) for agent in self.operational_agents],
            description="é¡¹ç›®ç»ç†ï¼Œåè°ƒå„ä¸“å®¶å·¥ä½œ",
            system_message="""
            ä½ æ˜¯é¡¹ç›®ç»ç†ï¼Œè´Ÿè´£ï¼š
            1. åˆ†æä»»åŠ¡éœ€æ±‚
            2. åˆ†é…å·¥ä½œç»™åˆé€‚çš„ä¸“å®¶
            3. æ±‡æ€»ä¸“å®¶æ„è§
            4. æå‡ºåˆæ­¥å»ºè®®
            """
        )
    
    def create_executive(self):
        return AssistantAgent(
            name="executive",
            model_client=OpenAIChatCompletionClient(model="gpt-4o"),
            tools=[AgentTool(self.manager)],
            description="é«˜çº§å†³ç­–è€…",
            system_message="""
            ä½ æ˜¯é«˜çº§å†³ç­–è€…ï¼Œè´Ÿè´£ï¼š
            1. å®¡æŸ¥ç®¡ç†å±‚å»ºè®®
            2. è€ƒè™‘æˆ˜ç•¥å› ç´ 
            3. åšå‡ºæœ€ç»ˆå†³ç­–
            4. è¯´æ˜å†³ç­–ç†ç”±
            """
        )
    
    async def make_decision(self, task: str):
        """æ‰§è¡Œå±‚æ¬¡åŒ–å†³ç­–æµç¨‹"""
        return await self.executive.run(task=task)

# ä½¿ç”¨ç¤ºä¾‹
team = HierarchicalTeam()
decision = await team.make_decision(
    "æˆ‘ä»¬å…¬å¸æ˜¯å¦åº”è¯¥æŠ•èµ„å¼€å‘ä¸€ä¸ªæ–°çš„AIäº§å“ï¼Ÿè¯·è¿›è¡Œå…¨é¢åˆ†æå¹¶ç»™å‡ºå»ºè®®ã€‚"
)
```

## 3. æ€§èƒ½ä¼˜åŒ–å®æˆ˜

### 3.1 æ¶ˆæ¯å¤„ç†ä¼˜åŒ–

#### æ‰¹å¤„ç†ä¼˜åŒ–

```python
class BatchProcessor:
    """æ‰¹å¤„ç†æ¶ˆæ¯ä¼˜åŒ–å™¨"""
    
    def __init__(self, batch_size: int = 10, timeout: float = 1.0):
        self.batch_size = batch_size
        self.timeout = timeout
        self.message_queue = asyncio.Queue()
        self.result_futures = {}
        self.processing_task = None
    
    async def process_message(self, message: Any, agent: AssistantAgent) -> Any:
        """æ‰¹å¤„ç†æ¶ˆæ¯"""
        
        # åˆ›å»ºç»“æœFuture
        message_id = str(uuid.uuid4())
        future = asyncio.Future()
        self.result_futures[message_id] = future
        
        # æ·»åŠ åˆ°é˜Ÿåˆ—
        await self.message_queue.put((message_id, message, agent))
        
        # å¯åŠ¨å¤„ç†ä»»åŠ¡ï¼ˆå¦‚æœæœªå¯åŠ¨ï¼‰
        if self.processing_task is None or self.processing_task.done():
            self.processing_task = asyncio.create_task(self._process_batch())
        
        # ç­‰å¾…ç»“æœ
        return await future
    
    async def _process_batch(self):
        """æ‰¹å¤„ç†é€»è¾‘"""
        batch = []
        
        try:
            # æ”¶é›†æ‰¹æ¬¡æ¶ˆæ¯
            while len(batch) < self.batch_size:
                try:
                    item = await asyncio.wait_for(
                        self.message_queue.get(), 
                        timeout=self.timeout
                    )
                    batch.append(item)
                except asyncio.TimeoutError:
                    break
            
            if not batch:
                return
            
            # æŒ‰ä»£ç†åˆ†ç»„
            agent_groups = {}
            for message_id, message, agent in batch:
                if agent not in agent_groups:
                    agent_groups[agent] = []
                agent_groups[agent].append((message_id, message))
            
            # å¹¶å‘å¤„ç†å„ç»„
            tasks = [
                self._process_agent_group(agent, messages)
                for agent, messages in agent_groups.items()
            ]
            
            await asyncio.gather(*tasks, return_exceptions=True)
            
        except Exception as e:
            # å¤„ç†å¼‚å¸¸ï¼Œé€šçŸ¥æ‰€æœ‰ç­‰å¾…çš„Future
            for message_id, _, _ in batch:
                if message_id in self.result_futures:
                    self.result_futures[message_id].set_exception(e)
                    del self.result_futures[message_id]
    
    async def _process_agent_group(self, agent: AssistantAgent, messages: List[Tuple[str, Any]]):
        """å¤„ç†å•ä¸ªä»£ç†çš„æ¶ˆæ¯ç»„"""
        
        for message_id, message in messages:
            try:
                # å¤„ç†å•ä¸ªæ¶ˆæ¯
                result = await agent.run(task=str(message))
                
                # è®¾ç½®ç»“æœ
                if message_id in self.result_futures:
                    self.result_futures[message_id].set_result(result)
                    del self.result_futures[message_id]
                    
            except Exception as e:
                if message_id in self.result_futures:
                    self.result_futures[message_id].set_exception(e)
                    del self.result_futures[message_id]

# ä½¿ç”¨ç¤ºä¾‹
batch_processor = BatchProcessor(batch_size=5, timeout=0.5)
agent = AssistantAgent("assistant", model_client)

# æ‰¹é‡å¤„ç†æ¶ˆæ¯
tasks = [
    batch_processor.process_message(f"é—®é¢˜{i}", agent)
    for i in range(20)
]

results = await asyncio.gather(*tasks)
```

#### è¿æ¥æ± ä¼˜åŒ–

```python
class ModelClientPool:
    """æ¨¡å‹å®¢æˆ·ç«¯è¿æ¥æ± """
    
    def __init__(self, max_connections: int = 10, model_config: dict = None):
        self.max_connections = max_connections
        self.model_config = model_config or {}
        self.available_clients = asyncio.Queue(maxsize=max_connections)
        self.total_clients = 0
        self.lock = asyncio.Lock()
        self.stats = {
            'requests': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
    
    async def get_client(self) -> OpenAIChatCompletionClient:
        """è·å–å®¢æˆ·ç«¯è¿æ¥"""
        
        try:
            # å°è¯•ä»æ± ä¸­è·å–
            client = self.available_clients.get_nowait()
            self.stats['cache_hits'] += 1
            return client
        except asyncio.QueueEmpty:
            # åˆ›å»ºæ–°è¿æ¥
            async with self.lock:
                if self.total_clients < self.max_connections:
                    client = self._create_client()
                    self.total_clients += 1
                    self.stats['cache_misses'] += 1
                    return client
                else:
                    # ç­‰å¾…å¯ç”¨è¿æ¥
                    client = await self.available_clients.get()
                    self.stats['cache_hits'] += 1
                    return client
    
    async def return_client(self, client: OpenAIChatCompletionClient):
        """å½’è¿˜å®¢æˆ·ç«¯è¿æ¥"""
        try:
            self.available_clients.put_nowait(client)
        except asyncio.QueueFull:
            # æ± å·²æ»¡ï¼Œå…³é—­è¿æ¥
            await self._close_client(client)
            async with self.lock:
                self.total_clients -= 1
    
    def _create_client(self) -> OpenAIChatCompletionClient:
        """åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯"""
        return OpenAIChatCompletionClient(
            model=self.model_config.get('model', 'gpt-4o-mini'),
            api_key=self.model_config.get('api_key'),
            base_url=self.model_config.get('base_url'),
            max_retries=self.model_config.get('max_retries', 3),
            timeout=self.model_config.get('timeout', 30.0)
        )
    
    async def _close_client(self, client: OpenAIChatCompletionClient):
        """å…³é—­å®¢æˆ·ç«¯è¿æ¥"""
        # å®ç°å®¢æˆ·ç«¯æ¸…ç†é€»è¾‘
        pass
    
    def get_stats(self) -> dict:
        """è·å–è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            'total_clients': self.total_clients,
            'available_clients': self.available_clients.qsize(),
            'hit_rate': self.stats['cache_hits'] / max(1, self.stats['requests'])
        }

# ä½¿ç”¨ç¤ºä¾‹
client_pool = ModelClientPool(max_connections=5, model_config={
    'model': 'gpt-4o-mini',
    'api_key': 'your-api-key'
})

class PooledAgent(AssistantAgent):
    """ä½¿ç”¨è¿æ¥æ± çš„ä»£ç†"""
    
    def __init__(self, name: str, client_pool: ModelClientPool):
        self.client_pool = client_pool
        super().__init__(
            name=name,
            model_client=None,  # å°†åœ¨è¿è¡Œæ—¶è·å–
            description="ä½¿ç”¨è¿æ¥æ± çš„é«˜æ€§èƒ½ä»£ç†"
        )
    
    async def on_messages(self, messages, cancellation_token):
        """é‡å†™æ¶ˆæ¯å¤„ç†ä»¥ä½¿ç”¨è¿æ¥æ± """
        
        # è·å–å®¢æˆ·ç«¯
        client = await self.client_pool.get_client()
        
        try:
            # ä¸´æ—¶è®¾ç½®å®¢æˆ·ç«¯
            original_client = self._model_client
            self._model_client = client
            
            # å¤„ç†æ¶ˆæ¯
            result = await super().on_messages(messages, cancellation_token)
            
            return result
        finally:
            # å½’è¿˜å®¢æˆ·ç«¯
            await self.client_pool.return_client(client)
            self._model_client = original_client
```

### 3.2 å†…å­˜ä¼˜åŒ–

#### å¯¹è±¡æ± åŒ–

```python
class MessageContextPool:
    """æ¶ˆæ¯ä¸Šä¸‹æ–‡å¯¹è±¡æ± """
    
    def __init__(self, max_size: int = 100):
        self.max_size = max_size
        self.pool = []
        self.lock = threading.Lock()
        self.created_count = 0
        self.reused_count = 0
    
    def get_context(self) -> MessageContext:
        """è·å–æ¶ˆæ¯ä¸Šä¸‹æ–‡å¯¹è±¡"""
        
        with self.lock:
            if self.pool:
                context = self.pool.pop()
                self.reused_count += 1
                return context
            else:
                context = MessageContext()
                self.created_count += 1
                return context
    
    def return_context(self, context: MessageContext):
        """å½’è¿˜æ¶ˆæ¯ä¸Šä¸‹æ–‡å¯¹è±¡"""
        
        # é‡ç½®å¯¹è±¡çŠ¶æ€
        context.reset()
        
        with self.lock:
            if len(self.pool) < self.max_size:
                self.pool.append(context)
    
    def get_stats(self) -> dict:
        """è·å–æ± ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'created_count': self.created_count,
            'reused_count': self.reused_count,
            'pool_size': len(self.pool),
            'reuse_rate': self.reused_count / max(1, self.created_count + self.reused_count)
        }

class MessageContext:
    """å¯é‡ç”¨çš„æ¶ˆæ¯ä¸Šä¸‹æ–‡"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """é‡ç½®å¯¹è±¡çŠ¶æ€"""
        self.message_id = None
        self.sender = None
        self.recipient = None
        self.timestamp = None
        self.properties = {}
        self.cancellation_token = None
```

#### å†…å­˜ç›‘æ§

```python
class MemoryMonitor:
    """å†…å­˜ä½¿ç”¨ç›‘æ§å™¨"""
    
    def __init__(self, warning_threshold: float = 0.8, critical_threshold: float = 0.9):
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
        self.callbacks = {
            'warning': [],
            'critical': [],
            'normal': []
        }
    
    def add_callback(self, level: str, callback: Callable):
        """æ·»åŠ å†…å­˜çŠ¶æ€å›è°ƒ"""
        if level in self.callbacks:
            self.callbacks[level].append(callback)
    
    def check_memory(self) -> dict:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        
        # è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
        memory_info = psutil.virtual_memory()
        usage_percent = memory_info.percent / 100.0
        
        # è·å–è¿›ç¨‹å†…å­˜ä¿¡æ¯
        process = psutil.Process()
        process_memory = process.memory_info()
        
        status = {
            'system_usage_percent': usage_percent,
            'process_memory_mb': process_memory.rss / 1024 / 1024,
            'available_memory_mb': memory_info.available / 1024 / 1024,
            'level': 'normal'
        }
        
        # ç¡®å®šå†…å­˜çŠ¶æ€çº§åˆ«
        if usage_percent >= self.critical_threshold:
            status['level'] = 'critical'
        elif usage_percent >= self.warning_threshold:
            status['level'] = 'warning'
        
        # è§¦å‘å›è°ƒ
        for callback in self.callbacks[status['level']]:
            try:
                callback(status)
            except Exception as e:
                logger.error(f"å†…å­˜ç›‘æ§å›è°ƒå¤±è´¥: {e}")
        
        return status
    
    async def start_monitoring(self, interval: float = 30.0):
        """å¯åŠ¨å†…å­˜ç›‘æ§"""
        
        while True:
            try:
                self.check_memory()
                await asyncio.sleep(interval)
            except Exception as e:
                logger.error(f"å†…å­˜ç›‘æ§å¼‚å¸¸: {e}")
                await asyncio.sleep(interval)

# ä½¿ç”¨ç¤ºä¾‹
memory_monitor = MemoryMonitor()

def on_memory_warning(status):
    logger.warning(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {status['system_usage_percent']:.1%}")
    # è§¦å‘åƒåœ¾å›æ”¶
    gc.collect()

def on_memory_critical(status):
    logger.critical(f"å†…å­˜ä½¿ç”¨ç‡å±é™©: {status['system_usage_percent']:.1%}")
    # æ¸…ç†ç¼“å­˜
    clear_caches()
    # é™åˆ¶æ–°è¯·æ±‚
    enable_backpressure()

memory_monitor.add_callback('warning', on_memory_warning)
memory_monitor.add_callback('critical', on_memory_critical)

# å¯åŠ¨ç›‘æ§
asyncio.create_task(memory_monitor.start_monitoring())
```

## 4. ç”Ÿäº§éƒ¨ç½²å®æˆ˜

### 4.1 å®¹å™¨åŒ–éƒ¨ç½²

#### Dockerfileæœ€ä½³å®è·µ

```dockerfile
# å¤šé˜¶æ®µæ„å»º - Pythonç‰ˆæœ¬
FROM python:3.11-slim as builder

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir --user -r requirements.txt

# ç”Ÿäº§é•œåƒ
FROM python:3.11-slim

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r autogen && useradd -r -g autogen autogen

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# ä»builderé˜¶æ®µå¤åˆ¶ä¾èµ–
COPY --from=builder /root/.local /home/autogen/.local

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY --chown=autogen:autogen . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PATH=/home/autogen/.local/bin:$PATH
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')" || exit 1

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER autogen

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Docker Composeé…ç½®

```yaml
version: '3.8'

services:
  # AutoGenåº”ç”¨
  autogen-app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/autogen
    depends_on:
      - redis
      - postgres
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    restart: unless-stopped

  # PostgreSQLæ•°æ®åº“
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=autogen
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped

  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped

  # Grafanaå¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    restart: unless-stopped

volumes:
  redis_data:
  postgres_data:
  prometheus_data:
  grafana_data:
```

### 4.2 Kuberneteséƒ¨ç½²

#### éƒ¨ç½²é…ç½®

```yaml
# autogen-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: autogen-app
  labels:
    app: autogen-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: autogen-app
  template:
    metadata:
      labels:
        app: autogen-app
    spec:
      containers:
      - name: autogen-app
        image: autogen-app:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: autogen-secrets
              key: openai-api-key
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: autogen-secrets
              key: database-url
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: logs
        emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: autogen-service
spec:
  selector:
    app: autogen-app
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: autogen-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: autogen-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

#### é…ç½®ç®¡ç†

```yaml
# autogen-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: autogen-config
data:
  app.yaml: |
    server:
      host: "0.0.0.0"
      port: 8000
      workers: 4
    
    logging:
      level: "INFO"
      format: "json"
      
    cache:
      type: "redis"
      ttl: 3600
      
    monitoring:
      enabled: true
      metrics_port: 9090
      
    agents:
      max_concurrent: 100
      timeout: 30
      retry_attempts: 3

---
apiVersion: v1
kind: Secret
metadata:
  name: autogen-secrets
type: Opaque
stringData:
  openai-api-key: "your-openai-api-key"
  database-url: "postgresql://user:pass@postgres:5432/autogen"
  redis-password: "your-redis-password"
```

### 4.3 ç›‘æ§å’Œå‘Šè­¦

#### Prometheusé…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "autogen_rules.yml"

scrape_configs:
  - job_name: 'autogen-app'
    static_configs:
      - targets: ['autogen-service:9090']
    metrics_path: /metrics
    scrape_interval: 10s

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

#### å‘Šè­¦è§„åˆ™

```yaml
# autogen_rules.yml
groups:
- name: autogen_alerts
  rules:
  # é«˜é”™è¯¯ç‡å‘Šè­¦
  - alert: HighErrorRate
    expr: rate(autogen_requests_failed_total[5m]) / rate(autogen_requests_total[5m]) > 0.05
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "AutoGenåº”ç”¨é”™è¯¯ç‡è¿‡é«˜"
      description: "é”™è¯¯ç‡ {{ $value | humanizePercentage }} è¶…è¿‡5%"

  # é«˜å“åº”æ—¶é—´å‘Šè­¦
  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(autogen_request_duration_seconds_bucket[5m])) > 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "AutoGenåº”ç”¨å“åº”æ—¶é—´è¿‡é•¿"
      description: "95%åˆ†ä½å“åº”æ—¶é—´ {{ $value }}s è¶…è¿‡2ç§’"

  # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
  - alert: HighMemoryUsage
    expr: (process_resident_memory_bytes / node_memory_MemTotal_bytes) > 0.8
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "AutoGenåº”ç”¨å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
      description: "å†…å­˜ä½¿ç”¨ç‡ {{ $value | humanizePercentage }} è¶…è¿‡80%"

  # ä»£ç†ç¦»çº¿å‘Šè­¦
  - alert: AgentOffline
    expr: autogen_agents_count{status="offline"} > 0
    for: 1m
    labels:
      severity: warning
    annotations:
      summary: "æœ‰ä»£ç†ç¦»çº¿"
      description: "{{ $value }}ä¸ªä»£ç†å¤„äºç¦»çº¿çŠ¶æ€"
```

#### Grafanaä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "AutoGenç›‘æ§ä»ªè¡¨æ¿",
    "panels": [
      {
        "title": "è¯·æ±‚QPS",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(autogen_requests_total[1m])",
            "legendFormat": "{{agent_type}}"
          }
        ]
      },
      {
        "title": "å“åº”æ—¶é—´åˆ†å¸ƒ",
        "type": "heatmap",
        "targets": [
          {
            "expr": "rate(autogen_request_duration_seconds_bucket[1m])",
            "format": "heatmap"
          }
        ]
      },
      {
        "title": "é”™è¯¯ç‡",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(autogen_requests_failed_total[5m]) / rate(autogen_requests_total[5m])",
            "legendFormat": "é”™è¯¯ç‡"
          }
        ]
      },
      {
        "title": "æ´»è·ƒä»£ç†æ•°",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(autogen_agents_count{status=\"online\"})",
            "legendFormat": "åœ¨çº¿ä»£ç†"
          }
        ]
      }
    ]
  }
}
```

## 5. æ•…éšœæ’æŸ¥æŒ‡å—

### 5.1 å¸¸è§é—®é¢˜è¯Šæ–­

#### æ€§èƒ½é—®é¢˜æ’æŸ¥

```python
class PerformanceDiagnostics:
    """æ€§èƒ½è¯Šæ–­å·¥å…·"""
    
    def __init__(self):
        self.metrics = {}
        self.profiler = None
    
    async def diagnose_slow_response(self, agent: AssistantAgent, test_message: str):
        """è¯Šæ–­å“åº”ç¼“æ…¢é—®é¢˜"""
        
        print("ğŸ” å¼€å§‹æ€§èƒ½è¯Šæ–­...")
        
        # 1. åŸºç¡€æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        result = await agent.run(task=test_message)
        total_time = time.time() - start_time
        
        print(f"ğŸ“Š æ€»å“åº”æ—¶é—´: {total_time:.2f}ç§’")
        
        # 2. åˆ†é˜¶æ®µè®¡æ—¶
        timings = await self._detailed_timing_analysis(agent, test_message)
        
        print("â±ï¸ è¯¦ç»†è®¡æ—¶åˆ†æ:")
        for stage, duration in timings.items():
            percentage = (duration / total_time) * 100
            print(f"  {stage}: {duration:.2f}s ({percentage:.1f}%)")
        
        # 3. èµ„æºä½¿ç”¨åˆ†æ
        resource_usage = self._analyze_resource_usage()
        print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {resource_usage['memory_mb']:.1f}MB")
        print(f"ğŸ”¥ CPUä½¿ç”¨: {resource_usage['cpu_percent']:.1f}%")
        
        # 4. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        suggestions = self._generate_optimization_suggestions(timings, resource_usage)
        
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for suggestion in suggestions:
            print(f"  â€¢ {suggestion}")
        
        return {
            'total_time': total_time,
            'timings': timings,
            'resource_usage': resource_usage,
            'suggestions': suggestions
        }
    
    async def _detailed_timing_analysis(self, agent: AssistantAgent, message: str):
        """è¯¦ç»†çš„è®¡æ—¶åˆ†æ"""
        
        timings = {}
        
        # æ¨¡æ‹Ÿå„é˜¶æ®µè®¡æ—¶
        start = time.time()
        
        # æ¶ˆæ¯é¢„å¤„ç†
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿé¢„å¤„ç†æ—¶é—´
        timings['message_preprocessing'] = time.time() - start
        
        # æ¨¡å‹è°ƒç”¨
        start = time.time()
        await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿæ¨¡å‹è°ƒç”¨æ—¶é—´
        timings['model_inference'] = time.time() - start
        
        # å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
        start = time.time()
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨æ—¶é—´
        timings['tool_execution'] = time.time() - start
        
        # å“åº”åå¤„ç†
        start = time.time()
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿåå¤„ç†æ—¶é—´
        timings['response_postprocessing'] = time.time() - start
        
        return timings
    
    def _analyze_resource_usage(self):
        """åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ"""
        
        process = psutil.Process()
        
        return {
            'memory_mb': process.memory_info().rss / 1024 / 1024,
            'cpu_percent': process.cpu_percent(),
            'thread_count': process.num_threads(),
            'open_files': len(process.open_files())
        }
    
    def _generate_optimization_suggestions(self, timings, resource_usage):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        
        suggestions = []
        
        # åŸºäºè®¡æ—¶åˆ†æçš„å»ºè®®
        if timings.get('model_inference', 0) > 1.0:
            suggestions.append("è€ƒè™‘ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹æˆ–å¯ç”¨æ¨¡å‹ç¼“å­˜")
        
        if timings.get('tool_execution', 0) > 0.5:
            suggestions.append("ä¼˜åŒ–å·¥å…·æ‰§è¡Œé€»è¾‘æˆ–ä½¿ç”¨å·¥å…·ç¼“å­˜")
        
        # åŸºäºèµ„æºä½¿ç”¨çš„å»ºè®®
        if resource_usage['memory_mb'] > 500:
            suggestions.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œè€ƒè™‘å¯ç”¨å¯¹è±¡æ± åŒ–")
        
        if resource_usage['cpu_percent'] > 80:
            suggestions.append("CPUä½¿ç”¨ç‡é«˜ï¼Œè€ƒè™‘å¯ç”¨å¼‚æ­¥å¤„ç†")
        
        if resource_usage['thread_count'] > 50:
            suggestions.append("çº¿ç¨‹æ•°è¿‡å¤šï¼Œæ£€æŸ¥æ˜¯å¦æœ‰çº¿ç¨‹æ³„æ¼")
        
        return suggestions

# ä½¿ç”¨ç¤ºä¾‹
diagnostics = PerformanceDiagnostics()
agent = AssistantAgent("test_agent", model_client)

# è¿è¡Œè¯Šæ–­
diagnosis = await diagnostics.diagnose_slow_response(
    agent, 
    "è¯·åˆ†æä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿"
)
```

#### å†…å­˜æ³„æ¼æ£€æµ‹

```python
import tracemalloc
import gc
from collections import defaultdict

class MemoryLeakDetector:
    """å†…å­˜æ³„æ¼æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.snapshots = []
        self.tracking = False
    
    def start_tracking(self):
        """å¼€å§‹å†…å­˜è¿½è¸ª"""
        tracemalloc.start()
        self.tracking = True
        self.take_snapshot("baseline")
        print("ğŸ” å¼€å§‹å†…å­˜æ³„æ¼æ£€æµ‹...")
    
    def take_snapshot(self, label: str):
        """æ‹æ‘„å†…å­˜å¿«ç…§"""
        if not self.tracking:
            return
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        snapshot = tracemalloc.take_snapshot()
        self.snapshots.append({
            'label': label,
            'snapshot': snapshot,
            'timestamp': time.time()
        })
        
        print(f"ğŸ“¸ æ‹æ‘„å†…å­˜å¿«ç…§: {label}")
    
    def analyze_leaks(self, top_n: int = 10):
        """åˆ†æå†…å­˜æ³„æ¼"""
        if len(self.snapshots) < 2:
            print("âŒ éœ€è¦è‡³å°‘2ä¸ªå¿«ç…§æ‰èƒ½åˆ†ææ³„æ¼")
            return
        
        print("\nğŸ” å†…å­˜æ³„æ¼åˆ†ææŠ¥å‘Š:")
        print("=" * 50)
        
        # æ¯”è¾ƒæœ€æ–°å’Œæœ€æ—§çš„å¿«ç…§
        first_snapshot = self.snapshots[0]['snapshot']
        last_snapshot = self.snapshots[-1]['snapshot']
        
        # è®¡ç®—å†…å­˜å¢é•¿
        top_stats = last_snapshot.compare_to(first_snapshot, 'lineno')
        
        print(f"ğŸ“Š å†…å­˜å¢é•¿æœ€å¤šçš„ {top_n} ä¸ªä½ç½®:")
        for index, stat in enumerate(top_stats[:top_n], 1):
            print(f"{index}. {stat}")
        
        # åˆ†æå¯¹è±¡ç±»å‹å¢é•¿
        self._analyze_object_growth()
        
        # ç”Ÿæˆä¿®å¤å»ºè®®
        suggestions = self._generate_leak_fix_suggestions(top_stats)
        
        print("\nğŸ’¡ ä¿®å¤å»ºè®®:")
        for suggestion in suggestions:
            print(f"  â€¢ {suggestion}")
    
    def _analyze_object_growth(self):
        """åˆ†æå¯¹è±¡ç±»å‹å¢é•¿"""
        
        print("\nğŸ“ˆ å¯¹è±¡ç±»å‹å¢é•¿åˆ†æ:")
        
        # è·å–å½“å‰å¯¹è±¡ç»Ÿè®¡
        obj_stats = defaultdict(int)
        for obj in gc.get_objects():
            obj_type = type(obj).__name__
            obj_stats[obj_type] += 1
        
        # æ˜¾ç¤ºæœ€å¤šçš„å¯¹è±¡ç±»å‹
        sorted_stats = sorted(obj_stats.items(), key=lambda x: x[1], reverse=True)
        for obj_type, count in sorted_stats[:10]:
            print(f"  {obj_type}: {count}")
    
    def _generate_leak_fix_suggestions(self, top_stats):
        """ç”Ÿæˆæ³„æ¼ä¿®å¤å»ºè®®"""
        
        suggestions = []
        
        for stat in top_stats[:5]:
            filename = stat.traceback.format()[-1]
            
            if 'asyncio' in filename:
                suggestions.append("æ£€æŸ¥å¼‚æ­¥ä»»åŠ¡æ˜¯å¦æ­£ç¡®æ¸…ç†ï¼Œé¿å…ä»»åŠ¡æ³„æ¼")
            elif 'cache' in filename.lower():
                suggestions.append("æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰TTLè®¾ç½®ï¼Œé¿å…ç¼“å­˜æ— é™å¢é•¿")
            elif 'list' in str(stat) or 'dict' in str(stat):
                suggestions.append("æ£€æŸ¥å®¹å™¨å¯¹è±¡æ˜¯å¦åŠæ—¶æ¸…ç†ï¼Œé¿å…å¼•ç”¨ç´¯ç§¯")
            elif 'agent' in filename.lower():
                suggestions.append("æ£€æŸ¥ä»£ç†å¯¹è±¡æ˜¯å¦æ­£ç¡®é‡Šæ”¾ï¼Œé¿å…ä»£ç†å®ä¾‹æ³„æ¼")
        
        if not suggestions:
            suggestions.append("å†…å­˜å¢é•¿å¯èƒ½æ˜¯æ­£å¸¸çš„ä¸šåŠ¡å¢é•¿ï¼Œç»§ç»­ç›‘æ§")
        
        return suggestions
    
    def stop_tracking(self):
        """åœæ­¢å†…å­˜è¿½è¸ª"""
        if self.tracking:
            tracemalloc.stop()
            self.tracking = False
            print("â¹ï¸ åœæ­¢å†…å­˜æ³„æ¼æ£€æµ‹")

# ä½¿ç”¨ç¤ºä¾‹
leak_detector = MemoryLeakDetector()

# å¼€å§‹æ£€æµ‹
leak_detector.start_tracking()

# è¿è¡Œä¸€äº›å¯èƒ½å¯¼è‡´å†…å­˜æ³„æ¼çš„æ“ä½œ
for i in range(100):
    agent = AssistantAgent(f"agent_{i}", model_client)
    # æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
    await agent.run(task="ç®€å•æµ‹è¯•")
    
    if i % 20 == 0:
        leak_detector.take_snapshot(f"iteration_{i}")

# åˆ†æç»“æœ
leak_detector.analyze_leaks()
leak_detector.stop_tracking()
```

### 5.2 æ—¥å¿—åˆ†æå·¥å…·

```python
import re
from datetime import datetime, timedelta
from collections import Counter, defaultdict

class LogAnalyzer:
    """æ—¥å¿—åˆ†æå·¥å…·"""
    
    def __init__(self, log_file: str):
        self.log_file = log_file
        self.patterns = {
            'error': re.compile(r'ERROR.*?(\w+Error|Exception).*?$', re.MULTILINE),
            'warning': re.compile(r'WARNING.*?$', re.MULTILINE),
            'performance': re.compile(r'å¤„ç†æ—¶é—´.*?(\d+\.?\d*).*?ms', re.MULTILINE),
            'agent': re.compile(r'ä»£ç†.*?(\w+).*?(æˆåŠŸ|å¤±è´¥)', re.MULTILINE),
            'timestamp': re.compile(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})')
        }
    
    def analyze_errors(self, hours: int = 24):
        """åˆ†æé”™è¯¯æ—¥å¿—"""
        
        print(f"ğŸ” åˆ†ææœ€è¿‘ {hours} å°æ—¶çš„é”™è¯¯æ—¥å¿—...")
        
        with open(self.log_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–é”™è¯¯ä¿¡æ¯
        errors = self.patterns['error'].findall(content)
        error_counter = Counter(errors)
        
        print(f"ğŸ“Š å‘ç° {len(errors)} ä¸ªé”™è¯¯:")
        for error_type, count in error_counter.most_common(10):
            print(f"  {error_type}: {count} æ¬¡")
        
        # åˆ†æé”™è¯¯è¶‹åŠ¿
        self._analyze_error_trends(content)
        
        return error_counter
    
    def analyze_performance(self):
        """åˆ†ææ€§èƒ½æ—¥å¿—"""
        
        print("âš¡ åˆ†ææ€§èƒ½æŒ‡æ ‡...")
        
        with open(self.log_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–å¤„ç†æ—¶é—´
        times = [float(t) for t in self.patterns['performance'].findall(content)]
        
        if times:
            avg_time = sum(times) / len(times)
            max_time = max(times)
            min_time = min(times)
            
            print(f"ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
            print(f"  å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f}ms")
            print(f"  æœ€å¤§å¤„ç†æ—¶é—´: {max_time:.2f}ms")
            print(f"  æœ€å°å¤„ç†æ—¶é—´: {min_time:.2f}ms")
            print(f"  æ€»è¯·æ±‚æ•°: {len(times)}")
            
            # åˆ†ææ…¢è¯·æ±‚
            slow_requests = [t for t in times if t > avg_time * 2]
            if slow_requests:
                print(f"  æ…¢è¯·æ±‚æ•°: {len(slow_requests)} ({len(slow_requests)/len(times)*100:.1f}%)")
        
        return times
    
    def analyze_agent_activity(self):
        """åˆ†æä»£ç†æ´»åŠ¨"""
        
        print("ğŸ¤– åˆ†æä»£ç†æ´»åŠ¨...")
        
        with open(self.log_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–ä»£ç†æ´»åŠ¨
        activities = self.patterns['agent'].findall(content)
        
        agent_stats = defaultdict(lambda: {'æˆåŠŸ': 0, 'å¤±è´¥': 0})
        for agent, status in activities:
            agent_stats[agent][status] += 1
        
        print("ğŸ“Š ä»£ç†æ´»åŠ¨ç»Ÿè®¡:")
        for agent, stats in agent_stats.items():
            total = stats['æˆåŠŸ'] + stats['å¤±è´¥']
            success_rate = stats['æˆåŠŸ'] / total * 100 if total > 0 else 0
            print(f"  {agent}: æˆåŠŸ {stats['æˆåŠŸ']}, å¤±è´¥ {stats['å¤±è´¥']}, æˆåŠŸç‡ {success_rate:.1f}%")
        
        return agent_stats
    
    def _analyze_error_trends(self, content: str):
        """åˆ†æé”™è¯¯è¶‹åŠ¿"""
        
        # æŒ‰å°æ—¶ç»Ÿè®¡é”™è¯¯
        hourly_errors = defaultdict(int)
        
        lines = content.split('\n')
        for line in lines:
            if 'ERROR' in line:
                timestamp_match = self.patterns['timestamp'].search(line)
                if timestamp_match:
                    timestamp_str = timestamp_match.group(1)
                    try:
                        dt = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                        hour_key = dt.strftime('%Y-%m-%d %H:00')
                        hourly_errors[hour_key] += 1
                    except ValueError:
                        continue
        
        if hourly_errors:
            print("\nğŸ“ˆ é”™è¯¯è¶‹åŠ¿ (æŒ‰å°æ—¶):")
            sorted_hours = sorted(hourly_errors.items())
            for hour, count in sorted_hours[-24:]:  # æœ€è¿‘24å°æ—¶
                print(f"  {hour}: {count} ä¸ªé”™è¯¯")
    
    def generate_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        
        print("ğŸ“‹ ç”Ÿæˆæ—¥å¿—åˆ†ææŠ¥å‘Š...")
        print("=" * 60)
        
        # é”™è¯¯åˆ†æ
        errors = self.analyze_errors()
        print()
        
        # æ€§èƒ½åˆ†æ
        performance = self.analyze_performance()
        print()
        
        # ä»£ç†æ´»åŠ¨åˆ†æ
        agents = self.analyze_agent_activity()
        print()
        
        # ç”Ÿæˆå»ºè®®
        suggestions = self._generate_suggestions(errors, performance, agents)
        
        print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for suggestion in suggestions:
            print(f"  â€¢ {suggestion}")
        
        return {
            'errors': errors,
            'performance': performance,
            'agents': agents,
            'suggestions': suggestions
        }
    
    def _generate_suggestions(self, errors, performance, agents):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        
        suggestions = []
        
        # åŸºäºé”™è¯¯åˆ†æçš„å»ºè®®
        if errors:
            most_common_error = errors.most_common(1)[0]
            if 'TimeoutError' in most_common_error[0]:
                suggestions.append("è¶…æ—¶é”™è¯¯è¾ƒå¤šï¼Œè€ƒè™‘å¢åŠ è¶…æ—¶æ—¶é—´æˆ–ä¼˜åŒ–å¤„ç†é€»è¾‘")
            elif 'ConnectionError' in most_common_error[0]:
                suggestions.append("è¿æ¥é”™è¯¯è¾ƒå¤šï¼Œæ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒæœåŠ¡å¯ç”¨æ€§")
        
        # åŸºäºæ€§èƒ½åˆ†æçš„å»ºè®®
        if performance:
            avg_time = sum(performance) / len(performance)
            if avg_time > 1000:  # è¶…è¿‡1ç§’
                suggestions.append("å¹³å‡å“åº”æ—¶é—´è¾ƒé•¿ï¼Œè€ƒè™‘æ€§èƒ½ä¼˜åŒ–")
        
        # åŸºäºä»£ç†æ´»åŠ¨çš„å»ºè®®
        for agent, stats in agents.items():
            total = stats['æˆåŠŸ'] + stats['å¤±è´¥']
            if total > 0:
                success_rate = stats['æˆåŠŸ'] / total
                if success_rate < 0.9:  # æˆåŠŸç‡ä½äº90%
                    suggestions.append(f"ä»£ç† {agent} æˆåŠŸç‡è¾ƒä½ï¼Œéœ€è¦æ£€æŸ¥å’Œä¼˜åŒ–")
        
        if not suggestions:
            suggestions.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œç»§ç»­ç›‘æ§")
        
        return suggestions

# ä½¿ç”¨ç¤ºä¾‹
analyzer = LogAnalyzer('/app/logs/autogen.log')
report = analyzer.generate_report()
```

## 6. æ€»ç»“

æœ¬å®æˆ˜æŒ‡å—æ¶µç›–äº†AutoGenæ¡†æ¶ä»å¼€å‘åˆ°ç”Ÿäº§éƒ¨ç½²çš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š

### 6.1 æ ¸å¿ƒè¦ç‚¹

1. **æ¶æ„è®¾è®¡**ï¼šéµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼Œåˆç†è®¾è®¡ä»£ç†å’Œå›¢é˜Ÿç»“æ„
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šä½¿ç”¨æ‰¹å¤„ç†ã€è¿æ¥æ± ã€å¯¹è±¡æ± ç­‰æŠ€æœ¯æå‡æ€§èƒ½
3. **å†…å­˜ç®¡ç†**ï¼šå®æ–½å†…å­˜ç›‘æ§å’Œæ³„æ¼æ£€æµ‹ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®š
4. **å®¹å™¨åŒ–éƒ¨ç½²**ï¼šä½¿ç”¨Dockerå’ŒKuberneteså®ç°å¯æ‰©å±•éƒ¨ç½²
5. **ç›‘æ§å‘Šè­¦**ï¼šå»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³»ï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³é—®é¢˜
6. **æ•…éšœæ’æŸ¥**ï¼šæŒæ¡å¸¸è§é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³æ–¹æ³•

### 6.2 æœ€ä½³å®è·µæ€»ç»“

- **å¼€å‘é˜¶æ®µ**ï¼šæ³¨é‡ä»£ç è´¨é‡ï¼Œç¼–å†™å……åˆ†çš„æµ‹è¯•
- **æµ‹è¯•é˜¶æ®µ**ï¼šè¿›è¡Œæ€§èƒ½æµ‹è¯•å’Œå‹åŠ›æµ‹è¯•
- **éƒ¨ç½²é˜¶æ®µ**ï¼šä½¿ç”¨æ¸è¿›å¼éƒ¨ç½²ç­–ç•¥
- **è¿ç»´é˜¶æ®µ**ï¼šå»ºç«‹å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶
- **ä¼˜åŒ–é˜¶æ®µ**ï¼šåŸºäºç›‘æ§æ•°æ®æŒç»­ä¼˜åŒ–æ€§èƒ½

é€šè¿‡éµå¾ªè¿™äº›å®æˆ˜ç»éªŒå’Œæœ€ä½³å®è·µï¼Œå¯ä»¥æ„å»ºå‡ºé«˜æ€§èƒ½ã€é«˜å¯ç”¨çš„AutoGenåº”ç”¨ç³»ç»Ÿã€‚

<function_calls>
<invoke name="todo_write">
<parameter name="merge">true

