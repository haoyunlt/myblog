---
title: "Milvus æºç æ·±åº¦å‰–æ"
date: 2024-12-19T10:00:00+08:00
draft: false
tags: ["Milvus", "å‘é‡æ•°æ®åº“", "æºç åˆ†æ", "åˆ†å¸ƒå¼ç³»ç»Ÿ", "AIåŸºç¡€è®¾æ–½"]
categories: ["milvus", "æŠ€æœ¯åˆ†æ"]
description: "æ·±å…¥å‰–æ Milvus å‘é‡æ•°æ®åº“çš„æºç å®ç°ï¼ŒåŒ…æ‹¬æ¶æ„è®¾è®¡ã€æ ¸å¿ƒæ¨¡å—å’Œæ€§èƒ½ä¼˜åŒ–"
weight: 310
slug: "milvus-source-analysis"
---

# Milvus æºç æ·±åº¦å‰–æ

## ç›®å½•

1. [æ¡†æ¶ä½¿ç”¨æ‰‹å†Œ](#1-æ¡†æ¶ä½¿ç”¨æ‰‹å†Œ)
2. [å¯¹å¤– API æ·±å…¥åˆ†æ](#2-å¯¹å¤–-api-æ·±å…¥åˆ†æ)
3. [æ•´ä½“æ¶æ„è®¾è®¡](#3-æ•´ä½“æ¶æ„è®¾è®¡)
4. [æ ¸å¿ƒæ¨¡å—åˆ†æ](#4-æ ¸å¿ƒæ¨¡å—åˆ†æ)
5. [å…³é”®æ•°æ®ç»“æ„](#5-å…³é”®æ•°æ®ç»“æ„)
6. [å®æˆ˜ç»éªŒæ€»ç»“](#6-å®æˆ˜ç»éªŒæ€»ç»“)

---

## 1. æ¡†æ¶ä½¿ç”¨æ‰‹å†Œ

### 1.1 Milvus ç®€ä»‹

Milvus æ˜¯ä¸€ä¸ªå¼€æºçš„å‘é‡æ•°æ®åº“ï¼Œä¸“ä¸º AI åº”ç”¨å’Œå‘é‡ç›¸ä¼¼åº¦æœç´¢è€Œè®¾è®¡ã€‚å®ƒé‡‡ç”¨äº‘åŸç”Ÿæ¶æ„ï¼Œæ”¯æŒå­˜å‚¨ä¸è®¡ç®—åˆ†ç¦»ï¼Œå…·å¤‡é«˜æ€§èƒ½ã€é«˜å¯ç”¨æ€§å’Œæ°´å¹³æ‰©å±•èƒ½åŠ›ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- ğŸš€ **é«˜æ€§èƒ½**ï¼šä¸‡äº¿çº§å‘é‡æ¯«ç§’çº§æœç´¢
- ğŸŒ **äº‘åŸç”Ÿ**ï¼šå­˜å‚¨è®¡ç®—åˆ†ç¦»ï¼Œæ”¯æŒ Kubernetes
- ğŸ”§ **å¤šç´¢å¼•æ”¯æŒ**ï¼šHNSWã€IVFã€FLATã€SCANNã€DiskANN
- ğŸ›¡ï¸ **é«˜å¯ç”¨**ï¼š99.9% å¯ç”¨æ€§ä¿è¯
- ğŸ“Š **å¤šæ•°æ®ç±»å‹**ï¼šå‘é‡ã€æ ‡é‡ã€VARCHAR æ”¯æŒ
- ğŸ” **æ··åˆæœç´¢**ï¼šè¯­ä¹‰æœç´¢ + å…¨æ–‡æœç´¢

### 1.2 å¿«é€Ÿå¼€å§‹

#### å®‰è£…éƒ¨ç½²

```bash
# ä½¿ç”¨ Docker Compose éƒ¨ç½²
wget https://github.com/milvus-io/milvus/releases/download/v2.3.0/milvus-standalone-docker-compose.yml -O docker-compose.yml
docker-compose up -d

# ä½¿ç”¨ Kubernetes Helm éƒ¨ç½²
helm repo add milvus https://milvus-io.github.io/milvus-helm/
helm install my-release milvus/milvus
```

#### Python SDK ä½¿ç”¨

```python
from pymilvus import MilvusClient

# è¿æ¥ Milvus
client = MilvusClient(
    uri="http://localhost:19530",
    token="username:password"  # å¯é€‰
)

# åˆ›å»ºé›†åˆ
client.create_collection(
    collection_name="demo_collection",
    dimension=768,
    metric_type="IP",
    consistency_level="Strong"
)

# æ’å…¥æ•°æ®
data = [
    {"id": 1, "vector": [0.1, 0.2, ...], "text": "sample text"},
    {"id": 2, "vector": [0.3, 0.4, ...], "text": "another text"}
]
client.insert(collection_name="demo_collection", data=data)

# å‘é‡æœç´¢
results = client.search(
    collection_name="demo_collection",
    data=[[0.1, 0.2, ...]],  # æŸ¥è¯¢å‘é‡
    limit=10,
    output_fields=["text"]
)
```

### 1.3 æ¶æ„æ¨¡å¼

Milvus æ”¯æŒä¸¤ç§éƒ¨ç½²æ¨¡å¼ï¼š

#### å•æœºæ¨¡å¼ (Standalone)
- æ‰€æœ‰ç»„ä»¶è¿è¡Œåœ¨å•ä¸ªè¿›ç¨‹ä¸­
- é€‚åˆå¼€å‘æµ‹è¯•å’Œå°è§„æ¨¡åº”ç”¨
- èµ„æºéœ€æ±‚è¾ƒä½

#### é›†ç¾¤æ¨¡å¼ (Cluster)
- å¾®æœåŠ¡æ¶æ„ï¼Œç»„ä»¶ç‹¬ç«‹éƒ¨ç½²
- æ”¯æŒæ°´å¹³æ‰©å±•å’Œé«˜å¯ç”¨
- é€‚åˆç”Ÿäº§ç¯å¢ƒ

---

## 2. å¯¹å¤– API æ·±å…¥åˆ†æ

### 2.1 API æ¶æ„æ¦‚è§ˆ

Milvus é€šè¿‡ Proxy ç»„ä»¶å¯¹å¤–æä¾›ç»Ÿä¸€çš„ API æœåŠ¡ï¼Œæ”¯æŒ gRPC å’Œ RESTful ä¸¤ç§åè®®ã€‚

```mermaid
graph TB
    Client[å®¢æˆ·ç«¯] --> Proxy[Proxy ç½‘å…³]
    Proxy --> RootCoord[RootCoord<br/>å…ƒæ•°æ®ç®¡ç†]
    Proxy --> DataCoord[DataCoord<br/>æ•°æ®åè°ƒ]
    Proxy --> QueryCoord[QueryCoord<br/>æŸ¥è¯¢åè°ƒ]
    Proxy --> DataNode[DataNode<br/>æ•°æ®èŠ‚ç‚¹]
    Proxy --> QueryNode[QueryNode<br/>æŸ¥è¯¢èŠ‚ç‚¹]
```

### 2.2 æ ¸å¿ƒ API æ¥å£åˆ†æ

#### 2.2.1 é›†åˆç®¡ç† API

**CreateCollection - åˆ›å»ºé›†åˆ**

```go
// æ¥å£å®šä¹‰ï¼šinternal/proxy/impl.go
func (node *Proxy) CreateCollection(ctx context.Context, request *milvuspb.CreateCollectionRequest) (*commonpb.Status, error) {
    // 1. å‚æ•°éªŒè¯
    if err := merr.CheckHealthy(node.GetStateCode()); err != nil {
        return merr.Status(err), nil
    }
    
    // 2. æƒé™æ£€æŸ¥
    if err := node.checkAuth(ctx, request.GetDbName(), request.GetCollectionName(), commonpb.ObjectType_Collection, commonpb.ObjectPrivilege_PrivilegeCreateCollection); err != nil {
        return merr.Status(err), nil
    }
    
    // 3. åˆ›å»ºä»»åŠ¡å¹¶è°ƒåº¦
    cct := &createCollectionTask{
        baseTask: baseTask{
            ctx:  ctx,
            done: make(chan error, 1),
        },
        Condition: NewTaskCondition(ctx),
        CreateCollectionRequest: request,
        mixCoord: node.mixCoord,
    }
    
    // 4. æäº¤åˆ°ä»»åŠ¡è°ƒåº¦å™¨
    if err := node.sched.ddQueue.Enqueue(cct); err != nil {
        return merr.Status(err), nil
    }
    
    // 5. ç­‰å¾…ä»»åŠ¡å®Œæˆ
    if err := cct.WaitToFinish(); err != nil {
        return merr.Status(err), nil
    }
    
    return cct.result, nil
}
```

**å…³é”®è°ƒç”¨é“¾è·¯ï¼š**
1. `Proxy.CreateCollection()` - API å…¥å£
2. `createCollectionTask.Execute()` - ä»»åŠ¡æ‰§è¡Œ
3. `MixCoord.CreateCollection()` - åè°ƒå™¨å¤„ç†
4. `RootCoord.CreateCollection()` - å…ƒæ•°æ®å­˜å‚¨

#### 2.2.2 æ•°æ®æ“ä½œ API

**Insert - æ•°æ®æ’å…¥**

```go
// æ¥å£å®šä¹‰ï¼šinternal/proxy/impl.go
func (node *Proxy) Insert(ctx context.Context, request *milvuspb.InsertRequest) (*milvuspb.MutationResult, error) {
    // 1. å¥åº·æ£€æŸ¥
    if err := merr.CheckHealthy(node.GetStateCode()); err != nil {
        return &milvuspb.MutationResult{Status: merr.Status(err)}, nil
    }
    
    // 2. é€Ÿç‡é™åˆ¶æ£€æŸ¥
    if err := node.rateLimiter.Check(internalpb.RateType_DMLInsert, 1); err != nil {
        return &milvuspb.MutationResult{Status: merr.Status(err)}, nil
    }
    
    // 3. åˆ›å»ºæ’å…¥ä»»åŠ¡
    it := &insertTask{
        baseTask: baseTask{
            ctx:  ctx,
            done: make(chan error, 1),
        },
        Condition: NewTaskCondition(ctx),
        insertMsg: &msgstream.InsertMsg{
            BaseMsg: msgstream.BaseMsg{
                Ctx: ctx,
            },
            InsertRequest: *request,
        },
    }
    
    // 4. ä»»åŠ¡é¢„å¤„ç†
    if err := it.PreExecute(ctx); err != nil {
        return &milvuspb.MutationResult{Status: merr.Status(err)}, nil
    }
    
    // 5. æäº¤åˆ° DML é˜Ÿåˆ—
    if err := node.sched.dmlQueue.Enqueue(it); err != nil {
        return &milvuspb.MutationResult{Status: merr.Status(err)}, nil
    }
    
    // 6. ç­‰å¾…æ‰§è¡Œå®Œæˆ
    if err := it.WaitToFinish(); err != nil {
        return &milvuspb.MutationResult{Status: merr.Status(err)}, nil
    }
    
    return it.result, nil
}
```

**Insert è°ƒç”¨æ—¶åºå›¾ï¼š**

```mermaid
sequenceDiagram
    participant Client
    participant Proxy
    participant DataCoord
    participant DataNode
    participant MsgStream
    
    Client->>Proxy: Insert Request
    Proxy->>Proxy: éªŒè¯å‚æ•°å’Œæƒé™
    Proxy->>DataCoord: è·å– Segment ä¿¡æ¯
    DataCoord-->>Proxy: è¿”å› Segment ID
    Proxy->>MsgStream: å‘é€ Insert æ¶ˆæ¯
    MsgStream->>DataNode: æ¶ˆè´¹ Insert æ¶ˆæ¯
    DataNode->>DataNode: å†™å…¥æ•°æ®åˆ° Segment
    DataNode-->>Proxy: è¿”å›æ’å…¥ç»“æœ
    Proxy-->>Client: è¿”å› MutationResult
```

#### 2.2.3 æŸ¥è¯¢æœç´¢ API

**Search - å‘é‡æœç´¢**

```go
// æ¥å£å®šä¹‰ï¼šinternal/proxy/impl.go
func (node *Proxy) Search(ctx context.Context, request *milvuspb.SearchRequest) (*milvuspb.SearchResults, error) {
    // 1. å¥åº·æ£€æŸ¥å’Œæƒé™éªŒè¯
    if err := merr.CheckHealthy(node.GetStateCode()); err != nil {
        return &milvuspb.SearchResults{Status: merr.Status(err)}, nil
    }
    
    // 2. é€Ÿç‡é™åˆ¶
    if err := node.rateLimiter.Check(internalpb.RateType_DQLSearch, 1); err != nil {
        return &milvuspb.SearchResults{Status: merr.Status(err)}, nil
    }
    
    // 3. åˆ›å»ºæœç´¢ä»»åŠ¡
    st := &searchTask{
        baseTask: baseTask{
            ctx:  ctx,
            done: make(chan error, 1),
        },
        Condition: NewTaskCondition(ctx),
        SearchRequest: request,
        queryCoord: node.queryCoord,
        queryNodes: node.queryNodes,
    }
    
    // 4. ä»»åŠ¡é¢„å¤„ç† - è§£ææŸ¥è¯¢å‚æ•°
    if err := st.PreExecute(ctx); err != nil {
        return &milvuspb.SearchResults{Status: merr.Status(err)}, nil
    }
    
    // 5. æäº¤åˆ°æŸ¥è¯¢é˜Ÿåˆ—
    if err := node.sched.dqQueue.Enqueue(st); err != nil {
        return &milvuspb.SearchResults{Status: merr.Status(err)}, nil
    }
    
    // 6. ç­‰å¾…æœç´¢å®Œæˆ
    if err := st.WaitToFinish(); err != nil {
        return &milvuspb.SearchResults{Status: merr.Status(err)}, nil
    }
    
    return st.result, nil
}
```

**Search æ‰§è¡Œæµç¨‹ï¼š**

```go
// æœç´¢ä»»åŠ¡æ‰§è¡Œé€»è¾‘ï¼šinternal/proxy/task_search.go
func (st *searchTask) Execute(ctx context.Context) error {
    // 1. è·å–é›†åˆä¿¡æ¯
    collInfo, err := st.getCollectionInfo(ctx)
    if err != nil {
        return err
    }
    
    // 2. æŸ¥è¯¢åˆ†ç‰‡ä¿¡æ¯
    shards, err := st.getShards(ctx, collInfo.CollectionID)
    if err != nil {
        return err
    }
    
    // 3. å¹¶è¡ŒæŸ¥è¯¢å„ä¸ª QueryNode
    var wg sync.WaitGroup
    resultCh := make(chan *internalpb.SearchResults, len(shards))
    
    for _, shard := range shards {
        wg.Add(1)
        go func(s *shardInfo) {
            defer wg.Done()
            // è°ƒç”¨ QueryNode æ‰§è¡Œæœç´¢
            result, err := st.queryNode.Search(ctx, &internalpb.SearchRequest{
                Base: commonpbutil.NewMsgBase(),
                ReqID: st.ReqID,
                DbID: collInfo.DbID,
                CollectionID: collInfo.CollectionID,
                PartitionIDs: st.PartitionIDs,
                Dsl: st.Dsl,
                PlaceholderGroup: st.PlaceholderGroup,
                DslType: st.DslType,
                SerializedExprPlan: st.serializedExprPlan,
                OutputFieldsId: st.OutputFieldsId,
                TravelTimestamp: st.TravelTimestamp,
                GuaranteeTimestamp: st.GuaranteeTimestamp,
            })
            if err == nil {
                resultCh <- result
            }
        }(shard)
    }
    
    // 4. ç­‰å¾…æ‰€æœ‰æŸ¥è¯¢å®Œæˆ
    wg.Wait()
    close(resultCh)
    
    // 5. åˆå¹¶æŸ¥è¯¢ç»“æœ
    var results []*internalpb.SearchResults
    for result := range resultCh {
        results = append(results, result)
    }
    
    // 6. ç»“æœèšåˆå’Œæ’åº
    st.result = st.reduceSearchResults(results)
    return nil
}
```

### 2.3 API æ‹¦æˆªå™¨æœºåˆ¶

Milvus ä½¿ç”¨æ‹¦æˆªå™¨æ¨¡å¼å®ç°æ¨ªåˆ‡å…³æ³¨ç‚¹ï¼š

```go
// æ•°æ®åº“æ‹¦æˆªå™¨ï¼šinternal/proxy/database_interceptor.go
func DatabaseInterceptor() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req any, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface{}, err error) {
        filledCtx, filledReq := fillDatabase(ctx, req)
        return handler(filledCtx, filledReq)
    }
}

// è®¤è¯æ‹¦æˆªå™¨ï¼šinternal/proxy/authentication_interceptor.go
func AuthenticationInterceptor() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
        // éªŒè¯ç”¨æˆ·èº«ä»½
        if err := validateAuth(ctx, req); err != nil {
            return nil, err
        }
        return handler(ctx, req)
    }
}

// é€Ÿç‡é™åˆ¶æ‹¦æˆªå™¨ï¼šinternal/proxy/rate_limit_interceptor.go
func RateLimitInterceptor() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
        // æ£€æŸ¥é€Ÿç‡é™åˆ¶
        if err := checkRateLimit(ctx, req); err != nil {
            return nil, err
        }
        return handler(ctx, req)
    }
}
```

---

## 3. æ•´ä½“æ¶æ„è®¾è®¡

### 3.1 ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    subgraph "å®¢æˆ·ç«¯å±‚"
        SDK[Python/Go/Java SDK]
        REST[RESTful API]
    end
    
    subgraph "æ¥å…¥å±‚"
        LB[è´Ÿè½½å‡è¡¡å™¨]
        Proxy1[Proxy-1]
        Proxy2[Proxy-2]
        ProxyN[Proxy-N]
    end
    
    subgraph "åè°ƒå±‚"
        RootCoord[RootCoord<br/>å…ƒæ•°æ®ç®¡ç†]
        DataCoord[DataCoord<br/>æ•°æ®åè°ƒ]
        QueryCoord[QueryCoord<br/>æŸ¥è¯¢åè°ƒ]
    end
    
    subgraph "æ‰§è¡Œå±‚"
        DataNode1[DataNode-1]
        DataNode2[DataNode-2]
        QueryNode1[QueryNode-1]
        QueryNode2[QueryNode-2]
    end
    
    subgraph "å­˜å‚¨å±‚"
        MetaStore[(å…ƒæ•°æ®å­˜å‚¨<br/>etcd)]
        MsgQueue[æ¶ˆæ¯é˜Ÿåˆ—<br/>Pulsar/Kafka]
        ObjectStore[(å¯¹è±¡å­˜å‚¨<br/>MinIO/S3)]
    end
    
    SDK --> LB
    REST --> LB
    LB --> Proxy1
    LB --> Proxy2
    LB --> ProxyN
    
    Proxy1 --> RootCoord
    Proxy1 --> DataCoord
    Proxy1 --> QueryCoord
    
    RootCoord --> MetaStore
    DataCoord --> DataNode1
    DataCoord --> DataNode2
    QueryCoord --> QueryNode1
    QueryCoord --> QueryNode2
    
    DataNode1 --> MsgQueue
    DataNode1 --> ObjectStore
    QueryNode1 --> ObjectStore
```

### 3.2 æ ¸å¿ƒç»„ä»¶äº¤äº’æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant Client
    participant Proxy
    participant RootCoord
    participant DataCoord
    participant QueryCoord
    participant DataNode
    participant QueryNode
    participant Storage
    
    Note over Client,Storage: æ•°æ®æ’å…¥æµç¨‹
    Client->>Proxy: Insert Request
    Proxy->>RootCoord: éªŒè¯é›†åˆä¿¡æ¯
    RootCoord-->>Proxy: é›†åˆå…ƒæ•°æ®
    Proxy->>DataCoord: åˆ†é… Segment
    DataCoord-->>Proxy: Segment ä¿¡æ¯
    Proxy->>DataNode: å†™å…¥æ•°æ®
    DataNode->>Storage: æŒä¹…åŒ–æ•°æ®
    DataNode-->>Proxy: å†™å…¥ç¡®è®¤
    Proxy-->>Client: Insert Response
    
    Note over Client,Storage: æ•°æ®æŸ¥è¯¢æµç¨‹
    Client->>Proxy: Search Request
    Proxy->>RootCoord: è·å–é›†åˆä¿¡æ¯
    RootCoord-->>Proxy: é›†åˆå…ƒæ•°æ®
    Proxy->>QueryCoord: è·å–æŸ¥è¯¢è®¡åˆ’
    QueryCoord-->>Proxy: æŸ¥è¯¢åˆ†ç‰‡ä¿¡æ¯
    Proxy->>QueryNode: æ‰§è¡Œå‘é‡æœç´¢
    QueryNode->>Storage: è¯»å–ç´¢å¼•æ•°æ®
    Storage-->>QueryNode: è¿”å›æ•°æ®
    QueryNode-->>Proxy: æœç´¢ç»“æœ
    Proxy-->>Client: Search Response
```

### 3.3 æ•°æ®æµæ¶æ„

```mermaid
graph LR
    subgraph "æ•°æ®å†™å…¥æµ"
        Insert[æ•°æ®æ’å…¥] --> Proxy1[Proxy]
        Proxy1 --> MsgStream[æ¶ˆæ¯æµ]
        MsgStream --> DataNode1[DataNode]
        DataNode1 --> Segment[Segment æ–‡ä»¶]
        Segment --> ObjectStorage[(å¯¹è±¡å­˜å‚¨)]
    end
    
    subgraph "æ•°æ®æŸ¥è¯¢æµ"
        Search[å‘é‡æœç´¢] --> Proxy2[Proxy]
        Proxy2 --> QueryNode1[QueryNode]
        QueryNode1 --> Index[ç´¢å¼•æ–‡ä»¶]
        Index --> ObjectStorage
    end
    
    subgraph "å…ƒæ•°æ®æµ"
        MetaOp[å…ƒæ•°æ®æ“ä½œ] --> RootCoord1[RootCoord]
        RootCoord1 --> etcd[(etcd)]
    end
```

---

## 4. æ ¸å¿ƒæ¨¡å—åˆ†æ

### 4.1 Proxy æ¨¡å— - API ç½‘å…³

#### 4.1.1 æ¨¡å—æ¶æ„

```mermaid
graph TB
    subgraph "Proxy å†…éƒ¨æ¶æ„"
        API[gRPC/HTTP API]
        Auth[è®¤è¯æˆæƒ]
        RateLimit[é€Ÿç‡é™åˆ¶]
        TaskScheduler[ä»»åŠ¡è°ƒåº¦å™¨]
        
        subgraph "ä»»åŠ¡é˜Ÿåˆ—"
            DDLQueue[DDL é˜Ÿåˆ—]
            DMLQueue[DML é˜Ÿåˆ—]
            DQLQueue[DQL é˜Ÿåˆ—]
        end
        
        subgraph "å®¢æˆ·ç«¯ç®¡ç†"
            ConnMgr[è¿æ¥ç®¡ç†]
            ShardMgr[åˆ†ç‰‡ç®¡ç†]
            LBPolicy[è´Ÿè½½å‡è¡¡]
        end
    end
    
    API --> Auth
    Auth --> RateLimit
    RateLimit --> TaskScheduler
    TaskScheduler --> DDLQueue
    TaskScheduler --> DMLQueue
    TaskScheduler --> DQLQueue
```

#### 4.1.2 æ ¸å¿ƒæ•°æ®ç»“æ„

```go
// Proxy ä¸»ç»“æ„ï¼šinternal/proxy/proxy.go
type Proxy struct {
    milvuspb.UnimplementedMilvusServiceServer
    
    ctx    context.Context
    cancel context.CancelFunc
    wg     sync.WaitGroup
    
    // åŸºç¡€é…ç½®
    initParams *internalpb.InitParams
    ip         string
    port       int
    stateCode  atomic.Int32
    address    string
    
    // åè°ƒå™¨å®¢æˆ·ç«¯
    mixCoord types.MixCoordClient
    
    // é™æµå™¨
    simpleLimiter *SimpleLimiter
    
    // é€šé“ç®¡ç†å™¨
    chMgr channelsMgr
    
    // ä»»åŠ¡è°ƒåº¦å™¨
    sched *taskScheduler
    
    // ID å’Œæ—¶é—´æˆ³åˆ†é…å™¨
    rowIDAllocator *allocator.IDAllocator
    tsoAllocator   *timestampAllocator
    
    // æŒ‡æ ‡ç¼“å­˜ç®¡ç†å™¨
    metricsCacheManager *metricsinfo.MetricsCacheManager
    
    // ä¼šè¯å’Œåˆ†ç‰‡ç®¡ç†
    session  *sessionutil.Session
    shardMgr shardClientMgr
    
    // æœç´¢ç»“æœé€šé“
    searchResultCh chan *internalpb.SearchResults
    
    // å›è°ƒå‡½æ•°
    startCallbacks []func()
    closeCallbacks []func()
    
    // è´Ÿè½½å‡è¡¡ç­–ç•¥
    lbPolicy LBPolicy
    
    // èµ„æºç®¡ç†å™¨
    resourceManager resource.Manager
    
    // åŠŸèƒ½å¼€å…³
    enableMaterializedView   bool
    enableComplexDeleteLimit bool
    
    // æ…¢æŸ¥è¯¢ç¼“å­˜
    slowQueries *expirable.LRU[Timestamp, *metricsinfo.SlowQuery]
}
```

#### 4.1.3 ä»»åŠ¡è°ƒåº¦æœºåˆ¶

```go
// ä»»åŠ¡è°ƒåº¦å™¨ï¼šinternal/proxy/task_scheduler.go
type taskScheduler struct {
    ddQueue  *BaseTaskQueue  // DDL ä»»åŠ¡é˜Ÿåˆ—
    dmlQueue *BaseTaskQueue  // DML ä»»åŠ¡é˜Ÿåˆ—  
    dqQueue  *BaseTaskQueue  // DQL ä»»åŠ¡é˜Ÿåˆ—
    
    wg     sync.WaitGroup
    ctx    context.Context
    cancel context.CancelFunc
}

// åŸºç¡€ä»»åŠ¡æ¥å£
type task interface {
    TraceCtx() context.Context
    ID() UniqueID
    SetID(uid UniqueID)
    Name() string
    Type() commonpb.MsgType
    BeginTs() Timestamp
    EndTs() Timestamp
    SetTs(ts Timestamp)
    OnEnqueue() error
    PreExecute(ctx context.Context) error
    Execute(ctx context.Context) error
    PostExecute(ctx context.Context) error
    WaitToFinish() error
    Notify(err error)
}

// ä»»åŠ¡é˜Ÿåˆ—å¤„ç†é€»è¾‘
func (queue *BaseTaskQueue) processTask(t task) {
    // 1. ä»»åŠ¡é¢„å¤„ç†
    if err := t.PreExecute(queue.ctx); err != nil {
        t.Notify(err)
        return
    }
    
    // 2. æ‰§è¡Œä»»åŠ¡
    if err := t.Execute(queue.ctx); err != nil {
        t.Notify(err)
        return
    }
    
    // 3. ä»»åŠ¡åå¤„ç†
    if err := t.PostExecute(queue.ctx); err != nil {
        t.Notify(err)
        return
    }
    
    // 4. é€šçŸ¥ä»»åŠ¡å®Œæˆ
    t.Notify(nil)
}
```

### 4.2 RootCoord æ¨¡å— - å…ƒæ•°æ®ç®¡ç†

#### 4.2.1 æ¨¡å—èŒè´£

RootCoord æ˜¯ Milvus çš„å…ƒæ•°æ®ç®¡ç†ä¸­å¿ƒï¼Œè´Ÿè´£ï¼š
- é›†åˆå’Œåˆ†åŒºçš„å…ƒæ•°æ®ç®¡ç†
- Schema å®šä¹‰å’Œç‰ˆæœ¬æ§åˆ¶
- å…¨å±€ ID åˆ†é…
- æ—¶é—´æˆ³åˆ†é…
- æ•°æ®å®šä¹‰è¯­è¨€ (DDL) æ“ä½œåè°ƒ

#### 4.2.2 æ ¸å¿ƒæ•°æ®ç»“æ„

```go
// RootCoord ä¸»ç»“æ„ï¼šinternal/rootcoord/root_coord.go
type Core struct {
    ctx    context.Context
    cancel context.CancelFunc
    wg     sync.WaitGroup
    
    // åŸºç¡€ä¿¡æ¯
    etcdCli   *clientv3.Client
    address   string
    port      int
    stateCode atomic.Int32
    
    // å…ƒæ•°æ®å­˜å‚¨
    metaTable  *metaTable
    scheduler  *taskScheduler
    
    // ID åˆ†é…å™¨
    idAllocator       *allocator.GlobalIDAllocator
    tsoAllocator      *tso.GlobalTSOAllocator
    
    // ä»£ç†ç®¡ç†
    proxyClientManager *proxyClientManager
    proxyWatcher       *proxyWatcher
    
    // å¯¼å…¥ç®¡ç†
    importManager *importManager
    
    // é…é¢ç®¡ç†
    quotaCenter *QuotaCenter
    
    // ä¼šè¯
    session *sessionutil.Session
    
    // å·¥å‚
    factory dependency.Factory
}

// å…ƒæ•°æ®è¡¨ï¼šinternal/rootcoord/meta_table.go
type metaTable struct {
    ctx    context.Context
    catalog metastore.RootCoordCatalog
    
    // é›†åˆä¿¡æ¯ç¼“å­˜
    collID2Meta  map[typeutil.UniqueID]*model.Collection
    collName2ID  map[string]typeutil.UniqueID
    collAlias2ID map[string]typeutil.UniqueID
    
    // åˆ†åŒºä¿¡æ¯ç¼“å­˜
    partID2Meta map[typeutil.UniqueID]*model.Partition
    
    // æ•°æ®åº“ä¿¡æ¯
    dbName2ID map[string]typeutil.UniqueID
    dbID2Meta map[typeutil.UniqueID]*model.Database
    
    // è¯»å†™é”
    ddLock sync.RWMutex
}
```

#### 4.2.3 é›†åˆåˆ›å»ºæµç¨‹

```go
// åˆ›å»ºé›†åˆä»»åŠ¡ï¼šinternal/rootcoord/create_collection_task.go
type createCollectionTask struct {
    baseTask
    Req *milvuspb.CreateCollectionRequest
    
    // å†…éƒ¨çŠ¶æ€
    collectionID   typeutil.UniqueID
    partitionID    typeutil.UniqueID
    schema         *schemapb.CollectionSchema
    virtualChannels []string
    physicalChannels []string
}

func (t *createCollectionTask) Execute(ctx context.Context) error {
    // 1. åˆ†é…é›†åˆ ID
    collectionID, err := t.core.idAllocator.AllocOne()
    if err != nil {
        return err
    }
    t.collectionID = collectionID
    
    // 2. åˆ†é…åˆ†åŒº ID
    partitionID, err := t.core.idAllocator.AllocOne()
    if err != nil {
        return err
    }
    t.partitionID = partitionID
    
    // 3. éªŒè¯å’Œå¤„ç† Schema
    if err := t.validateSchema(); err != nil {
        return err
    }
    
    // 4. åˆ†é…è™šæ‹Ÿé€šé“
    t.virtualChannels = t.core.chanTimeTick.getDmlChannelNames(t.Req.ShardsNum)
    
    // 5. åˆ›å»ºé›†åˆå…ƒæ•°æ®
    collection := &model.Collection{
        CollectionID:         t.collectionID,
        Name:                t.Req.CollectionName,
        Description:         t.Req.Description,
        AutoID:              t.schema.AutoID,
        Fields:              model.UnmarshalFieldModels(t.schema.Fields),
        VirtualChannelNames: t.virtualChannels,
        PhysicalChannelNames: t.physicalChannels,
        ShardsNum:           t.Req.ShardsNum,
        ConsistencyLevel:    t.Req.ConsistencyLevel,
        CreateTime:          t.GetTs(),
        State:               pb.CollectionState_CollectionCreating,
        StartPositions:      t.startPositions,
    }
    
    // 6. æŒä¹…åŒ–åˆ°å…ƒæ•°æ®å­˜å‚¨
    if err := t.core.meta.AddCollection(ctx, collection); err != nil {
        return err
    }
    
    // 7. é€šçŸ¥ DataCoord åˆ›å»ºé›†åˆ
    if err := t.core.broker.CreateCollection(ctx, collection); err != nil {
        return err
    }
    
    return nil
}
```

### 4.3 DataCoord æ¨¡å— - æ•°æ®åè°ƒ

#### 4.3.1 æ¨¡å—æ¶æ„

```mermaid
graph TB
    subgraph "DataCoord æ¶æ„"
        API[gRPC API]
        
        subgraph "æ ¸å¿ƒç®¡ç†å™¨"
            SegmentMgr[Segment ç®¡ç†å™¨]
            ChannelMgr[Channel ç®¡ç†å™¨]
            SessionMgr[Session ç®¡ç†å™¨]
            CompactionMgr[å‹ç¼©ç®¡ç†å™¨]
            GCMgr[åƒåœ¾å›æ”¶ç®¡ç†å™¨]
        end
        
        subgraph "è°ƒåº¦å™¨"
            CompactionScheduler[å‹ç¼©è°ƒåº¦å™¨]
            ImportScheduler[å¯¼å…¥è°ƒåº¦å™¨]
            IndexScheduler[ç´¢å¼•è°ƒåº¦å™¨]
        end
        
        subgraph "å­˜å‚¨"
            MetaStore[(å…ƒæ•°æ®å­˜å‚¨)]
            MessageQueue[æ¶ˆæ¯é˜Ÿåˆ—]
        end
    end
    
    API --> SegmentMgr
    API --> ChannelMgr
    SegmentMgr --> CompactionScheduler
    ChannelMgr --> MessageQueue
    CompactionScheduler --> CompactionMgr
```

#### 4.3.2 Segment ç®¡ç†

```go
// Segment ç®¡ç†å™¨ï¼šinternal/datacoord/segment_manager.go
type SegmentManager struct {
    meta      *meta
    allocator allocator.Allocator
    
    // Segment åˆ†é…ç­–ç•¥
    segmentSealPolicy   []segmentSealPolicy
    channelSealPolicies map[string][]segmentSealPolicy
    
    // ç»Ÿè®¡ä¿¡æ¯
    estimatePolicy ChannelSegmentPolicy
    allocPolicy    ChannelSegmentPolicy
    
    // å¹¶å‘æ§åˆ¶
    mu sync.RWMutex
}

// Segment ä¿¡æ¯ç»“æ„
type SegmentInfo struct {
    SegmentInfo *datapb.SegmentInfo
    currRows    int64
    allocations []*allocation
    lastFlushTs typeutil.Timestamp
    
    // çŠ¶æ€ç®¡ç†
    isCompacting bool
    size         int64
    lastExpireTime typeutil.Timestamp
}

// Segment åˆ†é…é€»è¾‘
func (s *SegmentManager) AllocSegment(ctx context.Context, collectionID, partitionID typeutil.UniqueID, channelName string, requestRows int64) (*SegmentInfo, error) {
    // 1. æŸ¥æ‰¾å¯ç”¨çš„ Growing Segment
    segment := s.getGrowingSegment(collectionID, partitionID, channelName)
    
    // 2. å¦‚æœæ²¡æœ‰å¯ç”¨ Segmentï¼Œåˆ›å»ºæ–°çš„
    if segment == nil {
        segmentID, err := s.allocator.AllocOne()
        if err != nil {
            return nil, err
        }
        
        segment = &SegmentInfo{
            SegmentInfo: &datapb.SegmentInfo{
                ID:            segmentID,
                CollectionID:  collectionID,
                PartitionID:   partitionID,
                InsertChannel: channelName,
                State:         commonpb.SegmentState_Growing,
                MaxRowNum:     Params.DataCoordCfg.SegmentMaxSize.GetAsInt64(),
                CreatedByNode: Params.DataCoordCfg.GetNodeID(),
            },
        }
        
        // 3. æ³¨å†Œåˆ°å…ƒæ•°æ®
        if err := s.meta.AddSegment(ctx, segment); err != nil {
            return nil, err
        }
    }
    
    // 4. åˆ†é…è¡Œæ•°
    segment.currRows += requestRows
    
    // 5. æ£€æŸ¥æ˜¯å¦éœ€è¦ Seal
    if s.shouldSealSegment(segment) {
        s.sealSegment(ctx, segment)
    }
    
    return segment, nil
}
```

#### 4.3.3 å‹ç¼©æœºåˆ¶

```go
// å‹ç¼©ç®¡ç†å™¨ï¼šinternal/datacoord/compaction_manager.go
type CompactionManager struct {
    meta      *meta
    sessions  *SessionManager
    allocator allocator.Allocator
    
    // å‹ç¼©ä»»åŠ¡é˜Ÿåˆ—
    compactionHandler map[int64]*compactionPlanHandler
    
    // å‹ç¼©ç­–ç•¥
    levelZeroCompactionPolicy CompactionPolicy
    mixCompactionPolicy       CompactionPolicy
    
    mu sync.RWMutex
}

// å‹ç¼©ä»»åŠ¡
type compactionTask struct {
    triggerID     int64
    planID        int64
    dataNodeID    int64
    plan          *datapb.CompactionPlan
    state         datapb.CompactionTaskState
    
    startTime time.Time
    endTime   time.Time
}

// è§¦å‘å‹ç¼©é€»è¾‘
func (cm *CompactionManager) TriggerCompaction(collectionID int64) error {
    // 1. è·å–é›†åˆçš„æ‰€æœ‰ Segment
    segments := cm.meta.GetSegmentsByCollection(collectionID)
    
    // 2. æŒ‰å‹ç¼©ç­–ç•¥åˆ†ç»„
    groups := cm.groupSegmentsForCompaction(segments)
    
    // 3. ä¸ºæ¯ç»„åˆ›å»ºå‹ç¼©è®¡åˆ’
    for _, group := range groups {
        plan := &datapb.CompactionPlan{
            PlanID:        cm.allocator.AllocOne(),
            Type:          datapb.CompactionType_MixCompaction,
            SegmentBinlogs: group.segments,
            TimeoutInSeconds: 3600,
            Collection:    collectionID,
            Channel:       group.channel,
        }
        
        // 4. åˆ†é… DataNode æ‰§è¡Œå‹ç¼©
        nodeID := cm.selectDataNode(group.channel)
        if err := cm.sessions.Compaction(nodeID, plan); err != nil {
            return err
        }
        
        // 5. è®°å½•å‹ç¼©ä»»åŠ¡
        cm.addCompactionTask(plan.PlanID, nodeID, plan)
    }
    
    return nil
}
```

### 4.4 QueryCoord æ¨¡å— - æŸ¥è¯¢åè°ƒ

#### 4.4.1 æ¨¡å—èŒè´£

QueryCoord è´Ÿè´£æŸ¥è¯¢ç›¸å…³çš„åè°ƒå·¥ä½œï¼š
- ç®¡ç† QueryNode é›†ç¾¤
- è´Ÿè½½å‡è¡¡å’Œåˆ†ç‰‡åˆ†é…
- æŸ¥è¯¢ä»»åŠ¡è°ƒåº¦
- å‰¯æœ¬ç®¡ç†

#### 4.4.2 æ ¸å¿ƒæ¶æ„

```go
// QueryCoord ä¸»ç»“æ„ï¼šinternal/querycoordv2/server.go
type Server struct {
    ctx    context.Context
    cancel context.CancelFunc
    wg     sync.WaitGroup
    
    // åŸºç¡€ä¿¡æ¯
    etcdCli *clientv3.Client
    address string
    port    int
    
    // æ ¸å¿ƒç®¡ç†å™¨
    meta         *meta.Meta
    dist         *meta.DistributionManager
    targetMgr    *meta.TargetManager
    broker       meta.Broker
    
    // è°ƒåº¦å™¨
    jobScheduler  *job.Scheduler
    taskScheduler *task.Scheduler
    
    // è§‚å¯Ÿè€…
    nodeMgr     *session.NodeManager
    observers   []observers.Observer
    
    // æ£€æŸ¥å™¨
    checkerController *checkers.CheckerController
    
    // è´Ÿè½½å‡è¡¡å™¨
    balancer balance.Balance
    
    // ä¼šè¯
    session *sessionutil.Session
}

// åˆ†å¸ƒå¼ç®¡ç†å™¨ï¼šinternal/querycoordv2/meta/dist_manager.go
type DistributionManager struct {
    // Segment åˆ†å¸ƒ
    segmentDist map[int64]*meta.Segment  // nodeID -> segments
    channelDist map[int64]*meta.DmChannel // nodeID -> channels
    leaderView  map[int64]*meta.LeaderView // nodeID -> leader view
    
    // è¯»å†™é”
    rwmutex sync.RWMutex
}
```

#### 4.4.3 è´Ÿè½½å‡è¡¡æœºåˆ¶

```go
// è´Ÿè½½å‡è¡¡å™¨ï¼šinternal/querycoordv2/balance/balance.go
type Balance interface {
    AssignSegment(collectionID int64, segments []*meta.Segment, nodes []int64) []SegmentAssignPlan
    BalanceReplica(replica *meta.Replica) ([]SegmentAssignPlan, []ChannelAssignPlan)
}

// è½®è¯¢è´Ÿè½½å‡è¡¡å™¨
type RoundRobinBalancer struct {
    scheduler task.Scheduler
    meta      *meta.Meta
    dist      *meta.DistributionManager
}

func (b *RoundRobinBalancer) AssignSegment(collectionID int64, segments []*meta.Segment, nodes []int64) []SegmentAssignPlan {
    plans := make([]SegmentAssignPlan, 0, len(segments))
    
    // 1. è·å–èŠ‚ç‚¹è´Ÿè½½ä¿¡æ¯
    nodeLoads := make(map[int64]int64)
    for _, nodeID := range nodes {
        nodeLoads[nodeID] = b.getNodeLoad(nodeID)
    }
    
    // 2. æŒ‰è´Ÿè½½æ’åºèŠ‚ç‚¹
    sort.Slice(nodes, func(i, j int) bool {
        return nodeLoads[nodes[i]] < nodeLoads[nodes[j]]
    })
    
    // 3. è½®è¯¢åˆ†é… Segment
    nodeIndex := 0
    for _, segment := range segments {
        targetNode := nodes[nodeIndex]
        plans = append(plans, SegmentAssignPlan{
            Segment: segment,
            From:    -1,
            To:      targetNode,
        })
        
        nodeIndex = (nodeIndex + 1) % len(nodes)
        nodeLoads[targetNode]++
    }
    
    return plans
}
```

### 4.5 DataNode æ¨¡å— - æ•°æ®èŠ‚ç‚¹

#### 4.5.1 æ•°æ®å†™å…¥æµæ°´çº¿

```go
// æ•°æ®èŠ‚ç‚¹ï¼šinternal/datanode/data_node.go
type DataNode struct {
    ctx    context.Context
    cancel context.CancelFunc
    
    // åŸºç¡€ä¿¡æ¯
    Role       string
    NodeID     typeutil.UniqueID
    address    string
    port       int
    stateCode  atomic.Int32
    
    // æµæ°´çº¿ç®¡ç†
    flowgraphManager *pipeline.FlowgraphManager
    
    // å†™ç¼“å†²åŒºç®¡ç†
    writeBufferManager writebuffer.BufferManager
    
    // åŒæ­¥ç®¡ç†å™¨
    syncMgr syncmgr.SyncManager
    
    // å‹ç¼©å™¨
    compactionExecutor *compactor.Executor
    
    // ä¼šè¯
    session *sessionutil.Session
}

// æ•°æ®å†™å…¥æµæ°´çº¿ï¼šinternal/datanode/pipeline/flow_graph.go
type DataSyncService struct {
    ctx    context.Context
    cancel context.CancelFunc
    
    // æµå›¾èŠ‚ç‚¹
    dmStreamNode   *DmInputNode
    insertBufferNode *InsertBufferNode
    deleteBufferNode *DeleteBufferNode
    ttNode         *TimeTickNode
    
    // é€šé“ä¿¡æ¯
    vchannelName   string
    metacache      metacache.MetaCache
    
    // å†™ç¼“å†²åŒº
    writeBuffer    writebuffer.WriteBuffer
    
    // åŒæ­¥å™¨
    syncMgr        syncmgr.SyncManager
}

// æ’å…¥ç¼“å†²èŠ‚ç‚¹å¤„ç†é€»è¾‘
func (ibn *InsertBufferNode) Operate(in []Msg) []Msg {
    // 1. è§£ææ’å…¥æ¶ˆæ¯
    insertMsgs := ibn.parseInsertMsgs(in)
    
    // 2. å†™å…¥ç¼“å†²åŒº
    for _, msg := range insertMsgs {
        // åˆ†é… Segment
        segmentID := ibn.allocateSegment(msg.CollectionID, msg.PartitionID)
        
        // å†™å…¥æ•°æ®åˆ°ç¼“å†²åŒº
        ibn.writeBuffer.BufferData(segmentID, msg.RowData)
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·ç›˜
        if ibn.shouldFlush(segmentID) {
            ibn.triggerFlush(segmentID)
        }
    }
    
    return in
}
```

#### 4.5.2 æ•°æ®åˆ·ç›˜æœºåˆ¶

```go
// åŒæ­¥ç®¡ç†å™¨ï¼šinternal/datanode/syncmgr/sync_manager.go
type SyncManager interface {
    SyncData(ctx context.Context, task SyncTask) *SyncTask
}

type syncManager struct {
    // ä»»åŠ¡é˜Ÿåˆ—
    tasks chan SyncTask
    
    // å·¥ä½œåç¨‹æ± 
    workers []Worker
    
    // å…ƒæ•°æ®ç¼“å­˜
    metacache metacache.MetaCache
    
    // åˆ†é…å™¨
    allocator allocator.Allocator
    
    // å­˜å‚¨å®¢æˆ·ç«¯
    chunkManager storage.ChunkManager
}

// åŒæ­¥ä»»åŠ¡
type SyncTask struct {
    segmentID    int64
    collectionID int64
    partitionID  int64
    channelName  string
    
    // æ•°æ®
    insertData   *storage.InsertData
    deleteData   *storage.DeleteData
    
    // æ—¶é—´æˆ³
    startPosition *msgpb.MsgPosition
    endPosition   *msgpb.MsgPosition
    
    // å›è°ƒ
    done chan error
}

// æ‰§è¡ŒåŒæ­¥ä»»åŠ¡
func (sm *syncManager) sync(task *SyncTask) error {
    // 1. åºåˆ—åŒ–æ•°æ®
    insertLogs, statsLogs, err := sm.serializeInsertData(task.insertData)
    if err != nil {
        return err
    }
    
    deleteLogs, err := sm.serializeDeleteData(task.deleteData)
    if err != nil {
        return err
    }
    
    // 2. ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨
    insertPaths := make([]string, len(insertLogs))
    for i, log := range insertLogs {
        path := sm.generateInsertLogPath(task.segmentID, log.FieldID)
        if err := sm.chunkManager.Write(path, log.Data); err != nil {
            return err
        }
        insertPaths[i] = path
    }
    
    // 3. æ›´æ–°å…ƒæ•°æ®
    segmentInfo := &datapb.SegmentInfo{
        ID:           task.segmentID,
        CollectionID: task.collectionID,
        PartitionID:  task.partitionID,
        InsertChannel: task.channelName,
        NumOfRows:    task.insertData.GetRowNum(),
        Binlogs:      insertPaths,
        Deltalogs:    deletePaths,
        Statslogs:    statsLogs,
        StartPosition: task.startPosition,
        DmlPosition:   task.endPosition,
    }
    
    // 4. é€šçŸ¥ DataCoord
    if err := sm.reportSegment(segmentInfo); err != nil {
        return err
    }
    
    return nil
}
```

### 4.6 QueryNode æ¨¡å— - æŸ¥è¯¢èŠ‚ç‚¹

#### 4.6.1 æŸ¥è¯¢æ‰§è¡Œå¼•æ“

```go
// æŸ¥è¯¢èŠ‚ç‚¹ï¼šinternal/querynodev2/server.go
type QueryNode struct {
    ctx    context.Context
    cancel context.CancelFunc
    
    // åŸºç¡€ä¿¡æ¯
    address   string
    port      int
    nodeID    typeutil.UniqueID
    stateCode atomic.Int32
    
    // æ ¸å¿ƒç®¡ç†å™¨
    manager      *segment.Manager
    delegators   map[string]*delegator.ShardDelegator
    
    // æŸ¥è¯¢æ‰§è¡Œå™¨
    scheduler    *task.Scheduler
    
    // æœ¬åœ°å·¥ä½œå™¨
    workers      *LocalWorker
    
    // ä¼šè¯
    session      *sessionutil.Session
}

// Segment ç®¡ç†å™¨ï¼šinternal/querynodev2/segments/manager.go
type Manager struct {
    // Segment å­˜å‚¨
    growing map[int64]Segment  // segmentID -> growing segment
    sealed  map[int64]Segment  // segmentID -> sealed segment
    
    // é›†åˆä¿¡æ¯
    collection *Collection
    
    // åŠ è½½å™¨
    loader *Loader
    
    // è¯»å†™é”
    mu sync.RWMutex
}

// æŸ¥è¯¢æ‰§è¡Œé€»è¾‘
func (qn *QueryNode) Search(ctx context.Context, req *querypb.SearchRequest) (*internalpb.SearchResults, error) {
    // 1. è·å–åˆ†ç‰‡å§”æ‰˜å™¨
    delegator := qn.delegators[req.GetDmlChannels()[0]]
    if delegator == nil {
        return nil, errors.New("delegator not found")
    }
    
    // 2. åˆ›å»ºæœç´¢ä»»åŠ¡
    searchTask := &searchTask{
        req:       req,
        delegator: delegator,
        result:    make(chan *internalpb.SearchResults, 1),
    }
    
    // 3. æäº¤ä»»åŠ¡åˆ°è°ƒåº¦å™¨
    if err := qn.scheduler.Add(searchTask); err != nil {
        return nil, err
    }
    
    // 4. ç­‰å¾…ç»“æœ
    select {
    case result := <-searchTask.result:
        return result, nil
    case <-ctx.Done():
        return nil, ctx.Err()
    }
}

// åˆ†ç‰‡å§”æ‰˜å™¨æ‰§è¡Œæœç´¢
func (sd *ShardDelegator) Search(ctx context.Context, req *querypb.SearchRequest) (*internalpb.SearchResults, error) {
    // 1. è·å–æœç´¢ Segment åˆ—è¡¨
    sealedSegments := sd.getSearchableSegments(req.GetReq().GetCollectionID())
    growingSegments := sd.getGrowingSegments(req.GetReq().GetCollectionID())
    
    // 2. å¹¶è¡Œæœç´¢ Sealed Segment
    var wg sync.WaitGroup
    sealedResults := make([]*internalpb.SearchResults, len(sealedSegments))
    
    for i, segment := range sealedSegments {
        wg.Add(1)
        go func(idx int, seg Segment) {
            defer wg.Done()
            result, err := seg.Search(ctx, req)
            if err == nil {
                sealedResults[idx] = result
            }
        }(i, segment)
    }
    
    // 3. æœç´¢ Growing Segment
    growingResults := make([]*internalpb.SearchResults, len(growingSegments))
    for i, segment := range growingSegments {
        result, err := segment.Search(ctx, req)
        if err == nil {
            growingResults[i] = result
        }
    }
    
    // 4. ç­‰å¾…æ‰€æœ‰æœç´¢å®Œæˆ
    wg.Wait()
    
    // 5. åˆå¹¶æœç´¢ç»“æœ
    allResults := append(sealedResults, growingResults...)
    finalResult := sd.reduceSearchResults(allResults, req.GetReq().GetTopk())
    
    return finalResult, nil
}
```

---

## 5. å…³é”®æ•°æ®ç»“æ„

### 5.1 æ ¸å¿ƒæ•°æ®æ¨¡å‹

#### 5.1.1 é›†åˆ (Collection) æ¨¡å‹

```go
// é›†åˆæ¨¡å‹ï¼šinternal/metastore/model/collection.go
type Collection struct {
    CollectionID         int64                 `json:"collectionID"`
    Name                string                `json:"name"`
    Description         string                `json:"description"`
    AutoID              bool                  `json:"autoID"`
    Fields              []*Field              `json:"fields"`
    VirtualChannelNames []string              `json:"virtualChannelNames"`
    PhysicalChannelNames []string             `json:"physicalChannelNames"`
    ShardsNum           int32                 `json:"shardsNum"`
    ConsistencyLevel    commonpb.ConsistencyLevel `json:"consistencyLevel"`
    CreateTime          uint64                `json:"createTime"`
    StartPositions      []*commonpb.KeyDataPair `json:"startPositions"`
    Properties          map[string]string     `json:"properties"`
    State               pb.CollectionState    `json:"state"`
    Partitions          []*Partition          `json:"partitions"`
}

// å­—æ®µæ¨¡å‹
type Field struct {
    FieldID      int64                `json:"fieldID"`
    Name         string               `json:"name"`
    IsPrimaryKey bool                 `json:"isPrimaryKey"`
    Description  string               `json:"description"`
    DataType     schemapb.DataType    `json:"dataType"`
    TypeParams   map[string]string    `json:"typeParams"`
    IndexParams  map[string]string    `json:"indexParams"`
    AutoID       bool                 `json:"autoID"`
}
```

#### 5.1.2 Segment æ•°æ®ç»“æ„

```go
// Segment ä¿¡æ¯ï¼špkg/proto/datapb/data_coord.proto
type SegmentInfo struct {
    ID                int64                    `protobuf:"varint,1,opt,name=ID,proto3" json:"ID,omitempty"`
    CollectionID      int64                    `protobuf:"varint,2,opt,name=collectionID,proto3" json:"collectionID,omitempty"`
    PartitionID       int64                    `protobuf:"varint,3,opt,name=partitionID,proto3" json:"partitionID,omitempty"`
    InsertChannel     string                   `protobuf:"bytes,4,opt,name=insert_channel,json=insertChannel,proto3" json:"insert_channel,omitempty"`
    NumOfRows         int64                    `protobuf:"varint,5,opt,name=num_of_rows,json=numOfRows,proto3" json:"num_of_rows,omitempty"`
    State             commonpb.SegmentState    `protobuf:"varint,6,opt,name=state,proto3,enum=milvus.proto.common.SegmentState" json:"state,omitempty"`
    MaxRowNum         int64                    `protobuf:"varint,7,opt,name=max_row_num,json=maxRowNum,proto3" json:"max_row_num,omitempty"`
    LastExpireTime    uint64                   `protobuf:"varint,8,opt,name=last_expire_time,json=lastExpireTime,proto3" json:"last_expire_time,omitempty"`
    StartPosition     *msgpb.MsgPosition       `protobuf:"bytes,9,opt,name=start_position,json=startPosition,proto3" json:"start_position,omitempty"`
    DmlPosition       *msgpb.MsgPosition       `protobuf:"bytes,10,opt,name=dml_position,json=dmlPosition,proto3" json:"dml_position,omitempty"`
    Binlogs           []*FieldBinlog           `protobuf:"bytes,11,rep,name=binlogs,proto3" json:"binlogs,omitempty"`
    Statslogs         []*FieldBinlog           `protobuf:"bytes,12,rep,name=statslogs,proto3" json:"statslogs,omitempty"`
    Deltalogs         []*FieldBinlog           `protobuf:"bytes,13,rep,name=deltalogs,proto3" json:"deltalogs,omitempty"`
    CreatedByNode     int64                    `protobuf:"varint,14,opt,name=created_by_node,json=createdByNode,proto3" json:"created_by_node,omitempty"`
    SegmentSize       int64                    `protobuf:"varint,15,opt,name=segment_size,json=segmentSize,proto3" json:"segment_size,omitempty"`
    IndexInfos        []*SegmentIndexInfo      `protobuf:"bytes,16,rep,name=index_infos,json=indexInfos,proto3" json:"index_infos,omitempty"`
}

// Segment çŠ¶æ€æšä¸¾
type SegmentState int32
const (
    SegmentState_SegmentStateNone SegmentState = 0
    SegmentState_NotExist         SegmentState = 1
    SegmentState_Growing          SegmentState = 2
    SegmentState_Sealed           SegmentState = 3
    SegmentState_Flushed          SegmentState = 4
    SegmentState_Flushing         SegmentState = 5
    SegmentState_Dropped          SegmentState = 6
    SegmentState_Importing        SegmentState = 7
)
```

#### 5.1.3 ç´¢å¼•æ•°æ®ç»“æ„

```go
// ç´¢å¼•ä¿¡æ¯ï¼špkg/proto/indexpb/index_coord.proto
type IndexInfo struct {
    CollectionID    int64             `protobuf:"varint,1,opt,name=collectionID,proto3" json:"collectionID,omitempty"`
    FieldID         int64             `protobuf:"varint,2,opt,name=fieldID,proto3" json:"fieldID,omitempty"`
    IndexName       string            `protobuf:"bytes,3,opt,name=index_name,json=indexName,proto3" json:"index_name,omitempty"`
    IndexID         int64             `protobuf:"varint,4,opt,name=indexID,proto3" json:"indexID,omitempty"`
    TypeParams      []*commonpb.KeyValuePair `protobuf:"bytes,5,rep,name=type_params,json=typeParams,proto3" json:"type_params,omitempty"`
    IndexParams     []*commonpb.KeyValuePair `protobuf:"bytes,6,rep,name=index_params,json=indexParams,proto3" json:"index_params,omitempty"`
    IndexedRows     int64             `protobuf:"varint,7,opt,name=indexed_rows,json=indexedRows,proto3" json:"indexed_rows,omitempty"`
    TotalRows       int64             `protobuf:"varint,8,opt,name=total_rows,json=totalRows,proto3" json:"total_rows,omitempty"`
    State           commonpb.IndexState `protobuf:"varint,9,opt,name=state,proto3,enum=milvus.proto.common.IndexState" json:"state,omitempty"`
    IndexStateFailReason string       `protobuf:"bytes,10,opt,name=index_state_fail_reason,json=indexStateFailReason,proto3" json:"index_state_fail_reason,omitempty"`
    IsAutoIndex     bool              `protobuf:"varint,11,opt,name=is_auto_index,json=isAutoIndex,proto3" json:"is_auto_index,omitempty"`
    UserIndexParams []*commonpb.KeyValuePair `protobuf:"bytes,12,rep,name=user_index_params,json=userIndexParams,proto3" json:"user_index_params,omitempty"`
}
```

### 5.2 æ¶ˆæ¯ç³»ç»Ÿæ•°æ®ç»“æ„

#### 5.2.1 æ¶ˆæ¯åŸºç¡€ç»“æ„

```go
// æ¶ˆæ¯åŸºç¡€æ¥å£ï¼špkg/mq/msgstream/msg.go
type TsMsg interface {
    TraceCtx() context.Context
    SetTraceCtx(ctx context.Context)
    ID() UniqueID
    BeginTs() Timestamp
    EndTs() Timestamp
    Type() MsgType
    SourceID() int64
    HashKeys() []uint32
    Marshal(TsMsg) (MarshalType, error)
    Unmarshal(MarshalType) (TsMsg, error)
    Position() *MsgPosition
    SetPosition(*MsgPosition)
    Size() int
}

// æ’å…¥æ¶ˆæ¯
type InsertMsg struct {
    BaseMsg
    InsertRequest milvuspb.InsertRequest
    
    // å†…éƒ¨å­—æ®µ
    HashValues  []uint32
    Timestamps  []uint64
    RowIDs      []int64
    RowData     []*commonpb.Blob
}

// åˆ é™¤æ¶ˆæ¯
type DeleteMsg struct {
    BaseMsg
    DeleteRequest milvuspb.DeleteRequest
    
    // å†…éƒ¨å­—æ®µ
    HashValues []uint32
    Timestamps []uint64
    PrimaryKeys *schemapb.IDs
}

// æœç´¢æ¶ˆæ¯
type SearchMsg struct {
    BaseMsg
    SearchRequest milvuspb.SearchRequest
    
    // æŸ¥è¯¢è®¡åˆ’
    PlaceholderGroup []byte
    DslType         commonpb.DslType
    SerializedExprPlan []byte
}
```

### 5.3 å­˜å‚¨æ•°æ®ç»“æ„

#### 5.3.1 Binlog æ ¼å¼

```go
// Binlog äº‹ä»¶ï¼šinternal/storage/event.go
type Event interface {
    EventType() EventTypeCode
    Timestamp() Timestamp
}

// æ’å…¥äº‹ä»¶æ•°æ®
type InsertEventData struct {
    StartTimestamp Timestamp
    EndTimestamp   Timestamp
    
    // æ•°æ®å­—æ®µ
    Data map[FieldID]FieldData
}

// åˆ é™¤äº‹ä»¶æ•°æ®  
type DeleteEventData struct {
    StartTimestamp Timestamp
    EndTimestamp   Timestamp
    
    // åˆ é™¤çš„ä¸»é”®
    Pks         *schemapb.IDs
    Tss         []Timestamp
}

// å­—æ®µæ•°æ®æ¥å£
type FieldData interface {
    GetMemorySize() int
    RowNum() int
    GetNullMask() []bool
    AppendRow(interface{}) error
    GetRow(int) interface{}
}
```

### 5.4 ç±»ç»§æ‰¿å…³ç³»å›¾

```mermaid
classDiagram
    class Component {
        <<interface>>
        +GetComponentStates() ComponentStates
        +GetStatisticsChannel() string
        +GetTimeTickChannel() string
    }
    
    class Proxy {
        -stateCode atomic.Int32
        -mixCoord types.MixCoordClient
        -sched *taskScheduler
        +CreateCollection()
        +Insert()
        +Search()
    }
    
    class RootCoord {
        -metaTable *metaTable
        -idAllocator *allocator.GlobalIDAllocator
        +CreateCollection()
        +DropCollection()
        +AllocID()
    }
    
    class DataCoord {
        -segmentManager *SegmentManager
        -compactionHandler *CompactionHandler
        +AssignSegmentID()
        +Flush()
        +Compaction()
    }
    
    class QueryCoord {
        -meta *meta.Meta
        -dist *meta.DistributionManager
        -balancer balance.Balance
        +LoadCollection()
        +ReleaseCollection()
        +LoadBalance()
    }
    
    class DataNode {
        -flowgraphManager *pipeline.FlowgraphManager
        -writeBufferManager writebuffer.BufferManager
        +Flush()
        +Compaction()
    }
    
    class QueryNode {
        -manager *segment.Manager
        -delegators map[string]*delegator.ShardDelegator
        +Search()
        +Query()
        +LoadSegments()
    }
    
    Component <|-- Proxy
    Component <|-- RootCoord
    Component <|-- DataCoord
    Component <|-- QueryCoord
    Component <|-- DataNode
    Component <|-- QueryNode
    
    class Task {
        <<interface>>
        +ID() UniqueID
        +Type() MsgType
        +Execute() error
    }
    
    class BaseTask {
        -ctx context.Context
        -id UniqueID
        -ts Timestamp
    }
    
    class CreateCollectionTask {
        +Execute() error
        +PreExecute() error
    }
    
    class InsertTask {
        +Execute() error
        +PreExecute() error
    }
    
    class SearchTask {
        +Execute() error
        +PreExecute() error
    }
    
    Task <|-- BaseTask
    BaseTask <|-- CreateCollectionTask
    BaseTask <|-- InsertTask
    BaseTask <|-- SearchTask
```

---

## 6. å®æˆ˜ç»éªŒæ€»ç»“

### 6.1 æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

#### 6.1.1 ç´¢å¼•é€‰æ‹©ç­–ç•¥

**HNSW ç´¢å¼• - é«˜ç²¾åº¦åœºæ™¯**
```yaml
index_params:
  index_type: "HNSW"
  metric_type: "L2"
  params:
    M: 16              # è¿æ¥æ•°ï¼Œå½±å“ç²¾åº¦å’Œå†…å­˜
    efConstruction: 200 # æ„å»ºæ—¶æœç´¢æ·±åº¦
    ef: 64             # æŸ¥è¯¢æ—¶æœç´¢æ·±åº¦
```

**IVF ç´¢å¼• - å¹³è¡¡æ€§èƒ½**
```yaml
index_params:
  index_type: "IVF_FLAT"
  metric_type: "IP"
  params:
    nlist: 1024        # èšç±»ä¸­å¿ƒæ•°é‡
    nprobe: 16         # æŸ¥è¯¢æ—¶æ¢æµ‹çš„èšç±»æ•°
```

**DiskANN ç´¢å¼• - å¤§è§„æ¨¡æ•°æ®**
```yaml
index_params:
  index_type: "DISKANN"
  metric_type: "L2"
  params:
    max_degree: 56     # å›¾çš„æœ€å¤§åº¦æ•°
    search_list_size: 100 # æœç´¢åˆ—è¡¨å¤§å°
```

#### 6.1.2 é›†åˆè®¾è®¡åŸåˆ™

**åˆ†ç‰‡ç­–ç•¥**
```python
# æ ¹æ®æ•°æ®é‡å’ŒæŸ¥è¯¢ QPS ç¡®å®šåˆ†ç‰‡æ•°
def calculate_shard_num(data_size_gb, qps):
    # æ¯ä¸ªåˆ†ç‰‡å»ºè®®å¤„ç† 1-10GB æ•°æ®
    shard_by_size = max(1, data_size_gb // 5)
    
    # æ¯ä¸ªåˆ†ç‰‡å»ºè®®å¤„ç† 100-1000 QPS
    shard_by_qps = max(1, qps // 500)
    
    return min(16, max(shard_by_size, shard_by_qps))

# åˆ›å»ºé›†åˆæ—¶æŒ‡å®šåˆ†ç‰‡æ•°
collection_schema = {
    "collection_name": "my_collection",
    "dimension": 768,
    "shard_num": calculate_shard_num(100, 2000)  # 4 ä¸ªåˆ†ç‰‡
}
```

**å­—æ®µè®¾è®¡**
```python
# åˆç†è®¾è®¡ Schema
schema = CollectionSchema([
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=768),
    FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=50),  # ç”¨äºè¿‡æ»¤
    FieldSchema(name="timestamp", dtype=DataType.INT64),  # æ—¶é—´èŒƒå›´æŸ¥è¯¢
    FieldSchema(name="metadata", dtype=DataType.JSON)     # çµæ´»çš„å…ƒæ•°æ®
])
```

#### 6.1.3 æŸ¥è¯¢ä¼˜åŒ–æŠ€å·§

**æ··åˆæœç´¢ä¼˜åŒ–**
```python
# ä½¿ç”¨è¡¨è¾¾å¼è¿‡æ»¤å‡å°‘æœç´¢èŒƒå›´
search_params = {
    "metric_type": "L2",
    "params": {"nprobe": 16},
    "expr": "category in ['tech', 'science'] and timestamp > 1640995200"
}

# æ‰¹é‡æŸ¥è¯¢æé«˜ååé‡
batch_vectors = [vector1, vector2, vector3, ...]
results = collection.search(
    data=batch_vectors,
    anns_field="vector",
    param=search_params,
    limit=10,
    output_fields=["id", "category"]
)
```

### 6.2 è¿ç»´ç›‘æ§è¦ç‚¹

#### 6.2.1 å…³é”®æŒ‡æ ‡ç›‘æ§

**ç³»ç»Ÿçº§æŒ‡æ ‡**
```yaml
# Prometheus ç›‘æ§é…ç½®
metrics:
  - milvus_proxy_search_vectors_count     # æœç´¢å‘é‡æ•°
  - milvus_proxy_insert_vectors_count     # æ’å…¥å‘é‡æ•°
  - milvus_proxy_search_latency_bucket    # æœç´¢å»¶è¿Ÿåˆ†å¸ƒ
  - milvus_querynode_search_latency       # QueryNode æœç´¢å»¶è¿Ÿ
  - milvus_datanode_flush_buffer_count    # DataNode åˆ·ç›˜æ¬¡æ•°
  - milvus_rootcoord_ddl_req_count        # DDL è¯·æ±‚æ•°é‡
```

**ä¸šåŠ¡çº§æŒ‡æ ‡**
```python
# è‡ªå®šä¹‰ç›‘æ§æŒ‡æ ‡
class MilvusMonitor:
    def __init__(self):
        self.search_success_rate = Counter('milvus_search_success_total')
        self.search_error_rate = Counter('milvus_search_error_total')
        self.insert_throughput = Histogram('milvus_insert_throughput')
    
    def record_search(self, success: bool, latency: float):
        if success:
            self.search_success_rate.inc()
        else:
            self.search_error_rate.inc()
    
    def record_insert(self, batch_size: int, duration: float):
        throughput = batch_size / duration
        self.insert_throughput.observe(throughput)
```

#### 6.2.2 æ•…éšœæ’æŸ¥æ‰‹å†Œ

**å¸¸è§é—®é¢˜è¯Šæ–­**

1. **æœç´¢å»¶è¿Ÿè¿‡é«˜**
```bash
# æ£€æŸ¥ QueryNode è´Ÿè½½
kubectl top pods -l component=querynode

# æŸ¥çœ‹ç´¢å¼•æ„å»ºçŠ¶æ€
curl -X GET "http://milvus:9091/api/v1/index/progress?collection_name=my_collection"

# æ£€æŸ¥ Segment åˆ†å¸ƒ
curl -X GET "http://milvus:9091/api/v1/querycoord/segments"
```

2. **æ•°æ®æ’å…¥å¤±è´¥**
```bash
# æ£€æŸ¥ DataNode çŠ¶æ€
kubectl logs -l component=datanode --tail=100

# æŸ¥çœ‹æ¶ˆæ¯é˜Ÿåˆ—ç§¯å‹
kubectl exec -it pulsar-broker-0 -- bin/pulsar-admin topics stats persistent://public/default/milvus-insert

# æ£€æŸ¥å¯¹è±¡å­˜å‚¨è¿æ¥
kubectl exec -it datanode-0 -- curl -I http://minio:9000/minio/health/live
```

3. **å†…å­˜ä½¿ç”¨è¿‡é«˜**
```bash
# æŸ¥çœ‹å„ç»„ä»¶å†…å­˜ä½¿ç”¨
kubectl top pods -l app=milvus

# æ£€æŸ¥ Segment åŠ è½½æƒ…å†µ
curl -X GET "http://milvus:9091/api/v1/querynode/segments/memory"

# è°ƒæ•´å†…å­˜é…ç½®
kubectl patch configmap milvus-config --patch '
data:
  milvus.yaml: |
    queryNode:
      loadMemoryUsageRatio: 0.7  # é™ä½å†…å­˜ä½¿ç”¨æ¯”ä¾‹
'
```

### 6.3 æ‰©å®¹å’Œå®¹é‡è§„åˆ’

#### 6.3.1 æ°´å¹³æ‰©å®¹ç­–ç•¥

**QueryNode æ‰©å®¹**
```yaml
# å¢åŠ  QueryNode å‰¯æœ¬æ•°
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-querynode
spec:
  replicas: 6  # ä» 3 å¢åŠ åˆ° 6
  template:
    spec:
      containers:
      - name: querynode
        resources:
          requests:
            memory: "8Gi"
            cpu: "2"
          limits:
            memory: "16Gi"
            cpu: "4"
```

**DataNode æ‰©å®¹**
```yaml
# DataNode æ‰©å®¹é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-datanode
spec:
  replicas: 4  # å¢åŠ  DataNode æ•°é‡
  template:
    spec:
      containers:
      - name: datanode
        env:
        - name: DATANODE_MEMORY_LIMIT
          value: "32Gi"
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
```

#### 6.3.2 å®¹é‡è§„åˆ’å…¬å¼

**å­˜å‚¨å®¹é‡è®¡ç®—**
```python
def calculate_storage_capacity(
    vector_count: int,
    vector_dim: int,
    replica_count: int = 1,
    compression_ratio: float = 0.3,
    metadata_overhead: float = 0.1
):
    """
    è®¡ç®—å­˜å‚¨å®¹é‡éœ€æ±‚
    
    Args:
        vector_count: å‘é‡æ•°é‡
        vector_dim: å‘é‡ç»´åº¦
        replica_count: å‰¯æœ¬æ•°é‡
        compression_ratio: å‹ç¼©æ¯”ä¾‹
        metadata_overhead: å…ƒæ•°æ®å¼€é”€æ¯”ä¾‹
    """
    # åŸå§‹å‘é‡æ•°æ®å¤§å° (float32 = 4 bytes)
    raw_size_gb = vector_count * vector_dim * 4 / (1024**3)
    
    # è€ƒè™‘å‹ç¼©å’Œå‰¯æœ¬
    compressed_size = raw_size_gb * compression_ratio * replica_count
    
    # ç´¢å¼•å¼€é”€ (é€šå¸¸æ˜¯åŸå§‹æ•°æ®çš„ 1.2-2 å€)
    index_overhead = compressed_size * 1.5
    
    # å…ƒæ•°æ®å¼€é”€
    metadata_size = compressed_size * metadata_overhead
    
    # æ€»å­˜å‚¨éœ€æ±‚
    total_storage = compressed_size + index_overhead + metadata_size
    
    return {
        "raw_data_gb": raw_size_gb,
        "compressed_gb": compressed_size,
        "index_gb": index_overhead,
        "metadata_gb": metadata_size,
        "total_gb": total_storage,
        "recommended_gb": total_storage * 1.3  # 30% ç¼“å†²
    }

# ç¤ºä¾‹è®¡ç®—
capacity = calculate_storage_capacity(
    vector_count=100_000_000,  # 1äº¿å‘é‡
    vector_dim=768,
    replica_count=2,
    compression_ratio=0.25
)
print(f"æ¨èå­˜å‚¨å®¹é‡: {capacity['recommended_gb']:.2f} GB")
```

**å†…å­˜å®¹é‡è§„åˆ’**
```python
def calculate_memory_requirements(
    active_vectors: int,
    vector_dim: int,
    query_qps: int,
    search_topk: int = 100
):
    """
    è®¡ç®—å†…å­˜éœ€æ±‚
    
    Args:
        active_vectors: æ´»è·ƒå‘é‡æ•°é‡ (ç»å¸¸è¢«æŸ¥è¯¢çš„)
        vector_dim: å‘é‡ç»´åº¦
        query_qps: æŸ¥è¯¢ QPS
        search_topk: æœç´¢è¿”å›çš„ top-k ç»“æœæ•°
    """
    # QueryNode å†…å­˜éœ€æ±‚
    # 1. å‘é‡æ•°æ®å†…å­˜
    vector_memory_gb = active_vectors * vector_dim * 4 / (1024**3)
    
    # 2. ç´¢å¼•å†…å­˜ (HNSW çº¦ä¸ºå‘é‡æ•°æ®çš„ 1.5 å€)
    index_memory_gb = vector_memory_gb * 1.5
    
    # 3. æŸ¥è¯¢ç¼“å­˜å†…å­˜
    query_cache_gb = query_qps * search_topk * vector_dim * 4 / (1024**3) * 10  # 10ç§’ç¼“å­˜
    
    # 4. ç³»ç»Ÿå¼€é”€
    system_overhead_gb = (vector_memory_gb + index_memory_gb) * 0.2
    
    total_memory_gb = vector_memory_gb + index_memory_gb + query_cache_gb + system_overhead_gb
    
    return {
        "vector_data_gb": vector_memory_gb,
        "index_gb": index_memory_gb,
        "query_cache_gb": query_cache_gb,
        "system_overhead_gb": system_overhead_gb,
        "total_gb": total_memory_gb,
        "recommended_per_node_gb": total_memory_gb / 3,  # å‡è®¾ 3 ä¸ª QueryNode
    }
```

### 6.4 å®‰å…¨å’Œæƒé™ç®¡ç†

#### 6.4.1 RBAC é…ç½®

**ç”¨æˆ·å’Œè§’è‰²ç®¡ç†**
```python
from pymilvus import connections, utility

# è¿æ¥ç®¡ç†å‘˜è´¦æˆ·
connections.connect(
    alias="admin",
    host="localhost",
    port="19530",
    user="root",
    password="admin_password"
)

# åˆ›å»ºè§’è‰²
utility.create_role("data_scientist", using="admin")
utility.create_role("application_user", using="admin")

# åˆ›å»ºç”¨æˆ·
utility.create_user("alice", "alice_password", using="admin")
utility.create_user("bob", "bob_password", using="admin")

# åˆ†é…è§’è‰²
utility.add_user_to_role("alice", "data_scientist", using="admin")
utility.add_user_to_role("bob", "application_user", using="admin")

# æˆäºˆæƒé™
utility.grant_role_privilege(
    role_name="data_scientist",
    object_type="Collection",
    object_name="research_vectors",
    privilege="Search",
    using="admin"
)

utility.grant_role_privilege(
    role_name="application_user", 
    object_type="Collection",
    object_name="prod_vectors",
    privilege="Search",
    using="admin"
)
```

#### 6.4.2 ç½‘ç»œå®‰å…¨é…ç½®

**TLS åŠ å¯†é…ç½®**
```yaml
# milvus.yaml é…ç½®æ–‡ä»¶
tls:
  serverPemPath: "/etc/milvus/tls/server.pem"
  serverKeyPath: "/etc/milvus/tls/server.key"
  caPemPath: "/etc/milvus/tls/ca.pem"

proxy:
  http:
    enabled: true
    port: 9091
  grpc:
    serverMaxRecvSize: 268435456
    serverMaxSendSize: 268435456
    clientMaxRecvSize: 268435456
    clientMaxSendSize: 268435456

common:
  security:
    authorizationEnabled: true
    tlsMode: 2  # å¼ºåˆ¶ TLS
```

### 6.5 æ•°æ®è¿ç§»å’Œå¤‡ä»½

#### 6.5.1 æ•°æ®å¤‡ä»½ç­–ç•¥

**å…ƒæ•°æ®å¤‡ä»½**
```bash
#!/bin/bash
# å¤‡ä»½ etcd å…ƒæ•°æ®
ETCD_ENDPOINTS="etcd-0:2379,etcd-1:2379,etcd-2:2379"
BACKUP_DIR="/backup/milvus/$(date +%Y%m%d_%H%M%S)"

mkdir -p $BACKUP_DIR

# å¤‡ä»½ etcd æ•°æ®
etcdctl --endpoints=$ETCD_ENDPOINTS snapshot save $BACKUP_DIR/etcd_snapshot.db

# å¤‡ä»½ Milvus é…ç½®
kubectl get configmap milvus-config -o yaml > $BACKUP_DIR/milvus-config.yaml

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
tar -czf $BACKUP_DIR.tar.gz $BACKUP_DIR
rm -rf $BACKUP_DIR

echo "Backup completed: $BACKUP_DIR.tar.gz"
```

**å‘é‡æ•°æ®å¤‡ä»½**
```python
import os
from milvus_backup import MilvusBackup

class VectorDataBackup:
    def __init__(self, milvus_host, backup_storage):
        self.backup = MilvusBackup(
            milvus_endpoint=f"{milvus_host}:19530",
            storage_config={
                "type": "s3",
                "endpoint": backup_storage["endpoint"],
                "access_key": backup_storage["access_key"],
                "secret_key": backup_storage["secret_key"],
                "bucket": backup_storage["bucket"]
            }
        )
    
    def backup_collection(self, collection_name, backup_name=None):
        if not backup_name:
            backup_name = f"{collection_name}_{int(time.time())}"
        
        # åˆ›å»ºå¤‡ä»½
        job_id = self.backup.create_backup(
            backup_name=backup_name,
            collection_names=[collection_name]
        )
        
        # ç­‰å¾…å¤‡ä»½å®Œæˆ
        while True:
            status = self.backup.get_backup_status(job_id)
            if status["state"] == "Completed":
                break
            elif status["state"] == "Failed":
                raise Exception(f"Backup failed: {status['error']}")
            time.sleep(10)
        
        return backup_name
    
    def restore_collection(self, backup_name, target_collection=None):
        # æ¢å¤æ•°æ®
        job_id = self.backup.restore_backup(
            backup_name=backup_name,
            target_collection=target_collection
        )
        
        # ç›‘æ§æ¢å¤è¿›åº¦
        while True:
            status = self.backup.get_restore_status(job_id)
            if status["state"] == "Completed":
                break
            elif status["state"] == "Failed":
                raise Exception(f"Restore failed: {status['error']}")
            time.sleep(10)
        
        return True
```

#### 6.5.2 æ•°æ®è¿ç§»å·¥å…·

**è·¨é›†ç¾¤è¿ç§»**
```python
class MilvusMigration:
    def __init__(self, source_config, target_config):
        self.source = MilvusClient(**source_config)
        self.target = MilvusClient(**target_config)
    
    def migrate_collection(self, collection_name, batch_size=1000):
        # 1. è·å–æºé›†åˆä¿¡æ¯
        source_info = self.source.describe_collection(collection_name)
        
        # 2. åœ¨ç›®æ ‡åˆ›å»ºé›†åˆ
        self.target.create_collection(
            collection_name=collection_name,
            schema=source_info["schema"],
            index_params=source_info["index_params"]
        )
        
        # 3. åˆ†æ‰¹è¿ç§»æ•°æ®
        offset = 0
        while True:
            # ä»æºè¯»å–æ•°æ®
            results = self.source.query(
                collection_name=collection_name,
                expr="",
                limit=batch_size,
                offset=offset,
                output_fields=["*"]
            )
            
            if not results:
                break
            
            # å†™å…¥ç›®æ ‡
            self.target.insert(
                collection_name=collection_name,
                data=results
            )
            
            offset += batch_size
            print(f"Migrated {offset} records")
        
        # 4. æ„å»ºç´¢å¼•
        self.target.create_index(
            collection_name=collection_name,
            field_name="vector",
            index_params=source_info["index_params"]
        )
        
        print(f"Migration completed for {collection_name}")

# ä½¿ç”¨ç¤ºä¾‹
migration = MilvusMigration(
    source_config={
        "uri": "http://source-milvus:19530",
        "token": "source_credentials"
    },
    target_config={
        "uri": "http://target-milvus:19530", 
        "token": "target_credentials"
    }
)

migration.migrate_collection("my_vectors")
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£æ·±å…¥å‰–æäº† Milvus å‘é‡æ•°æ®åº“çš„æºç æ¶æ„ï¼Œæ¶µç›–äº†ä»æ¡†æ¶ä½¿ç”¨åˆ°æ ¸å¿ƒæ¨¡å—å®ç°çš„å„ä¸ªå±‚é¢ã€‚é€šè¿‡è¯¦ç»†çš„ä»£ç åˆ†æã€æ¶æ„å›¾è§£å’Œå®æˆ˜ç»éªŒåˆ†äº«ï¼Œå¸®åŠ©å¼€å‘è€…å…¨é¢ç†è§£ Milvus çš„å†…éƒ¨æœºåˆ¶ã€‚

**æ ¸å¿ƒè¦ç‚¹å›é¡¾ï¼š**

1. **æ¶æ„è®¾è®¡**ï¼šMilvus é‡‡ç”¨äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„ï¼Œå®ç°å­˜å‚¨è®¡ç®—åˆ†ç¦»ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•
2. **API è®¾è®¡**ï¼šé€šè¿‡ Proxy æä¾›ç»Ÿä¸€çš„ gRPC/HTTP æ¥å£ï¼Œä½¿ç”¨æ‹¦æˆªå™¨å®ç°æ¨ªåˆ‡å…³æ³¨ç‚¹
3. **æ•°æ®æµ**ï¼šæ’å…¥æ•°æ®é€šè¿‡æ¶ˆæ¯é˜Ÿåˆ—å¼‚æ­¥å¤„ç†ï¼ŒæŸ¥è¯¢è¯·æ±‚å¹¶è¡Œåˆ†å‘åˆ°å¤šä¸ªèŠ‚ç‚¹
4. **å­˜å‚¨å¼•æ“**ï¼šæ”¯æŒå¤šç§å‘é‡ç´¢å¼•ç®—æ³•ï¼Œé’ˆå¯¹ä¸åŒåœºæ™¯ä¼˜åŒ–æ€§èƒ½
5. **è¿ç»´ç›‘æ§**ï¼šæä¾›å®Œæ•´çš„ç›‘æ§æŒ‡æ ‡å’Œæ•…éšœæ’æŸ¥å·¥å…·

**æœ€ä½³å®è·µå»ºè®®ï¼š**

- æ ¹æ®æ•°æ®è§„æ¨¡å’ŒæŸ¥è¯¢æ¨¡å¼é€‰æ‹©åˆé€‚çš„ç´¢å¼•ç±»å‹
- åˆç†è®¾è®¡é›†åˆ Schema å’Œåˆ†ç‰‡ç­–ç•¥
- å»ºç«‹å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶
- åˆ¶å®šæ•°æ®å¤‡ä»½å’Œè¿ç§»æ–¹æ¡ˆ
- é…ç½®é€‚å½“çš„å®‰å…¨å’Œæƒé™æ§åˆ¶

å¸Œæœ›æœ¬æ–‡æ¡£èƒ½å¤Ÿå¸®åŠ©æ‚¨æ·±å…¥ç†è§£ Milvus çš„æŠ€æœ¯å†…å¹•ï¼Œåœ¨å®é™…é¡¹ç›®ä¸­æ›´å¥½åœ°åº”ç”¨è¿™ä¸€å¼ºå¤§çš„å‘é‡æ•°æ®åº“ç³»ç»Ÿã€‚
