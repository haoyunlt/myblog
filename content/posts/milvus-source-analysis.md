---
title: "Milvus æºç æ·±åº¦å‰–æ"
date: 2024-12-19T10:00:00+08:00
draft: false
tags: ["Milvus", "å‘é‡æ•°æ®åº“", "æºç åˆ†æ", "åˆ†å¸ƒå¼ç³»ç»Ÿ", "AIåŸºç¡€è®¾æ–½"]
categories: ["milvus", "æŠ€æœ¯åˆ†æ"]
description: "æ·±å…¥å‰–æ Milvus å‘é‡æ•°æ®åº“çš„æºç å®ç°ï¼ŒåŒ…æ‹¬æ¶æ„è®¾è®¡ã€æ ¸å¿ƒæ¨¡å—å’Œæ€§èƒ½ä¼˜åŒ–"
weight: 310
slug: "milvus-source-analysis"
---

# Milvus æºç æ·±åº¦å‰–æ

## ç›®å½•

1. [æ¡†æ¶ä½¿ç”¨æ‰‹å†Œ](#1-æ¡†æ¶ä½¿ç”¨æ‰‹å†Œ)
2. [å¯¹å¤– API æ·±å…¥åˆ†æ](#2-å¯¹å¤–-api-æ·±å…¥åˆ†æ)
3. [æ•´ä½“æ¶æ„è®¾è®¡](#3-æ•´ä½“æ¶æ„è®¾è®¡)
4. [æ ¸å¿ƒæ¨¡å—åˆ†æ](#4-æ ¸å¿ƒæ¨¡å—åˆ†æ)
5. [å…³é”®æ•°æ®ç»“æ„](#5-å…³é”®æ•°æ®ç»“æ„)
6. [å®æˆ˜ç»éªŒæ€»ç»“](#6-å®æˆ˜ç»éªŒæ€»ç»“)

---

## 1. æ¡†æ¶ä½¿ç”¨æ‰‹å†Œ

### 1.1 Milvus ç®€ä»‹

Milvus æ˜¯ä¸€ä¸ªå¼€æºçš„å‘é‡æ•°æ®åº“ï¼Œä¸“ä¸º AI åº”ç”¨å’Œå‘é‡ç›¸ä¼¼åº¦æœç´¢è€Œè®¾è®¡ã€‚å®ƒé‡‡ç”¨äº‘åŸç”Ÿæ¶æ„ï¼Œæ”¯æŒå­˜å‚¨ä¸è®¡ç®—åˆ†ç¦»ï¼Œå…·å¤‡é«˜æ€§èƒ½ã€é«˜å¯ç”¨æ€§å’Œæ°´å¹³æ‰©å±•èƒ½åŠ›ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- ğŸš€ **é«˜æ€§èƒ½**ï¼šä¸‡äº¿çº§å‘é‡æ¯«ç§’çº§æœç´¢
- ğŸŒ **äº‘åŸç”Ÿ**ï¼šå­˜å‚¨è®¡ç®—åˆ†ç¦»ï¼Œæ”¯æŒ Kubernetes
- ğŸ”§ **å¤šç´¢å¼•æ”¯æŒ**ï¼šHNSWã€IVFã€FLATã€SCANNã€DiskANN
- ğŸ›¡ï¸ **é«˜å¯ç”¨**ï¼š99.9% å¯ç”¨æ€§ä¿è¯
- ğŸ“Š **å¤šæ•°æ®ç±»å‹**ï¼šå‘é‡ã€æ ‡é‡ã€VARCHAR æ”¯æŒ
- ğŸ” **æ··åˆæœç´¢**ï¼šè¯­ä¹‰æœç´¢ + å…¨æ–‡æœç´¢

### 1.2 å¿«é€Ÿå¼€å§‹

#### 1.2.1 å®‰è£…éƒ¨ç½²

**Docker Compose å•æœºéƒ¨ç½²**
```bash
# ä¸‹è½½é…ç½®æ–‡ä»¶
wget https://github.com/milvus-io/milvus/releases/download/v2.3.0/milvus-standalone-docker-compose.yml -O docker-compose.yml

# å¯åŠ¨æœåŠ¡ï¼ˆåŒ…å« Milvusã€etcdã€MinIOï¼‰
docker-compose up -d

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs milvus-standalone
```

**Kubernetes é›†ç¾¤éƒ¨ç½²**
```bash
# æ·»åŠ  Helm ä»“åº“
helm repo add milvus https://milvus-io.github.io/milvus-helm/
helm repo update

# åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace milvus

# å®‰è£… Milvus é›†ç¾¤
helm install milvus-cluster milvus/milvus \
  --namespace milvus \
  --set cluster.enabled=true \
  --set etcd.replicaCount=3 \
  --set pulsar.enabled=true \
  --set minio.mode=distributed

# æ£€æŸ¥éƒ¨ç½²çŠ¶æ€
kubectl get pods -n milvus
```

**æºç ç¼–è¯‘éƒ¨ç½²**
```bash
# å…‹éš†æºç 
git clone https://github.com/milvus-io/milvus.git
cd milvus

# å®‰è£…ä¾èµ–
make install

# ç¼–è¯‘
make all

# å¯åŠ¨å•æœºæ¨¡å¼
./bin/milvus run standalone
```

#### 1.2.2 Python SDK è¯¦ç»†ä½¿ç”¨

**åŸºç¡€è¿æ¥å’Œè®¤è¯**
```python
from pymilvus import MilvusClient, connections, utility
import numpy as np

# æ–¹å¼1ï¼šä½¿ç”¨ MilvusClientï¼ˆæ¨èï¼‰
client = MilvusClient(
    uri="http://localhost:19530",  # Milvus æœåŠ¡åœ°å€
    token="username:password",     # è®¤è¯ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    db_name="default"              # æ•°æ®åº“åç§°
)

# æ–¹å¼2ï¼šä½¿ç”¨ä¼ ç»Ÿè¿æ¥æ–¹å¼
connections.connect(
    alias="default",
    host="localhost",
    port="19530",
    user="username",
    password="password"
)

# æ£€æŸ¥è¿æ¥çŠ¶æ€
print("Milvus server status:", utility.get_server_version())
```

**é›†åˆç®¡ç†å®Œæ•´ç¤ºä¾‹**
```python
from pymilvus import CollectionSchema, FieldSchema, DataType

# å®šä¹‰ Schema
def create_collection_schema():
    """
    åˆ›å»ºé›†åˆ Schemaï¼Œå®šä¹‰æ•°æ®ç»“æ„
    
    Returns:
        CollectionSchema: é›†åˆæ¨¡å¼å®šä¹‰
    """
    # å®šä¹‰å­—æ®µ
    fields = [
        # ä¸»é”®å­—æ®µï¼ˆè‡ªåŠ¨ç”ŸæˆIDï¼‰
        FieldSchema(
            name="id", 
            dtype=DataType.INT64, 
            is_primary=True, 
            auto_id=True,
            description="ä¸»é”®IDï¼Œè‡ªåŠ¨ç”Ÿæˆ"
        ),
        # å‘é‡å­—æ®µ
        FieldSchema(
            name="embedding", 
            dtype=DataType.FLOAT_VECTOR, 
            dim=768,  # å‘é‡ç»´åº¦
            description="æ–‡æœ¬åµŒå…¥å‘é‡"
        ),
        # æ ‡é‡å­—æ®µ
        FieldSchema(
            name="text", 
            dtype=DataType.VARCHAR, 
            max_length=1000,
            description="åŸå§‹æ–‡æœ¬å†…å®¹"
        ),
        FieldSchema(
            name="category", 
            dtype=DataType.VARCHAR, 
            max_length=50,
            description="æ–‡æ¡£åˆ†ç±»"
        ),
        FieldSchema(
            name="timestamp", 
            dtype=DataType.INT64,
            description="åˆ›å»ºæ—¶é—´æˆ³"
        ),
        # JSON å­—æ®µï¼ˆå­˜å‚¨å¤æ‚å…ƒæ•°æ®ï¼‰
        FieldSchema(
            name="metadata", 
            dtype=DataType.JSON,
            description="æ‰©å±•å…ƒæ•°æ®ä¿¡æ¯"
        )
    ]
    
    # åˆ›å»º Schema
    schema = CollectionSchema(
        fields=fields,
        description="æ–‡æ¡£å‘é‡æ£€ç´¢é›†åˆ",
        enable_dynamic_field=True  # æ”¯æŒåŠ¨æ€å­—æ®µ
    )
    
    return schema

# åˆ›å»ºé›†åˆ
collection_name = "document_search"
schema = create_collection_schema()

client.create_collection(
    collection_name=collection_name,
    schema=schema,
    shards_num=2,  # åˆ†ç‰‡æ•°é‡
    consistency_level="Strong"  # ä¸€è‡´æ€§çº§åˆ«
)

print(f"é›†åˆ {collection_name} åˆ›å»ºæˆåŠŸ")
```

**ç´¢å¼•ç®¡ç†å’Œä¼˜åŒ–**
```python
# åˆ›å»ºå‘é‡ç´¢å¼•
def create_vector_index():
    """
    ä¸ºå‘é‡å­—æ®µåˆ›å»ºç´¢å¼•ï¼Œæå‡æœç´¢æ€§èƒ½
    """
    # HNSW ç´¢å¼•ï¼ˆé«˜ç²¾åº¦ï¼‰
    hnsw_index_params = {
        "index_type": "HNSW",
        "metric_type": "L2",  # è·ç¦»åº¦é‡ï¼šL2, IP, COSINE
        "params": {
            "M": 16,              # è¿æ¥æ•°ï¼Œå½±å“ç²¾åº¦å’Œå†…å­˜ä½¿ç”¨
            "efConstruction": 200  # æ„å»ºæ—¶æœç´¢æ·±åº¦
        }
    }
    
    # IVF ç´¢å¼•ï¼ˆå¹³è¡¡æ€§èƒ½ï¼‰
    ivf_index_params = {
        "index_type": "IVF_FLAT",
        "metric_type": "IP",
        "params": {
            "nlist": 1024  # èšç±»ä¸­å¿ƒæ•°é‡
        }
    }
    
    # åˆ›å»ºç´¢å¼•
    client.create_index(
        collection_name=collection_name,
        field_name="embedding",
        index_params=hnsw_index_params
    )
    
    # åˆ›å»ºæ ‡é‡å­—æ®µç´¢å¼•ï¼ˆåŠ é€Ÿè¿‡æ»¤ï¼‰
    client.create_index(
        collection_name=collection_name,
        field_name="category",
        index_params={"index_type": "TRIE"}  # å­—ç¬¦ä¸²ç´¢å¼•
    )
    
    print("ç´¢å¼•åˆ›å»ºå®Œæˆ")

create_vector_index()
```

**æ•°æ®æ’å…¥å’Œæ‰¹å¤„ç†**
```python
import time
import random

def generate_sample_data(num_records=1000):
    """
    ç”Ÿæˆç¤ºä¾‹æ•°æ®
    
    Args:
        num_records (int): ç”Ÿæˆè®°å½•æ•°é‡
        
    Returns:
        list: åŒ…å«å‘é‡å’Œå…ƒæ•°æ®çš„è®°å½•åˆ—è¡¨
    """
    categories = ["æŠ€æœ¯", "ç§‘å­¦", "æ–‡å­¦", "å†å²", "è‰ºæœ¯"]
    
    data = []
    for i in range(num_records):
        # ç”Ÿæˆéšæœºå‘é‡ï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨çœŸå®çš„åµŒå…¥å‘é‡ï¼‰
        embedding = np.random.random(768).tolist()
        
        record = {
            "embedding": embedding,
            "text": f"è¿™æ˜¯ç¬¬ {i+1} æ¡æ–‡æ¡£å†…å®¹ï¼ŒåŒ…å«ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯",
            "category": random.choice(categories),
            "timestamp": int(time.time() * 1000),
            "metadata": {
                "source": "sample_generator",
                "version": "1.0",
                "tags": [f"tag_{j}" for j in range(random.randint(1, 5))]
            }
        }
        data.append(record)
    
    return data

# æ‰¹é‡æ’å…¥æ•°æ®
def batch_insert_data():
    """
    æ‰¹é‡æ’å…¥æ•°æ®ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®å¯¼å…¥
    """
    batch_size = 100  # æ‰¹æ¬¡å¤§å°
    total_records = 1000
    
    for i in range(0, total_records, batch_size):
        batch_data = generate_sample_data(batch_size)

# æ’å…¥æ•°æ®
        result = client.insert(
            collection_name=collection_name,
            data=batch_data
        )
        
        print(f"æ‰¹æ¬¡ {i//batch_size + 1}: æ’å…¥ {len(batch_data)} æ¡è®°å½•")
        print(f"æ’å…¥ç»“æœ: {result}")
    
    # åˆ·ç›˜ç¡®ä¿æ•°æ®æŒä¹…åŒ–
    client.flush(collection_name=collection_name)
    print("æ•°æ®æ’å…¥å®Œæˆå¹¶å·²åˆ·ç›˜")

batch_insert_data()
```

**é«˜çº§æœç´¢å’ŒæŸ¥è¯¢**
```python
def advanced_search_examples():
    """
    å±•ç¤ºå„ç§é«˜çº§æœç´¢åŠŸèƒ½
    """
    # 1. åŸºç¡€å‘é‡æœç´¢
    query_vector = np.random.random(768).tolist()
    
    basic_results = client.search(
        collection_name=collection_name,
        data=[query_vector],
        anns_field="embedding",
    limit=10,
        output_fields=["text", "category", "timestamp"]
    )
    
    print("åŸºç¡€æœç´¢ç»“æœ:")
    for hits in basic_results:
        for hit in hits:
            print(f"ID: {hit.id}, ç›¸ä¼¼åº¦: {hit.score:.4f}")
            print(f"æ–‡æœ¬: {hit.entity.get('text')}")
            print(f"åˆ†ç±»: {hit.entity.get('category')}")
            print("-" * 50)
    
    # 2. å¸¦è¿‡æ»¤æ¡ä»¶çš„æœç´¢
    filtered_results = client.search(
        collection_name=collection_name,
        data=[query_vector],
        anns_field="embedding",
        limit=5,
        expr='category in ["æŠ€æœ¯", "ç§‘å­¦"] and timestamp > 1640995200000',
        output_fields=["text", "category", "metadata"]
    )
    
    print("\nè¿‡æ»¤æœç´¢ç»“æœ:")
    for hits in filtered_results:
        for hit in hits:
            print(f"ID: {hit.id}, åˆ†ç±»: {hit.entity.get('category')}")
    
    # 3. èŒƒå›´æœç´¢
    range_results = client.search(
        collection_name=collection_name,
        data=[query_vector],
        anns_field="embedding",
        limit=20,
        search_params={
            "metric_type": "L2",
            "params": {"nprobe": 16, "radius": 0.8, "range_filter": 1.2}
        }
    )
    
    # 4. æ··åˆæœç´¢ï¼ˆå¤šå‘é‡ï¼‰
    multi_vectors = [np.random.random(768).tolist() for _ in range(3)]
    
    hybrid_results = client.search(
        collection_name=collection_name,
        data=multi_vectors,
        anns_field="embedding",
        limit=5,
        output_fields=["text", "category"]
    )
    
    print(f"\næ··åˆæœç´¢ç»“æœ: {len(hybrid_results)} ç»„ç»“æœ")

# åŠ è½½é›†åˆåˆ°å†…å­˜
client.load_collection(collection_name=collection_name)
print("é›†åˆå·²åŠ è½½åˆ°å†…å­˜")

# æ‰§è¡Œé«˜çº§æœç´¢
advanced_search_examples()
```

**Go SDK ä½¿ç”¨ç¤ºä¾‹**
```go
package main

import (
    "context"
    "fmt"
    "log"
    
    "github.com/milvus-io/milvus-sdk-go/v2/client"
    "github.com/milvus-io/milvus-sdk-go/v2/entity"
)

func main() {
    // è¿æ¥ Milvus
    ctx := context.Background()
    
    c, err := client.NewGrpcClient(ctx, "localhost:19530")
    if err != nil {
        log.Fatal("è¿æ¥å¤±è´¥:", err)
    }
    defer c.Close()
    
    // åˆ›å»ºé›†åˆ
    collectionName := "go_example"
    schema := createCollectionSchema()
    
    err = c.CreateCollection(ctx, schema, 2) // 2ä¸ªåˆ†ç‰‡
    if err != nil {
        log.Fatal("åˆ›å»ºé›†åˆå¤±è´¥:", err)
    }
    
    // åˆ›å»ºç´¢å¼•
    indexParams := entity.NewIndexHNSW(entity.L2, 16, 200)
    err = c.CreateIndex(ctx, collectionName, "vector", indexParams, false)
    if err != nil {
        log.Fatal("åˆ›å»ºç´¢å¼•å¤±è´¥:", err)
    }
    
    // æ’å…¥æ•°æ®
    insertData(ctx, c, collectionName)
    
    // åŠ è½½é›†åˆ
    err = c.LoadCollection(ctx, collectionName, false)
    if err != nil {
        log.Fatal("åŠ è½½é›†åˆå¤±è´¥:", err)
    }
    
    // æœç´¢
    searchVectors(ctx, c, collectionName)
}

func createCollectionSchema() *entity.Schema {
    // å®šä¹‰å­—æ®µ
    idField := entity.NewField().
        WithName("id").
        WithDataType(entity.FieldTypeInt64).
        WithIsPrimaryKey(true).
        WithIsAutoID(true)
    
    vectorField := entity.NewField().
        WithName("vector").
        WithDataType(entity.FieldTypeFloatVector).
        WithDim(128)
    
    textField := entity.NewField().
        WithName("text").
        WithDataType(entity.FieldTypeVarChar).
        WithMaxLength(500)
    
    // åˆ›å»º Schema
    schema := entity.NewSchema().
        WithName("go_example").
        WithDescription("Go SDK ç¤ºä¾‹é›†åˆ").
        WithField(idField).
        WithField(vectorField).
        WithField(textField)
    
    return schema
}

func insertData(ctx context.Context, c client.Client, collectionName string) {
    // å‡†å¤‡æ•°æ®
    vectors := make([][]float32, 1000)
    texts := make([]string, 1000)
    
    for i := 0; i < 1000; i++ {
        // ç”Ÿæˆéšæœºå‘é‡
        vector := make([]float32, 128)
        for j := 0; j < 128; j++ {
            vector[j] = rand.Float32()
        }
        vectors[i] = vector
        texts[i] = fmt.Sprintf("æ–‡æ¡£ %d", i)
    }
    
    // åˆ›å»ºåˆ—æ•°æ®
    vectorColumn := entity.NewColumnFloatVector("vector", 128, vectors)
    textColumn := entity.NewColumnVarChar("text", texts)
    
    // æ’å…¥æ•°æ®
    _, err := c.Insert(ctx, collectionName, "", vectorColumn, textColumn)
    if err != nil {
        log.Fatal("æ’å…¥æ•°æ®å¤±è´¥:", err)
    }
    
    // åˆ·ç›˜
    err = c.Flush(ctx, collectionName, false)
    if err != nil {
        log.Fatal("åˆ·ç›˜å¤±è´¥:", err)
    }
    
    fmt.Println("æ•°æ®æ’å…¥æˆåŠŸ")
}

func searchVectors(ctx context.Context, c client.Client, collectionName string) {
    // å‡†å¤‡æŸ¥è¯¢å‘é‡
    queryVector := make([]float32, 128)
    for i := 0; i < 128; i++ {
        queryVector[i] = rand.Float32()
    }
    
    // æœç´¢å‚æ•°
    searchParams := entity.NewIndexHNSWSearchParam(64) // ef å‚æ•°
    
    // æ‰§è¡Œæœç´¢
    results, err := c.Search(
        ctx,
        collectionName,
        []string{},  // åˆ†åŒºåç§°
        "id > 0",    // è¿‡æ»¤è¡¨è¾¾å¼
        []string{"text"}, // è¾“å‡ºå­—æ®µ
        []entity.Vector{entity.FloatVector(queryVector)},
        "vector",    // å‘é‡å­—æ®µå
        entity.L2,   // è·ç¦»åº¦é‡
        10,          // topK
        searchParams,
    )
    
    if err != nil {
        log.Fatal("æœç´¢å¤±è´¥:", err)
    }
    
    // å¤„ç†ç»“æœ
    for _, result := range results {
        fmt.Printf("æ‰¾åˆ° %d ä¸ªç»“æœ:\n", result.ResultCount)
        for i := 0; i < result.ResultCount; i++ {
            id := result.IDs.(*entity.ColumnInt64).Data()[i]
            score := result.Scores[i]
            text := result.Fields.GetColumn("text").(*entity.ColumnVarChar).Data()[i]
            
            fmt.Printf("ID: %d, ç›¸ä¼¼åº¦: %.4f, æ–‡æœ¬: %s\n", id, score, text)
        }
    }
}
```

### 1.3 æ¶æ„æ¨¡å¼

Milvus æ”¯æŒä¸¤ç§éƒ¨ç½²æ¨¡å¼ï¼š

#### å•æœºæ¨¡å¼ (Standalone)
- æ‰€æœ‰ç»„ä»¶è¿è¡Œåœ¨å•ä¸ªè¿›ç¨‹ä¸­
- é€‚åˆå¼€å‘æµ‹è¯•å’Œå°è§„æ¨¡åº”ç”¨
- èµ„æºéœ€æ±‚è¾ƒä½

#### é›†ç¾¤æ¨¡å¼ (Cluster)
- å¾®æœåŠ¡æ¶æ„ï¼Œç»„ä»¶ç‹¬ç«‹éƒ¨ç½²
- æ”¯æŒæ°´å¹³æ‰©å±•å’Œé«˜å¯ç”¨
- é€‚åˆç”Ÿäº§ç¯å¢ƒ

---

## 2. å¯¹å¤– API æ·±å…¥åˆ†æ

### 2.1 API æ¶æ„æ¦‚è§ˆ

Milvus é€šè¿‡ Proxy ç»„ä»¶å¯¹å¤–æä¾›ç»Ÿä¸€çš„ API æœåŠ¡ï¼Œæ”¯æŒ gRPC å’Œ RESTful ä¸¤ç§åè®®ã€‚

```mermaid
graph TB
    Client[å®¢æˆ·ç«¯] --> Proxy[Proxy ç½‘å…³]
    Proxy --> RootCoord[RootCoord<br/>å…ƒæ•°æ®ç®¡ç†]
    Proxy --> DataCoord[DataCoord<br/>æ•°æ®åè°ƒ]
    Proxy --> QueryCoord[QueryCoord<br/>æŸ¥è¯¢åè°ƒ]
    Proxy --> DataNode[DataNode<br/>æ•°æ®èŠ‚ç‚¹]
    Proxy --> QueryNode[QueryNode<br/>æŸ¥è¯¢èŠ‚ç‚¹]
```

### 2.2 æ ¸å¿ƒ API æ¥å£åˆ†æ

#### 2.2.1 é›†åˆç®¡ç† API

**CreateCollection - åˆ›å»ºé›†åˆ**

**API å…¥å£å‡½æ•°ï¼š**
```go
// æ–‡ä»¶ï¼šinternal/proxy/impl.go
// åŠŸèƒ½ï¼šåˆ›å»ºæ–°çš„é›†åˆï¼ŒåŒ…æ‹¬ Schema éªŒè¯ã€æƒé™æ£€æŸ¥å’Œä»»åŠ¡è°ƒåº¦
// å‚æ•°ï¼š
//   - ctx: ä¸Šä¸‹æ–‡ï¼Œç”¨äºè¶…æ—¶æ§åˆ¶å’Œé“¾è·¯è¿½è¸ª
//   - request: åˆ›å»ºé›†åˆè¯·æ±‚ï¼ŒåŒ…å«é›†åˆåç§°ã€Schemaã€åˆ†ç‰‡æ•°ç­‰ä¿¡æ¯
// è¿”å›å€¼ï¼š
//   - *commonpb.Status: æ“ä½œçŠ¶æ€ï¼ŒåŒ…å«é”™è¯¯ç å’Œé”™è¯¯ä¿¡æ¯
//   - error: Go æ ‡å‡†é”™è¯¯
func (node *Proxy) CreateCollection(ctx context.Context, request *milvuspb.CreateCollectionRequest) (*commonpb.Status, error) {
    // 1. å¥åº·çŠ¶æ€æ£€æŸ¥ - ç¡®ä¿ Proxy èŠ‚ç‚¹å¤„äºå¥åº·çŠ¶æ€
    if err := merr.CheckHealthy(node.GetStateCode()); err != nil {
        return merr.Status(err), nil
    }
    
    // 2. å¼€å¯é“¾è·¯è¿½è¸ª - ç”¨äºæ€§èƒ½ç›‘æ§å’Œé—®é¢˜æ’æŸ¥
    ctx, sp := otel.Tracer(typeutil.ProxyRole).Start(ctx, "Proxy-CreateCollection")
    defer sp.End()
    
    // 3. åˆ›å»ºä»»åŠ¡å¯¹è±¡ - å°è£…è¯·æ±‚å‚æ•°å’Œæ‰§è¡Œé€»è¾‘
    cct := &createCollectionTask{
        ctx:                     ctx,                // æ‰§è¡Œä¸Šä¸‹æ–‡
        Condition:               NewTaskCondition(ctx), // ä»»åŠ¡æ¡ä»¶æ§åˆ¶
        CreateCollectionRequest: request,            // åŸå§‹è¯·æ±‚
        mixCoord:                node.mixCoord,      // åè°ƒå™¨å®¢æˆ·ç«¯
    }

    // 4. è®°å½•è¯·æ±‚æ—¥å¿— - ä¾¿äºè°ƒè¯•å’Œç›‘æ§
    log := log.Ctx(ctx).With(
        zap.String("role", typeutil.ProxyRole),
        zap.String("db", request.DbName),
        zap.String("collection", request.CollectionName),
        zap.Int("len(schema)", len(request.Schema)),
        zap.Int32("shards_num", request.ShardsNum),
        zap.String("consistency_level", request.ConsistencyLevel.String()),
    )
    log.Info("CreateCollection request received")

    // 5. æäº¤åˆ° DDL ä»»åŠ¡é˜Ÿåˆ— - å¼‚æ­¥æ‰§è¡Œä»¥é¿å…é˜»å¡
    if err := node.sched.ddQueue.Enqueue(cct); err != nil {
        log.Warn("Failed to enqueue create collection task", zap.Error(err))
        return merr.Status(err), nil
    }
    
    // 6. ç­‰å¾…ä»»åŠ¡å®Œæˆ - åŒæ­¥ç­‰å¾…å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œç»“æœ
    if err := cct.WaitToFinish(); err != nil {
        log.Warn("Create collection task failed", zap.Error(err))
        return merr.Status(err), nil
    }
    
    return cct.result, nil
}
```

**ä»»åŠ¡é¢„å¤„ç†å‡½æ•°ï¼š**
```go
// æ–‡ä»¶ï¼šinternal/proxy/task.go
// åŠŸèƒ½ï¼šåœ¨æ‰§è¡Œåˆ›å»ºé›†åˆä»»åŠ¡å‰è¿›è¡Œå„ç§éªŒè¯å’Œé¢„å¤„ç†
// å‚æ•°ï¼šctx - æ‰§è¡Œä¸Šä¸‹æ–‡
// è¿”å›å€¼ï¼šerror - éªŒè¯å¤±è´¥æ—¶è¿”å›é”™è¯¯
func (t *createCollectionTask) PreExecute(ctx context.Context) error {
    // 1. è®¾ç½®æ¶ˆæ¯ç±»å‹å’ŒæºID
    t.Base.MsgType = commonpb.MsgType_CreateCollection
    t.Base.SourceID = paramtable.GetNodeID()

    // 2. è§£æ Schema - å°†å­—èŠ‚æ•°ç»„ååºåˆ—åŒ–ä¸ºç»“æ„ä½“
    t.schema = &schemapb.CollectionSchema{}
    err := proto.Unmarshal(t.Schema, t.schema)
    if err != nil {
        return fmt.Errorf("failed to unmarshal schema: %w", err)
    }
    t.schema.AutoID = false

    // 3. éªŒè¯å‡½æ•°å­—æ®µå®šä¹‰
    if err := validateFunction(t.schema); err != nil {
        return fmt.Errorf("function validation failed: %w", err)
    }

    // 4. éªŒè¯åˆ†ç‰‡æ•°é‡é™åˆ¶
    if t.ShardsNum > Params.ProxyCfg.MaxShardNum.GetAsInt32() {
        return fmt.Errorf("shards number %d exceeds maximum limit %d", 
            t.ShardsNum, Params.ProxyCfg.MaxShardNum.GetAsInt())
    }

    // 5. éªŒè¯å­—æ®µæ•°é‡é™åˆ¶
    totalFieldsNum := typeutil.GetTotalFieldsNum(t.schema)
    if totalFieldsNum > Params.ProxyCfg.MaxFieldNum.GetAsInt() {
        return fmt.Errorf("total fields number %d exceeds maximum limit %d", 
            totalFieldsNum, Params.ProxyCfg.MaxFieldNum.GetAsInt())
    }

    // 6. éªŒè¯å‘é‡å­—æ®µæ•°é‡
    vectorFields := len(typeutil.GetVectorFieldSchemas(t.schema))
    if vectorFields > Params.ProxyCfg.MaxVectorFieldNum.GetAsInt() {
        return fmt.Errorf("vector fields number %d exceeds maximum limit %d", 
            vectorFields, Params.ProxyCfg.MaxVectorFieldNum.GetAsInt())
    }
    if vectorFields == 0 {
        return merr.WrapErrParameterInvalidMsg("schema must contain at least one vector field")
    }

    // 7. éªŒè¯é›†åˆåç§°æ ¼å¼
    if err := validateCollectionName(t.schema.Name); err != nil {
        return fmt.Errorf("invalid collection name: %w", err)
    }

    // 8. éªŒè¯å­—æ®µåç§°å”¯ä¸€æ€§
    if err := validateDuplicatedFieldName(t.schema); err != nil {
        return fmt.Errorf("duplicated field name found: %w", err)
    }

    // 9. éªŒè¯ä¸»é”®å®šä¹‰
    if err := validatePrimaryKey(t.schema); err != nil {
        return fmt.Errorf("primary key validation failed: %w", err)
    }

    // 10. éªŒè¯åŠ¨æ€å­—æ®µé…ç½®
    if err := validateDynamicField(t.schema); err != nil {
        return fmt.Errorf("dynamic field validation failed: %w", err)
    }

    // 11. éªŒè¯è‡ªåŠ¨IDé…ç½®
    if err := ValidateFieldAutoID(t.schema); err != nil {
        return fmt.Errorf("auto ID validation failed: %w", err)
    }

    // 12. éªŒè¯å­—æ®µç±»å‹å®šä¹‰
    if err := validateFieldType(t.schema); err != nil {
        return fmt.Errorf("field type validation failed: %w", err)
    }

    // 13. é‡æ–°åºåˆ—åŒ– Schema
    t.CreateCollectionRequest.Schema, err = proto.Marshal(t.schema)
    if err != nil {
        return fmt.Errorf("failed to marshal schema: %w", err)
    }
    
    return nil
}
```

**ä»»åŠ¡æ‰§è¡Œå‡½æ•°ï¼š**
```go
// æ–‡ä»¶ï¼šinternal/proxy/task.go
// åŠŸèƒ½ï¼šæ‰§è¡Œåˆ›å»ºé›†åˆçš„æ ¸å¿ƒé€»è¾‘ï¼Œè°ƒç”¨ RootCoord æœåŠ¡
// å‚æ•°ï¼šctx - æ‰§è¡Œä¸Šä¸‹æ–‡
// è¿”å›å€¼ï¼šerror - æ‰§è¡Œå¤±è´¥æ—¶è¿”å›é”™è¯¯
func (t *createCollectionTask) Execute(ctx context.Context) error {
    var err error
    // è°ƒç”¨ MixCoordï¼ˆå®é™…æ˜¯ RootCoordï¼‰åˆ›å»ºé›†åˆ
    // MixCoord æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åè°ƒå™¨æ¥å£ï¼Œå°è£…äº†å¤šä¸ªåè°ƒå™¨çš„åŠŸèƒ½
    t.result, err = t.mixCoord.CreateCollection(ctx, t.CreateCollectionRequest)
    
    // æ£€æŸ¥ RPC è°ƒç”¨ç»“æœ
    return merr.CheckRPCCall(t.result, err)
}
```

**å…³é”®è°ƒç”¨é“¾è·¯å’Œæ—¶åºå›¾ï¼š**

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant Proxy as Proxy
    participant TaskQueue as ä»»åŠ¡é˜Ÿåˆ—
    participant Task as CreateCollectionTask
    participant RootCoord as RootCoord
    participant MetaStore as å…ƒæ•°æ®å­˜å‚¨
    participant DataCoord as DataCoord
    
    Client->>Proxy: CreateCollection Request
    Note over Proxy: 1. å¥åº·æ£€æŸ¥
    Note over Proxy: 2. åˆ›å»ºä»»åŠ¡å¯¹è±¡
    Proxy->>TaskQueue: Enqueue Task
    TaskQueue->>Task: Execute PreExecute
    Note over Task: 3. Schema éªŒè¯
    Note over Task: 4. å­—æ®µéªŒè¯
    Note over Task: 5. é™åˆ¶æ£€æŸ¥
    Task->>Task: Execute
    Task->>RootCoord: CreateCollection RPC
    RootCoord->>RootCoord: åˆ†é…é›†åˆID
    RootCoord->>RootCoord: åˆ†é…åˆ†åŒºID
    RootCoord->>RootCoord: åˆ†é…è™šæ‹Ÿé€šé“
    RootCoord->>MetaStore: ä¿å­˜é›†åˆå…ƒæ•°æ®
    MetaStore-->>RootCoord: ä¿å­˜æˆåŠŸ
    RootCoord->>DataCoord: é€šçŸ¥åˆ›å»ºé›†åˆ
    DataCoord-->>RootCoord: ç¡®è®¤
    RootCoord-->>Task: è¿”å›ç»“æœ
    Task-->>Proxy: ä»»åŠ¡å®Œæˆ
    Proxy-->>Client: CreateCollection Response
```

#### 2.2.2 æ•°æ®æ“ä½œ API

**Insert - æ•°æ®æ’å…¥**

**API å…¥å£å‡½æ•°ï¼š**
```go
// æ–‡ä»¶ï¼šinternal/proxy/impl.go
// åŠŸèƒ½ï¼šæ’å…¥æ•°æ®åˆ°æŒ‡å®šé›†åˆï¼Œæ”¯æŒæ‰¹é‡æ’å…¥å’Œåˆ†åŒºé”®è·¯ç”±
// å‚æ•°ï¼š
//   - ctx: ä¸Šä¸‹æ–‡ï¼Œç”¨äºè¶…æ—¶æ§åˆ¶å’Œé“¾è·¯è¿½è¸ª
//   - request: æ’å…¥è¯·æ±‚ï¼ŒåŒ…å«é›†åˆåç§°ã€åˆ†åŒºåç§°ã€å­—æ®µæ•°æ®ç­‰
// è¿”å›å€¼ï¼š
//   - *milvuspb.MutationResult: æ’å…¥ç»“æœï¼ŒåŒ…å«æ’å…¥çš„IDå’Œé”™è¯¯ä¿¡æ¯
//   - error: Go æ ‡å‡†é”™è¯¯
func (node *Proxy) Insert(ctx context.Context, request *milvuspb.InsertRequest) (*milvuspb.MutationResult, error) {
    // 1. å¼€å¯é“¾è·¯è¿½è¸ª
    ctx, sp := otel.Tracer(typeutil.ProxyRole).Start(ctx, "Proxy-Insert")
    defer sp.End()

    // 2. å¥åº·çŠ¶æ€æ£€æŸ¥
    if err := merr.CheckHealthy(node.GetStateCode()); err != nil {
        return &milvuspb.MutationResult{Status: merr.Status(err)}, nil
    }

    // 3. è®°å½•è¯·æ±‚æ—¥å¿—å’ŒæŒ‡æ ‡
    log := log.Ctx(ctx).With(
        zap.String("role", typeutil.ProxyRole),
        zap.String("db", request.DbName),
        zap.String("collection", request.CollectionName),
        zap.String("partition", request.PartitionName),
        zap.Int("len(FieldsData)", len(request.FieldsData)),
        zap.Int("len(HashKeys)", len(request.HashKeys)),
        zap.Uint32("NumRows", request.NumRows),
    )

    // 4. è®¾ç½®ç›‘æ§æŒ‡æ ‡
    metrics.GetStats(ctx).
        SetNodeID(paramtable.GetNodeID()).
        SetInboundLabel(metrics.InsertLabel).
        SetDatabaseName(request.GetDbName()).
        SetCollectionName(request.GetCollectionName())

    // 5. åˆ›å»ºæ’å…¥ä»»åŠ¡å¯¹è±¡
    it := &insertTask{
        ctx:       ctx,
        Condition: NewTaskCondition(ctx),
        insertMsg: &msgstream.InsertMsg{
            BaseMsg: msgstream.BaseMsg{
                HashValues: request.HashKeys, // ç”¨äºåˆ†ç‰‡è·¯ç”±çš„å“ˆå¸Œå€¼
            },
            InsertRequest: &msgpb.InsertRequest{
                Base: commonpbutil.NewMsgBase(
                    commonpbutil.WithMsgType(commonpb.MsgType_Insert),
                    commonpbutil.WithSourceID(paramtable.GetNodeID()),
                ),
                DbName:         request.GetDbName(),
                CollectionName: request.CollectionName,
                PartitionName:  request.PartitionName,
                FieldsData:     request.FieldsData,
                NumRows:        uint64(request.NumRows),
                Version:        msgpb.InsertDataVersion_ColumnBased,
                Namespace:      request.Namespace,
            },
        },
        idAllocator:     node.rowIDAllocator,    // IDåˆ†é…å™¨
        chMgr:           node.chMgr,             // é€šé“ç®¡ç†å™¨
        schemaTimestamp: request.SchemaTimestamp, // Schemaæ—¶é—´æˆ³
    }

    // 6. æ„é€ å¤±è´¥å“åº”çš„è¾…åŠ©å‡½æ•°
    constructFailedResponse := func(err error) *milvuspb.MutationResult {
        numRows := request.NumRows
        errIndex := make([]uint32, numRows)
        for i := uint32(0); i < numRows; i++ {
            errIndex[i] = i
        }
        return &milvuspb.MutationResult{
            Status:   merr.Status(err),
            ErrIndex: errIndex,
        }
    }

    // 7. æäº¤åˆ° DML ä»»åŠ¡é˜Ÿåˆ—
    if err := node.sched.dmQueue.Enqueue(it); err != nil {
        log.Warn("Failed to enqueue insert task", zap.Error(err))
        return constructFailedResponse(
            merr.WrapErrAsInputErrorWhen(err, merr.ErrCollectionNotFound, merr.ErrDatabaseNotFound)), nil
    }

    // 8. ç­‰å¾…ä»»åŠ¡å®Œæˆ
    if err := it.WaitToFinish(); err != nil {
        log.Warn("Failed to execute insert task", zap.Error(err))
        return constructFailedResponse(err), nil
    }

    // 9. å¤„ç†æ‰§è¡Œç»“æœ
    if it.result.GetStatus().GetErrorCode() != commonpb.ErrorCode_Success {
        // è®¾ç½®é”™è¯¯ç´¢å¼•
        numRows := request.NumRows
        errIndex := make([]uint32, numRows)
        for i := uint32(0); i < numRows; i++ {
            errIndex[i] = i
        }
        it.result.ErrIndex = errIndex
        log.Warn("Insert operation failed", zap.Uint32s("err_index", it.result.ErrIndex))
    }

    // 10. è®¾ç½®æ’å…¥è®¡æ•°
    it.result.InsertCnt = int64(request.NumRows)

    return it.result, nil
}
```

**æ’å…¥ä»»åŠ¡é¢„å¤„ç†ï¼š**
```go
// æ–‡ä»¶ï¼šinternal/proxy/task_insert.go
// åŠŸèƒ½ï¼šæ’å…¥ä»»åŠ¡çš„é¢„å¤„ç†ï¼ŒåŒ…æ‹¬SchemaéªŒè¯ã€åˆ†åŒºå¤„ç†ã€æ•°æ®éªŒè¯
func (it *insertTask) PreExecute(ctx context.Context) error {
    // 1. è®¾ç½®æ¶ˆæ¯åŸºç¡€ä¿¡æ¯
    it.insertMsg.Base.MsgType = commonpb.MsgType_Insert
    it.insertMsg.Base.SourceID = paramtable.GetNodeID()

    // 2. è·å–é›†åˆä¿¡æ¯å’ŒSchema
    collectionName := it.insertMsg.CollectionName
    collID, err := globalMetaCache.GetCollectionID(ctx, it.insertMsg.GetDbName(), collectionName)
    if err != nil {
        return fmt.Errorf("failed to get collection ID: %w", err)
    }
    it.insertMsg.CollectionID = collID

    schema, err := globalMetaCache.GetCollectionSchema(ctx, it.insertMsg.GetDbName(), collectionName)
    if err != nil {
        return fmt.Errorf("failed to get collection schema: %w", err)
    }
    it.schema = schema

    // 3. åˆ†é…è¡ŒIDï¼ˆå¦‚æœéœ€è¦ï¼‰
    if err := it.assignRowIDs(ctx); err != nil {
        return fmt.Errorf("failed to assign row IDs: %w", err)
    }

    // 4. å¤„ç†åˆ†åŒºé”®æ¨¡å¼
    partitionKeyMode, err := isPartitionKeyMode(ctx, it.insertMsg.GetDbName(), collectionName)
    if err != nil {
        return fmt.Errorf("failed to check partition key mode: %w", err)
    }

    if partitionKeyMode {
        // åˆ†åŒºé”®æ¨¡å¼ï¼šä»æ•°æ®ä¸­æå–åˆ†åŒºé”®
        fieldSchema, _ := typeutil.GetPartitionKeyFieldSchema(it.schema)
        it.partitionKeys, err = getPartitionKeyFieldData(fieldSchema, it.insertMsg)
        if err != nil {
            return fmt.Errorf("failed to get partition keys: %w", err)
        }
    } else {
        // éåˆ†åŒºé”®æ¨¡å¼ï¼šä½¿ç”¨æŒ‡å®šçš„åˆ†åŒºæˆ–é»˜è®¤åˆ†åŒº
        partitionTag := it.insertMsg.GetPartitionName()
        if len(partitionTag) <= 0 {
            // ä½¿ç”¨é»˜è®¤åˆ†åŒº
            pinfo, err := globalMetaCache.GetPartitionInfo(ctx, it.insertMsg.GetDbName(), collectionName, "")
            if err != nil {
                return fmt.Errorf("failed to get default partition info: %w", err)
            }
            partitionTag = pinfo.name
            it.insertMsg.PartitionName = partitionTag
        }

        // éªŒè¯åˆ†åŒºåç§°
        if err := validatePartitionTag(partitionTag, true); err != nil {
            return fmt.Errorf("invalid partition name %s: %w", partitionTag, err)
        }
    }

    // 5. æ•°æ®éªŒè¯
    validator := newValidateUtil(
        withNANCheck(),      // NaNå€¼æ£€æŸ¥
        withOverflowCheck(), // æº¢å‡ºæ£€æŸ¥
        withMaxLenCheck(),   // æœ€å¤§é•¿åº¦æ£€æŸ¥
        withMaxCapCheck(),   // æœ€å¤§å®¹é‡æ£€æŸ¥
    )
    if err := validator.Validate(it.insertMsg.GetFieldsData(), schema.schemaHelper, it.insertMsg.NRows()); err != nil {
        return merr.WrapErrAsInputError(fmt.Errorf("data validation failed: %w", err))
    }

    return nil
}
```

**Insert è°ƒç”¨æ—¶åºå›¾ï¼š**

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant Proxy as Proxy
    participant TaskQueue as DMLé˜Ÿåˆ—
    participant Task as InsertTask
    participant MetaCache as å…ƒæ•°æ®ç¼“å­˜
    participant IDAllocator as IDåˆ†é…å™¨
    participant ChannelMgr as é€šé“ç®¡ç†å™¨
    participant MsgStream as æ¶ˆæ¯æµ
    participant DataNode as DataNode
    
    Client->>Proxy: Insert Request
    Note over Proxy: 1. å¥åº·æ£€æŸ¥
    Note over Proxy: 2. åˆ›å»ºæ’å…¥ä»»åŠ¡
    Proxy->>TaskQueue: Enqueue InsertTask
    TaskQueue->>Task: Execute PreExecute
    Task->>MetaCache: GetCollectionID & Schema
    MetaCache-->>Task: è¿”å›é›†åˆä¿¡æ¯
    Task->>IDAllocator: åˆ†é…è¡ŒID
    IDAllocator-->>Task: è¿”å›IDèŒƒå›´
    Note over Task: 3. æ•°æ®éªŒè¯
    Note over Task: 4. åˆ†åŒºé”®å¤„ç†
    Task->>Task: Execute
    Task->>ChannelMgr: è·å–è™šæ‹Ÿé€šé“
    ChannelMgr-->>Task: è¿”å›é€šé“åˆ—è¡¨
    Task->>MsgStream: å‘é€æ’å…¥æ¶ˆæ¯
    MsgStream->>DataNode: æ¶ˆè´¹æ’å…¥æ¶ˆæ¯
    DataNode->>DataNode: å†™å…¥æ•°æ®åˆ°Segment
    DataNode-->>Task: è¿”å›æ’å…¥ç»“æœ
    Task-->>Proxy: ä»»åŠ¡å®Œæˆ
    Proxy-->>Client: Insert Response
```

#### 2.2.3 æŸ¥è¯¢æœç´¢ API

**Search - å‘é‡æœç´¢**

**API å…¥å£å‡½æ•°ï¼š**
```go
// æ–‡ä»¶ï¼šinternal/proxy/impl.go
// åŠŸèƒ½ï¼šæ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼Œæ”¯æŒæ··åˆæœç´¢ã€è¿‡æ»¤æ¡ä»¶ã€å¤šå‘é‡æŸ¥è¯¢
// å‚æ•°ï¼š
//   - ctx: ä¸Šä¸‹æ–‡ï¼Œç”¨äºè¶…æ—¶æ§åˆ¶å’Œé“¾è·¯è¿½è¸ª
//   - request: æœç´¢è¯·æ±‚ï¼ŒåŒ…å«æŸ¥è¯¢å‘é‡ã€æœç´¢å‚æ•°ã€è¿‡æ»¤æ¡ä»¶ç­‰
// è¿”å›å€¼ï¼š
//   - *milvuspb.SearchResults: æœç´¢ç»“æœï¼ŒåŒ…å«ç›¸ä¼¼å‘é‡å’Œç›¸ä¼¼åº¦åˆ†æ•°
//   - error: Go æ ‡å‡†é”™è¯¯
func (node *Proxy) Search(ctx context.Context, request *milvuspb.SearchRequest) (*milvuspb.SearchResults, error) {
    var err error
    rsp := &milvuspb.SearchResults{Status: merr.Success()}

    // 1. ä¼˜åŒ–æœç´¢ç­–ç•¥ - æ”¯æŒç»“æœå¤§å°ä¸è¶³æ—¶çš„é‡è¯•æœºåˆ¶
    optimizedSearch := true
    resultSizeInsufficient := false
    isTopkReduce := false
    isRecallEvaluation := false

    // 2. é‡è¯•æœºåˆ¶ - å¤„ç†ç»“æœä¸è¶³å’Œä¸€è‡´æ€§é‡æŸ¥è¯¢
    err2 := retry.Handle(ctx, func() (bool, error) {
        rsp, resultSizeInsufficient, isTopkReduce, isRecallEvaluation, err = 
            node.search(ctx, request, optimizedSearch, false)
        
        // å¦‚æœä¼˜åŒ–æœç´¢ç»“æœä¸è¶³ä¸”å¯ç”¨äº†ç»“æœé™åˆ¶æ£€æŸ¥ï¼Œåˆ™è¿›è¡Œéä¼˜åŒ–æœç´¢
        if merr.Ok(rsp.GetStatus()) && optimizedSearch && resultSizeInsufficient && 
           isTopkReduce && paramtable.Get().AutoIndexConfig.EnableResultLimitCheck.GetAsBool() {
            optimizedSearch = false
            rsp, resultSizeInsufficient, isTopkReduce, isRecallEvaluation, err = 
                node.search(ctx, request, optimizedSearch, false)
            
            // è®°å½•é‡è¯•æŒ‡æ ‡
            metrics.ProxyRetrySearchCount.WithLabelValues(
                strconv.FormatInt(paramtable.GetNodeID(), 10),
                metrics.SearchLabel,
                request.GetDbName(),
                request.GetCollectionName(),
            ).Inc()
        }
        
        // å¤„ç†ä¸€è‡´æ€§é‡æŸ¥è¯¢é”™è¯¯
        if errors.Is(merr.Error(rsp.GetStatus()), merr.ErrInconsistentRequery) {
            return true, merr.Error(rsp.GetStatus())
        }
        
        return false, err
    })

    if err2 != nil {
        err = err2
    }
    if err != nil {
        rsp.Status = merr.Status(err)
    }
    return rsp, nil
}
```

**æ ¸å¿ƒæœç´¢å‡½æ•°ï¼š**
```go
// æ–‡ä»¶ï¼šinternal/proxy/impl.go
// åŠŸèƒ½ï¼šæ‰§è¡Œå…·ä½“çš„æœç´¢é€»è¾‘
func (node *Proxy) search(ctx context.Context, request *milvuspb.SearchRequest, 
    optimizedSearch bool, isRecallEvaluation bool) (*milvuspb.SearchResults, bool, bool, bool, error) {
    
    // 1. è®¾ç½®ç›‘æ§æŒ‡æ ‡
    metrics.GetStats(ctx).
        SetNodeID(paramtable.GetNodeID()).
        SetInboundLabel(metrics.SearchLabel).
        SetDatabaseName(request.GetDbName()).
        SetCollectionName(request.GetCollectionName())

    // 2. è®°å½•æŸ¥è¯¢å‘é‡æ•°é‡
    metrics.ProxyReceivedNQ.WithLabelValues(
        strconv.FormatInt(paramtable.GetNodeID(), 10),
        metrics.SearchLabel,
        request.GetDbName(),
        request.GetCollectionName(),
    ).Add(float64(request.GetNq()))

    // 3. å¥åº·çŠ¶æ€æ£€æŸ¥
    if err := merr.CheckHealthy(node.GetStateCode()); err != nil {
        return &milvuspb.SearchResults{Status: merr.Status(err)}, false, false, false, nil
    }

    // 4. å¼€å¯é“¾è·¯è¿½è¸ª
    ctx, sp := otel.Tracer(typeutil.ProxyRole).Start(ctx, "Proxy-Search")
    defer sp.End()

    // 5. å¤„ç†ä¸»é”®æœç´¢ - æ ¹æ®ä¸»é”®è·å–å‘é‡è¿›è¡Œæœç´¢
    if request.SearchByPrimaryKeys {
        placeholderGroupBytes, err := node.getVectorPlaceholderGroupForSearchByPks(ctx, request)
        if err != nil {
            return &milvuspb.SearchResults{Status: merr.Status(err)}, false, false, false, nil
        }
        request.PlaceholderGroup = placeholderGroupBytes
    }

    // 6. åˆ›å»ºæœç´¢ä»»åŠ¡å¯¹è±¡
    qt := &searchTask{
        ctx:       ctx,
        Condition: NewTaskCondition(ctx),
        SearchRequest: &internalpb.SearchRequest{
            Base: commonpbutil.NewMsgBase(
                commonpbutil.WithMsgType(commonpb.MsgType_Search),
                commonpbutil.WithSourceID(paramtable.GetNodeID()),
            ),
            ReqID:              paramtable.GetNodeID(),
            IsTopkReduce:       optimizedSearch,      // æ˜¯å¦å¯ç”¨TopKä¼˜åŒ–
            IsRecallEvaluation: isRecallEvaluation,   // æ˜¯å¦è¿›è¡Œå¬å›è¯„ä¼°
        },
        request:                request,
        tr:                     timerecord.NewTimeRecorder("search"),
        mixCoord:               node.mixCoord,
        node:                   node,
        lb:                     node.lbPolicy,        // è´Ÿè½½å‡è¡¡ç­–ç•¥
        enableMaterializedView: node.enableMaterializedView,
        mustUsePartitionKey:    Params.ProxyCfg.MustUsePartitionKey.GetAsBool(),
    }

    // 7. è®°å½•è¯¦ç»†çš„æœç´¢æ—¥å¿—
    log := log.Ctx(ctx).With(
        zap.String("role", typeutil.ProxyRole),
        zap.String("db", request.DbName),
        zap.String("collection", request.CollectionName),
        zap.Strings("partitions", request.PartitionNames),
        zap.String("expr", request.Expr),
        zap.Uint64("guarantee_timestamp", request.GuaranteeTimestamp),
        zap.Uint64("travel_timestamp", request.TravelTimestamp),
        zap.Int64("nq", qt.Nq),
        zap.Int64("topk", qt.TopK),
        zap.Bool("use_default_consistency", request.UseDefaultConsistency),
    )

    // 8. æäº¤åˆ°æŸ¥è¯¢é˜Ÿåˆ—å¹¶ç­‰å¾…æ‰§è¡Œ
    // ... çœç•¥é˜Ÿåˆ—å¤„ç†é€»è¾‘
    
    return qt.result, qt.resultSizeInsufficient, qt.isTopkReduce, qt.isRecallEvaluation, nil
}
```

**æœç´¢ä»»åŠ¡é¢„å¤„ç†ï¼š**
```go
// æ–‡ä»¶ï¼šinternal/proxy/task_search.go
// åŠŸèƒ½ï¼šæœç´¢ä»»åŠ¡çš„é¢„å¤„ç†ï¼ŒåŒ…æ‹¬å‚æ•°éªŒè¯ã€åˆ†åŒºå¤„ç†ã€è¾“å‡ºå­—æ®µè§£æ
func (t *searchTask) PreExecute(ctx context.Context) error {
    // 1. å¼€å¯é“¾è·¯è¿½è¸ª
    ctx, sp := otel.Tracer(typeutil.ProxyRole).Start(ctx, "Proxy-Search-PreExecute")
    defer sp.End()

    // 2. è®¾ç½®è¯·æ±‚åŸºç¡€ä¿¡æ¯
    t.SearchRequest.IsAdvanced = len(t.request.GetSubReqs()) > 0
    t.Base.MsgType = commonpb.MsgType_Search
    t.Base.SourceID = paramtable.GetNodeID()

    // 3. è·å–é›†åˆä¿¡æ¯
    collectionName := t.request.CollectionName
    t.collectionName = collectionName
    collID, err := globalMetaCache.GetCollectionID(ctx, t.request.GetDbName(), collectionName)
    if err != nil {
        return merr.WrapErrAsInputErrorWhen(err, merr.ErrCollectionNotFound, merr.ErrDatabaseNotFound)
    }

    t.SearchRequest.DbID = 0
    t.SearchRequest.CollectionID = collID

    // 4. è·å–é›†åˆSchema
    t.schema, err = globalMetaCache.GetCollectionSchema(ctx, t.request.GetDbName(), collectionName)
    if err != nil {
        return fmt.Errorf("failed to get collection schema: %w", err)
    }

    // 5. æ£€æŸ¥åˆ†åŒºé”®æ¨¡å¼
    t.partitionKeyMode, err = isPartitionKeyMode(ctx, t.request.GetDbName(), collectionName)
    if err != nil {
        return fmt.Errorf("failed to check partition key mode: %w", err)
    }

    // åˆ†åŒºé”®æ¨¡å¼ä¸‹ä¸å…è®¸æ‰‹åŠ¨æŒ‡å®šåˆ†åŒº
    if t.partitionKeyMode && len(t.request.GetPartitionNames()) != 0 {
        return errors.New("cannot manually specify partition names in partition key mode")
    }

    // å¼ºåˆ¶ä½¿ç”¨åˆ†åŒºé”®çš„é…ç½®æ£€æŸ¥
    if t.mustUsePartitionKey && !t.partitionKeyMode {
        return merr.WrapErrAsInputError(merr.WrapErrParameterInvalidMsg(
            "must use partition key because mustUsePartitionKey config is enabled"))
    }

    // 6. å¤„ç†åˆ†åŒºåç§°åˆ°åˆ†åŒºIDçš„è½¬æ¢
    if !t.partitionKeyMode && len(t.request.GetPartitionNames()) > 0 {
        t.SearchRequest.PartitionIDs, err = getPartitionIDs(ctx, 
            t.request.GetDbName(), collectionName, t.request.GetPartitionNames())
        if err != nil {
            return fmt.Errorf("failed to get partition IDs: %w", err)
        }
    }

    // 7. è§£æè¾“å‡ºå­—æ®µ
    t.translatedOutputFields, t.userOutputFields, err = translateOutputFields(
        t.request.OutputFields, t.schema, false)
    if err != nil {
        return fmt.Errorf("failed to translate output fields: %w", err)
    }

    // 8. éªŒè¯é«˜çº§æœç´¢è¯·æ±‚æ•°é‡
    if t.SearchRequest.GetIsAdvanced() {
        if len(t.request.GetSubReqs()) > defaultMaxSearchRequest {
            return fmt.Errorf("maximum number of search requests is %d", defaultMaxSearchRequest)
        }
    }

    // 9. æ£€æŸ¥å’Œè®¾ç½®æŸ¥è¯¢å‘é‡æ•°é‡(nq)
    nq, err := t.checkNq(ctx)
    if err != nil {
        return fmt.Errorf("invalid nq parameter: %w", err)
    }
    t.SearchRequest.Nq = nq

    // 10. è®¾ç½®æ˜¯å¦å¿½ç•¥Growing Segment
    if t.SearchRequest.IgnoreGrowing, err = isIgnoreGrowing(t.request.SearchParams); err != nil {
        return fmt.Errorf("failed to parse ignore_growing parameter: %w", err)
    }

    // 11. è·å–è¾“å‡ºå­—æ®µID
    outputFieldIDs, err := getOutputFieldIDs(t.schema, t.translatedOutputFields)
    if err != nil {
        return fmt.Errorf("failed to get output field IDs: %w", err)
    }
    t.SearchRequest.OutputFieldsId = outputFieldIDs

    // 12. åˆå§‹åŒ–æœç´¢è¯·æ±‚å‚æ•°
    if t.SearchRequest.GetIsAdvanced() {
        err = t.initAdvancedSearchRequest(ctx)
    } else {
        err = t.initSearchRequest(ctx)
    }
    if err != nil {
        return fmt.Errorf("failed to initialize search request: %w", err)
    }

    return nil
}
```

**æœç´¢ä»»åŠ¡æ‰§è¡Œï¼š**
```go
// æ–‡ä»¶ï¼šinternal/proxy/task_search.go
// åŠŸèƒ½ï¼šæ‰§è¡Œæœç´¢ä»»åŠ¡ï¼Œé€šè¿‡è´Ÿè½½å‡è¡¡å™¨åˆ†å‘åˆ°å„ä¸ªQueryNode
func (t *searchTask) Execute(ctx context.Context) error {
    // 1. å¼€å¯é“¾è·¯è¿½è¸ª
    ctx, sp := otel.Tracer(typeutil.ProxyRole).Start(ctx, "Proxy-Search-Execute")
    defer sp.End()

    // 2. åˆ›å»ºæ€§èƒ½è®°å½•å™¨
    tr := timerecord.NewTimeRecorder(fmt.Sprintf("proxy execute search %d", t.ID()))
    defer tr.CtxElapse(ctx, "search execute done")

    // 3. é€šè¿‡è´Ÿè½½å‡è¡¡å™¨æ‰§è¡Œæœç´¢
    err := t.lb.Execute(ctx, CollectionWorkLoad{
        db:             t.request.GetDbName(),
        collectionID:   t.SearchRequest.CollectionID,
        collectionName: t.collectionName,
        nq:             t.Nq,
        exec:           t.searchShard,  // åˆ†ç‰‡æœç´¢å‡½æ•°
    })
    
    if err != nil {
        log.Ctx(ctx).Warn("search execute failed", zap.Error(err))
        return fmt.Errorf("failed to execute search: %w", err)
    }

    return nil
}
```

**Search æ‰§è¡Œæ—¶åºå›¾ï¼š**

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant Proxy as Proxy
    participant TaskQueue as æŸ¥è¯¢é˜Ÿåˆ—
    participant Task as SearchTask
    participant MetaCache as å…ƒæ•°æ®ç¼“å­˜
    participant LB as è´Ÿè½½å‡è¡¡å™¨
    participant QueryNode1 as QueryNode-1
    participant QueryNode2 as QueryNode-2
    participant Segment as Segment
    
    Client->>Proxy: Search Request
    Note over Proxy: 1. å¥åº·æ£€æŸ¥
    Note over Proxy: 2. é‡è¯•æœºåˆ¶å¤„ç†
    Proxy->>TaskQueue: Enqueue SearchTask
    TaskQueue->>Task: Execute PreExecute
    Task->>MetaCache: GetCollectionID & Schema
    MetaCache-->>Task: è¿”å›é›†åˆä¿¡æ¯
    Note over Task: 3. å‚æ•°éªŒè¯
    Note over Task: 4. åˆ†åŒºå¤„ç†
    Note over Task: 5. è¾“å‡ºå­—æ®µè§£æ
    Task->>Task: Execute
    Task->>LB: æ‰§è¡Œè´Ÿè½½å‡è¡¡æœç´¢
    LB->>QueryNode1: æœç´¢åˆ†ç‰‡1
    LB->>QueryNode2: æœç´¢åˆ†ç‰‡2
    
    par å¹¶è¡Œæœç´¢
        QueryNode1->>Segment: æœç´¢Sealed Segment
        Segment-->>QueryNode1: è¿”å›æœç´¢ç»“æœ
        QueryNode1->>Segment: æœç´¢Growing Segment
        Segment-->>QueryNode1: è¿”å›æœç´¢ç»“æœ
    and
        QueryNode2->>Segment: æœç´¢Sealed Segment
        Segment-->>QueryNode2: è¿”å›æœç´¢ç»“æœ
        QueryNode2->>Segment: æœç´¢Growing Segment
        Segment-->>QueryNode2: è¿”å›æœç´¢ç»“æœ
    end
    
    QueryNode1-->>LB: è¿”å›åˆ†ç‰‡1ç»“æœ
    QueryNode2-->>LB: è¿”å›åˆ†ç‰‡2ç»“æœ
    LB->>LB: åˆå¹¶å’Œæ’åºç»“æœ
    LB-->>Task: è¿”å›æœ€ç»ˆç»“æœ
    Task-->>Proxy: ä»»åŠ¡å®Œæˆ
    Proxy-->>Client: Search Response
```

### 2.3 API æ‹¦æˆªå™¨æœºåˆ¶

Milvus ä½¿ç”¨æ‹¦æˆªå™¨æ¨¡å¼å®ç°æ¨ªåˆ‡å…³æ³¨ç‚¹ï¼š

```go
// æ•°æ®åº“æ‹¦æˆªå™¨ï¼šinternal/proxy/database_interceptor.go
func DatabaseInterceptor() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req any, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface{}, err error) {
        filledCtx, filledReq := fillDatabase(ctx, req)
        return handler(filledCtx, filledReq)
    }
}

// è®¤è¯æ‹¦æˆªå™¨ï¼šinternal/proxy/authentication_interceptor.go
func AuthenticationInterceptor() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
        // éªŒè¯ç”¨æˆ·èº«ä»½
        if err := validateAuth(ctx, req); err != nil {
            return nil, err
        }
        return handler(ctx, req)
    }
}

// é€Ÿç‡é™åˆ¶æ‹¦æˆªå™¨ï¼šinternal/proxy/rate_limit_interceptor.go
func RateLimitInterceptor() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
        // æ£€æŸ¥é€Ÿç‡é™åˆ¶
        if err := checkRateLimit(ctx, req); err != nil {
            return nil, err
        }
        return handler(ctx, req)
    }
}
```

---

## 3. æ•´ä½“æ¶æ„è®¾è®¡

### 3.1 ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

Milvus é‡‡ç”¨äº‘åŸç”Ÿçš„å¾®æœåŠ¡æ¶æ„ï¼Œå®ç°äº†å­˜å‚¨ä¸è®¡ç®—åˆ†ç¦»ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•å’Œé«˜å¯ç”¨éƒ¨ç½²ã€‚æ•´ä¸ªç³»ç»Ÿåˆ†ä¸ºäº”ä¸ªå±‚æ¬¡ï¼šå®¢æˆ·ç«¯å±‚ã€æ¥å…¥å±‚ã€åè°ƒå±‚ã€æ‰§è¡Œå±‚å’Œå­˜å‚¨å±‚ã€‚

#### 3.1.1 è¯¦ç»†ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    subgraph "å®¢æˆ·ç«¯å±‚ - Client Layer"
        SDK[Python/Go/Java/Node.js SDK]
        REST[RESTful API]
        GRPC[gRPC Client]
        WebUI[Web UI / Attu]
    end
    
    subgraph "æ¥å…¥å±‚ - Access Layer"
        LB[è´Ÿè½½å‡è¡¡å™¨<br/>Load Balancer]
        subgraph "Proxy Cluster"
            Proxy1[Proxy-1<br/>API Gateway]
            Proxy2[Proxy-2<br/>API Gateway]
            ProxyN[Proxy-N<br/>API Gateway]
        end
    end
    
    subgraph "åè°ƒå±‚ - Coordinator Layer"
        RootCoord[RootCoord<br/>å…ƒæ•°æ®ç®¡ç†<br/>DDLæ“ä½œåè°ƒ]
        DataCoord[DataCoord<br/>æ•°æ®åè°ƒ<br/>Segmentç®¡ç†]
        QueryCoord[QueryCoord<br/>æŸ¥è¯¢åè°ƒ<br/>è´Ÿè½½å‡è¡¡]
        IndexCoord[IndexCoord<br/>ç´¢å¼•åè°ƒ<br/>ç´¢å¼•æ„å»ºç®¡ç†]
    end
    
    subgraph "æ‰§è¡Œå±‚ - Worker Layer"
        subgraph "Data Plane"
            DataNode1[DataNode-1<br/>æ•°æ®å†™å…¥<br/>Flush & Compaction]
            DataNode2[DataNode-2<br/>æ•°æ®å†™å…¥<br/>Flush & Compaction]
            DataNodeN[DataNode-N<br/>æ•°æ®å†™å…¥<br/>Flush & Compaction]
        end
        
        subgraph "Query Plane"
            QueryNode1[QueryNode-1<br/>å‘é‡æœç´¢<br/>æ ‡é‡æŸ¥è¯¢]
            QueryNode2[QueryNode-2<br/>å‘é‡æœç´¢<br/>æ ‡é‡æŸ¥è¯¢]
            QueryNodeN[QueryNode-N<br/>å‘é‡æœç´¢<br/>æ ‡é‡æŸ¥è¯¢]
        end
        
        subgraph "Index Plane"
            IndexNode1[IndexNode-1<br/>ç´¢å¼•æ„å»º<br/>ç´¢å¼•ä¼˜åŒ–]
            IndexNode2[IndexNode-2<br/>ç´¢å¼•æ„å»º<br/>ç´¢å¼•ä¼˜åŒ–]
        end
    end
    
    subgraph "å­˜å‚¨å±‚ - Storage Layer"
        subgraph "å…ƒæ•°æ®å­˜å‚¨"
            MetaStore[(etcd Cluster<br/>é›†åˆå…ƒæ•°æ®<br/>åˆ†åŒºä¿¡æ¯<br/>ç´¢å¼•å…ƒæ•°æ®)]
        end
        
        subgraph "æ¶ˆæ¯é˜Ÿåˆ—"
            MsgQueue[Pulsar/Kafka Cluster<br/>DMLæ¶ˆæ¯æµ<br/>DDLæ¶ˆæ¯æµ<br/>æ—¶é—´åŒæ­¥]
        end
        
        subgraph "å¯¹è±¡å­˜å‚¨"
            ObjectStore[(MinIO/S3<br/>Segmentæ–‡ä»¶<br/>ç´¢å¼•æ–‡ä»¶<br/>Binlogæ–‡ä»¶)]
        end
        
        subgraph "æœ¬åœ°å­˜å‚¨"
            LocalCache[æœ¬åœ°ç¼“å­˜<br/>å†…å­˜ä¸­çš„Segment<br/>ç´¢å¼•ç¼“å­˜]
        end
    end
    
    %% å®¢æˆ·ç«¯åˆ°æ¥å…¥å±‚è¿æ¥
    SDK --> LB
    REST --> LB
    GRPC --> LB
    WebUI --> LB
    
    %% è´Ÿè½½å‡è¡¡åˆ°Proxy
    LB --> Proxy1
    LB --> Proxy2
    LB --> ProxyN
    
    %% Proxyåˆ°åè°ƒå±‚
    Proxy1 --> RootCoord
    Proxy1 --> DataCoord
    Proxy1 --> QueryCoord
    Proxy1 --> IndexCoord
    
    %% åè°ƒå±‚åˆ°æ‰§è¡Œå±‚
    DataCoord --> DataNode1
    DataCoord --> DataNode2
    DataCoord --> DataNodeN
    
    QueryCoord --> QueryNode1
    QueryCoord --> QueryNode2
    QueryCoord --> QueryNodeN
    
    IndexCoord --> IndexNode1
    IndexCoord --> IndexNode2
    
    %% åè°ƒå±‚åˆ°å­˜å‚¨å±‚
    RootCoord --> MetaStore
    DataCoord --> MetaStore
    QueryCoord --> MetaStore
    IndexCoord --> MetaStore
    
    %% æ‰§è¡Œå±‚åˆ°å­˜å‚¨å±‚
    DataNode1 --> MsgQueue
    DataNode1 --> ObjectStore
    DataNode1 --> LocalCache
    
    QueryNode1 --> ObjectStore
    QueryNode1 --> LocalCache
    
    IndexNode1 --> ObjectStore
    
    %% å†…éƒ¨é€šä¿¡
    DataCoord -.-> QueryCoord
    QueryCoord -.-> DataCoord
    IndexCoord -.-> DataCoord
```

**æ¶æ„å±‚æ¬¡è¯¦ç»†è¯´æ˜ï¼š**

**1. å®¢æˆ·ç«¯å±‚ (Client Layer)**
- **å¤šè¯­è¨€SDKæ”¯æŒ**ï¼šæä¾›Pythonã€Goã€Javaã€Node.jsç­‰å¤šç§è¯­è¨€çš„SDK
- **åè®®æ”¯æŒ**ï¼šæ”¯æŒgRPCå’ŒRESTfulä¸¤ç§é€šä¿¡åè®®
- **ç®¡ç†ç•Œé¢**ï¼šAttuç­‰Web UIå·¥å…·ç”¨äºå¯è§†åŒ–ç®¡ç†

**2. æ¥å…¥å±‚ (Access Layer)**
- **è´Ÿè½½å‡è¡¡å™¨**ï¼šåˆ†å‘å®¢æˆ·ç«¯è¯·æ±‚åˆ°å¤šä¸ªProxyå®ä¾‹
- **Proxyé›†ç¾¤**ï¼šæ— çŠ¶æ€çš„APIç½‘å…³ï¼Œæä¾›ç»Ÿä¸€çš„æœåŠ¡å…¥å£
- **åŠŸèƒ½ç‰¹æ€§**ï¼š
  - è¯·æ±‚è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
  - è®¤è¯å’Œæƒé™æ§åˆ¶
  - é€Ÿç‡é™åˆ¶å’Œæµé‡æ§åˆ¶
  - åè®®è½¬æ¢å’Œå‚æ•°éªŒè¯

**3. åè°ƒå±‚ (Coordinator Layer)**
- **RootCoord**ï¼šå…ƒæ•°æ®ç®¡ç†ä¸­å¿ƒ
  - é›†åˆå’Œåˆ†åŒºçš„ç”Ÿå‘½å‘¨æœŸç®¡ç†
  - Schemaå®šä¹‰å’Œç‰ˆæœ¬æ§åˆ¶
  - å…¨å±€IDå’Œæ—¶é—´æˆ³åˆ†é…
  - DDLæ“ä½œçš„åè°ƒå’Œæ‰§è¡Œ

- **DataCoord**ï¼šæ•°æ®åè°ƒä¸­å¿ƒ
  - Segmentçš„åˆ†é…å’Œç®¡ç†
  - æ•°æ®åˆ·ç›˜å’Œå‹ç¼©è°ƒåº¦
  - Channelå’ŒDataNodeçš„æ˜ å°„
  - æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†

- **QueryCoord**ï¼šæŸ¥è¯¢åè°ƒä¸­å¿ƒ
  - QueryNodeé›†ç¾¤ç®¡ç†
  - è´Ÿè½½å‡è¡¡å’Œåˆ†ç‰‡åˆ†é…
  - æŸ¥è¯¢è®¡åˆ’ç”Ÿæˆå’Œä¼˜åŒ–
  - å‰¯æœ¬ç®¡ç†å’Œæ•…éšœè½¬ç§»

- **IndexCoord**ï¼šç´¢å¼•åè°ƒä¸­å¿ƒ
  - ç´¢å¼•æ„å»ºä»»åŠ¡è°ƒåº¦
  - IndexNodeé›†ç¾¤ç®¡ç†
  - ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç®¡ç†
  - ç´¢å¼•å…ƒæ•°æ®ç®¡ç†

**4. æ‰§è¡Œå±‚ (Worker Layer)**
- **DataNode**ï¼šæ•°æ®å†™å…¥å’Œå¤„ç†
  - å®æ—¶æ•°æ®å†™å…¥å’Œç¼“å­˜
  - æ•°æ®åˆ·ç›˜åˆ°å¯¹è±¡å­˜å‚¨
  - æ•°æ®å‹ç¼©å’Œåˆå¹¶
  - å¢é‡æ•°æ®å¤„ç†

- **QueryNode**ï¼šæŸ¥è¯¢æ‰§è¡Œå¼•æ“
  - å‘é‡ç›¸ä¼¼åº¦æœç´¢
  - æ ‡é‡æ•°æ®è¿‡æ»¤å’ŒæŸ¥è¯¢
  - å†…å­˜ä¸­çš„æ•°æ®ç®¡ç†
  - æŸ¥è¯¢ç»“æœèšåˆ

- **IndexNode**ï¼šç´¢å¼•æ„å»ºæœåŠ¡
  - å‘é‡ç´¢å¼•æ„å»º
  - ç´¢å¼•ä¼˜åŒ–å’Œæ›´æ–°
  - å¤šç§ç´¢å¼•ç®—æ³•æ”¯æŒ
  - åˆ†å¸ƒå¼ç´¢å¼•æ„å»º

**5. å­˜å‚¨å±‚ (Storage Layer)**
- **å…ƒæ•°æ®å­˜å‚¨(etcd)**ï¼š
  - é›†åˆå’Œåˆ†åŒºå…ƒæ•°æ®
  - ç´¢å¼•å®šä¹‰å’ŒçŠ¶æ€
  - ç³»ç»Ÿé…ç½®ä¿¡æ¯
  - åˆ†å¸ƒå¼é”å’Œåè°ƒ

- **æ¶ˆæ¯é˜Ÿåˆ—(Pulsar/Kafka)**ï¼š
  - DMLæ“ä½œçš„æ¶ˆæ¯æµ
  - æ—¶é—´åŒæ­¥å’Œä¸€è‡´æ€§ä¿è¯
  - æ•°æ®å˜æ›´æ—¥å¿—
  - ç»„ä»¶é—´å¼‚æ­¥é€šä¿¡

- **å¯¹è±¡å­˜å‚¨(MinIO/S3)**ï¼š
  - Segmentæ•°æ®æ–‡ä»¶
  - ç´¢å¼•æ–‡ä»¶å­˜å‚¨
  - Binlogå’ŒDeltalog
  - æ•°æ®å¤‡ä»½å’Œå½’æ¡£

- **æœ¬åœ°ç¼“å­˜**ï¼š
  - å†…å­˜ä¸­çš„çƒ­æ•°æ®
  - ç´¢å¼•ç¼“å­˜
  - æŸ¥è¯¢ç»“æœç¼“å­˜
  - å…ƒæ•°æ®ç¼“å­˜

### 3.2 æ ¸å¿ƒç»„ä»¶äº¤äº’æ—¶åºå›¾

#### 3.2.1 å®Œæ•´çš„æ•°æ®æ’å…¥æµç¨‹

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant LB as è´Ÿè½½å‡è¡¡å™¨
    participant Proxy as Proxy
    participant RootCoord as RootCoord
    participant DataCoord as DataCoord
    participant DataNode as DataNode
    participant MsgStream as æ¶ˆæ¯æµ
    participant ObjectStore as å¯¹è±¡å­˜å‚¨
    participant MetaStore as å…ƒæ•°æ®å­˜å‚¨
    
    Note over Client,MetaStore: æ•°æ®æ’å…¥å®Œæ•´æµç¨‹
    
    %% 1. å®¢æˆ·ç«¯è¯·æ±‚é˜¶æ®µ
    Client->>LB: Insert Request
    LB->>Proxy: è·¯ç”±åˆ°å¯ç”¨Proxy
    
    %% 2. è¯·æ±‚éªŒè¯é˜¶æ®µ
    Proxy->>Proxy: å¥åº·æ£€æŸ¥ & æƒé™éªŒè¯
    Proxy->>RootCoord: è·å–é›†åˆå…ƒæ•°æ®
    RootCoord->>MetaStore: æŸ¥è¯¢é›†åˆä¿¡æ¯
    MetaStore-->>RootCoord: è¿”å›é›†åˆSchema
    RootCoord-->>Proxy: é›†åˆå…ƒæ•°æ® & Schema
    
    %% 3. æ•°æ®é¢„å¤„ç†é˜¶æ®µ
    Proxy->>Proxy: æ•°æ®éªŒè¯ & IDåˆ†é…
    Proxy->>DataCoord: è¯·æ±‚Segmentåˆ†é…
    DataCoord->>MetaStore: æŸ¥è¯¢Segmentä¿¡æ¯
    DataCoord->>DataCoord: é€‰æ‹©æˆ–åˆ›å»ºSegment
    DataCoord-->>Proxy: è¿”å›Segment ID & Channel
    
    %% 4. æ•°æ®å†™å…¥é˜¶æ®µ
    Proxy->>MsgStream: å‘é€Insertæ¶ˆæ¯åˆ°DML Channel
    MsgStream->>DataNode: æ¶ˆè´¹Insertæ¶ˆæ¯
    DataNode->>DataNode: å†™å…¥æ•°æ®åˆ°å†…å­˜Buffer
    
    %% 5. æ•°æ®æŒä¹…åŒ–é˜¶æ®µ
    DataNode->>DataNode: æ£€æŸ¥Flushæ¡ä»¶
    alt éœ€è¦Flush
        DataNode->>ObjectStore: å†™å…¥Segmentæ–‡ä»¶
        DataNode->>ObjectStore: å†™å…¥Binlogæ–‡ä»¶
        DataNode->>DataCoord: æŠ¥å‘ŠSegmentçŠ¶æ€
        DataCoord->>MetaStore: æ›´æ–°Segmentå…ƒæ•°æ®
    end
    
    %% 6. å“åº”è¿”å›é˜¶æ®µ
    DataNode-->>Proxy: å†™å…¥ç¡®è®¤ (é€šè¿‡æ¶ˆæ¯æµ)
    Proxy-->>LB: Insert Response
    LB-->>Client: è¿”å›æ’å…¥ç»“æœ
```

#### 3.2.2 å®Œæ•´çš„å‘é‡æœç´¢æµç¨‹

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant LB as è´Ÿè½½å‡è¡¡å™¨
    participant Proxy as Proxy
    participant RootCoord as RootCoord
    participant QueryCoord as QueryCoord
    participant QueryNode1 as QueryNode-1
    participant QueryNode2 as QueryNode-2
    participant ObjectStore as å¯¹è±¡å­˜å‚¨
    participant LocalCache as æœ¬åœ°ç¼“å­˜
    participant MetaStore as å…ƒæ•°æ®å­˜å‚¨
    
    Note over Client,MetaStore: å‘é‡æœç´¢å®Œæ•´æµç¨‹
    
    %% 1. å®¢æˆ·ç«¯è¯·æ±‚é˜¶æ®µ
    Client->>LB: Search Request
    LB->>Proxy: è·¯ç”±åˆ°å¯ç”¨Proxy
    
    %% 2. è¯·æ±‚éªŒè¯é˜¶æ®µ
    Proxy->>Proxy: å¥åº·æ£€æŸ¥ & å‚æ•°éªŒè¯
    Proxy->>RootCoord: è·å–é›†åˆä¿¡æ¯
    RootCoord->>MetaStore: æŸ¥è¯¢é›†åˆå…ƒæ•°æ®
    MetaStore-->>RootCoord: è¿”å›é›†åˆä¿¡æ¯
    RootCoord-->>Proxy: é›†åˆå…ƒæ•°æ® & åˆ†åŒºä¿¡æ¯
    
    %% 3. æŸ¥è¯¢è§„åˆ’é˜¶æ®µ
    Proxy->>QueryCoord: è·å–æŸ¥è¯¢è®¡åˆ’
    QueryCoord->>MetaStore: æŸ¥è¯¢Segmentåˆ†å¸ƒ
    QueryCoord->>QueryCoord: ç”ŸæˆæŸ¥è¯¢è®¡åˆ’ & è´Ÿè½½å‡è¡¡
    QueryCoord-->>Proxy: è¿”å›æŸ¥è¯¢åˆ†ç‰‡ä¿¡æ¯
    
    %% 4. å¹¶è¡Œæœç´¢é˜¶æ®µ
    par åˆ†ç‰‡1æœç´¢
        Proxy->>QueryNode1: æ‰§è¡Œæœç´¢è¯·æ±‚
        QueryNode1->>LocalCache: æ£€æŸ¥ç¼“å­˜
        alt ç¼“å­˜æœªå‘½ä¸­
            QueryNode1->>ObjectStore: åŠ è½½Segmentæ•°æ®
            QueryNode1->>ObjectStore: åŠ è½½ç´¢å¼•æ–‡ä»¶
            QueryNode1->>LocalCache: ç¼“å­˜åŠ è½½çš„æ•°æ®
        end
        QueryNode1->>QueryNode1: æ‰§è¡Œå‘é‡æœç´¢
        QueryNode1->>QueryNode1: åº”ç”¨æ ‡é‡è¿‡æ»¤
        QueryNode1-->>Proxy: è¿”å›æœç´¢ç»“æœ1
    and åˆ†ç‰‡2æœç´¢
        Proxy->>QueryNode2: æ‰§è¡Œæœç´¢è¯·æ±‚
        QueryNode2->>LocalCache: æ£€æŸ¥ç¼“å­˜
        alt ç¼“å­˜æœªå‘½ä¸­
            QueryNode2->>ObjectStore: åŠ è½½Segmentæ•°æ®
            QueryNode2->>ObjectStore: åŠ è½½ç´¢å¼•æ–‡ä»¶
            QueryNode2->>LocalCache: ç¼“å­˜åŠ è½½çš„æ•°æ®
        end
        QueryNode2->>QueryNode2: æ‰§è¡Œå‘é‡æœç´¢
        QueryNode2->>QueryNode2: åº”ç”¨æ ‡é‡è¿‡æ»¤
        QueryNode2-->>Proxy: è¿”å›æœç´¢ç»“æœ2
    end
    
    %% 5. ç»“æœèšåˆé˜¶æ®µ
    Proxy->>Proxy: åˆå¹¶å¤šä¸ªåˆ†ç‰‡ç»“æœ
    Proxy->>Proxy: å…¨å±€æ’åº & TopKé€‰æ‹©
    Proxy->>Proxy: ç»“æœåå¤„ç† & å­—æ®µå¡«å……
    
    %% 6. å“åº”è¿”å›é˜¶æ®µ
    Proxy-->>LB: Search Response
    LB-->>Client: è¿”å›æœç´¢ç»“æœ
```

#### 3.2.3 é›†åˆåˆ›å»ºæµç¨‹

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant Proxy as Proxy
    participant RootCoord as RootCoord
    participant DataCoord as DataCoord
    participant QueryCoord as QueryCoord
    participant IndexCoord as IndexCoord
    participant MetaStore as å…ƒæ•°æ®å­˜å‚¨
    participant MsgStream as æ¶ˆæ¯æµ
    
    Note over Client,MsgStream: é›†åˆåˆ›å»ºå®Œæ•´æµç¨‹
    
    %% 1. å®¢æˆ·ç«¯è¯·æ±‚
    Client->>Proxy: CreateCollection Request
    Proxy->>Proxy: SchemaéªŒè¯ & å‚æ•°æ£€æŸ¥
    
    %% 2. RootCoordå¤„ç†
    Proxy->>RootCoord: åˆ›å»ºé›†åˆè¯·æ±‚
    RootCoord->>RootCoord: åˆ†é…é›†åˆID & åˆ†åŒºID
    RootCoord->>RootCoord: åˆ†é…è™šæ‹Ÿé€šé“
    RootCoord->>MetaStore: ä¿å­˜é›†åˆå…ƒæ•°æ®
    
    %% 3. é€šçŸ¥å…¶ä»–åè°ƒå™¨
    RootCoord->>DataCoord: é€šçŸ¥åˆ›å»ºé›†åˆ
    DataCoord->>DataCoord: åˆå§‹åŒ–Channelæ˜ å°„
    DataCoord->>MetaStore: ä¿å­˜æ•°æ®åè°ƒä¿¡æ¯
    
    RootCoord->>QueryCoord: é€šçŸ¥åˆ›å»ºé›†åˆ
    QueryCoord->>QueryCoord: åˆå§‹åŒ–æŸ¥è¯¢å…ƒæ•°æ®
    QueryCoord->>MetaStore: ä¿å­˜æŸ¥è¯¢åè°ƒä¿¡æ¯
    
    RootCoord->>IndexCoord: é€šçŸ¥åˆ›å»ºé›†åˆ
    IndexCoord->>IndexCoord: åˆå§‹åŒ–ç´¢å¼•å…ƒæ•°æ®
    IndexCoord->>MetaStore: ä¿å­˜ç´¢å¼•åè°ƒä¿¡æ¯
    
    %% 4. åˆ›å»ºæ¶ˆæ¯é€šé“
    RootCoord->>MsgStream: åˆ›å»ºDMLé€šé“
    RootCoord->>MsgStream: åˆ›å»ºDDLé€šé“
    
    %% 5. è¿”å›ç»“æœ
    RootCoord-->>Proxy: åˆ›å»ºæˆåŠŸå“åº”
    Proxy-->>Client: CreateCollection Response
```

### 3.3 æ•°æ®æµæ¶æ„

#### 3.3.1 å®Œæ•´æ•°æ®æµå›¾

```mermaid
graph TB
    subgraph "æ•°æ®å†™å…¥æµ - Write Path"
        Client1[å®¢æˆ·ç«¯åº”ç”¨] --> Proxy1[Proxyé›†ç¾¤]
        Proxy1 --> |1. éªŒè¯è¯·æ±‚| MetaCache1[å…ƒæ•°æ®ç¼“å­˜]
        Proxy1 --> |2. åˆ†é…ID| IDAllocator[IDåˆ†é…å™¨]
        Proxy1 --> |3. å‘é€æ¶ˆæ¯| MsgStream1[DMLæ¶ˆæ¯æµ]
        
        MsgStream1 --> |4. æ¶ˆè´¹æ¶ˆæ¯| DataNode1[DataNode-1]
        MsgStream1 --> |4. æ¶ˆè´¹æ¶ˆæ¯| DataNode2[DataNode-2]
        
        DataNode1 --> |5. å†™å…¥ç¼“å­˜| WriteBuffer1[å†™ç¼“å†²åŒº]
        DataNode2 --> |5. å†™å…¥ç¼“å­˜| WriteBuffer2[å†™ç¼“å†²åŒº]
        
        WriteBuffer1 --> |6. åˆ·ç›˜è§¦å‘| FlushManager1[åˆ·ç›˜ç®¡ç†å™¨]
        WriteBuffer2 --> |6. åˆ·ç›˜è§¦å‘| FlushManager2[åˆ·ç›˜ç®¡ç†å™¨]
        
        FlushManager1 --> |7. æŒä¹…åŒ–| ObjectStore1[(å¯¹è±¡å­˜å‚¨)]
        FlushManager2 --> |7. æŒä¹…åŒ–| ObjectStore1
        
        FlushManager1 --> |8. æ›´æ–°å…ƒæ•°æ®| DataCoord1[DataCoord]
        FlushManager2 --> |8. æ›´æ–°å…ƒæ•°æ®| DataCoord1
        
        DataCoord1 --> |9. ä¿å­˜çŠ¶æ€| MetaStore1[(å…ƒæ•°æ®å­˜å‚¨)]
    end
    
    subgraph "æ•°æ®æŸ¥è¯¢æµ - Read Path"
        Client2[å®¢æˆ·ç«¯åº”ç”¨] --> Proxy2[Proxyé›†ç¾¤]
        Proxy2 --> |1. è·å–å…ƒæ•°æ®| MetaCache2[å…ƒæ•°æ®ç¼“å­˜]
        Proxy2 --> |2. æŸ¥è¯¢è§„åˆ’| QueryCoord1[QueryCoord]
        
        QueryCoord1 --> |3. è´Ÿè½½å‡è¡¡| QueryNode1[QueryNode-1]
        QueryCoord1 --> |3. è´Ÿè½½å‡è¡¡| QueryNode2[QueryNode-2]
        
        QueryNode1 --> |4. æ£€æŸ¥ç¼“å­˜| LocalCache1[æœ¬åœ°ç¼“å­˜]
        QueryNode2 --> |4. æ£€æŸ¥ç¼“å­˜| LocalCache2[æœ¬åœ°ç¼“å­˜]
        
        LocalCache1 --> |5. ç¼“å­˜æœªå‘½ä¸­| ObjectStore2[(å¯¹è±¡å­˜å‚¨)]
        LocalCache2 --> |5. ç¼“å­˜æœªå‘½ä¸­| ObjectStore2
        
        ObjectStore2 --> |6. åŠ è½½æ•°æ®| SegmentLoader1[SegmentåŠ è½½å™¨]
        ObjectStore2 --> |6. åŠ è½½æ•°æ®| SegmentLoader2[SegmentåŠ è½½å™¨]
        
        SegmentLoader1 --> |7. æ‰§è¡Œæœç´¢| SearchEngine1[æœç´¢å¼•æ“]
        SegmentLoader2 --> |7. æ‰§è¡Œæœç´¢| SearchEngine2[æœç´¢å¼•æ“]
        
        SearchEngine1 --> |8. è¿”å›ç»“æœ| Proxy2
        SearchEngine2 --> |8. è¿”å›ç»“æœ| Proxy2
        
        Proxy2 --> |9. ç»“æœèšåˆ| ResultMerger[ç»“æœåˆå¹¶å™¨]
        ResultMerger --> Client2
    end
    
    subgraph "å…ƒæ•°æ®æµ - Metadata Path"
        Client3[ç®¡ç†å®¢æˆ·ç«¯] --> Proxy3[Proxyé›†ç¾¤]
        Proxy3 --> |1. DDLè¯·æ±‚| RootCoord1[RootCoord]
        
        RootCoord1 --> |2. åˆ†é…èµ„æº| ResourceAllocator[èµ„æºåˆ†é…å™¨]
        RootCoord1 --> |3. ä¿å­˜å…ƒæ•°æ®| MetaStore2[(å…ƒæ•°æ®å­˜å‚¨)]
        
        RootCoord1 --> |4. é€šçŸ¥åè°ƒå™¨| DataCoord2[DataCoord]
        RootCoord1 --> |4. é€šçŸ¥åè°ƒå™¨| QueryCoord2[QueryCoord]
        RootCoord1 --> |4. é€šçŸ¥åè°ƒå™¨| IndexCoord1[IndexCoord]
        
        DataCoord2 --> |5. æ›´æ–°æ˜ å°„| ChannelManager[é€šé“ç®¡ç†å™¨]
        QueryCoord2 --> |5. æ›´æ–°åˆ†å¸ƒ| DistributionManager[åˆ†å¸ƒç®¡ç†å™¨]
        IndexCoord1 --> |5. è°ƒåº¦ä»»åŠ¡| IndexScheduler[ç´¢å¼•è°ƒåº¦å™¨]
        
        ChannelManager --> MsgStream2[æ¶ˆæ¯æµ]
        IndexScheduler --> IndexNode1[IndexNode]
        IndexNode1 --> ObjectStore3[(å¯¹è±¡å­˜å‚¨)]
    end
    
    subgraph "ç›‘æ§æµ - Monitoring Path"
        AllComponents[æ‰€æœ‰ç»„ä»¶] --> |æŒ‡æ ‡ä¸ŠæŠ¥| MetricsCollector[æŒ‡æ ‡æ”¶é›†å™¨]
        MetricsCollector --> |å­˜å‚¨æŒ‡æ ‡| MetricsStore[(æŒ‡æ ‡å­˜å‚¨)]
        MetricsStore --> |å¯è§†åŒ–| Dashboard[ç›‘æ§é¢æ¿]
        
        AllComponents --> |æ—¥å¿—è¾“å‡º| LogCollector[æ—¥å¿—æ”¶é›†å™¨]
        LogCollector --> |å­˜å‚¨æ—¥å¿—| LogStore[(æ—¥å¿—å­˜å‚¨)]
        LogStore --> |æ—¥å¿—åˆ†æ| LogAnalyzer[æ—¥å¿—åˆ†æå™¨]
    end
    
    %% æ•°æ®æµè¿æ¥
    ObjectStore1 -.-> ObjectStore2
    ObjectStore2 -.-> ObjectStore3
    MetaStore1 -.-> MetaStore2
    MsgStream1 -.-> MsgStream2
```

#### 3.3.2 æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†

```mermaid
stateDiagram-v2
    [*] --> Growing: æ•°æ®æ’å…¥
    
    Growing --> Growing: æŒç»­å†™å…¥
    Growing --> Sealed: è¾¾åˆ°å¤§å°é™åˆ¶/æ—¶é—´é™åˆ¶
    Growing --> Sealed: æ‰‹åŠ¨Flush
    
    Sealed --> Flushed: åˆ·ç›˜å®Œæˆ
    Flushed --> Indexed: ç´¢å¼•æ„å»º
    Indexed --> Compacted: å‹ç¼©åˆå¹¶
    
    Compacted --> Compacted: ç»§ç»­å‹ç¼©
    Compacted --> Dropped: æ•°æ®åˆ é™¤
    
    Dropped --> [*]: æ¸…ç†å®Œæˆ
    
    note right of Growing
        çŠ¶æ€ï¼šå†…å­˜ä¸­
        æ“ä½œï¼šå†™å…¥ã€æŸ¥è¯¢
        ä½ç½®ï¼šDataNodeç¼“å­˜
    end note
    
    note right of Sealed
        çŠ¶æ€ï¼šåªè¯»
        æ“ä½œï¼šå‡†å¤‡åˆ·ç›˜
        ä½ç½®ï¼šDataNodeç¼“å­˜
    end note
    
    note right of Flushed
        çŠ¶æ€ï¼šæŒä¹…åŒ–
        æ“ä½œï¼šæŸ¥è¯¢
        ä½ç½®ï¼šå¯¹è±¡å­˜å‚¨
    end note
    
    note right of Indexed
        çŠ¶æ€ï¼šå·²ç´¢å¼•
        æ“ä½œï¼šé«˜æ•ˆæŸ¥è¯¢
        ä½ç½®ï¼šå¯¹è±¡å­˜å‚¨+ç¼“å­˜
    end note
    
    note right of Compacted
        çŠ¶æ€ï¼šå·²å‹ç¼©
        æ“ä½œï¼šä¼˜åŒ–æŸ¥è¯¢
        ä½ç½®ï¼šå¯¹è±¡å­˜å‚¨
    end note
```

#### 3.3.3 æ¶ˆæ¯æµæ¶æ„

```mermaid
graph TB
    subgraph "æ¶ˆæ¯ç”Ÿäº§è€… - Producers"
        Proxy1[Proxy-1] --> |DMLæ¶ˆæ¯| VChannel1[VChannel-1]
        Proxy2[Proxy-2] --> |DMLæ¶ˆæ¯| VChannel2[VChannel-2]
        RootCoord[RootCoord] --> |DDLæ¶ˆæ¯| DDLChannel[DDL Channel]
        DataCoord[DataCoord] --> |æ§åˆ¶æ¶ˆæ¯| ControlChannel[Control Channel]
    end
    
    subgraph "æ¶ˆæ¯é˜Ÿåˆ— - Message Queue"
        VChannel1 --> PChannel1[PChannel-1]
        VChannel2 --> PChannel1
        VChannel1 --> PChannel2[PChannel-2]
        VChannel2 --> PChannel2
        
        DDLChannel --> DDLTopic[DDL Topic]
        ControlChannel --> ControlTopic[Control Topic]
        
        subgraph "Pulsar/Kafkaé›†ç¾¤"
            PChannel1 --> Partition1[åˆ†åŒº1]
            PChannel1 --> Partition2[åˆ†åŒº2]
            PChannel2 --> Partition3[åˆ†åŒº3]
            PChannel2 --> Partition4[åˆ†åŒº4]
        end
    end
    
    subgraph "æ¶ˆæ¯æ¶ˆè´¹è€… - Consumers"
        Partition1 --> DataNode1[DataNode-1]
        Partition2 --> DataNode1
        Partition3 --> DataNode2[DataNode-2]
        Partition4 --> DataNode2
        
        DDLTopic --> QueryCoord[QueryCoord]
        DDLTopic --> DataCoordConsumer[DataCoord]
        DDLTopic --> IndexCoord[IndexCoord]
        
        ControlTopic --> DataNode1
        ControlTopic --> DataNode2
        ControlTopic --> QueryNode1[QueryNode-1]
        ControlTopic --> QueryNode2[QueryNode-2]
    end
    
    subgraph "æ—¶é—´åŒæ­¥ - Time Synchronization"
        TimeTickProducer[TimeTickç”Ÿäº§è€…] --> TimeTickChannel[TimeTick Channel]
        TimeTickChannel --> TimeTickConsumer1[DataNodeæ¶ˆè´¹è€…]
        TimeTickChannel --> TimeTickConsumer2[QueryNodeæ¶ˆè´¹è€…]
        
        TimeTickConsumer1 --> Watermark1[æ°´ä½çº¿1]
        TimeTickConsumer2 --> Watermark2[æ°´ä½çº¿2]
    end
```

**æ•°æ®æµæ¶æ„å…³é”®ç‰¹æ€§ï¼š**

1. **å†™å…¥è·¯å¾„ä¼˜åŒ–**ï¼š
   - æ‰¹é‡å†™å…¥å‡å°‘ç½‘ç»œå¼€é”€
   - å†…å­˜ç¼“å†²æå‡å†™å…¥æ€§èƒ½
   - å¼‚æ­¥åˆ·ç›˜ä¿è¯æ•°æ®æŒä¹…æ€§
   - å‹ç¼©åˆå¹¶ä¼˜åŒ–å­˜å‚¨æ•ˆç‡

2. **æŸ¥è¯¢è·¯å¾„ä¼˜åŒ–**ï¼š
   - å¤šçº§ç¼“å­˜åŠ é€Ÿæ•°æ®è®¿é—®
   - å¹¶è¡ŒæŸ¥è¯¢æå‡æœç´¢æ€§èƒ½
   - æ™ºèƒ½è´Ÿè½½å‡è¡¡åˆ†æ•£æŸ¥è¯¢å‹åŠ›
   - ç»“æœèšåˆä¿è¯æŸ¥è¯¢å‡†ç¡®æ€§

3. **å…ƒæ•°æ®ç®¡ç†**ï¼š
   - é›†ä¸­å¼å…ƒæ•°æ®å­˜å‚¨
   - åˆ†å¸ƒå¼ç¼“å­˜æå‡è®¿é—®é€Ÿåº¦
   - ç‰ˆæœ¬æ§åˆ¶æ”¯æŒSchemaæ¼”è¿›
   - ä¸€è‡´æ€§ä¿è¯æ•°æ®æ­£ç¡®æ€§

4. **æ¶ˆæ¯æµè®¾è®¡**ï¼š
   - è™šæ‹Ÿé€šé“æ”¯æŒé€»è¾‘åˆ†ç‰‡
   - ç‰©ç†é€šé“å®ç°è´Ÿè½½å‡è¡¡
   - æ—¶é—´åŒæ­¥ä¿è¯æ•°æ®ä¸€è‡´æ€§
   - æ¶ˆæ¯æŒä¹…åŒ–é˜²æ­¢æ•°æ®ä¸¢å¤±

---

## 4. æ ¸å¿ƒæ¨¡å—åˆ†æ

### 4.1 Proxy æ¨¡å— - API ç½‘å…³

#### 4.1.1 æ¨¡å—æ¶æ„

```mermaid
graph TB
    subgraph "Proxy å†…éƒ¨æ¶æ„"
        API[gRPC/HTTP API]
        Auth[è®¤è¯æˆæƒ]
        RateLimit[é€Ÿç‡é™åˆ¶]
        TaskScheduler[ä»»åŠ¡è°ƒåº¦å™¨]
        
        subgraph "ä»»åŠ¡é˜Ÿåˆ—"
            DDLQueue[DDL é˜Ÿåˆ—]
            DMLQueue[DML é˜Ÿåˆ—]
            DQLQueue[DQL é˜Ÿåˆ—]
        end
        
        subgraph "å®¢æˆ·ç«¯ç®¡ç†"
            ConnMgr[è¿æ¥ç®¡ç†]
            ShardMgr[åˆ†ç‰‡ç®¡ç†]
            LBPolicy[è´Ÿè½½å‡è¡¡]
        end
    end
    
    API --> Auth
    Auth --> RateLimit
    RateLimit --> TaskScheduler
    TaskScheduler --> DDLQueue
    TaskScheduler --> DMLQueue
    TaskScheduler --> DQLQueue
```

#### 4.1.2 æ ¸å¿ƒæ•°æ®ç»“æ„

```go
// Proxy ä¸»ç»“æ„ï¼šinternal/proxy/proxy.go
type Proxy struct {
    milvuspb.UnimplementedMilvusServiceServer
    
    ctx    context.Context
    cancel context.CancelFunc
    wg     sync.WaitGroup
    
    // åŸºç¡€é…ç½®
    initParams *internalpb.InitParams
    ip         string
    port       int
    stateCode  atomic.Int32
    address    string
    
    // åè°ƒå™¨å®¢æˆ·ç«¯
    mixCoord types.MixCoordClient
    
    // é™æµå™¨
    simpleLimiter *SimpleLimiter
    
    // é€šé“ç®¡ç†å™¨
    chMgr channelsMgr
    
    // ä»»åŠ¡è°ƒåº¦å™¨
    sched *taskScheduler
    
    // ID å’Œæ—¶é—´æˆ³åˆ†é…å™¨
    rowIDAllocator *allocator.IDAllocator
    tsoAllocator   *timestampAllocator
    
    // æŒ‡æ ‡ç¼“å­˜ç®¡ç†å™¨
    metricsCacheManager *metricsinfo.MetricsCacheManager
    
    // ä¼šè¯å’Œåˆ†ç‰‡ç®¡ç†
    session  *sessionutil.Session
    shardMgr shardClientMgr
    
    // æœç´¢ç»“æœé€šé“
    searchResultCh chan *internalpb.SearchResults
    
    // å›è°ƒå‡½æ•°
    startCallbacks []func()
    closeCallbacks []func()
    
    // è´Ÿè½½å‡è¡¡ç­–ç•¥
    lbPolicy LBPolicy
    
    // èµ„æºç®¡ç†å™¨
    resourceManager resource.Manager
    
    // åŠŸèƒ½å¼€å…³
    enableMaterializedView   bool
    enableComplexDeleteLimit bool
    
    // æ…¢æŸ¥è¯¢ç¼“å­˜
    slowQueries *expirable.LRU[Timestamp, *metricsinfo.SlowQuery]
}
```

#### 4.1.3 ä»»åŠ¡è°ƒåº¦æœºåˆ¶

```go
// ä»»åŠ¡è°ƒåº¦å™¨ï¼šinternal/proxy/task_scheduler.go
type taskScheduler struct {
    ddQueue  *BaseTaskQueue  // DDL ä»»åŠ¡é˜Ÿåˆ—
    dmlQueue *BaseTaskQueue  // DML ä»»åŠ¡é˜Ÿåˆ—  
    dqQueue  *BaseTaskQueue  // DQL ä»»åŠ¡é˜Ÿåˆ—
    
    wg     sync.WaitGroup
    ctx    context.Context
    cancel context.CancelFunc
}

// åŸºç¡€ä»»åŠ¡æ¥å£
type task interface {
    TraceCtx() context.Context
    ID() UniqueID
    SetID(uid UniqueID)
    Name() string
    Type() commonpb.MsgType
    BeginTs() Timestamp
    EndTs() Timestamp
    SetTs(ts Timestamp)
    OnEnqueue() error
    PreExecute(ctx context.Context) error
    Execute(ctx context.Context) error
    PostExecute(ctx context.Context) error
    WaitToFinish() error
    Notify(err error)
}

// ä»»åŠ¡é˜Ÿåˆ—å¤„ç†é€»è¾‘
func (queue *BaseTaskQueue) processTask(t task) {
    // 1. ä»»åŠ¡é¢„å¤„ç†
    if err := t.PreExecute(queue.ctx); err != nil {
        t.Notify(err)
        return
    }
    
    // 2. æ‰§è¡Œä»»åŠ¡
    if err := t.Execute(queue.ctx); err != nil {
        t.Notify(err)
        return
    }
    
    // 3. ä»»åŠ¡åå¤„ç†
    if err := t.PostExecute(queue.ctx); err != nil {
        t.Notify(err)
        return
    }
    
    // 4. é€šçŸ¥ä»»åŠ¡å®Œæˆ
    t.Notify(nil)
}
```

### 4.2 RootCoord æ¨¡å— - å…ƒæ•°æ®ç®¡ç†

#### 4.2.1 æ¨¡å—èŒè´£

RootCoord æ˜¯ Milvus çš„å…ƒæ•°æ®ç®¡ç†ä¸­å¿ƒï¼Œè´Ÿè´£ï¼š
- é›†åˆå’Œåˆ†åŒºçš„å…ƒæ•°æ®ç®¡ç†
- Schema å®šä¹‰å’Œç‰ˆæœ¬æ§åˆ¶
- å…¨å±€ ID åˆ†é…
- æ—¶é—´æˆ³åˆ†é…
- æ•°æ®å®šä¹‰è¯­è¨€ (DDL) æ“ä½œåè°ƒ

#### 4.2.2 æ ¸å¿ƒæ•°æ®ç»“æ„

```go
// RootCoord ä¸»ç»“æ„ï¼šinternal/rootcoord/root_coord.go
type Core struct {
    ctx    context.Context
    cancel context.CancelFunc
    wg     sync.WaitGroup
    
    // åŸºç¡€ä¿¡æ¯
    etcdCli   *clientv3.Client
    address   string
    port      int
    stateCode atomic.Int32
    
    // å…ƒæ•°æ®å­˜å‚¨
    metaTable  *metaTable
    scheduler  *taskScheduler
    
    // ID åˆ†é…å™¨
    idAllocator       *allocator.GlobalIDAllocator
    tsoAllocator      *tso.GlobalTSOAllocator
    
    // ä»£ç†ç®¡ç†
    proxyClientManager *proxyClientManager
    proxyWatcher       *proxyWatcher
    
    // å¯¼å…¥ç®¡ç†
    importManager *importManager
    
    // é…é¢ç®¡ç†
    quotaCenter *QuotaCenter
    
    // ä¼šè¯
    session *sessionutil.Session
    
    // å·¥å‚
    factory dependency.Factory
}

// å…ƒæ•°æ®è¡¨ï¼šinternal/rootcoord/meta_table.go
type metaTable struct {
    ctx    context.Context
    catalog metastore.RootCoordCatalog
    
    // é›†åˆä¿¡æ¯ç¼“å­˜
    collID2Meta  map[typeutil.UniqueID]*model.Collection
    collName2ID  map[string]typeutil.UniqueID
    collAlias2ID map[string]typeutil.UniqueID
    
    // åˆ†åŒºä¿¡æ¯ç¼“å­˜
    partID2Meta map[typeutil.UniqueID]*model.Partition
    
    // æ•°æ®åº“ä¿¡æ¯
    dbName2ID map[string]typeutil.UniqueID
    dbID2Meta map[typeutil.UniqueID]*model.Database
    
    // è¯»å†™é”
    ddLock sync.RWMutex
}
```

#### 4.2.3 é›†åˆåˆ›å»ºæµç¨‹

```go
// åˆ›å»ºé›†åˆä»»åŠ¡ï¼šinternal/rootcoord/create_collection_task.go
type createCollectionTask struct {
    baseTask
    Req *milvuspb.CreateCollectionRequest
    
    // å†…éƒ¨çŠ¶æ€
    collectionID   typeutil.UniqueID
    partitionID    typeutil.UniqueID
    schema         *schemapb.CollectionSchema
    virtualChannels []string
    physicalChannels []string
}

func (t *createCollectionTask) Execute(ctx context.Context) error {
    // 1. åˆ†é…é›†åˆ ID
    collectionID, err := t.core.idAllocator.AllocOne()
    if err != nil {
        return err
    }
    t.collectionID = collectionID
    
    // 2. åˆ†é…åˆ†åŒº ID
    partitionID, err := t.core.idAllocator.AllocOne()
    if err != nil {
        return err
    }
    t.partitionID = partitionID
    
    // 3. éªŒè¯å’Œå¤„ç† Schema
    if err := t.validateSchema(); err != nil {
        return err
    }
    
    // 4. åˆ†é…è™šæ‹Ÿé€šé“
    t.virtualChannels = t.core.chanTimeTick.getDmlChannelNames(t.Req.ShardsNum)
    
    // 5. åˆ›å»ºé›†åˆå…ƒæ•°æ®
    collection := &model.Collection{
        CollectionID:         t.collectionID,
        Name:                t.Req.CollectionName,
        Description:         t.Req.Description,
        AutoID:              t.schema.AutoID,
        Fields:              model.UnmarshalFieldModels(t.schema.Fields),
        VirtualChannelNames: t.virtualChannels,
        PhysicalChannelNames: t.physicalChannels,
        ShardsNum:           t.Req.ShardsNum,
        ConsistencyLevel:    t.Req.ConsistencyLevel,
        CreateTime:          t.GetTs(),
        State:               pb.CollectionState_CollectionCreating,
        StartPositions:      t.startPositions,
    }
    
    // 6. æŒä¹…åŒ–åˆ°å…ƒæ•°æ®å­˜å‚¨
    if err := t.core.meta.AddCollection(ctx, collection); err != nil {
        return err
    }
    
    // 7. é€šçŸ¥ DataCoord åˆ›å»ºé›†åˆ
    if err := t.core.broker.CreateCollection(ctx, collection); err != nil {
        return err
    }
    
    return nil
}
```

### 4.3 DataCoord æ¨¡å— - æ•°æ®åè°ƒ

#### 4.3.1 æ¨¡å—æ¶æ„

```mermaid
graph TB
    subgraph "DataCoord æ¶æ„"
        API[gRPC API]
        
        subgraph "æ ¸å¿ƒç®¡ç†å™¨"
            SegmentMgr[Segment ç®¡ç†å™¨]
            ChannelMgr[Channel ç®¡ç†å™¨]
            SessionMgr[Session ç®¡ç†å™¨]
            CompactionMgr[å‹ç¼©ç®¡ç†å™¨]
            GCMgr[åƒåœ¾å›æ”¶ç®¡ç†å™¨]
        end
        
        subgraph "è°ƒåº¦å™¨"
            CompactionScheduler[å‹ç¼©è°ƒåº¦å™¨]
            ImportScheduler[å¯¼å…¥è°ƒåº¦å™¨]
            IndexScheduler[ç´¢å¼•è°ƒåº¦å™¨]
        end
        
        subgraph "å­˜å‚¨"
            MetaStore[(å…ƒæ•°æ®å­˜å‚¨)]
            MessageQueue[æ¶ˆæ¯é˜Ÿåˆ—]
        end
    end
    
    API --> SegmentMgr
    API --> ChannelMgr
    SegmentMgr --> CompactionScheduler
    ChannelMgr --> MessageQueue
    CompactionScheduler --> CompactionMgr
```

#### 4.3.2 Segment ç®¡ç†

```go
// Segment ç®¡ç†å™¨ï¼šinternal/datacoord/segment_manager.go
type SegmentManager struct {
    meta      *meta
    allocator allocator.Allocator
    
    // Segment åˆ†é…ç­–ç•¥
    segmentSealPolicy   []segmentSealPolicy
    channelSealPolicies map[string][]segmentSealPolicy
    
    // ç»Ÿè®¡ä¿¡æ¯
    estimatePolicy ChannelSegmentPolicy
    allocPolicy    ChannelSegmentPolicy
    
    // å¹¶å‘æ§åˆ¶
    mu sync.RWMutex
}

// Segment ä¿¡æ¯ç»“æ„
type SegmentInfo struct {
    SegmentInfo *datapb.SegmentInfo
    currRows    int64
    allocations []*allocation
    lastFlushTs typeutil.Timestamp
    
    // çŠ¶æ€ç®¡ç†
    isCompacting bool
    size         int64
    lastExpireTime typeutil.Timestamp
}

// Segment åˆ†é…é€»è¾‘
func (s *SegmentManager) AllocSegment(ctx context.Context, collectionID, partitionID typeutil.UniqueID, channelName string, requestRows int64) (*SegmentInfo, error) {
    // 1. æŸ¥æ‰¾å¯ç”¨çš„ Growing Segment
    segment := s.getGrowingSegment(collectionID, partitionID, channelName)
    
    // 2. å¦‚æœæ²¡æœ‰å¯ç”¨ Segmentï¼Œåˆ›å»ºæ–°çš„
    if segment == nil {
        segmentID, err := s.allocator.AllocOne()
        if err != nil {
            return nil, err
        }
        
        segment = &SegmentInfo{
            SegmentInfo: &datapb.SegmentInfo{
                ID:            segmentID,
                CollectionID:  collectionID,
                PartitionID:   partitionID,
                InsertChannel: channelName,
                State:         commonpb.SegmentState_Growing,
                MaxRowNum:     Params.DataCoordCfg.SegmentMaxSize.GetAsInt64(),
                CreatedByNode: Params.DataCoordCfg.GetNodeID(),
            },
        }
        
        // 3. æ³¨å†Œåˆ°å…ƒæ•°æ®
        if err := s.meta.AddSegment(ctx, segment); err != nil {
            return nil, err
        }
    }
    
    // 4. åˆ†é…è¡Œæ•°
    segment.currRows += requestRows
    
    // 5. æ£€æŸ¥æ˜¯å¦éœ€è¦ Seal
    if s.shouldSealSegment(segment) {
        s.sealSegment(ctx, segment)
    }
    
    return segment, nil
}
```

#### 4.3.3 å‹ç¼©æœºåˆ¶

```go
// å‹ç¼©ç®¡ç†å™¨ï¼šinternal/datacoord/compaction_manager.go
type CompactionManager struct {
    meta      *meta
    sessions  *SessionManager
    allocator allocator.Allocator
    
    // å‹ç¼©ä»»åŠ¡é˜Ÿåˆ—
    compactionHandler map[int64]*compactionPlanHandler
    
    // å‹ç¼©ç­–ç•¥
    levelZeroCompactionPolicy CompactionPolicy
    mixCompactionPolicy       CompactionPolicy
    
    mu sync.RWMutex
}

// å‹ç¼©ä»»åŠ¡
type compactionTask struct {
    triggerID     int64
    planID        int64
    dataNodeID    int64
    plan          *datapb.CompactionPlan
    state         datapb.CompactionTaskState
    
    startTime time.Time
    endTime   time.Time
}

// è§¦å‘å‹ç¼©é€»è¾‘
func (cm *CompactionManager) TriggerCompaction(collectionID int64) error {
    // 1. è·å–é›†åˆçš„æ‰€æœ‰ Segment
    segments := cm.meta.GetSegmentsByCollection(collectionID)
    
    // 2. æŒ‰å‹ç¼©ç­–ç•¥åˆ†ç»„
    groups := cm.groupSegmentsForCompaction(segments)
    
    // 3. ä¸ºæ¯ç»„åˆ›å»ºå‹ç¼©è®¡åˆ’
    for _, group := range groups {
        plan := &datapb.CompactionPlan{
            PlanID:        cm.allocator.AllocOne(),
            Type:          datapb.CompactionType_MixCompaction,
            SegmentBinlogs: group.segments,
            TimeoutInSeconds: 3600,
            Collection:    collectionID,
            Channel:       group.channel,
        }
        
        // 4. åˆ†é… DataNode æ‰§è¡Œå‹ç¼©
        nodeID := cm.selectDataNode(group.channel)
        if err := cm.sessions.Compaction(nodeID, plan); err != nil {
            return err
        }
        
        // 5. è®°å½•å‹ç¼©ä»»åŠ¡
        cm.addCompactionTask(plan.PlanID, nodeID, plan)
    }
    
    return nil
}
```

### 4.4 QueryCoord æ¨¡å— - æŸ¥è¯¢åè°ƒ

#### 4.4.1 æ¨¡å—èŒè´£

QueryCoord è´Ÿè´£æŸ¥è¯¢ç›¸å…³çš„åè°ƒå·¥ä½œï¼š
- ç®¡ç† QueryNode é›†ç¾¤
- è´Ÿè½½å‡è¡¡å’Œåˆ†ç‰‡åˆ†é…
- æŸ¥è¯¢ä»»åŠ¡è°ƒåº¦
- å‰¯æœ¬ç®¡ç†

#### 4.4.2 æ ¸å¿ƒæ¶æ„

```go
// QueryCoord ä¸»ç»“æ„ï¼šinternal/querycoordv2/server.go
type Server struct {
    ctx    context.Context
    cancel context.CancelFunc
    wg     sync.WaitGroup
    
    // åŸºç¡€ä¿¡æ¯
    etcdCli *clientv3.Client
    address string
    port    int
    
    // æ ¸å¿ƒç®¡ç†å™¨
    meta         *meta.Meta
    dist         *meta.DistributionManager
    targetMgr    *meta.TargetManager
    broker       meta.Broker
    
    // è°ƒåº¦å™¨
    jobScheduler  *job.Scheduler
    taskScheduler *task.Scheduler
    
    // è§‚å¯Ÿè€…
    nodeMgr     *session.NodeManager
    observers   []observers.Observer
    
    // æ£€æŸ¥å™¨
    checkerController *checkers.CheckerController
    
    // è´Ÿè½½å‡è¡¡å™¨
    balancer balance.Balance
    
    // ä¼šè¯
    session *sessionutil.Session
}

// åˆ†å¸ƒå¼ç®¡ç†å™¨ï¼šinternal/querycoordv2/meta/dist_manager.go
type DistributionManager struct {
    // Segment åˆ†å¸ƒ
    segmentDist map[int64]*meta.Segment  // nodeID -> segments
    channelDist map[int64]*meta.DmChannel // nodeID -> channels
    leaderView  map[int64]*meta.LeaderView // nodeID -> leader view
    
    // è¯»å†™é”
    rwmutex sync.RWMutex
}
```

#### 4.4.3 è´Ÿè½½å‡è¡¡æœºåˆ¶

```go
// è´Ÿè½½å‡è¡¡å™¨ï¼šinternal/querycoordv2/balance/balance.go
type Balance interface {
    AssignSegment(collectionID int64, segments []*meta.Segment, nodes []int64) []SegmentAssignPlan
    BalanceReplica(replica *meta.Replica) ([]SegmentAssignPlan, []ChannelAssignPlan)
}

// è½®è¯¢è´Ÿè½½å‡è¡¡å™¨
type RoundRobinBalancer struct {
    scheduler task.Scheduler
    meta      *meta.Meta
    dist      *meta.DistributionManager
}

func (b *RoundRobinBalancer) AssignSegment(collectionID int64, segments []*meta.Segment, nodes []int64) []SegmentAssignPlan {
    plans := make([]SegmentAssignPlan, 0, len(segments))
    
    // 1. è·å–èŠ‚ç‚¹è´Ÿè½½ä¿¡æ¯
    nodeLoads := make(map[int64]int64)
    for _, nodeID := range nodes {
        nodeLoads[nodeID] = b.getNodeLoad(nodeID)
    }
    
    // 2. æŒ‰è´Ÿè½½æ’åºèŠ‚ç‚¹
    sort.Slice(nodes, func(i, j int) bool {
        return nodeLoads[nodes[i]] < nodeLoads[nodes[j]]
    })
    
    // 3. è½®è¯¢åˆ†é… Segment
    nodeIndex := 0
    for _, segment := range segments {
        targetNode := nodes[nodeIndex]
        plans = append(plans, SegmentAssignPlan{
            Segment: segment,
            From:    -1,
            To:      targetNode,
        })
        
        nodeIndex = (nodeIndex + 1) % len(nodes)
        nodeLoads[targetNode]++
    }
    
    return plans
}
```

### 4.5 DataNode æ¨¡å— - æ•°æ®èŠ‚ç‚¹

#### 4.5.1 æ•°æ®å†™å…¥æµæ°´çº¿

```go
// æ•°æ®èŠ‚ç‚¹ï¼šinternal/datanode/data_node.go
type DataNode struct {
    ctx    context.Context
    cancel context.CancelFunc
    
    // åŸºç¡€ä¿¡æ¯
    Role       string
    NodeID     typeutil.UniqueID
    address    string
    port       int
    stateCode  atomic.Int32
    
    // æµæ°´çº¿ç®¡ç†
    flowgraphManager *pipeline.FlowgraphManager
    
    // å†™ç¼“å†²åŒºç®¡ç†
    writeBufferManager writebuffer.BufferManager
    
    // åŒæ­¥ç®¡ç†å™¨
    syncMgr syncmgr.SyncManager
    
    // å‹ç¼©å™¨
    compactionExecutor *compactor.Executor
    
    // ä¼šè¯
    session *sessionutil.Session
}

// æ•°æ®å†™å…¥æµæ°´çº¿ï¼šinternal/datanode/pipeline/flow_graph.go
type DataSyncService struct {
    ctx    context.Context
    cancel context.CancelFunc
    
    // æµå›¾èŠ‚ç‚¹
    dmStreamNode   *DmInputNode
    insertBufferNode *InsertBufferNode
    deleteBufferNode *DeleteBufferNode
    ttNode         *TimeTickNode
    
    // é€šé“ä¿¡æ¯
    vchannelName   string
    metacache      metacache.MetaCache
    
    // å†™ç¼“å†²åŒº
    writeBuffer    writebuffer.WriteBuffer
    
    // åŒæ­¥å™¨
    syncMgr        syncmgr.SyncManager
}

// æ’å…¥ç¼“å†²èŠ‚ç‚¹å¤„ç†é€»è¾‘
func (ibn *InsertBufferNode) Operate(in []Msg) []Msg {
    // 1. è§£ææ’å…¥æ¶ˆæ¯
    insertMsgs := ibn.parseInsertMsgs(in)
    
    // 2. å†™å…¥ç¼“å†²åŒº
    for _, msg := range insertMsgs {
        // åˆ†é… Segment
        segmentID := ibn.allocateSegment(msg.CollectionID, msg.PartitionID)
        
        // å†™å…¥æ•°æ®åˆ°ç¼“å†²åŒº
        ibn.writeBuffer.BufferData(segmentID, msg.RowData)
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·ç›˜
        if ibn.shouldFlush(segmentID) {
            ibn.triggerFlush(segmentID)
        }
    }
    
    return in
}
```

#### 4.5.2 æ•°æ®åˆ·ç›˜æœºåˆ¶

```go
// åŒæ­¥ç®¡ç†å™¨ï¼šinternal/datanode/syncmgr/sync_manager.go
type SyncManager interface {
    SyncData(ctx context.Context, task SyncTask) *SyncTask
}

type syncManager struct {
    // ä»»åŠ¡é˜Ÿåˆ—
    tasks chan SyncTask
    
    // å·¥ä½œåç¨‹æ± 
    workers []Worker
    
    // å…ƒæ•°æ®ç¼“å­˜
    metacache metacache.MetaCache
    
    // åˆ†é…å™¨
    allocator allocator.Allocator
    
    // å­˜å‚¨å®¢æˆ·ç«¯
    chunkManager storage.ChunkManager
}

// åŒæ­¥ä»»åŠ¡
type SyncTask struct {
    segmentID    int64
    collectionID int64
    partitionID  int64
    channelName  string
    
    // æ•°æ®
    insertData   *storage.InsertData
    deleteData   *storage.DeleteData
    
    // æ—¶é—´æˆ³
    startPosition *msgpb.MsgPosition
    endPosition   *msgpb.MsgPosition
    
    // å›è°ƒ
    done chan error
}

// æ‰§è¡ŒåŒæ­¥ä»»åŠ¡
func (sm *syncManager) sync(task *SyncTask) error {
    // 1. åºåˆ—åŒ–æ•°æ®
    insertLogs, statsLogs, err := sm.serializeInsertData(task.insertData)
    if err != nil {
        return err
    }
    
    deleteLogs, err := sm.serializeDeleteData(task.deleteData)
    if err != nil {
        return err
    }
    
    // 2. ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨
    insertPaths := make([]string, len(insertLogs))
    for i, log := range insertLogs {
        path := sm.generateInsertLogPath(task.segmentID, log.FieldID)
        if err := sm.chunkManager.Write(path, log.Data); err != nil {
            return err
        }
        insertPaths[i] = path
    }
    
    // 3. æ›´æ–°å…ƒæ•°æ®
    segmentInfo := &datapb.SegmentInfo{
        ID:           task.segmentID,
        CollectionID: task.collectionID,
        PartitionID:  task.partitionID,
        InsertChannel: task.channelName,
        NumOfRows:    task.insertData.GetRowNum(),
        Binlogs:      insertPaths,
        Deltalogs:    deletePaths,
        Statslogs:    statsLogs,
        StartPosition: task.startPosition,
        DmlPosition:   task.endPosition,
    }
    
    // 4. é€šçŸ¥ DataCoord
    if err := sm.reportSegment(segmentInfo); err != nil {
        return err
    }
    
    return nil
}
```

### 4.6 QueryNode æ¨¡å— - æŸ¥è¯¢èŠ‚ç‚¹

#### 4.6.1 æŸ¥è¯¢æ‰§è¡Œå¼•æ“

```go
// æŸ¥è¯¢èŠ‚ç‚¹ï¼šinternal/querynodev2/server.go
type QueryNode struct {
    ctx    context.Context
    cancel context.CancelFunc
    
    // åŸºç¡€ä¿¡æ¯
    address   string
    port      int
    nodeID    typeutil.UniqueID
    stateCode atomic.Int32
    
    // æ ¸å¿ƒç®¡ç†å™¨
    manager      *segment.Manager
    delegators   map[string]*delegator.ShardDelegator
    
    // æŸ¥è¯¢æ‰§è¡Œå™¨
    scheduler    *task.Scheduler
    
    // æœ¬åœ°å·¥ä½œå™¨
    workers      *LocalWorker
    
    // ä¼šè¯
    session      *sessionutil.Session
}

// Segment ç®¡ç†å™¨ï¼šinternal/querynodev2/segments/manager.go
type Manager struct {
    // Segment å­˜å‚¨
    growing map[int64]Segment  // segmentID -> growing segment
    sealed  map[int64]Segment  // segmentID -> sealed segment
    
    // é›†åˆä¿¡æ¯
    collection *Collection
    
    // åŠ è½½å™¨
    loader *Loader
    
    // è¯»å†™é”
    mu sync.RWMutex
}

// æŸ¥è¯¢æ‰§è¡Œé€»è¾‘
func (qn *QueryNode) Search(ctx context.Context, req *querypb.SearchRequest) (*internalpb.SearchResults, error) {
    // 1. è·å–åˆ†ç‰‡å§”æ‰˜å™¨
    delegator := qn.delegators[req.GetDmlChannels()[0]]
    if delegator == nil {
        return nil, errors.New("delegator not found")
    }
    
    // 2. åˆ›å»ºæœç´¢ä»»åŠ¡
    searchTask := &searchTask{
        req:       req,
        delegator: delegator,
        result:    make(chan *internalpb.SearchResults, 1),
    }
    
    // 3. æäº¤ä»»åŠ¡åˆ°è°ƒåº¦å™¨
    if err := qn.scheduler.Add(searchTask); err != nil {
        return nil, err
    }
    
    // 4. ç­‰å¾…ç»“æœ
    select {
    case result := <-searchTask.result:
        return result, nil
    case <-ctx.Done():
        return nil, ctx.Err()
    }
}

// åˆ†ç‰‡å§”æ‰˜å™¨æ‰§è¡Œæœç´¢
func (sd *ShardDelegator) Search(ctx context.Context, req *querypb.SearchRequest) (*internalpb.SearchResults, error) {
    // 1. è·å–æœç´¢ Segment åˆ—è¡¨
    sealedSegments := sd.getSearchableSegments(req.GetReq().GetCollectionID())
    growingSegments := sd.getGrowingSegments(req.GetReq().GetCollectionID())
    
    // 2. å¹¶è¡Œæœç´¢ Sealed Segment
    var wg sync.WaitGroup
    sealedResults := make([]*internalpb.SearchResults, len(sealedSegments))
    
    for i, segment := range sealedSegments {
        wg.Add(1)
        go func(idx int, seg Segment) {
            defer wg.Done()
            result, err := seg.Search(ctx, req)
            if err == nil {
                sealedResults[idx] = result
            }
        }(i, segment)
    }
    
    // 3. æœç´¢ Growing Segment
    growingResults := make([]*internalpb.SearchResults, len(growingSegments))
    for i, segment := range growingSegments {
        result, err := segment.Search(ctx, req)
        if err == nil {
            growingResults[i] = result
        }
    }
    
    // 4. ç­‰å¾…æ‰€æœ‰æœç´¢å®Œæˆ
    wg.Wait()
    
    // 5. åˆå¹¶æœç´¢ç»“æœ
    allResults := append(sealedResults, growingResults...)
    finalResult := sd.reduceSearchResults(allResults, req.GetReq().GetTopk())
    
    return finalResult, nil
}
```

---

## 5. å…³é”®æ•°æ®ç»“æ„

### 5.1 æ ¸å¿ƒæ•°æ®æ¨¡å‹

#### 5.1.1 é›†åˆ (Collection) æ¨¡å‹

```go
// é›†åˆæ¨¡å‹ï¼šinternal/metastore/model/collection.go
type Collection struct {
    CollectionID         int64                 `json:"collectionID"`
    Name                string                `json:"name"`
    Description         string                `json:"description"`
    AutoID              bool                  `json:"autoID"`
    Fields              []*Field              `json:"fields"`
    VirtualChannelNames []string              `json:"virtualChannelNames"`
    PhysicalChannelNames []string             `json:"physicalChannelNames"`
    ShardsNum           int32                 `json:"shardsNum"`
    ConsistencyLevel    commonpb.ConsistencyLevel `json:"consistencyLevel"`
    CreateTime          uint64                `json:"createTime"`
    StartPositions      []*commonpb.KeyDataPair `json:"startPositions"`
    Properties          map[string]string     `json:"properties"`
    State               pb.CollectionState    `json:"state"`
    Partitions          []*Partition          `json:"partitions"`
}

// å­—æ®µæ¨¡å‹
type Field struct {
    FieldID      int64                `json:"fieldID"`
    Name         string               `json:"name"`
    IsPrimaryKey bool                 `json:"isPrimaryKey"`
    Description  string               `json:"description"`
    DataType     schemapb.DataType    `json:"dataType"`
    TypeParams   map[string]string    `json:"typeParams"`
    IndexParams  map[string]string    `json:"indexParams"`
    AutoID       bool                 `json:"autoID"`
}
```

#### 5.1.2 Segment æ•°æ®ç»“æ„

```go
// Segment ä¿¡æ¯ï¼špkg/proto/datapb/data_coord.proto
type SegmentInfo struct {
    ID                int64                    `protobuf:"varint,1,opt,name=ID,proto3" json:"ID,omitempty"`
    CollectionID      int64                    `protobuf:"varint,2,opt,name=collectionID,proto3" json:"collectionID,omitempty"`
    PartitionID       int64                    `protobuf:"varint,3,opt,name=partitionID,proto3" json:"partitionID,omitempty"`
    InsertChannel     string                   `protobuf:"bytes,4,opt,name=insert_channel,json=insertChannel,proto3" json:"insert_channel,omitempty"`
    NumOfRows         int64                    `protobuf:"varint,5,opt,name=num_of_rows,json=numOfRows,proto3" json:"num_of_rows,omitempty"`
    State             commonpb.SegmentState    `protobuf:"varint,6,opt,name=state,proto3,enum=milvus.proto.common.SegmentState" json:"state,omitempty"`
    MaxRowNum         int64                    `protobuf:"varint,7,opt,name=max_row_num,json=maxRowNum,proto3" json:"max_row_num,omitempty"`
    LastExpireTime    uint64                   `protobuf:"varint,8,opt,name=last_expire_time,json=lastExpireTime,proto3" json:"last_expire_time,omitempty"`
    StartPosition     *msgpb.MsgPosition       `protobuf:"bytes,9,opt,name=start_position,json=startPosition,proto3" json:"start_position,omitempty"`
    DmlPosition       *msgpb.MsgPosition       `protobuf:"bytes,10,opt,name=dml_position,json=dmlPosition,proto3" json:"dml_position,omitempty"`
    Binlogs           []*FieldBinlog           `protobuf:"bytes,11,rep,name=binlogs,proto3" json:"binlogs,omitempty"`
    Statslogs         []*FieldBinlog           `protobuf:"bytes,12,rep,name=statslogs,proto3" json:"statslogs,omitempty"`
    Deltalogs         []*FieldBinlog           `protobuf:"bytes,13,rep,name=deltalogs,proto3" json:"deltalogs,omitempty"`
    CreatedByNode     int64                    `protobuf:"varint,14,opt,name=created_by_node,json=createdByNode,proto3" json:"created_by_node,omitempty"`
    SegmentSize       int64                    `protobuf:"varint,15,opt,name=segment_size,json=segmentSize,proto3" json:"segment_size,omitempty"`
    IndexInfos        []*SegmentIndexInfo      `protobuf:"bytes,16,rep,name=index_infos,json=indexInfos,proto3" json:"index_infos,omitempty"`
}

// Segment çŠ¶æ€æšä¸¾
type SegmentState int32
const (
    SegmentState_SegmentStateNone SegmentState = 0
    SegmentState_NotExist         SegmentState = 1
    SegmentState_Growing          SegmentState = 2
    SegmentState_Sealed           SegmentState = 3
    SegmentState_Flushed          SegmentState = 4
    SegmentState_Flushing         SegmentState = 5
    SegmentState_Dropped          SegmentState = 6
    SegmentState_Importing        SegmentState = 7
)
```

#### 5.1.3 ç´¢å¼•æ•°æ®ç»“æ„

```go
// ç´¢å¼•ä¿¡æ¯ï¼špkg/proto/indexpb/index_coord.proto
type IndexInfo struct {
    CollectionID    int64             `protobuf:"varint,1,opt,name=collectionID,proto3" json:"collectionID,omitempty"`
    FieldID         int64             `protobuf:"varint,2,opt,name=fieldID,proto3" json:"fieldID,omitempty"`
    IndexName       string            `protobuf:"bytes,3,opt,name=index_name,json=indexName,proto3" json:"index_name,omitempty"`
    IndexID         int64             `protobuf:"varint,4,opt,name=indexID,proto3" json:"indexID,omitempty"`
    TypeParams      []*commonpb.KeyValuePair `protobuf:"bytes,5,rep,name=type_params,json=typeParams,proto3" json:"type_params,omitempty"`
    IndexParams     []*commonpb.KeyValuePair `protobuf:"bytes,6,rep,name=index_params,json=indexParams,proto3" json:"index_params,omitempty"`
    IndexedRows     int64             `protobuf:"varint,7,opt,name=indexed_rows,json=indexedRows,proto3" json:"indexed_rows,omitempty"`
    TotalRows       int64             `protobuf:"varint,8,opt,name=total_rows,json=totalRows,proto3" json:"total_rows,omitempty"`
    State           commonpb.IndexState `protobuf:"varint,9,opt,name=state,proto3,enum=milvus.proto.common.IndexState" json:"state,omitempty"`
    IndexStateFailReason string       `protobuf:"bytes,10,opt,name=index_state_fail_reason,json=indexStateFailReason,proto3" json:"index_state_fail_reason,omitempty"`
    IsAutoIndex     bool              `protobuf:"varint,11,opt,name=is_auto_index,json=isAutoIndex,proto3" json:"is_auto_index,omitempty"`
    UserIndexParams []*commonpb.KeyValuePair `protobuf:"bytes,12,rep,name=user_index_params,json=userIndexParams,proto3" json:"user_index_params,omitempty"`
}
```

### 5.2 æ¶ˆæ¯ç³»ç»Ÿæ•°æ®ç»“æ„

#### 5.2.1 æ¶ˆæ¯åŸºç¡€ç»“æ„

```go
// æ¶ˆæ¯åŸºç¡€æ¥å£ï¼špkg/mq/msgstream/msg.go
type TsMsg interface {
    TraceCtx() context.Context
    SetTraceCtx(ctx context.Context)
    ID() UniqueID
    BeginTs() Timestamp
    EndTs() Timestamp
    Type() MsgType
    SourceID() int64
    HashKeys() []uint32
    Marshal(TsMsg) (MarshalType, error)
    Unmarshal(MarshalType) (TsMsg, error)
    Position() *MsgPosition
    SetPosition(*MsgPosition)
    Size() int
}

// æ’å…¥æ¶ˆæ¯
type InsertMsg struct {
    BaseMsg
    InsertRequest milvuspb.InsertRequest
    
    // å†…éƒ¨å­—æ®µ
    HashValues  []uint32
    Timestamps  []uint64
    RowIDs      []int64
    RowData     []*commonpb.Blob
}

// åˆ é™¤æ¶ˆæ¯
type DeleteMsg struct {
    BaseMsg
    DeleteRequest milvuspb.DeleteRequest
    
    // å†…éƒ¨å­—æ®µ
    HashValues []uint32
    Timestamps []uint64
    PrimaryKeys *schemapb.IDs
}

// æœç´¢æ¶ˆæ¯
type SearchMsg struct {
    BaseMsg
    SearchRequest milvuspb.SearchRequest
    
    // æŸ¥è¯¢è®¡åˆ’
    PlaceholderGroup []byte
    DslType         commonpb.DslType
    SerializedExprPlan []byte
}
```

### 5.3 å­˜å‚¨æ•°æ®ç»“æ„

#### 5.3.1 Binlog æ ¼å¼

```go
// Binlog äº‹ä»¶ï¼šinternal/storage/event.go
type Event interface {
    EventType() EventTypeCode
    Timestamp() Timestamp
}

// æ’å…¥äº‹ä»¶æ•°æ®
type InsertEventData struct {
    StartTimestamp Timestamp
    EndTimestamp   Timestamp
    
    // æ•°æ®å­—æ®µ
    Data map[FieldID]FieldData
}

// åˆ é™¤äº‹ä»¶æ•°æ®  
type DeleteEventData struct {
    StartTimestamp Timestamp
    EndTimestamp   Timestamp
    
    // åˆ é™¤çš„ä¸»é”®
    Pks         *schemapb.IDs
    Tss         []Timestamp
}

// å­—æ®µæ•°æ®æ¥å£
type FieldData interface {
    GetMemorySize() int
    RowNum() int
    GetNullMask() []bool
    AppendRow(interface{}) error
    GetRow(int) interface{}
}
```

### 5.4 ç±»ç»§æ‰¿å…³ç³»å›¾

```mermaid
classDiagram
    class Component {
        <<interface>>
        +GetComponentStates() ComponentStates
        +GetStatisticsChannel() string
        +GetTimeTickChannel() string
    }
    
    class Proxy {
        -stateCode atomic.Int32
        -mixCoord types.MixCoordClient
        -sched *taskScheduler
        +CreateCollection()
        +Insert()
        +Search()
    }
    
    class RootCoord {
        -metaTable *metaTable
        -idAllocator *allocator.GlobalIDAllocator
        +CreateCollection()
        +DropCollection()
        +AllocID()
    }
    
    class DataCoord {
        -segmentManager *SegmentManager
        -compactionHandler *CompactionHandler
        +AssignSegmentID()
        +Flush()
        +Compaction()
    }
    
    class QueryCoord {
        -meta *meta.Meta
        -dist *meta.DistributionManager
        -balancer balance.Balance
        +LoadCollection()
        +ReleaseCollection()
        +LoadBalance()
    }
    
    class DataNode {
        -flowgraphManager *pipeline.FlowgraphManager
        -writeBufferManager writebuffer.BufferManager
        +Flush()
        +Compaction()
    }
    
    class QueryNode {
        -manager *segment.Manager
        -delegators map[string]*delegator.ShardDelegator
        +Search()
        +Query()
        +LoadSegments()
    }
    
    Component <|-- Proxy
    Component <|-- RootCoord
    Component <|-- DataCoord
    Component <|-- QueryCoord
    Component <|-- DataNode
    Component <|-- QueryNode
    
    class Task {
        <<interface>>
        +ID() UniqueID
        +Type() MsgType
        +Execute() error
    }
    
    class BaseTask {
        -ctx context.Context
        -id UniqueID
        -ts Timestamp
    }
    
    class CreateCollectionTask {
        +Execute() error
        +PreExecute() error
    }
    
    class InsertTask {
        +Execute() error
        +PreExecute() error
    }
    
    class SearchTask {
        +Execute() error
        +PreExecute() error
    }
    
    Task <|-- BaseTask
    BaseTask <|-- CreateCollectionTask
    BaseTask <|-- InsertTask
    BaseTask <|-- SearchTask
```

---

## 6. å®æˆ˜ç»éªŒæ€»ç»“

### 6.1 æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

#### 6.1.1 ç´¢å¼•é€‰æ‹©ç­–ç•¥

**HNSW ç´¢å¼• - é«˜ç²¾åº¦åœºæ™¯**
```yaml
index_params:
  index_type: "HNSW"
  metric_type: "L2"
  params:
    M: 16              # è¿æ¥æ•°ï¼Œå½±å“ç²¾åº¦å’Œå†…å­˜
    efConstruction: 200 # æ„å»ºæ—¶æœç´¢æ·±åº¦
    ef: 64             # æŸ¥è¯¢æ—¶æœç´¢æ·±åº¦
```

**IVF ç´¢å¼• - å¹³è¡¡æ€§èƒ½**
```yaml
index_params:
  index_type: "IVF_FLAT"
  metric_type: "IP"
  params:
    nlist: 1024        # èšç±»ä¸­å¿ƒæ•°é‡
    nprobe: 16         # æŸ¥è¯¢æ—¶æ¢æµ‹çš„èšç±»æ•°
```

**DiskANN ç´¢å¼• - å¤§è§„æ¨¡æ•°æ®**
```yaml
index_params:
  index_type: "DISKANN"
  metric_type: "L2"
  params:
    max_degree: 56     # å›¾çš„æœ€å¤§åº¦æ•°
    search_list_size: 100 # æœç´¢åˆ—è¡¨å¤§å°
```

#### 6.1.2 é›†åˆè®¾è®¡åŸåˆ™

**åˆ†ç‰‡ç­–ç•¥**
```python
# æ ¹æ®æ•°æ®é‡å’ŒæŸ¥è¯¢ QPS ç¡®å®šåˆ†ç‰‡æ•°
def calculate_shard_num(data_size_gb, qps):
    # æ¯ä¸ªåˆ†ç‰‡å»ºè®®å¤„ç† 1-10GB æ•°æ®
    shard_by_size = max(1, data_size_gb // 5)
    
    # æ¯ä¸ªåˆ†ç‰‡å»ºè®®å¤„ç† 100-1000 QPS
    shard_by_qps = max(1, qps // 500)
    
    return min(16, max(shard_by_size, shard_by_qps))

# åˆ›å»ºé›†åˆæ—¶æŒ‡å®šåˆ†ç‰‡æ•°
collection_schema = {
    "collection_name": "my_collection",
    "dimension": 768,
    "shard_num": calculate_shard_num(100, 2000)  # 4 ä¸ªåˆ†ç‰‡
}
```

**å­—æ®µè®¾è®¡**
```python
# åˆç†è®¾è®¡ Schema
schema = CollectionSchema([
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=768),
    FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=50),  # ç”¨äºè¿‡æ»¤
    FieldSchema(name="timestamp", dtype=DataType.INT64),  # æ—¶é—´èŒƒå›´æŸ¥è¯¢
    FieldSchema(name="metadata", dtype=DataType.JSON)     # çµæ´»çš„å…ƒæ•°æ®
])
```

#### 6.1.3 æŸ¥è¯¢ä¼˜åŒ–æŠ€å·§

**æ··åˆæœç´¢ä¼˜åŒ–**
```python
# ä½¿ç”¨è¡¨è¾¾å¼è¿‡æ»¤å‡å°‘æœç´¢èŒƒå›´
search_params = {
    "metric_type": "L2",
    "params": {"nprobe": 16},
    "expr": "category in ['tech', 'science'] and timestamp > 1640995200"
}

# æ‰¹é‡æŸ¥è¯¢æé«˜ååé‡
batch_vectors = [vector1, vector2, vector3, ...]
results = collection.search(
    data=batch_vectors,
    anns_field="vector",
    param=search_params,
    limit=10,
    output_fields=["id", "category"]
)
```

### 6.2 è¿ç»´ç›‘æ§è¦ç‚¹

#### 6.2.1 å…³é”®æŒ‡æ ‡ç›‘æ§

**ç³»ç»Ÿçº§æŒ‡æ ‡**
```yaml
# Prometheus ç›‘æ§é…ç½®
metrics:
  - milvus_proxy_search_vectors_count     # æœç´¢å‘é‡æ•°
  - milvus_proxy_insert_vectors_count     # æ’å…¥å‘é‡æ•°
  - milvus_proxy_search_latency_bucket    # æœç´¢å»¶è¿Ÿåˆ†å¸ƒ
  - milvus_querynode_search_latency       # QueryNode æœç´¢å»¶è¿Ÿ
  - milvus_datanode_flush_buffer_count    # DataNode åˆ·ç›˜æ¬¡æ•°
  - milvus_rootcoord_ddl_req_count        # DDL è¯·æ±‚æ•°é‡
```

**ä¸šåŠ¡çº§æŒ‡æ ‡**
```python
# è‡ªå®šä¹‰ç›‘æ§æŒ‡æ ‡
class MilvusMonitor:
    def __init__(self):
        self.search_success_rate = Counter('milvus_search_success_total')
        self.search_error_rate = Counter('milvus_search_error_total')
        self.insert_throughput = Histogram('milvus_insert_throughput')
    
    def record_search(self, success: bool, latency: float):
        if success:
            self.search_success_rate.inc()
        else:
            self.search_error_rate.inc()
    
    def record_insert(self, batch_size: int, duration: float):
        throughput = batch_size / duration
        self.insert_throughput.observe(throughput)
```

#### 6.2.2 æ•…éšœæ’æŸ¥æ‰‹å†Œ

**å¸¸è§é—®é¢˜è¯Šæ–­**

1. **æœç´¢å»¶è¿Ÿè¿‡é«˜**
```bash
# æ£€æŸ¥ QueryNode è´Ÿè½½
kubectl top pods -l component=querynode

# æŸ¥çœ‹ç´¢å¼•æ„å»ºçŠ¶æ€
curl -X GET "http://milvus:9091/api/v1/index/progress?collection_name=my_collection"

# æ£€æŸ¥ Segment åˆ†å¸ƒ
curl -X GET "http://milvus:9091/api/v1/querycoord/segments"
```

2. **æ•°æ®æ’å…¥å¤±è´¥**
```bash
# æ£€æŸ¥ DataNode çŠ¶æ€
kubectl logs -l component=datanode --tail=100

# æŸ¥çœ‹æ¶ˆæ¯é˜Ÿåˆ—ç§¯å‹
kubectl exec -it pulsar-broker-0 -- bin/pulsar-admin topics stats persistent://public/default/milvus-insert

# æ£€æŸ¥å¯¹è±¡å­˜å‚¨è¿æ¥
kubectl exec -it datanode-0 -- curl -I http://minio:9000/minio/health/live
```

3. **å†…å­˜ä½¿ç”¨è¿‡é«˜**
```bash
# æŸ¥çœ‹å„ç»„ä»¶å†…å­˜ä½¿ç”¨
kubectl top pods -l app=milvus

# æ£€æŸ¥ Segment åŠ è½½æƒ…å†µ
curl -X GET "http://milvus:9091/api/v1/querynode/segments/memory"

# è°ƒæ•´å†…å­˜é…ç½®
kubectl patch configmap milvus-config --patch '
data:
  milvus.yaml: |
    queryNode:
      loadMemoryUsageRatio: 0.7  # é™ä½å†…å­˜ä½¿ç”¨æ¯”ä¾‹
'
```

### 6.3 æ‰©å®¹å’Œå®¹é‡è§„åˆ’

#### 6.3.1 æ°´å¹³æ‰©å®¹ç­–ç•¥

**QueryNode æ‰©å®¹**
```yaml
# å¢åŠ  QueryNode å‰¯æœ¬æ•°
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-querynode
spec:
  replicas: 6  # ä» 3 å¢åŠ åˆ° 6
  template:
    spec:
      containers:
      - name: querynode
        resources:
          requests:
            memory: "8Gi"
            cpu: "2"
          limits:
            memory: "16Gi"
            cpu: "4"
```

**DataNode æ‰©å®¹**
```yaml
# DataNode æ‰©å®¹é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-datanode
spec:
  replicas: 4  # å¢åŠ  DataNode æ•°é‡
  template:
    spec:
      containers:
      - name: datanode
        env:
        - name: DATANODE_MEMORY_LIMIT
          value: "32Gi"
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
```

#### 6.3.2 å®¹é‡è§„åˆ’å…¬å¼

**å­˜å‚¨å®¹é‡è®¡ç®—**
```python
def calculate_storage_capacity(
    vector_count: int,
    vector_dim: int,
    replica_count: int = 1,
    compression_ratio: float = 0.3,
    metadata_overhead: float = 0.1
):
    """
    è®¡ç®—å­˜å‚¨å®¹é‡éœ€æ±‚
    
    Args:
        vector_count: å‘é‡æ•°é‡
        vector_dim: å‘é‡ç»´åº¦
        replica_count: å‰¯æœ¬æ•°é‡
        compression_ratio: å‹ç¼©æ¯”ä¾‹
        metadata_overhead: å…ƒæ•°æ®å¼€é”€æ¯”ä¾‹
    """
    # åŸå§‹å‘é‡æ•°æ®å¤§å° (float32 = 4 bytes)
    raw_size_gb = vector_count * vector_dim * 4 / (1024**3)
    
    # è€ƒè™‘å‹ç¼©å’Œå‰¯æœ¬
    compressed_size = raw_size_gb * compression_ratio * replica_count
    
    # ç´¢å¼•å¼€é”€ (é€šå¸¸æ˜¯åŸå§‹æ•°æ®çš„ 1.2-2 å€)
    index_overhead = compressed_size * 1.5
    
    # å…ƒæ•°æ®å¼€é”€
    metadata_size = compressed_size * metadata_overhead
    
    # æ€»å­˜å‚¨éœ€æ±‚
    total_storage = compressed_size + index_overhead + metadata_size
    
    return {
        "raw_data_gb": raw_size_gb,
        "compressed_gb": compressed_size,
        "index_gb": index_overhead,
        "metadata_gb": metadata_size,
        "total_gb": total_storage,
        "recommended_gb": total_storage * 1.3  # 30% ç¼“å†²
    }

# ç¤ºä¾‹è®¡ç®—
capacity = calculate_storage_capacity(
    vector_count=100_000_000,  # 1äº¿å‘é‡
    vector_dim=768,
    replica_count=2,
    compression_ratio=0.25
)
print(f"æ¨èå­˜å‚¨å®¹é‡: {capacity['recommended_gb']:.2f} GB")
```

**å†…å­˜å®¹é‡è§„åˆ’**
```python
def calculate_memory_requirements(
    active_vectors: int,
    vector_dim: int,
    query_qps: int,
    search_topk: int = 100
):
    """
    è®¡ç®—å†…å­˜éœ€æ±‚
    
    Args:
        active_vectors: æ´»è·ƒå‘é‡æ•°é‡ (ç»å¸¸è¢«æŸ¥è¯¢çš„)
        vector_dim: å‘é‡ç»´åº¦
        query_qps: æŸ¥è¯¢ QPS
        search_topk: æœç´¢è¿”å›çš„ top-k ç»“æœæ•°
    """
    # QueryNode å†…å­˜éœ€æ±‚
    # 1. å‘é‡æ•°æ®å†…å­˜
    vector_memory_gb = active_vectors * vector_dim * 4 / (1024**3)
    
    # 2. ç´¢å¼•å†…å­˜ (HNSW çº¦ä¸ºå‘é‡æ•°æ®çš„ 1.5 å€)
    index_memory_gb = vector_memory_gb * 1.5
    
    # 3. æŸ¥è¯¢ç¼“å­˜å†…å­˜
    query_cache_gb = query_qps * search_topk * vector_dim * 4 / (1024**3) * 10  # 10ç§’ç¼“å­˜
    
    # 4. ç³»ç»Ÿå¼€é”€
    system_overhead_gb = (vector_memory_gb + index_memory_gb) * 0.2
    
    total_memory_gb = vector_memory_gb + index_memory_gb + query_cache_gb + system_overhead_gb
    
    return {
        "vector_data_gb": vector_memory_gb,
        "index_gb": index_memory_gb,
        "query_cache_gb": query_cache_gb,
        "system_overhead_gb": system_overhead_gb,
        "total_gb": total_memory_gb,
        "recommended_per_node_gb": total_memory_gb / 3,  # å‡è®¾ 3 ä¸ª QueryNode
    }
```

### 6.4 å®‰å…¨å’Œæƒé™ç®¡ç†

#### 6.4.1 RBAC é…ç½®

**ç”¨æˆ·å’Œè§’è‰²ç®¡ç†**
```python
from pymilvus import connections, utility

# è¿æ¥ç®¡ç†å‘˜è´¦æˆ·
connections.connect(
    alias="admin",
    host="localhost",
    port="19530",
    user="root",
    password="admin_password"
)

# åˆ›å»ºè§’è‰²
utility.create_role("data_scientist", using="admin")
utility.create_role("application_user", using="admin")

# åˆ›å»ºç”¨æˆ·
utility.create_user("alice", "alice_password", using="admin")
utility.create_user("bob", "bob_password", using="admin")

# åˆ†é…è§’è‰²
utility.add_user_to_role("alice", "data_scientist", using="admin")
utility.add_user_to_role("bob", "application_user", using="admin")

# æˆäºˆæƒé™
utility.grant_role_privilege(
    role_name="data_scientist",
    object_type="Collection",
    object_name="research_vectors",
    privilege="Search",
    using="admin"
)

utility.grant_role_privilege(
    role_name="application_user", 
    object_type="Collection",
    object_name="prod_vectors",
    privilege="Search",
    using="admin"
)
```

#### 6.4.2 ç½‘ç»œå®‰å…¨é…ç½®

**TLS åŠ å¯†é…ç½®**
```yaml
# milvus.yaml é…ç½®æ–‡ä»¶
tls:
  serverPemPath: "/etc/milvus/tls/server.pem"
  serverKeyPath: "/etc/milvus/tls/server.key"
  caPemPath: "/etc/milvus/tls/ca.pem"

proxy:
  http:
    enabled: true
    port: 9091
  grpc:
    serverMaxRecvSize: 268435456
    serverMaxSendSize: 268435456
    clientMaxRecvSize: 268435456
    clientMaxSendSize: 268435456

common:
  security:
    authorizationEnabled: true
    tlsMode: 2  # å¼ºåˆ¶ TLS
```

### 6.5 æ•°æ®è¿ç§»å’Œå¤‡ä»½

#### 6.5.1 æ•°æ®å¤‡ä»½ç­–ç•¥

**å…ƒæ•°æ®å¤‡ä»½**
```bash
#!/bin/bash
# å¤‡ä»½ etcd å…ƒæ•°æ®
ETCD_ENDPOINTS="etcd-0:2379,etcd-1:2379,etcd-2:2379"
BACKUP_DIR="/backup/milvus/$(date +%Y%m%d_%H%M%S)"

mkdir -p $BACKUP_DIR

# å¤‡ä»½ etcd æ•°æ®
etcdctl --endpoints=$ETCD_ENDPOINTS snapshot save $BACKUP_DIR/etcd_snapshot.db

# å¤‡ä»½ Milvus é…ç½®
kubectl get configmap milvus-config -o yaml > $BACKUP_DIR/milvus-config.yaml

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
tar -czf $BACKUP_DIR.tar.gz $BACKUP_DIR
rm -rf $BACKUP_DIR

echo "Backup completed: $BACKUP_DIR.tar.gz"
```

**å‘é‡æ•°æ®å¤‡ä»½**
```python
import os
from milvus_backup import MilvusBackup

class VectorDataBackup:
    def __init__(self, milvus_host, backup_storage):
        self.backup = MilvusBackup(
            milvus_endpoint=f"{milvus_host}:19530",
            storage_config={
                "type": "s3",
                "endpoint": backup_storage["endpoint"],
                "access_key": backup_storage["access_key"],
                "secret_key": backup_storage["secret_key"],
                "bucket": backup_storage["bucket"]
            }
        )
    
    def backup_collection(self, collection_name, backup_name=None):
        if not backup_name:
            backup_name = f"{collection_name}_{int(time.time())}"
        
        # åˆ›å»ºå¤‡ä»½
        job_id = self.backup.create_backup(
            backup_name=backup_name,
            collection_names=[collection_name]
        )
        
        # ç­‰å¾…å¤‡ä»½å®Œæˆ
        while True:
            status = self.backup.get_backup_status(job_id)
            if status["state"] == "Completed":
                break
            elif status["state"] == "Failed":
                raise Exception(f"Backup failed: {status['error']}")
            time.sleep(10)
        
        return backup_name
    
    def restore_collection(self, backup_name, target_collection=None):
        # æ¢å¤æ•°æ®
        job_id = self.backup.restore_backup(
            backup_name=backup_name,
            target_collection=target_collection
        )
        
        # ç›‘æ§æ¢å¤è¿›åº¦
        while True:
            status = self.backup.get_restore_status(job_id)
            if status["state"] == "Completed":
                break
            elif status["state"] == "Failed":
                raise Exception(f"Restore failed: {status['error']}")
            time.sleep(10)
        
        return True
```

#### 6.5.2 æ•°æ®è¿ç§»å·¥å…·

**è·¨é›†ç¾¤è¿ç§»**
```python
class MilvusMigration:
    def __init__(self, source_config, target_config):
        self.source = MilvusClient(**source_config)
        self.target = MilvusClient(**target_config)
    
    def migrate_collection(self, collection_name, batch_size=1000):
        # 1. è·å–æºé›†åˆä¿¡æ¯
        source_info = self.source.describe_collection(collection_name)
        
        # 2. åœ¨ç›®æ ‡åˆ›å»ºé›†åˆ
        self.target.create_collection(
            collection_name=collection_name,
            schema=source_info["schema"],
            index_params=source_info["index_params"]
        )
        
        # 3. åˆ†æ‰¹è¿ç§»æ•°æ®
        offset = 0
        while True:
            # ä»æºè¯»å–æ•°æ®
            results = self.source.query(
                collection_name=collection_name,
                expr="",
                limit=batch_size,
                offset=offset,
                output_fields=["*"]
            )
            
            if not results:
                break
            
            # å†™å…¥ç›®æ ‡
            self.target.insert(
                collection_name=collection_name,
                data=results
            )
            
            offset += batch_size
            print(f"Migrated {offset} records")
        
        # 4. æ„å»ºç´¢å¼•
        self.target.create_index(
            collection_name=collection_name,
            field_name="vector",
            index_params=source_info["index_params"]
        )
        
        print(f"Migration completed for {collection_name}")

# ä½¿ç”¨ç¤ºä¾‹
migration = MilvusMigration(
    source_config={
        "uri": "http://source-milvus:19530",
        "token": "source_credentials"
    },
    target_config={
        "uri": "http://target-milvus:19530", 
        "token": "target_credentials"
    }
)

migration.migrate_collection("my_vectors")
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£æ·±å…¥å‰–æäº† Milvus å‘é‡æ•°æ®åº“çš„æºç æ¶æ„ï¼Œæ¶µç›–äº†ä»æ¡†æ¶ä½¿ç”¨åˆ°æ ¸å¿ƒæ¨¡å—å®ç°çš„å„ä¸ªå±‚é¢ã€‚é€šè¿‡è¯¦ç»†çš„ä»£ç åˆ†æã€æ¶æ„å›¾è§£å’Œå®æˆ˜ç»éªŒåˆ†äº«ï¼Œå¸®åŠ©å¼€å‘è€…å…¨é¢ç†è§£ Milvus çš„å†…éƒ¨æœºåˆ¶ã€‚

**æ ¸å¿ƒè¦ç‚¹å›é¡¾ï¼š**

1. **æ¶æ„è®¾è®¡**ï¼šMilvus é‡‡ç”¨äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„ï¼Œå®ç°å­˜å‚¨è®¡ç®—åˆ†ç¦»ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•
2. **API è®¾è®¡**ï¼šé€šè¿‡ Proxy æä¾›ç»Ÿä¸€çš„ gRPC/HTTP æ¥å£ï¼Œä½¿ç”¨æ‹¦æˆªå™¨å®ç°æ¨ªåˆ‡å…³æ³¨ç‚¹
3. **æ•°æ®æµ**ï¼šæ’å…¥æ•°æ®é€šè¿‡æ¶ˆæ¯é˜Ÿåˆ—å¼‚æ­¥å¤„ç†ï¼ŒæŸ¥è¯¢è¯·æ±‚å¹¶è¡Œåˆ†å‘åˆ°å¤šä¸ªèŠ‚ç‚¹
4. **å­˜å‚¨å¼•æ“**ï¼šæ”¯æŒå¤šç§å‘é‡ç´¢å¼•ç®—æ³•ï¼Œé’ˆå¯¹ä¸åŒåœºæ™¯ä¼˜åŒ–æ€§èƒ½
5. **è¿ç»´ç›‘æ§**ï¼šæä¾›å®Œæ•´çš„ç›‘æ§æŒ‡æ ‡å’Œæ•…éšœæ’æŸ¥å·¥å…·

**æœ€ä½³å®è·µå»ºè®®ï¼š**

- æ ¹æ®æ•°æ®è§„æ¨¡å’ŒæŸ¥è¯¢æ¨¡å¼é€‰æ‹©åˆé€‚çš„ç´¢å¼•ç±»å‹
- åˆç†è®¾è®¡é›†åˆ Schema å’Œåˆ†ç‰‡ç­–ç•¥
- å»ºç«‹å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶
- åˆ¶å®šæ•°æ®å¤‡ä»½å’Œè¿ç§»æ–¹æ¡ˆ
- é…ç½®é€‚å½“çš„å®‰å…¨å’Œæƒé™æ§åˆ¶

å¸Œæœ›æœ¬æ–‡æ¡£èƒ½å¤Ÿå¸®åŠ©æ‚¨æ·±å…¥ç†è§£ Milvus çš„æŠ€æœ¯å†…å¹•ï¼Œåœ¨å®é™…é¡¹ç›®ä¸­æ›´å¥½åœ°åº”ç”¨è¿™ä¸€å¼ºå¤§çš„å‘é‡æ•°æ®åº“ç³»ç»Ÿã€‚
