---
title: "VoiceHelperæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ - éƒ¨ç½²ä¸è¿ç»´"
date: "2025-09-22T14:00:00+08:00"
draft: false
description: "VoiceHelperç³»ç»Ÿéƒ¨ç½²ä¸è¿ç»´æŒ‡å—ï¼Œæ¶µç›–å®¹å™¨åŒ–éƒ¨ç½²ã€CI/CDæµæ°´çº¿ã€è¿ç»´è‡ªåŠ¨åŒ–ç­‰å…³é”®æŠ€æœ¯"
slug: "voicehelper-deployment-operations"
author: "tommie blog"
categories: ["voicehelper", "AI", "è¿ç»´éƒ¨ç½²"]
tags: ["VoiceHelper", "Docker", "Kubernetes", "CI/CD", "è¿ç»´è‡ªåŠ¨åŒ–", "éƒ¨ç½²"]
showComments: false
toc: true
tocOpen: false
showReadingTime: true
showWordCount: true
pinned: true
weight: 10
# æ€§èƒ½ä¼˜åŒ–é…ç½®
paginated: true
lazyLoad: true
performanceOptimized: true
---

# VoiceHelperéƒ¨ç½²ä¸è¿ç»´

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»VoiceHelperæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ç³»ç»Ÿçš„éƒ¨ç½²ä¸è¿ç»´æ–¹æ¡ˆï¼Œæ¶µç›–å®¹å™¨åŒ–éƒ¨ç½²ã€CI/CDæµæ°´çº¿ã€è¿ç»´è‡ªåŠ¨åŒ–ç­‰å…³é”®æŠ€æœ¯ã€‚

## 9. éƒ¨ç½²ä¸è¿ç»´

### 9.1 å®¹å™¨åŒ–éƒ¨ç½²

#### 9.1.0 éƒ¨ç½²æ¶æ„æ€»è§ˆ

```mermaid
graph TB
    subgraph "å¼€å‘ç¯å¢ƒ"
        Dev[å¼€å‘è€…]
        Git[Gitä»“åº“]
        IDE[å¼€å‘å·¥å…·]
    end
    
    subgraph "CI/CDæµæ°´çº¿"
        CI[æŒç»­é›†æˆ]
        Build[é•œåƒæ„å»º]
        Test[è‡ªåŠ¨åŒ–æµ‹è¯•]
        Deploy[è‡ªåŠ¨éƒ¨ç½²]
    end
    
    subgraph "å®¹å™¨åŒ–å¹³å°"
        Docker[Dockerå®¹å™¨]
        K8s[Kubernetesé›†ç¾¤]
        Helm[HelmåŒ…ç®¡ç†]
        Registry[é•œåƒä»“åº“]
    end
    
    subgraph "ç”Ÿäº§ç¯å¢ƒ"
        LB[è´Ÿè½½å‡è¡¡å™¨]
        Frontend[å‰ç«¯æœåŠ¡]
        Backend[åç«¯æœåŠ¡]
        AI[AIæœåŠ¡]
        DB[(æ•°æ®åº“é›†ç¾¤)]
    end
    
    subgraph "ç›‘æ§è¿ç»´"
        Monitor[ç›‘æ§ç³»ç»Ÿ]
        Log[æ—¥å¿—ç³»ç»Ÿ]
        Alert[å‘Šè­¦ç³»ç»Ÿ]
        Backup[å¤‡ä»½ç³»ç»Ÿ]
    end
    
    Dev --> Git
    Git --> CI
    CI --> Build
    Build --> Test
    Test --> Deploy
    
    Build --> Registry
    Deploy --> K8s
    K8s --> Helm
    
    K8s --> LB
    LB --> Frontend
    LB --> Backend
    LB --> AI
    Backend --> DB
    
    K8s --> Monitor
    K8s --> Log
    Monitor --> Alert
    DB --> Backup
    
    style CI fill:#e1f5fe
    style K8s fill:#f3e5f5
    style LB fill:#fff3e0
    style Monitor fill:#e8f5e8
```
  </div>
</div>

#### 9.1.1 Dockerå®¹å™¨åŒ–é…ç½®

```dockerfile
# åç«¯æœåŠ¡Dockerfile
# æ–‡ä»¶è·¯å¾„: backend/Dockerfile
FROM golang:1.21-alpine AS builder

WORKDIR /app

# å®‰è£…ä¾èµ–
RUN apk add --no-cache git ca-certificates tzdata

# å¤åˆ¶go modæ–‡ä»¶
COPY go.mod go.sum ./
RUN go mod download

# å¤åˆ¶æºä»£ç 
COPY . .

# æ„å»ºåº”ç”¨
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main ./cmd/server

# è¿è¡Œé˜¶æ®µ
FROM alpine:latest

RUN apk --no-cache add ca-certificates tzdata
WORKDIR /root/

# å¤åˆ¶æ„å»ºçš„äºŒè¿›åˆ¶æ–‡ä»¶
COPY --from=builder /app/main .
COPY --from=builder /app/configs ./configs

# è®¾ç½®æ—¶åŒº
ENV TZ=Asia/Shanghai

# æš´éœ²ç«¯å£
EXPOSE 8080

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8080/health || exit 1

# å¯åŠ¨åº”ç”¨
CMD ["./main"]
```

```dockerfile
# å‰ç«¯åº”ç”¨Dockerfile
# æ–‡ä»¶è·¯å¾„: frontend/Dockerfile
FROM node:18-alpine AS builder

WORKDIR /app

# å¤åˆ¶packageæ–‡ä»¶
COPY package*.json ./
RUN npm ci --only=production

# å¤åˆ¶æºä»£ç 
COPY . .

# æ„å»ºåº”ç”¨
RUN npm run build

# è¿è¡Œé˜¶æ®µ
FROM nginx:alpine

# å¤åˆ¶æ„å»ºæ–‡ä»¶
COPY --from=builder /app/out /usr/share/nginx/html

# å¤åˆ¶nginxé…ç½®
COPY nginx.conf /etc/nginx/nginx.conf

# æš´éœ²ç«¯å£
EXPOSE 80

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost/health || exit 1

CMD ["nginx", "-g", "daemon off;"]
```

```dockerfile
# AIæœåŠ¡Dockerfile
# æ–‡ä»¶è·¯å¾„: algo/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶requirementsæ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶æºä»£ç 
COPY . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨åº”ç”¨
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### 9.1.2 Docker Composeé…ç½®

```yaml
# Docker Composeé…ç½®
# æ–‡ä»¶è·¯å¾„: docker-compose.yml
version: '3.8'

services:
  # å‰ç«¯æœåŠ¡
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    environment:
      - NODE_ENV=production
    depends_on:
      - backend
    networks:
      - voicehelper-network
    restart: unless-stopped

  # åç«¯æœåŠ¡
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - GO_ENV=production
      - DATABASE_URL=postgres://user:password@postgres:5432/voicehelper
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - redis
    networks:
      - voicehelper-network
    restart: unless-stopped
    volumes:
      - ./configs:/root/configs:ro

  # AIæœåŠ¡
  ai-service:
    build:
      context: ./algo
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - PYTHON_ENV=production
      - MILVUS_HOST=milvus
      - MILVUS_PORT=19530
    depends_on:
      - milvus
    networks:
      - voicehelper-network
    restart: unless-stopped
    volumes:
      - ./models:/app/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # PostgreSQLæ•°æ®åº“
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=voicehelper
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    networks:
      - voicehelper-network
    restart: unless-stopped
    ports:
      - "5432:5432"

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --requirepass redispassword
    volumes:
      - redis_data:/data
    networks:
      - voicehelper-network
    restart: unless-stopped
    ports:
      - "6379:6379"

  # Milvuså‘é‡æ•°æ®åº“
  milvus:
    image: milvusdb/milvus:v2.3.4
    command: ["milvus", "run", "standalone"]
    environment:
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=minio:9000
    volumes:
      - milvus_data:/var/lib/milvus
    depends_on:
      - etcd
      - minio
    networks:
      - voicehelper-network
    restart: unless-stopped
    ports:
      - "19530:19530"

  # Etcd (Milvusä¾èµ–)
  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    volumes:
      - etcd_data:/etcd
    networks:
      - voicehelper-network
    restart: unless-stopped

  # MinIOå¯¹è±¡å­˜å‚¨
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin123
    volumes:
      - minio_data:/data
    networks:
      - voicehelper-network
    restart: unless-stopped
    ports:
      - "9000:9000"
      - "9001:9001"

  # Neo4jå›¾æ•°æ®åº“
  neo4j:
    image: neo4j:5.0
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_PLUGINS=["apoc"]
    volumes:
      - neo4j_data:/data
    networks:
      - voicehelper-network
    restart: unless-stopped
    ports:
      - "7474:7474"
      - "7687:7687"

  # Nginxè´Ÿè½½å‡è¡¡
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - frontend
      - backend
    networks:
      - voicehelper-network
    restart: unless-stopped

  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
      - prometheus_data:/prometheus
    networks:
      - voicehelper-network
    restart: unless-stopped

  # Grafanaå¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - voicehelper-network
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  milvus_data:
  etcd_data:
  minio_data:
  neo4j_data:
  prometheus_data:
  grafana_data:

networks:
  voicehelper-network:
    driver: bridge
```

### 9.2 Kuberneteséƒ¨ç½²

#### 9.2.1 Kuberneteséƒ¨ç½²é…ç½®

```yaml
# Namespaceé…ç½®
# æ–‡ä»¶è·¯å¾„: k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: voicehelper
  labels:
    name: voicehelper
---
# ConfigMapé…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: voicehelper-config
  namespace: voicehelper
data:
  database.url: "postgres://user:password@postgres:5432/voicehelper"
  redis.url: "redis://redis:6379"
  milvus.host: "milvus"
  milvus.port: "19530"
---
# Secreté…ç½®
apiVersion: v1
kind: Secret
metadata:
  name: voicehelper-secrets
  namespace: voicehelper
type: Opaque
data:
  database-password: cGFzc3dvcmQ=  # base64 encoded
  redis-password: cmVkaXNwYXNzd29yZA==
  jwt-secret: and0c2VjcmV0a2V5
```

```yaml
# åç«¯æœåŠ¡éƒ¨ç½²
# æ–‡ä»¶è·¯å¾„: k8s/backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: voicehelper-backend
  namespace: voicehelper
  labels:
    app: voicehelper-backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: voicehelper-backend
  template:
    metadata:
      labels:
        app: voicehelper-backend
    spec:
      containers:
      - name: backend
        image: voicehelper/backend:latest
        ports:
        - containerPort: 8080
        env:
        - name: GO_ENV
          value: "production"
        - name: DATABASE_URL
          valueFrom:
            configMapKeyRef:
              name: voicehelper-config
              key: database.url
        - name: REDIS_URL
          valueFrom:
            configMapKeyRef:
              name: voicehelper-config
              key: redis.url
        - name: JWT_SECRET
          valueFrom:
            secretKeyRef:
              name: voicehelper-secrets
              key: jwt-secret
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: config-volume
          mountPath: /root/configs
          readOnly: true
      volumes:
      - name: config-volume
        configMap:
          name: voicehelper-config
---
apiVersion: v1
kind: Service
metadata:
  name: voicehelper-backend-service
  namespace: voicehelper
spec:
  selector:
    app: voicehelper-backend
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
  type: ClusterIP
```

```yaml
# AIæœåŠ¡éƒ¨ç½²
# æ–‡ä»¶è·¯å¾„: k8s/ai-service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: voicehelper-ai-service
  namespace: voicehelper
  labels:
    app: voicehelper-ai-service
spec:
  replicas: 2
  selector:
    matchLabels:
      app: voicehelper-ai-service
  template:
    metadata:
      labels:
        app: voicehelper-ai-service
    spec:
      containers:
      - name: ai-service
        image: voicehelper/ai-service:latest
        ports:
        - containerPort: 8000
        env:
        - name: PYTHON_ENV
          value: "production"
        - name: MILVUS_HOST
          valueFrom:
            configMapKeyRef:
              name: voicehelper-config
              key: milvus.host
        - name: MILVUS_PORT
          valueFrom:
            configMapKeyRef:
              name: voicehelper-config
              key: milvus.port
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
            nvidia.com/gpu: 1
          limits:
            memory: "2Gi"
            cpu: "1000m"
            nvidia.com/gpu: 1
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        volumeMounts:
        - name: model-volume
          mountPath: /app/models
          readOnly: true
      volumes:
      - name: model-volume
        persistentVolumeClaim:
          claimName: model-storage-pvc
      nodeSelector:
        accelerator: nvidia-tesla-v100
---
apiVersion: v1
kind: Service
metadata:
  name: voicehelper-ai-service
  namespace: voicehelper
spec:
  selector:
    app: voicehelper-ai-service
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 8000
  type: ClusterIP
```

```yaml
# æ•°æ®åº“éƒ¨ç½²
# æ–‡ä»¶è·¯å¾„: k8s/database-deployment.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: voicehelper
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15-alpine
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          value: voicehelper
        - name: POSTGRES_USER
          value: user
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: voicehelper-secrets
              key: database-password
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        - name: init-script
          mountPath: /docker-entrypoint-initdb.d
      volumes:
      - name: init-script
        configMap:
          name: postgres-init-script
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 20Gi
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: voicehelper
spec:
  selector:
    app: postgres
  ports:
  - protocol: TCP
    port: 5432
    targetPort: 5432
  type: ClusterIP
```

#### 9.2.2 Helm Charté…ç½®

```yaml
# Helm Charté…ç½®
# æ–‡ä»¶è·¯å¾„: helm/voicehelper/Chart.yaml
apiVersion: v2
name: voicehelper
description: VoiceHelperæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ç³»ç»Ÿ
type: application
version: 1.0.0
appVersion: "1.0.0"
dependencies:
  - name: postgresql
    version: 12.1.9
    repository: https://charts.bitnami.com/bitnami
  - name: redis
    version: 17.4.3
    repository: https://charts.bitnami.com/bitnami
  - name: prometheus
    version: 15.18.0
    repository: https://prometheus-community.github.io/helm-charts
  - name: grafana
    version: 6.44.11
    repository: https://grafana.github.io/helm-charts
```

```yaml
# Helm Valuesé…ç½®
# æ–‡ä»¶è·¯å¾„: helm/voicehelper/values.yaml
global:
  imageRegistry: ""
  imagePullSecrets: []

replicaCount:
  backend: 3
  frontend: 2
  aiService: 2

image:
  backend:
    repository: voicehelper/backend
    tag: "latest"
    pullPolicy: IfNotPresent
  frontend:
    repository: voicehelper/frontend
    tag: "latest"
    pullPolicy: IfNotPresent
  aiService:
    repository: voicehelper/ai-service
    tag: "latest"
    pullPolicy: IfNotPresent

service:
  type: ClusterIP
  backend:
    port: 8080
  frontend:
    port: 80
  aiService:
    port: 8000

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "100"
  hosts:
    - host: api.voicehelper.com
      paths:
        - path: /
          pathType: Prefix
          service: backend
    - host: voicehelper.com
      paths:
        - path: /
          pathType: Prefix
          service: frontend
  tls:
    - secretName: voicehelper-tls
      hosts:
        - voicehelper.com
        - api.voicehelper.com

resources:
  backend:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 256Mi
  frontend:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  aiService:
    limits:
      cpu: 1000m
      memory: 2Gi
      nvidia.com/gpu: 1
    requests:
      cpu: 500m
      memory: 1Gi
      nvidia.com/gpu: 1

autoscaling:
  enabled: true
  backend:
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  aiService:
    minReplicas: 2
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80

nodeSelector:
  aiService:
    accelerator: nvidia-tesla-v100

tolerations: []

affinity:
  backend:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - voicehelper-backend
          topologyKey: kubernetes.io/hostname

# å¤–éƒ¨ä¾èµ–é…ç½®
postgresql:
  enabled: true
  auth:
    postgresPassword: "password"
    database: "voicehelper"
  primary:
    persistence:
      enabled: true
      size: 20Gi

redis:
  enabled: true
  auth:
    enabled: true
    password: "redispassword"
  master:
    persistence:
      enabled: true
      size: 8Gi

prometheus:
  enabled: true
  server:
    persistentVolume:
      size: 20Gi

grafana:
  enabled: true
  persistence:
    enabled: true
    size: 10Gi
  adminPassword: "admin123"
```

### 9.3 CI/CDæµæ°´çº¿

#### 9.3.0 CI/CDæµæ°´çº¿æ¶æ„å›¾

```mermaid
sequenceDiagram
    participant Dev as å¼€å‘è€…
    participant Git as Gitä»“åº“
    participant CI as CIç³»ç»Ÿ
    participant Build as æ„å»ºç³»ç»Ÿ
    participant Test as æµ‹è¯•ç³»ç»Ÿ
    participant Security as å®‰å…¨æ‰«æ
    participant Registry as é•œåƒä»“åº“
    participant Staging as æµ‹è¯•ç¯å¢ƒ
    participant Prod as ç”Ÿäº§ç¯å¢ƒ
    participant Monitor as ç›‘æ§ç³»ç»Ÿ

    Dev->>Git: æ¨é€ä»£ç 
    Git->>CI: è§¦å‘æµæ°´çº¿
    
    par å¹¶è¡Œæ‰§è¡Œ
        CI->>Build: æ„å»ºåç«¯æœåŠ¡
        CI->>Build: æ„å»ºå‰ç«¯åº”ç”¨
        CI->>Build: æ„å»ºAIæœåŠ¡
    end
    
    par å¹¶è¡Œæµ‹è¯•
        Build->>Test: å•å…ƒæµ‹è¯•
        Build->>Test: é›†æˆæµ‹è¯•
        Build->>Security: å®‰å…¨æ‰«æ
    end
    
    alt æµ‹è¯•é€šè¿‡
        Test->>Build: æµ‹è¯•æˆåŠŸ
        Security->>Build: å®‰å…¨æ£€æŸ¥é€šè¿‡
        
        Build->>Registry: æ¨é€é•œåƒ
        Registry->>Staging: éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
        
        Staging->>Test: å†’çƒŸæµ‹è¯•
        Test->>Staging: æµ‹è¯•é€šè¿‡
        
        alt ç”Ÿäº§éƒ¨ç½²
            Staging->>Prod: éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
            Prod->>Monitor: å¯åŠ¨ç›‘æ§
            Monitor->>Dev: éƒ¨ç½²æˆåŠŸé€šçŸ¥
        end
        
    else æµ‹è¯•å¤±è´¥
        Test->>Dev: å‘é€å¤±è´¥é€šçŸ¥
        Security->>Dev: å‘é€å®‰å…¨é—®é¢˜é€šçŸ¥
    end
```
  </div>
</div>

#### 9.3.1 GitHub Actionsé…ç½®

```yaml
# GitHub Actionså·¥ä½œæµ
# æ–‡ä»¶è·¯å¾„: .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v4

    - name: Set up Go
      uses: actions/setup-go@v4
      with:
        go-version: '1.21'

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    # åç«¯æµ‹è¯•
    - name: Test Backend
      run: |
        cd backend
        go mod download
        go test -v -race -coverprofile=coverage.out ./...
        go tool cover -html=coverage.out -o coverage.html

    # å‰ç«¯æµ‹è¯•
    - name: Test Frontend
      run: |
        cd frontend
        npm ci
        npm run test:ci
        npm run build

    # AIæœåŠ¡æµ‹è¯•
    - name: Test AI Service
      run: |
        cd algo
        pip install -r requirements.txt
        python -m pytest tests/ -v --cov=./ --cov-report=xml

    # ä¸Šä¼ æµ‹è¯•è¦†ç›–ç‡
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        files: ./backend/coverage.out,./algo/coverage.xml

  security-scan:
    runs-on: ubuntu-latest
    needs: test
    steps:
    - uses: actions/checkout@v4

    # Goå®‰å…¨æ‰«æ
    - name: Run Gosec Security Scanner
      uses: securecodewarrior/github-action-gosec@master
      with:
        args: './backend/...'

    # Node.jså®‰å…¨æ‰«æ
    - name: Run npm audit
      run: |
        cd frontend
        npm audit --audit-level moderate

    # Pythonå®‰å…¨æ‰«æ
    - name: Run Safety
      run: |
        cd algo
        pip install safety
        safety check -r requirements.txt

    # Dockeré•œåƒå®‰å…¨æ‰«æ
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'voicehelper/backend:latest'
        format: 'sarif'
        output: 'trivy-results.sarif'

  build-and-push:
    runs-on: ubuntu-latest
    needs: [test, security-scan]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    strategy:
      matrix:
        service: [backend, frontend, ai-service]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: ./${{ matrix.service }}
        file: ./${{ matrix.service }}/Dockerfile
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-staging:
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.ref == 'refs/heads/develop'
    environment: staging

    steps:
    - uses: actions/checkout@v4

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG_STAGING }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig

    - name: Deploy to staging
      run: |
        helm upgrade --install voicehelper-staging ./helm/voicehelper \
          --namespace voicehelper-staging \
          --create-namespace \
          --set image.backend.tag=${{ github.sha }} \
          --set image.frontend.tag=${{ github.sha }} \
          --set image.aiService.tag=${{ github.sha }} \
          --values ./helm/voicehelper/values-staging.yaml

  deploy-production:
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.ref == 'refs/heads/main'
    environment: production

    steps:
    - uses: actions/checkout@v4

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG_PRODUCTION }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig

    - name: Deploy to production
      run: |
        helm upgrade --install voicehelper ./helm/voicehelper \
          --namespace voicehelper \
          --create-namespace \
          --set image.backend.tag=${{ github.sha }} \
          --set image.frontend.tag=${{ github.sha }} \
          --set image.aiService.tag=${{ github.sha }} \
          --values ./helm/voicehelper/values-production.yaml

    - name: Run smoke tests
      run: |
        kubectl wait --for=condition=ready pod -l app=voicehelper-backend -n voicehelper --timeout=300s
        kubectl run smoke-test --rm -i --restart=Never --image=curlimages/curl -- \
          curl -f http://voicehelper-backend-service:8080/health

  notify:
    runs-on: ubuntu-latest
    needs: [deploy-staging, deploy-production]
    if: always()
    
    steps:
    - name: Notify deployment status
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#deployments'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        fields: repo,message,commit,author,action,eventName,ref,workflow
```

#### 9.3.2 GitLab CI/CDé…ç½®

```yaml
# GitLab CI/CDé…ç½®
# æ–‡ä»¶è·¯å¾„: .gitlab-ci.yml
stages:
  - test
  - security
  - build
  - deploy-staging
  - deploy-production

variables:
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"
  REGISTRY: $CI_REGISTRY
  IMAGE_TAG: $CI_COMMIT_SHA

# æµ‹è¯•é˜¶æ®µ
test-backend:
  stage: test
  image: golang:1.21
  services:
    - postgres:15
    - redis:7
  variables:
    POSTGRES_DB: test
    POSTGRES_USER: postgres
    POSTGRES_PASSWORD: postgres
    REDIS_URL: redis://redis:6379
  script:
    - cd backend
    - go mod download
    - go test -v -race -coverprofile=coverage.out ./...
    - go tool cover -func=coverage.out
  coverage: '/total:\s+\(statements\)\s+(\d+\.\d+\%)/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: backend/coverage.xml

test-frontend:
  stage: test
  image: node:18
  script:
    - cd frontend
    - npm ci
    - npm run test:ci
    - npm run build
  artifacts:
    reports:
      junit: frontend/junit.xml
      coverage_report:
        coverage_format: cobertura
        path: frontend/coverage/cobertura-coverage.xml

test-ai-service:
  stage: test
  image: python:3.11
  script:
    - cd algo
    - pip install -r requirements.txt
    - python -m pytest tests/ -v --cov=./ --cov-report=xml --junitxml=junit.xml
  artifacts:
    reports:
      junit: algo/junit.xml
      coverage_report:
        coverage_format: cobertura
        path: algo/coverage.xml

# å®‰å…¨æ‰«æ
security-scan:
  stage: security
  image: docker:latest
  services:
    - docker:dind
  script:
    - docker run --rm -v "$PWD:/src" securecodewarrior/gosec /src/backend/...
    - docker run --rm -v "$PWD:/src" aquasec/trivy fs --security-checks vuln /src
  allow_failure: true

# æ„å»ºé•œåƒ
.build-template: &build-template
  stage: build
  image: docker:latest
  services:
    - docker:dind
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    - docker build -t $REGISTRY/$CI_PROJECT_PATH/$SERVICE:$IMAGE_TAG ./$SERVICE
    - docker push $REGISTRY/$CI_PROJECT_PATH/$SERVICE:$IMAGE_TAG
    - docker tag $REGISTRY/$CI_PROJECT_PATH/$SERVICE:$IMAGE_TAG $REGISTRY/$CI_PROJECT_PATH/$SERVICE:latest
    - docker push $REGISTRY/$CI_PROJECT_PATH/$SERVICE:latest
  only:
    - main
    - develop

build-backend:
  <<: *build-template
  variables:
    SERVICE: backend

build-frontend:
  <<: *build-template
  variables:
    SERVICE: frontend

build-ai-service:
  <<: *build-template
  variables:
    SERVICE: ai-service

# éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
deploy-staging:
  stage: deploy-staging
  image: bitnami/kubectl:latest
  environment:
    name: staging
    url: https://staging.voicehelper.com
  script:
    - kubectl config use-context $KUBE_CONTEXT_STAGING
    - helm upgrade --install voicehelper-staging ./helm/voicehelper
        --namespace voicehelper-staging
        --create-namespace
        --set image.backend.tag=$IMAGE_TAG
        --set image.frontend.tag=$IMAGE_TAG
        --set image.aiService.tag=$IMAGE_TAG
        --values ./helm/voicehelper/values-staging.yaml
  only:
    - develop

# éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
deploy-production:
  stage: deploy-production
  image: bitnami/kubectl:latest
  environment:
    name: production
    url: https://voicehelper.com
  script:
    - kubectl config use-context $KUBE_CONTEXT_PRODUCTION
    - helm upgrade --install voicehelper ./helm/voicehelper
        --namespace voicehelper
        --create-namespace
        --set image.backend.tag=$IMAGE_TAG
        --set image.frontend.tag=$IMAGE_TAG
        --set image.aiService.tag=$IMAGE_TAG
        --values ./helm/voicehelper/values-production.yaml
  when: manual
  only:
    - main
```

### 9.4 è¿ç»´è‡ªåŠ¨åŒ–

#### 9.4.1 è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

```bash
#!/bin/bash
# è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬
# æ–‡ä»¶è·¯å¾„: scripts/deploy.sh

set -e

# é…ç½®å˜é‡
ENVIRONMENT=${1:-staging}
VERSION=${2:-latest}
NAMESPACE="voicehelper-${ENVIRONMENT}"

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log_info "æ£€æŸ¥ä¾èµ–..."
    
    if ! command -v kubectl &> /dev/null; then
        log_error "kubectlæœªå®‰è£…"
        exit 1
    fi
    
    if ! command -v helm &> /dev/null; then
        log_error "helmæœªå®‰è£…"
        exit 1
    fi
    
    if ! kubectl cluster-info &> /dev/null; then
        log_error "æ— æ³•è¿æ¥åˆ°Kubernetesé›†ç¾¤"
        exit 1
    fi
    
    log_info "ä¾èµ–æ£€æŸ¥å®Œæˆ"
}

# é¢„éƒ¨ç½²æ£€æŸ¥
pre_deploy_check() {
    log_info "æ‰§è¡Œé¢„éƒ¨ç½²æ£€æŸ¥..."
    
    # æ£€æŸ¥é•œåƒæ˜¯å¦å­˜åœ¨
    local images=("backend" "frontend" "ai-service")
    for image in "${images[@]}"; do
        if ! docker manifest inspect "ghcr.io/voicehelper/${image}:${VERSION}" &> /dev/null; then
            log_error "é•œåƒ ghcr.io/voicehelper/${image}:${VERSION} ä¸å­˜åœ¨"
            exit 1
        fi
    done
    
    # æ£€æŸ¥å‘½åç©ºé—´
    if ! kubectl get namespace "${NAMESPACE}" &> /dev/null; then
        log_info "åˆ›å»ºå‘½åç©ºé—´ ${NAMESPACE}"
        kubectl create namespace "${NAMESPACE}"
    fi
    
    log_info "é¢„éƒ¨ç½²æ£€æŸ¥å®Œæˆ"
}

# å¤‡ä»½å½“å‰éƒ¨ç½²
backup_current_deployment() {
    log_info "å¤‡ä»½å½“å‰éƒ¨ç½²..."
    
    local backup_dir="backups/$(date +%Y%m%d_%H%M%S)"
    mkdir -p "${backup_dir}"
    
    # å¤‡ä»½Helm release
    helm get values voicehelper -n "${NAMESPACE}" > "${backup_dir}/values.yaml" 2>/dev/null || true
    helm get manifest voicehelper -n "${NAMESPACE}" > "${backup_dir}/manifest.yaml" 2>/dev/null || true
    
    # å¤‡ä»½æ•°æ®åº“
    if kubectl get pod -n "${NAMESPACE}" -l app=postgres &> /dev/null; then
        log_info "å¤‡ä»½æ•°æ®åº“..."
        kubectl exec -n "${NAMESPACE}" -it deployment/postgres -- pg_dump -U user voicehelper > "${backup_dir}/database.sql"
    fi
    
    log_info "å¤‡ä»½å®Œæˆ: ${backup_dir}"
}

# æ‰§è¡Œéƒ¨ç½²
deploy() {
    log_info "å¼€å§‹éƒ¨ç½² VoiceHelper ${VERSION} åˆ° ${ENVIRONMENT} ç¯å¢ƒ..."
    
    # æ›´æ–°Helmä¾èµ–
    helm dependency update ./helm/voicehelper
    
    # æ‰§è¡Œéƒ¨ç½²
    helm upgrade --install voicehelper ./helm/voicehelper \
        --namespace "${NAMESPACE}" \
        --create-namespace \
        --set image.backend.tag="${VERSION}" \
        --set image.frontend.tag="${VERSION}" \
        --set image.aiService.tag="${VERSION}" \
        --values "./helm/voicehelper/values-${ENVIRONMENT}.yaml" \
        --wait \
        --timeout=600s
    
    log_info "éƒ¨ç½²å®Œæˆ"
}

# éƒ¨ç½²åéªŒè¯
post_deploy_verification() {
    log_info "æ‰§è¡Œéƒ¨ç½²åéªŒè¯..."
    
    # ç­‰å¾…Podå°±ç»ª
    log_info "ç­‰å¾…Podå°±ç»ª..."
    kubectl wait --for=condition=ready pod -l app=voicehelper-backend -n "${NAMESPACE}" --timeout=300s
    kubectl wait --for=condition=ready pod -l app=voicehelper-frontend -n "${NAMESPACE}" --timeout=300s
    kubectl wait --for=condition=ready pod -l app=voicehelper-ai-service -n "${NAMESPACE}" --timeout=300s
    
    # å¥åº·æ£€æŸ¥
    log_info "æ‰§è¡Œå¥åº·æ£€æŸ¥..."
    local backend_pod=$(kubectl get pod -n "${NAMESPACE}" -l app=voicehelper-backend -o jsonpath='{.items[0].metadata.name}')
    
    if kubectl exec -n "${NAMESPACE}" "${backend_pod}" -- curl -f http://localhost:8080/health; then
        log_info "åç«¯æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡"
    else
        log_error "åç«¯æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥"
        exit 1
    fi
    
    # è¿è¡ŒçƒŸé›¾æµ‹è¯•
    log_info "è¿è¡ŒçƒŸé›¾æµ‹è¯•..."
    kubectl run smoke-test --rm -i --restart=Never --image=curlimages/curl -n "${NAMESPACE}" -- \
        curl -f "http://voicehelper-backend-service:8080/api/health"
    
    log_info "éƒ¨ç½²éªŒè¯å®Œæˆ"
}

# å›æ»šå‡½æ•°
rollback() {
    log_warn "æ‰§è¡Œå›æ»š..."
    helm rollback voicehelper -n "${NAMESPACE}"
    log_info "å›æ»šå®Œæˆ"
}

# æ¸…ç†å‡½æ•°
cleanup() {
    log_info "æ¸…ç†ä¸´æ—¶èµ„æº..."
    kubectl delete pod smoke-test -n "${NAMESPACE}" --ignore-not-found=true
}

# ä¸»å‡½æ•°
main() {
    log_info "å¼€å§‹éƒ¨ç½²æµç¨‹..."
    
    # è®¾ç½®é”™è¯¯å¤„ç†
    trap 'log_error "éƒ¨ç½²å¤±è´¥ï¼Œæ‰§è¡Œæ¸…ç†..."; cleanup; exit 1' ERR
    trap 'cleanup' EXIT
    
    check_dependencies
    pre_deploy_check
    backup_current_deployment
    deploy
    post_deploy_verification
    
    log_info "éƒ¨ç½²æˆåŠŸå®Œæˆï¼"
}

# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    echo "ç”¨æ³•: $0 [ç¯å¢ƒ] [ç‰ˆæœ¬]"
    echo ""
    echo "å‚æ•°:"
    echo "  ç¯å¢ƒ    éƒ¨ç½²ç¯å¢ƒ (staging|production)ï¼Œé»˜è®¤: staging"
    echo "  ç‰ˆæœ¬    é•œåƒç‰ˆæœ¬æ ‡ç­¾ï¼Œé»˜è®¤: latest"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  $0 staging v1.2.3"
    echo "  $0 production latest"
    echo ""
    echo "é€‰é¡¹:"
    echo "  -h, --help     æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯"
    echo "  -r, --rollback å›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬"
}

# å¤„ç†å‘½ä»¤è¡Œå‚æ•°
case "${1:-}" in
    -h|--help)
        show_help
        exit 0
        ;;
    -r|--rollback)
        rollback
        exit 0
        ;;
    *)
        main
        ;;
esac
```

#### 9.4.2 ç›‘æ§å’Œå‘Šè­¦è‡ªåŠ¨åŒ–

```python
# è‡ªåŠ¨åŒ–ç›‘æ§è„šæœ¬
# æ–‡ä»¶è·¯å¾„: scripts/monitoring_automation.py
import asyncio
import aiohttp
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any
import yaml
import json

class MonitoringAutomation:
    """ç›‘æ§è‡ªåŠ¨åŒ–ç³»ç»Ÿ"""
    
    def __init__(self, config_file: str):
        with open(config_file, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.logger = logging.getLogger(__name__)
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def check_service_health(self) -> Dict[str, bool]:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        health_status = {}
        
        for service in self.config['services']:
            try:
                url = f"{service['url']}/health"
                async with self.session.get(url, timeout=10) as response:
                    health_status[service['name']] = response.status == 200
            except Exception as e:
                self.logger.error(f"Health check failed for {service['name']}: {e}")
                health_status[service['name']] = False
        
        return health_status
    
    async def check_metrics(self) -> Dict[str, Any]:
        """æ£€æŸ¥å…³é”®æŒ‡æ ‡"""
        metrics = {}
        
        # æŸ¥è¯¢PrometheusæŒ‡æ ‡
        prometheus_url = self.config['prometheus']['url']
        queries = self.config['prometheus']['queries']
        
        for metric_name, query in queries.items():
            try:
                url = f"{prometheus_url}/api/v1/query"
                params = {'query': query}
                
                async with self.session.get(url, params=params) as response:
                    data = await response.json()
                    
                    if data['status'] == 'success' and data['data']['result']:
                        value = float(data['data']['result'][0]['value'][1])
                        metrics[metric_name] = value
                    else:
                        metrics[metric_name] = None
                        
            except Exception as e:
                self.logger.error(f"Failed to query metric {metric_name}: {e}")
                metrics[metric_name] = None
        
        return metrics
    
    async def auto_scale_services(self, metrics: Dict[str, Any]):
        """è‡ªåŠ¨æ‰©ç¼©å®¹æœåŠ¡"""
        scaling_rules = self.config['auto_scaling']
        
        for rule in scaling_rules:
            service_name = rule['service']
            metric_name = rule['metric']
            threshold = rule['threshold']
            action = rule['action']
            
            if metric_name in metrics and metrics[metric_name] is not None:
                current_value = metrics[metric_name]
                
                if current_value > threshold:
                    await self._scale_service(service_name, action)
    
    async def _scale_service(self, service_name: str, action: str):
        """æ‰§è¡ŒæœåŠ¡æ‰©ç¼©å®¹"""
        try:
            if action == 'scale_up':
                # æ‰©å®¹é€»è¾‘
                cmd = f"kubectl scale deployment {service_name} --replicas=5 -n voicehelper"
            elif action == 'scale_down':
                # ç¼©å®¹é€»è¾‘
                cmd = f"kubectl scale deployment {service_name} --replicas=2 -n voicehelper"
            else:
                return
            
            import subprocess
            result = subprocess.run(cmd.split(), capture_output=True, text=True)
            
            if result.returncode == 0:
                self.logger.info(f"Successfully scaled {service_name}: {action}")
            else:
                self.logger.error(f"Failed to scale {service_name}: {result.stderr}")
                
        except Exception as e:
            self.logger.error(f"Error scaling service {service_name}: {e}")
    
    async def cleanup_old_data(self):
        """æ¸…ç†æ—§æ•°æ®"""
        cleanup_tasks = self.config['cleanup']
        
        for task in cleanup_tasks:
            if task['type'] == 'database':
                await self._cleanup_database(task)
            elif task['type'] == 'logs':
                await self._cleanup_logs(task)
            elif task['type'] == 'metrics':
                await self._cleanup_metrics(task)
    
    async def _cleanup_database(self, task: Dict[str, Any]):
        """æ¸…ç†æ•°æ®åº“æ•°æ®"""
        try:
            # è¿™é‡Œåº”è¯¥è¿æ¥åˆ°æ•°æ®åº“æ‰§è¡Œæ¸…ç†
            table = task['table']
            retention_days = task['retention_days']
            
            # ç¤ºä¾‹ï¼šåˆ é™¤è¶…è¿‡ä¿ç•™æœŸçš„æ•°æ®
            cutoff_date = datetime.now() - timedelta(days=retention_days)
            
            self.logger.info(f"Cleaning up {table} data older than {cutoff_date}")
            # å®é™…çš„æ•°æ®åº“æ¸…ç†é€»è¾‘
            
        except Exception as e:
            self.logger.error(f"Database cleanup failed: {e}")
    
    async def _cleanup_logs(self, task: Dict[str, Any]):
        """æ¸…ç†æ—¥å¿—æ–‡ä»¶"""
        try:
            import os
            import glob
            
            log_pattern = task['pattern']
            retention_days = task['retention_days']
            
            cutoff_time = datetime.now() - timedelta(days=retention_days)
            
            for log_file in glob.glob(log_pattern):
                file_time = datetime.fromtimestamp(os.path.getmtime(log_file))
                if file_time < cutoff_time:
                    os.remove(log_file)
                    self.logger.info(f"Removed old log file: {log_file}")
                    
        except Exception as e:
            self.logger.error(f"Log cleanup failed: {e}")
    
    async def generate_report(self, health_status: Dict[str, bool], metrics: Dict[str, Any]):
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'health_status': health_status,
            'metrics': metrics,
            'summary': {
                'healthy_services': sum(health_status.values()),
                'total_services': len(health_status),
                'critical_metrics': []
            }
        }
        
        # æ£€æŸ¥å…³é”®æŒ‡æ ‡
        critical_thresholds = self.config['critical_thresholds']
        for metric_name, threshold in critical_thresholds.items():
            if metric_name in metrics and metrics[metric_name] is not None:
                if metrics[metric_name] > threshold:
                    report['summary']['critical_metrics'].append({
                        'metric': metric_name,
                        'value': metrics[metric_name],
                        'threshold': threshold
                    })
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"reports/monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.logger.info(f"Monitoring report saved: {report_file}")
        
        return report
    
    async def run_monitoring_cycle(self):
        """è¿è¡Œä¸€æ¬¡å®Œæ•´çš„ç›‘æ§å‘¨æœŸ"""
        self.logger.info("Starting monitoring cycle...")
        
        try:
            # æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
            health_status = await self.check_service_health()
            
            # æ£€æŸ¥æŒ‡æ ‡
            metrics = await self.check_metrics()
            
            # è‡ªåŠ¨æ‰©ç¼©å®¹
            await self.auto_scale_services(metrics)
            
            # æ¸…ç†æ—§æ•°æ®
            await self.cleanup_old_data()
            
            # ç”ŸæˆæŠ¥å‘Š
            report = await self.generate_report(health_status, metrics)
            
            # å‘é€å‘Šè­¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
            await self._send_alerts_if_needed(report)
            
            self.logger.info("Monitoring cycle completed successfully")
            
        except Exception as e:
            self.logger.error(f"Monitoring cycle failed: {e}")
    
    async def _send_alerts_if_needed(self, report: Dict[str, Any]):
        """æ ¹æ®éœ€è¦å‘é€å‘Šè­¦"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æœåŠ¡ä¸å¥åº·
        unhealthy_services = [
            service for service, healthy in report['health_status'].items()
            if not healthy
        ]
        
        if unhealthy_services:
            await self._send_alert(
                "Service Health Alert",
                f"Unhealthy services detected: {', '.join(unhealthy_services)}"
            )
        
        # æ£€æŸ¥å…³é”®æŒ‡æ ‡
        if report['summary']['critical_metrics']:
            critical_metrics_text = '\n'.join([
                f"- {metric['metric']}: {metric['value']} (threshold: {metric['threshold']})"
                for metric in report['summary']['critical_metrics']
            ])
            
            await self._send_alert(
                "Critical Metrics Alert",
                f"Critical metrics detected:\n{critical_metrics_text}"
            )
    
    async def _send_alert(self, title: str, message: str):
        """å‘é€å‘Šè­¦"""
        alert_config = self.config['alerts']
        
        # å‘é€åˆ°é’‰é’‰
        if 'dingtalk' in alert_config:
            await self._send_dingtalk_alert(title, message, alert_config['dingtalk'])
        
        # å‘é€é‚®ä»¶
        if 'email' in alert_config:
            await self._send_email_alert(title, message, alert_config['email'])
    
    async def _send_dingtalk_alert(self, title: str, message: str, config: Dict[str, str]):
        """å‘é€é’‰é’‰å‘Šè­¦"""
        webhook_url = config['webhook_url']
        
        payload = {
            "msgtype": "text",
            "text": {
                "content": f"ğŸš¨ {title}\n\n{message}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            }
        }
        
        try:
            async with self.session.post(webhook_url, json=payload) as response:
                if response.status == 200:
                    self.logger.info("DingTalk alert sent successfully")
                else:
                    self.logger.error(f"Failed to send DingTalk alert: {response.status}")
        except Exception as e:
            self.logger.error(f"Error sending DingTalk alert: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    async with MonitoringAutomation('config/monitoring.yaml') as monitor:
        # è¿è¡Œç›‘æ§å‘¨æœŸ
        await monitor.run_monitoring_cycle()

if __name__ == "__main__":
    asyncio.run(main())
```

#### 9.4.3 ç¾éš¾æ¢å¤è‡ªåŠ¨åŒ–

```bash
#!/bin/bash
# ç¾éš¾æ¢å¤è„šæœ¬
# æ–‡ä»¶è·¯å¾„: scripts/disaster_recovery.sh

set -e

# é…ç½®å˜é‡
BACKUP_LOCATION=${BACKUP_LOCATION:-"s3://voicehelper-backups"}
RECOVERY_NAMESPACE=${RECOVERY_NAMESPACE:-"voicehelper-recovery"}
RECOVERY_TYPE=${1:-"full"}  # full, database, files

# æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"
}

# æ£€æŸ¥å¤‡ä»½å®Œæ•´æ€§
check_backup_integrity() {
    log "æ£€æŸ¥å¤‡ä»½å®Œæ•´æ€§..."
    
    local backup_date=${2:-$(date -d "yesterday" +%Y%m%d)}
    local backup_path="${BACKUP_LOCATION}/${backup_date}"
    
    # æ£€æŸ¥æ•°æ®åº“å¤‡ä»½
    if aws s3 ls "${backup_path}/database.sql.gz" > /dev/null 2>&1; then
        log "æ•°æ®åº“å¤‡ä»½æ–‡ä»¶å­˜åœ¨"
    else
        log "é”™è¯¯: æ•°æ®åº“å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨"
        exit 1
    fi
    
    # æ£€æŸ¥æ–‡ä»¶å¤‡ä»½
    if aws s3 ls "${backup_path}/files.tar.gz" > /dev/null 2>&1; then
        log "æ–‡ä»¶å¤‡ä»½å­˜åœ¨"
    else
        log "é”™è¯¯: æ–‡ä»¶å¤‡ä»½ä¸å­˜åœ¨"
        exit 1
    fi
    
    # æ£€æŸ¥é…ç½®å¤‡ä»½
    if aws s3 ls "${backup_path}/configs.tar.gz" > /dev/null 2>&1; then
        log "é…ç½®å¤‡ä»½å­˜åœ¨"
    else
        log "é”™è¯¯: é…ç½®å¤‡ä»½ä¸å­˜åœ¨"
        exit 1
    fi
    
    log "å¤‡ä»½å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡"
}

# æ¢å¤æ•°æ®åº“
recover_database() {
    log "å¼€å§‹æ¢å¤æ•°æ®åº“..."
    
    local backup_date=${1:-$(date -d "yesterday" +%Y%m%d)}
    local backup_path="${BACKUP_LOCATION}/${backup_date}"
    
    # ä¸‹è½½æ•°æ®åº“å¤‡ä»½
    aws s3 cp "${backup_path}/database.sql.gz" /tmp/database.sql.gz
    gunzip /tmp/database.sql.gz
    
    # åˆ›å»ºä¸´æ—¶PostgreSQLå®ä¾‹
    kubectl run postgres-recovery --image=postgres:15 \
        --env="POSTGRES_PASSWORD=recovery123" \
        --env="POSTGRES_DB=voicehelper" \
        -n "${RECOVERY_NAMESPACE}" \
        --restart=Never
    
    # ç­‰å¾…PostgreSQLå¯åŠ¨
    kubectl wait --for=condition=ready pod/postgres-recovery -n "${RECOVERY_NAMESPACE}" --timeout=300s
    
    # æ¢å¤æ•°æ®åº“
    kubectl cp /tmp/database.sql "${RECOVERY_NAMESPACE}/postgres-recovery:/tmp/database.sql"
    kubectl exec -n "${RECOVERY_NAMESPACE}" postgres-recovery -- \
        psql -U postgres -d voicehelper -f /tmp/database.sql
    
    log "æ•°æ®åº“æ¢å¤å®Œæˆ"
}

# æ¢å¤æ–‡ä»¶
recover_files() {
    log "å¼€å§‹æ¢å¤æ–‡ä»¶..."
    
    local backup_date=${1:-$(date -d "yesterday" +%Y%m%d)}
    local backup_path="${BACKUP_LOCATION}/${backup_date}"
    
    # ä¸‹è½½æ–‡ä»¶å¤‡ä»½
    aws s3 cp "${backup_path}/files.tar.gz" /tmp/files.tar.gz
    
    # åˆ›å»ºæ¢å¤PVC
    kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: recovery-files-pvc
  namespace: ${RECOVERY_NAMESPACE}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
EOF
    
    # åˆ›å»ºæ¢å¤Pod
    kubectl run file-recovery --image=alpine \
        --command -- sleep 3600 \
        -n "${RECOVERY_NAMESPACE}" \
        --restart=Never
    
    kubectl patch pod file-recovery -n "${RECOVERY_NAMESPACE}" -p '
    {
        "spec": {
            "volumes": [
                {
                    "name": "recovery-storage",
                    "persistentVolumeClaim": {
                        "claimName": "recovery-files-pvc"
                    }
                }
            ],
            "containers": [
                {
                    "name": "file-recovery",
                    "volumeMounts": [
                        {
                            "name": "recovery-storage",
                            "mountPath": "/recovery"
                        }
                    ]
                }
            ]
        }
    }'
    
    # ç­‰å¾…Podå¯åŠ¨
    kubectl wait --for=condition=ready pod/file-recovery -n "${RECOVERY_NAMESPACE}" --timeout=300s
    
    # å¤åˆ¶å¹¶è§£å‹æ–‡ä»¶
    kubectl cp /tmp/files.tar.gz "${RECOVERY_NAMESPACE}/file-recovery:/recovery/files.tar.gz"
    kubectl exec -n "${RECOVERY_NAMESPACE}" file-recovery -- \
        tar -xzf /recovery/files.tar.gz -C /recovery/
    
    log "æ–‡ä»¶æ¢å¤å®Œæˆ"
}

# æ¢å¤é…ç½®
recover_configs() {
    log "å¼€å§‹æ¢å¤é…ç½®..."
    
    local backup_date=${1:-$(date -d "yesterday" +%Y%m%d)}
    local backup_path="${BACKUP_LOCATION}/${backup_date}"
    
    # ä¸‹è½½é…ç½®å¤‡ä»½
    aws s3 cp "${backup_path}/configs.tar.gz" /tmp/configs.tar.gz
    tar -xzf /tmp/configs.tar.gz -C /tmp/
    
    # æ¢å¤ConfigMapså’ŒSecrets
    kubectl apply -f /tmp/configs/ -n "${RECOVERY_NAMESPACE}"
    
    log "é…ç½®æ¢å¤å®Œæˆ"
}

# éªŒè¯æ¢å¤
verify_recovery() {
    log "éªŒè¯æ¢å¤ç»“æœ..."
    
    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    kubectl exec -n "${RECOVERY_NAMESPACE}" postgres-recovery -- \
        psql -U postgres -d voicehelper -c "SELECT COUNT(*) FROM users;"
    
    # æ£€æŸ¥æ–‡ä»¶
    kubectl exec -n "${RECOVERY_NAMESPACE}" file-recovery -- \
        ls -la /recovery/
    
    # æ£€æŸ¥é…ç½®
    kubectl get configmaps,secrets -n "${RECOVERY_NAMESPACE}"
    
    log "æ¢å¤éªŒè¯å®Œæˆ"
}

# æ¸…ç†æ¢å¤ç¯å¢ƒ
cleanup_recovery() {
    log "æ¸…ç†æ¢å¤ç¯å¢ƒ..."
    
    kubectl delete namespace "${RECOVERY_NAMESPACE}" --ignore-not-found=true
    rm -f /tmp/database.sql /tmp/files.tar.gz /tmp/configs.tar.gz
    rm -rf /tmp/configs/
    
    log "æ¸…ç†å®Œæˆ"
}

# å®Œæ•´æ¢å¤æµç¨‹
full_recovery() {
    local backup_date=$1
    
    log "å¼€å§‹å®Œæ•´æ¢å¤æµç¨‹..."
    
    # åˆ›å»ºæ¢å¤å‘½åç©ºé—´
    kubectl create namespace "${RECOVERY_NAMESPACE}" --dry-run=client -o yaml | kubectl apply -f -
    
    check_backup_integrity "${backup_date}"
    recover_database "${backup_date}"
    recover_files "${backup_date}"
    recover_configs "${backup_date}"
    verify_recovery
    
    log "å®Œæ•´æ¢å¤æµç¨‹å®Œæˆ"
    log "æ¢å¤çš„èµ„æºä½äºå‘½åç©ºé—´: ${RECOVERY_NAMESPACE}"
    log "è¯·éªŒè¯æ¢å¤ç»“æœåæ‰‹åŠ¨æ¸…ç†æ¢å¤ç¯å¢ƒ"
}

# ä¸»å‡½æ•°
main() {
    case "${RECOVERY_TYPE}" in
        "full")
            full_recovery "$2"
            ;;
        "database")
            recover_database "$2"
            ;;
        "files")
            recover_files "$2"
            ;;
        "configs")
            recover_configs "$2"
            ;;
        "cleanup")
            cleanup_recovery
            ;;
        *)
            echo "ç”¨æ³•: $0 [full|database|files|configs|cleanup] [backup_date]"
            echo "ç¤ºä¾‹: $0 full 20231201"
            exit 1
            ;;
    esac
}

# é”™è¯¯å¤„ç†
trap 'log "æ¢å¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œæ‰§è¡Œæ¸…ç†..."; cleanup_recovery; exit 1' ERR

main "$@"
```

## ç›¸å…³æ–‡æ¡£

- [ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ](/posts/voicehelper-architecture-overview/)
- [å‰ç«¯æ¨¡å—æ·±åº¦è§£æ](/posts/voicehelper-frontend-modules/)
- [åç«¯æœåŠ¡æ ¸å¿ƒå®ç°](/posts/voicehelper-backend-services/)
- [AIç®—æ³•å¼•æ“æ·±åº¦åˆ†æ](/posts/voicehelper-ai-algorithms/)
- [æ•°æ®å­˜å‚¨æ¶æ„](/posts/voicehelper-data-storage/)
- [ç³»ç»Ÿäº¤äº’æ—¶åºå›¾](/posts/voicehelper-system-interactions/)
- [ç¬¬ä¸‰æ–¹é›†æˆä¸æ‰©å±•](/posts/voicehelper-third-party-integration/)
- [æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§](/posts/voicehelper-performance-optimization/)
- [æ€»ç»“ä¸æœ€ä½³å®è·µ](/posts/voicehelper-best-practices/)
- [é¡¹ç›®åŠŸèƒ½æ¸…å•](/posts/voicehelper-feature-inventory/)
- [ç‰ˆæœ¬è¿­ä»£å†ç¨‹](/posts/voicehelper-version-history/)
- [ç«äº‰åŠ›åˆ†æ](/posts/voicehelper-competitive-analysis/)
- [APIæ¥å£æ¸…å•](/posts/voicehelper-api-reference/)
- [é”™è¯¯ç ç³»ç»Ÿ](/posts/voicehelper-error-codes/)
- [ç‰ˆæœ¬è¿­ä»£è®¡åˆ’](/posts/voicehelper-version-roadmap/)
