---
title: "QwenAgent-04 - LLMæ¨¡å—æ¶æ„åˆ†æ"
date: 2025-09-28T00:47:16+08:00
draft: false
tags: ['æºç åˆ†æ', 'æŠ€æœ¯æ–‡æ¡£']
categories: ['æŠ€æœ¯åˆ†æ']
description: "QwenAgent-04 - LLMæ¨¡å—æ¶æ„åˆ†æçš„æ·±å…¥æŠ€æœ¯åˆ†ææ–‡æ¡£"
keywords: ['æºç åˆ†æ', 'æŠ€æœ¯æ–‡æ¡£']
author: "æŠ€æœ¯åˆ†æå¸ˆ"
weight: 1
---

## ğŸ“ æ¦‚è¿°

LLMæ¨¡å—æ˜¯Qwen-Agentæ¡†æ¶çš„æ ¸å¿ƒé©±åŠ¨åŠ›ï¼Œè´Ÿè´£ä¸å„ç§å¤§è¯­è¨€æ¨¡å‹æœåŠ¡è¿›è¡Œäº¤äº’ã€‚æœ¬æ¨¡å—é€šè¿‡ç»Ÿä¸€çš„æŠ½è±¡æ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹æœåŠ¡æä¾›å•†ï¼ŒåŒ…æ‹¬DashScopeã€OpenAIã€Azureç­‰ï¼Œå¹¶æä¾›å‡½æ•°è°ƒç”¨ã€æµå¼è¾“å‡ºã€å¤šæ¨¡æ€æ”¯æŒç­‰é«˜çº§åŠŸèƒ½ã€‚

## ğŸ—ï¸ LLMæ¨¡å—æ¶æ„è®¾è®¡

### æ ¸å¿ƒç±»ç»§æ‰¿å…³ç³»å›¾

```mermaid
classDiagram
    class BaseChatModel {
        <<abstract>>
        +model: str
        +generate_cfg: dict
        +model_type: str
        +use_raw_api: bool
        +cache: Optional[Cache]
        +support_multimodal_input: bool
        +support_multimodal_output: bool
        +support_audio_input: bool
        +chat(messages, functions, stream)
        +quick_chat(prompt)
        +_chat_with_functions(...)*
        +_chat_stream(...)*
        +_chat_no_stream(...)*
        +_preprocess_messages(...)
        +_postprocess_messages(...)
        +raw_chat(messages, functions, stream)
        +quick_chat_oai(messages, tools)
    }
    
    class BaseFnCallModel {
        <<abstract>>
        +fncall_prompt: FnCallPrompt
        +_preprocess_messages(...)
        +_postprocess_messages(...)
        +_remove_fncall_messages(...)
        +_chat_with_functions(...)*
    }
    
    class QwenChatAtDS {
        +model: str = 'qwen-max'
        +_chat_stream(messages, delta_stream, generate_cfg)
        +_chat_no_stream(messages, generate_cfg)  
        +_continue_assistant_response(...)
        +_delta_stream_output(response)
        +_full_stream_output(response)
    }
    
    class TextChatAtOAI {
        +model: str = 'gpt-4o-mini'
        +_complete_create: function
        +_chat_complete_create: function
        +_chat_stream(messages, delta_stream, generate_cfg)
        +_chat_no_stream(messages, generate_cfg)
        +_delta_stream_output(response)
        +_full_stream_output(response)
    }
    
    class QwenVLChatAtDS {
        +support_multimodal_input: bool = True
        +_chat_stream(...)
        +_chat_no_stream(...)
    }
    
    class QwenAudioChatAtDS {
        +support_audio_input: bool = True
        +_chat_stream(...)
        +_chat_no_stream(...)
    }
    
    class TextChatAtAzure {
        +_chat_stream(...)
        +_chat_no_stream(...)
    }
    
    class Transformers {
        +_chat_stream(...)
        +_chat_no_stream(...)
    }
    
    class OpenVINO {
        +_chat_stream(...)
        +_chat_no_stream(...)
    }
    
    BaseChatModel <|-- BaseFnCallModel
    BaseFnCallModel <|-- QwenChatAtDS
    BaseFnCallModel <|-- TextChatAtOAI
    BaseFnCallModel <|-- QwenVLChatAtDS
    BaseFnCallModel <|-- QwenAudioChatAtDS
    BaseFnCallModel <|-- TextChatAtAzure
    BaseChatModel <|-- Transformers
    BaseChatModel <|-- OpenVINO
    
    note for BaseChatModel "ç»Ÿä¸€çš„LLMæŠ½è±¡æ¥å£"
    note for BaseFnCallModel "æ”¯æŒå‡½æ•°è°ƒç”¨çš„æ¨¡å‹åŸºç±»"
    note for QwenChatAtDS "DashScopeæœåŠ¡é›†æˆ"
    note for TextChatAtOAI "OpenAIå…¼å®¹æ¥å£"
```

### æ¨¡å—ç»„ä»¶å…³ç³»å›¾

```mermaid
graph TB
    subgraph "LLMæŠ½è±¡å±‚"
        A[BaseChatModel] --> B[ç»Ÿä¸€æ¥å£å®šä¹‰]
        C[BaseFnCallModel] --> D[å‡½æ•°è°ƒç”¨æ”¯æŒ]
        E[LLM_REGISTRY] --> F[æ¨¡å‹æ³¨å†Œç®¡ç†]
    end
    
    subgraph "æ¨¡å‹æœåŠ¡å®ç°"
        G[QwenChatAtDS] --> H[DashScope API]
        I[TextChatAtOAI] --> J[OpenAI API]
        K[QwenVLChatAtDS] --> L[å¤šæ¨¡æ€æ”¯æŒ]
        M[QwenAudioChatAtDS] --> N[éŸ³é¢‘æ”¯æŒ]
        O[TextChatAtAzure] --> P[Azure API]
        Q[Transformers] --> R[æœ¬åœ°æ¨¡å‹]
        S[OpenVINO] --> T[æ¨ç†åŠ é€Ÿ]
    end
    
    subgraph "åŠŸèƒ½ç»„ä»¶"
        U[Function Calling] --> V[å·¥å…·è°ƒç”¨è§£æ]
        W[Stream Processing] --> X[æµå¼è¾“å‡º]
        Y[Message Processing] --> Z[æ¶ˆæ¯é¢„åå¤„ç†]
        AA[Cache System] --> BB[å“åº”ç¼“å­˜]
        CC[Retry Mechanism] --> DD[é”™è¯¯é‡è¯•]
    end
    
    A --> G
    A --> I  
    A --> Q
    C --> G
    C --> I
    C --> K
    C --> M
    C --> O
    
    G --> U
    I --> U
    G --> W
    I --> W
    A --> Y
    A --> AA
    A --> CC
    
    style A fill:#e1f5fe
    style C fill:#f3e5f5
    style G fill:#e8f5e8
    style I fill:#fff3e0
```

## ğŸ” BaseChatModelåŸºç±»è¯¦ç»†åˆ†æ

### BaseChatModelæ ¸å¿ƒå±æ€§å’Œæ–¹æ³•

```python
class BaseChatModel(ABC):
    """LLMåŸºç¡€æŠ½è±¡ç±» - å®šä¹‰ç»Ÿä¸€çš„æ¨¡å‹äº¤äº’æ¥å£
    
    è®¾è®¡ç›®æ ‡:
        1. ä¸ºæ‰€æœ‰LLMæä¾›ç»Ÿä¸€çš„æ¥å£æŠ½è±¡
        2. æ”¯æŒå¤šç§è¾“å…¥è¾“å‡ºæ¨¡å¼ï¼ˆæ–‡æœ¬ã€å¤šæ¨¡æ€ã€éŸ³é¢‘ï¼‰
        3. æä¾›å®Œæ•´çš„ç¼“å­˜å’Œé‡è¯•æœºåˆ¶
        4. æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§å¤„ç†æ¨¡å¼
        5. å…¼å®¹å¤šç§APIæ ¼å¼å’Œè°ƒç”¨æ–¹å¼
    
    æ ¸å¿ƒèŒè´£:
        - æ¶ˆæ¯æ ¼å¼æ ‡å‡†åŒ–å’Œè½¬æ¢
        - è¾“å…¥é•¿åº¦ç®¡ç†å’Œæˆªæ–­
        - å“åº”ç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–
        - é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
        - æµå¼è¾“å‡ºç®¡ç†
    """
    
    # æ¨¡å‹èƒ½åŠ›å±æ€§
    @property
    def support_multimodal_input(self) -> bool:
        """æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰ï¼‰"""
        return False
        
    @property  
    def support_multimodal_output(self) -> bool:
        """æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€è¾“å‡ºï¼ˆé™¤æ–‡æœ¬å¤–çš„å…¶ä»–æ ¼å¼ï¼‰"""
        return False
        
    @property
    def support_audio_input(self) -> bool:
        """æ˜¯å¦æ”¯æŒéŸ³é¢‘è¾“å…¥"""
        return False
    
    def __init__(self, cfg: Optional[Dict] = None):
        """BaseChatModelåˆå§‹åŒ–
        
        åˆå§‹åŒ–æµç¨‹:
            1. è§£æåŸºç¡€é…ç½®ï¼ˆæ¨¡å‹åç§°ã€ç±»å‹ç­‰ï¼‰
            2. è®¾ç½®ç”Ÿæˆå‚æ•°å’Œç¼“å­˜é…ç½®  
            3. åˆå§‹åŒ–é‡è¯•æœºåˆ¶
            4. é…ç½®åŸç”ŸAPIæ¨¡å¼
        
        å…³é”®é…ç½®é¡¹:
            - model: æ¨¡å‹åç§°
            - model_type: æ¨¡å‹ç±»å‹æ ‡è¯†
            - generate_cfg: ç”Ÿæˆå‚æ•°é…ç½®
            - cache_dir: ç¼“å­˜ç›®å½•é…ç½®
            - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            - use_raw_api: æ˜¯å¦ä½¿ç”¨åŸç”ŸAPI
        """
        cfg = cfg or {}
        
        # 1. åŸºç¡€é…ç½®
        self.model = cfg.get('model', '').strip()
        self.model_type = cfg.get('model_type', '')
        
        # 2. ç”Ÿæˆé…ç½®å¤„ç†
        generate_cfg = copy.deepcopy(cfg.get('generate_cfg', {}))
        cache_dir = cfg.get('cache_dir', generate_cfg.pop('cache_dir', None))
        self.max_retries = generate_cfg.pop('max_retries', 0)
        self.generate_cfg = generate_cfg
        
        # 3. DashScopeç‰¹æ®Šé…ç½®
        if 'dashscope' in self.model_type:
            self.generate_cfg['incremental_output'] = True
        
        # 4. åŸç”ŸAPIæ¨¡å¼é…ç½®
        self.use_raw_api = os.getenv('QWEN_AGENT_USE_RAW_API', 'false').lower() == 'true'
        if 'use_raw_api' in generate_cfg:
            self.use_raw_api = generate_cfg.pop('use_raw_api')
        elif self.model_type == 'qwen_dashscope':
            # Qwen3-Maxé»˜è®¤ä½¿ç”¨åŸç”ŸAPI
            if self.model == 'qwen3-max' and (not self.use_raw_api):
                logger.info('Setting `use_raw_api` to True when using `Qwen3-Max`')
                self.use_raw_api = True
        
        # 5. ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–
        if cache_dir:
            try:
                import diskcache
                os.makedirs(cache_dir, exist_ok=True)
                self.cache = diskcache.Cache(directory=cache_dir)
            except ImportError:
                logger.warning('Caching disabled because diskcache is not installed.')
                self.cache = None
        else:
            self.cache = None
```

### chat()æ–¹æ³•å®Œæ•´å®ç°åˆ†æ

```python
def chat(
    self,
    messages: List[Union[Message, Dict]],
    functions: Optional[List[Dict]] = None,
    stream: bool = True,
    delta_stream: bool = False,
    extra_generate_cfg: Optional[Dict] = None,
) -> Union[List[Message], List[Dict], Iterator[List[Message]], Iterator[List[Dict]]]:
    """LLMèŠå¤©çš„æ ¸å¿ƒæ¥å£ - ç»Ÿä¸€å¤„ç†æ‰€æœ‰LLMäº¤äº’
    
    å¤„ç†æµç¨‹è¯¦è§£:
        1. è¾“å…¥æ ¼å¼ç»Ÿä¸€åŒ– - å°†Dictå’ŒMessageç»Ÿä¸€ä¸ºMessageç±»å‹
        2. ç¼“å­˜æŸ¥æ‰¾ - æ£€æŸ¥æ˜¯å¦æœ‰å·²ç¼“å­˜çš„å“åº”
        3. é…ç½®åˆå¹¶ - åˆå¹¶åŸºç¡€é…ç½®å’Œè°ƒç”¨é…ç½®
        4. éšæœºç§å­è®¾ç½® - ç¡®ä¿ç»“æœå¯å¤ç°
        5. è¯­è¨€æ£€æµ‹ - è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯è¯­è¨€
        6. ç³»ç»Ÿæ¶ˆæ¯å¤„ç† - æ·»åŠ é»˜è®¤ç³»ç»Ÿæ¶ˆæ¯
        7. è¾“å…¥é•¿åº¦ç®¡ç† - æˆªæ–­è¿‡é•¿çš„è¾“å…¥
        8. å‡½æ•°è°ƒç”¨æ¨¡å¼æ£€æµ‹ - åˆ¤æ–­æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
        9. æ¶ˆæ¯é¢„å¤„ç† - å¤šæ¨¡æ€å†…å®¹å¤„ç†
        10. æ¨¡å‹æœåŠ¡è°ƒç”¨ - å…·ä½“çš„APIè°ƒç”¨
        11. é‡è¯•æœºåˆ¶ - é”™è¯¯æ—¶çš„é‡è¯•é€»è¾‘
        12. å“åº”åå¤„ç† - æ ¼å¼åŒ–å’Œç¼“å­˜
    
    å‚æ•°è¯´æ˜:
        messages: è¾“å…¥æ¶ˆæ¯åˆ—è¡¨ï¼Œæ”¯æŒDictæˆ–Messageå¯¹è±¡
        functions: å¯ç”¨å‡½æ•°åˆ—è¡¨ï¼Œç”¨äºå‡½æ•°è°ƒç”¨
        stream: æ˜¯å¦æµå¼è¾“å‡º
        delta_stream: æ˜¯å¦å¢é‡æµå¼è¾“å‡ºï¼ˆå·²å¼ƒç”¨ï¼‰
        extra_generate_cfg: é¢å¤–çš„ç”Ÿæˆé…ç½®
    
    è¿”å›å€¼è¯´æ˜:
        - éæµå¼: è¿”å›Messageæˆ–Dictåˆ—è¡¨
        - æµå¼: è¿”å›Messageæˆ–Dictåˆ—è¡¨çš„è¿­ä»£å™¨
    """
    
    # 1. è¾“å…¥æ¶ˆæ¯ç»Ÿä¸€åŒ–
    messages = copy.deepcopy(messages)
    _return_message_type = 'dict'  # è®°å½•è¿”å›ç±»å‹
    new_messages = []
    
    for msg in messages:
        if isinstance(msg, dict):
            new_messages.append(Message(**msg))
        else:
            new_messages.append(msg)
            _return_message_type = 'message'
    messages = new_messages
    
    if not messages:
        raise ValueError('Messages can not be empty.')
    
    # 2. ç¼“å­˜æŸ¥æ‰¾
    if self.cache is not None:
        cache_key = dict(messages=messages, functions=functions, extra_generate_cfg=extra_generate_cfg)
        cache_key: str = json_dumps_compact(cache_key, sort_keys=True)
        cache_value: str = self.cache.get(cache_key)
        if cache_value:
            cache_value: List[dict] = json.loads(cache_value)
            if _return_message_type == 'message':
                cache_value: List[Message] = [Message(**m) for m in cache_value]
            if stream:
                cache_value: Iterator[List[Union[Message, dict]]] = iter([cache_value])
            return cache_value
    
    # 3. å¼ƒç”¨è­¦å‘Šå¤„ç†
    if stream and delta_stream:
        logger.warning(
            'Support for `delta_stream=True` is deprecated. '
            'Please use `stream=True and delta_stream=False` or `stream=False` instead.'
        )
    
    # 4. é…ç½®åˆå¹¶å’Œå¤„ç†
    generate_cfg = merge_generate_cfgs(base_generate_cfg=self.generate_cfg, new_generate_cfg=extra_generate_cfg)
    
    # 5. éšæœºç§å­è®¾ç½®
    if 'seed' not in generate_cfg:
        generate_cfg['seed'] = random.randint(a=0, b=2**30)
    
    # 6. è¯­è¨€æ£€æµ‹
    if 'lang' in generate_cfg:
        lang: Literal['en', 'zh'] = generate_cfg.pop('lang')
    else:
        lang: Literal['en', 'zh'] = 'zh' if has_chinese_messages(messages) else 'en'
    
    if not stream and 'incremental_output' in generate_cfg:
        generate_cfg.pop('incremental_output')
    
    # 7. ç³»ç»Ÿæ¶ˆæ¯å¤„ç†
    if DEFAULT_SYSTEM_MESSAGE and messages[0].role != SYSTEM:
        messages = [Message(role=SYSTEM, content=DEFAULT_SYSTEM_MESSAGE)] + messages
    
    # 8. è¾“å…¥é•¿åº¦ç®¡ç†
    max_input_tokens = generate_cfg.pop('max_input_tokens', DEFAULT_MAX_INPUT_TOKENS)
    if max_input_tokens > 0:
        messages = _truncate_input_messages_roughly(
            messages=messages,
            max_tokens=max_input_tokens,
        )
    
    # 9. å‡½æ•°è°ƒç”¨æ¨¡å¼æ£€æµ‹
    if functions:
        fncall_mode = True
    else:
        fncall_mode = False
    
    # function_choiceå‚æ•°éªŒè¯
    if 'function_choice' in generate_cfg:
        fn_choice = generate_cfg['function_choice']
        valid_fn_choices = [f.get('name', f.get('name_for_model', None)) for f in (functions or [])]
        valid_fn_choices = ['auto', 'none'] + [f for f in valid_fn_choices if f]
        if fn_choice not in valid_fn_choices:
            raise ValueError(f'The value of function_choice must be one of: {valid_fn_choices}. '
                           f'But function_choice="{fn_choice}" is received.')
        if fn_choice == 'none':
            fncall_mode = False
    
    # 10. æ¶ˆæ¯é¢„å¤„ç†
    messages = self._preprocess_messages(messages,
                                       lang=lang,
                                       generate_cfg=generate_cfg,
                                       functions=functions,
                                       use_raw_api=self.use_raw_api)
    
    if not self.support_multimodal_input:
        messages = [format_as_text_message(msg, add_upload_info=False) for msg in messages]
    
    # 11. åŸç”ŸAPIæ¨¡å¼å¤„ç†
    if self.use_raw_api:
        logger.debug('`use_raw_api` takes effect.')
        assert stream and (not delta_stream), '`use_raw_api` only support full stream!!!'
        return self.raw_chat(messages=messages, functions=functions, stream=stream, generate_cfg=generate_cfg)
    
    # 12. æ¸…ç†éå‡½æ•°è°ƒç”¨æ¨¡å¼çš„å‚æ•°
    if not fncall_mode:
        for k in ['parallel_function_calls', 'function_choice', 'thought_in_content']:
            if k in generate_cfg:
                del generate_cfg[k]
    
    # 13. å®šä¹‰æ¨¡å‹æœåŠ¡è°ƒç”¨å‡½æ•°
    def _call_model_service():
        if fncall_mode:
            return self._chat_with_functions(
                messages=messages,
                functions=functions,
                stream=stream,
                delta_stream=delta_stream,
                generate_cfg=generate_cfg,
                lang=lang,
            )
        else:
            # TODO: ä¼˜åŒ–ä»£ç ç»“æ„
            if messages[-1].role == ASSISTANT:
                # ç»­å†™æ¨¡å¼
                assert not delta_stream, 'Continuation mode does not currently support `delta_stream`'
                return self._continue_assistant_response(messages, generate_cfg=generate_cfg, stream=stream)
            else:
                return self._chat(
                    messages,
                    stream=stream,
                    delta_stream=delta_stream,
                    generate_cfg=generate_cfg,
                )
    
    # 14. é‡è¯•æœºåˆ¶åº”ç”¨
    if stream and delta_stream:
        # å¢é‡æµå¼æ— é‡è¯•
        output = _call_model_service()
    elif stream and (not delta_stream):
        # å…¨é‡æµå¼æ”¯æŒé‡è¯•
        output = retry_model_service_iterator(_call_model_service, max_retries=self.max_retries)
    else:
        # éæµå¼æ”¯æŒé‡è¯•
        output = retry_model_service(_call_model_service, max_retries=self.max_retries)
    
    # 15. å“åº”å¤„ç†å’Œç¼“å­˜
    if isinstance(output, list):
        # éæµå¼å¤„ç†
        assert not stream
        logger.debug(f'LLM Output: \n{pformat([_.model_dump() for _ in output], indent=2)}')
        output = self._postprocess_messages(output, fncall_mode=fncall_mode, generate_cfg=generate_cfg)
        if not self.support_multimodal_output:
            output = _format_as_text_messages(messages=output)
        if self.cache:
            self.cache.set(cache_key, json_dumps_compact(output))
        return self._convert_messages_to_target_type(output, _return_message_type)
    else:
        # æµå¼å¤„ç†
        assert stream
        if delta_stream:
            # å¢é‡æµå¼çš„ç‰¹æ®Šå¤„ç†
            generate_cfg = copy.deepcopy(generate_cfg)
            assert 'skip_stopword_postproc' not in generate_cfg
            generate_cfg['skip_stopword_postproc'] = True
            
        output = self._postprocess_messages_iterator(output, fncall_mode=fncall_mode, generate_cfg=generate_cfg)
        
        def _format_and_cache() -> Iterator[List[Message]]:
            o = []
            for o in output:
                if o:
                    if not self.support_multimodal_output:
                        o = _format_as_text_messages(messages=o)
                    yield o
            if o and (self.cache is not None):
                self.cache.set(cache_key, json_dumps_compact(o))
        
        return self._convert_messages_iterator_to_target_type(_format_and_cache(), _return_message_type)
```

## ğŸ”§ BaseFnCallModelå‡½æ•°è°ƒç”¨æ¨¡å‹

### å‡½æ•°è°ƒç”¨å¤„ç†æœºåˆ¶

```python
class BaseFnCallModel(BaseChatModel, ABC):
    """æ”¯æŒå‡½æ•°è°ƒç”¨çš„LLMåŸºç±»
    
    æ ¸å¿ƒåŠŸèƒ½:
        1. å‡½æ•°è°ƒç”¨æç¤ºæ¨¡æ¿ç®¡ç†
        2. æ¶ˆæ¯æ ¼å¼è½¬æ¢ï¼ˆå‡½æ•°è°ƒç”¨ <-> æ™®é€šå¯¹è¯ï¼‰
        3. å¹¶è¡Œå‡½æ•°è°ƒç”¨æ”¯æŒ
        4. æ€ç»´é“¾å†…å®¹å¤„ç†
    
    æç¤ºæ¨¡æ¿ç±»å‹:
        - qwen: QwenåŸç”Ÿå‡½æ•°è°ƒç”¨æ ¼å¼
        - nous: Nousç ”ç©¶æ ¼å¼ï¼ˆæ¨èï¼‰
    """
    
    def __init__(self, cfg: Optional[Dict] = None):
        """å‡½æ•°è°ƒç”¨æ¨¡å‹åˆå§‹åŒ–
        
        åˆå§‹åŒ–è¿‡ç¨‹:
            1. é€‰æ‹©å‡½æ•°è°ƒç”¨æç¤ºæ¨¡æ¿ç±»å‹
            2. åŠ è½½å¯¹åº”çš„æç¤ºå¤„ç†å™¨
            3. é…ç½®åœç”¨è¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        """
        super().__init__(cfg)
        
        # 1. è·å–å‡½æ•°è°ƒç”¨æç¤ºç±»å‹
        fncall_prompt_type = self.generate_cfg.get('fncall_prompt_type', 'nous')
        
        # 2. åŠ è½½å¯¹åº”çš„æç¤ºå¤„ç†å™¨
        if fncall_prompt_type == 'qwen':
            from qwen_agent.llm.fncall_prompts.qwen_fncall_prompt import FN_STOP_WORDS, QwenFnCallPrompt
            self.fncall_prompt = QwenFnCallPrompt()
            # æ·»åŠ å‡½æ•°è°ƒç”¨åœç”¨è¯
            stop = self.generate_cfg.get('stop', [])
            self.generate_cfg['stop'] = stop + [x for x in FN_STOP_WORDS if x not in stop]
        elif fncall_prompt_type == 'nous':
            from qwen_agent.llm.fncall_prompts.nous_fncall_prompt import NousFnCallPrompt
            self.fncall_prompt = NousFnCallPrompt()
        else:
            raise NotImplementedError(f'Unsupported fncall_prompt_type: {fncall_prompt_type}')
        
        # 3. æ¸…ç†é…ç½®ä¸­çš„æç¤ºç±»å‹å‚æ•°
        if 'fncall_prompt_type' in self.generate_cfg:
            del self.generate_cfg['fncall_prompt_type']
    
    def _preprocess_messages(
        self,
        messages: List[Message],
        lang: Literal['en', 'zh'],
        generate_cfg: dict,
        functions: Optional[List[Dict]] = None,
        use_raw_api: bool = False,
    ) -> List[Message]:
        """å‡½æ•°è°ƒç”¨æ¶ˆæ¯é¢„å¤„ç†
        
        å¤„ç†é€»è¾‘:
            1. è°ƒç”¨çˆ¶ç±»è¿›è¡ŒåŸºç¡€é¢„å¤„ç†
            2. å¦‚æœä½¿ç”¨åŸç”ŸAPIï¼Œç›´æ¥è¿”å›
            3. å¦‚æœæ²¡æœ‰å‡½æ•°æˆ–function_choice=noneï¼Œç§»é™¤å‡½æ•°è°ƒç”¨æ¶ˆæ¯
            4. å¦åˆ™ï¼Œä½¿ç”¨æç¤ºæ¨¡æ¿å¤„ç†å‡½æ•°è°ƒç”¨æ¶ˆæ¯
        
        å‚æ•°è¯´æ˜:
            messages: è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            lang: è¯­è¨€ç±»å‹
            generate_cfg: ç”Ÿæˆé…ç½®
            functions: å¯ç”¨å‡½æ•°åˆ—è¡¨
            use_raw_api: æ˜¯å¦ä½¿ç”¨åŸç”ŸAPI
        """
        # 1. çˆ¶ç±»åŸºç¡€é¢„å¤„ç†
        messages = super()._preprocess_messages(messages, lang=lang, generate_cfg=generate_cfg, functions=functions)
        
        # 2. åŸç”ŸAPIæ¨¡å¼ç›´æ¥è¿”å›
        if use_raw_api:
            return messages
        
        # 3. å¤„ç†å‡½æ•°è°ƒç”¨ç›¸å…³é€»è¾‘
        if (not functions) or (generate_cfg.get('function_choice', 'auto') == 'none'):
            # ç§»é™¤å‡½æ•°è°ƒç”¨æ¶ˆæ¯
            messages = self._remove_fncall_messages(messages, lang=lang)
        else:
            # ä½¿ç”¨å‡½æ•°è°ƒç”¨æç¤ºæ¨¡æ¿å¤„ç†
            messages = self.fncall_prompt.preprocess_fncall_messages(
                messages=messages,
                functions=functions,
                lang=lang,
                parallel_function_calls=generate_cfg.get('parallel_function_calls', False),
                function_choice=generate_cfg.get('function_choice', 'auto'),
            )
        
        return messages
    
    def _postprocess_messages(
        self,
        messages: List[Message],
        fncall_mode: bool,
        generate_cfg: dict,
    ) -> List[Message]:
        """å‡½æ•°è°ƒç”¨æ¶ˆæ¯åå¤„ç†
        
        å¤„ç†é€»è¾‘:
            1. è°ƒç”¨çˆ¶ç±»è¿›è¡ŒåŸºç¡€åå¤„ç†
            2. å¦‚æœæ˜¯å‡½æ•°è°ƒç”¨æ¨¡å¼ï¼Œä½¿ç”¨æç¤ºæ¨¡æ¿åå¤„ç†
        """
        # 1. çˆ¶ç±»åŸºç¡€åå¤„ç†
        messages = super()._postprocess_messages(messages, fncall_mode=fncall_mode, generate_cfg=generate_cfg)
        
        # 2. å‡½æ•°è°ƒç”¨æ¨¡å¼çš„ç‰¹æ®Šåå¤„ç†
        if fncall_mode:
            messages = self.fncall_prompt.postprocess_fncall_messages(
                messages=messages,
                parallel_function_calls=generate_cfg.get('parallel_function_calls', False),
                function_choice=generate_cfg.get('function_choice', 'auto'),
                thought_in_content=generate_cfg.get('thought_in_content', False),
            )
        
        return messages
    
    def _remove_fncall_messages(self, messages: List[Message], lang: Literal['en', 'zh']) -> List[Message]:
        """ç§»é™¤å‡½æ•°è°ƒç”¨æ¶ˆæ¯ï¼Œè½¬æ¢ä¸ºæ™®é€šç”¨æˆ·æ¶ˆæ¯
        
        ä½¿ç”¨åœºæ™¯:
            å½“function_choice="none"æ—¶ï¼Œéœ€è¦å°†å‡½æ•°è°ƒç”¨ç›¸å…³çš„æ¶ˆæ¯
            è½¬æ¢ä¸ºæ™®é€šçš„ç”¨æˆ·æ¶ˆæ¯ï¼Œé¿å…æ¨¡å‹ç»§ç»­å°è¯•ç”Ÿæˆå‡½æ•°è°ƒç”¨
        
        è½¬æ¢é€»è¾‘:
            - function_callæ¶ˆæ¯ -> è½¬æ¢ä¸ºæè¿°å·¥å…·è°ƒç”¨çš„ç”¨æˆ·æ¶ˆæ¯
            - FUNCTIONæ¶ˆæ¯ -> è½¬æ¢ä¸ºæè¿°å·¥å…·ç»“æœçš„ç”¨æˆ·æ¶ˆæ¯
        """
        new_messages = []
        for msg in messages:
            if (msg.role == FUNCTION) or msg.function_call:
                # ç¡®ä¿å‰ä¸€æ¡æ¶ˆæ¯æ˜¯ç”¨æˆ·æ¶ˆæ¯
                if (not new_messages) or (new_messages[-1].role != USER):
                    new_messages.append(Message(role=USER, content=[]))
                
                if msg.function_call:
                    # å¤„ç†å‡½æ•°è°ƒç”¨æ¶ˆæ¯
                    tool_name = msg.function_call.name
                    tool_args = msg.function_call.arguments
                    if lang == 'zh':
                        tool_text = f'\n\nå·¥å…·"{tool_name}"è¢«è°ƒç”¨æ—¶ä½¿ç”¨äº†ä»¥ä¸‹å‚æ•°ï¼š\n{tool_args}'
                    else:
                        tool_text = f'\n\nThe tool "{tool_name}" was called with these arguments: \n{tool_args}'
                else:
                    # å¤„ç†å‡½æ•°ç»“æœæ¶ˆæ¯
                    assert msg.role == FUNCTION
                    if lang == 'zh':
                        tool_text = f'\n\nå·¥å…·"{msg.name}"è¿”å›äº†ä»¥ä¸‹ç»“æœï¼š\n{msg.content}'
                    else:
                        tool_text = f'\n\nThe tool "{msg.name}" returned: \n{msg.content}'
                
                # å°†å·¥å…·ä¿¡æ¯æ·»åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯ä¸­
                if isinstance(new_messages[-1].content, str):
                    new_messages[-1].content += tool_text
                else:
                    new_messages[-1].content.append(ContentItem(text=tool_text))
            else:
                # æ™®é€šæ¶ˆæ¯ç›´æ¥æ·»åŠ 
                new_messages.append(msg)
        
        return new_messages
```

## ğŸŒ å…·ä½“æ¨¡å‹æœåŠ¡å®ç°

### 1. QwenChatAtDS - DashScopeæœåŠ¡å®ç°

```python
@register_llm('qwen_dashscope')
class QwenChatAtDS(BaseFnCallModel):
    """DashScopeæœåŠ¡çš„Qwenæ¨¡å‹å®ç°
    
    ç‰¹ç‚¹:
        1. é›†æˆé˜¿é‡Œäº‘DashScopeæœåŠ¡
        2. æ”¯æŒæµå¼å’Œéæµå¼è¾“å‡º
        3. æ”¯æŒå‡½æ•°è°ƒç”¨å’Œæ¨ç†å†…å®¹
        4. æä¾›å®Œæ•´çš„é”™è¯¯å¤„ç†
    
    æ”¯æŒçš„åŠŸèƒ½:
        - æ–‡æœ¬å¯¹è¯
        - å‡½æ•°è°ƒç”¨
        - æµå¼è¾“å‡º
        - å¢é‡å’Œå…¨é‡æµå¼
        - æ¨ç†å†…å®¹ï¼ˆreasoning_contentï¼‰
    """
    
    def __init__(self, cfg: Optional[Dict] = None):
        """DashScopeæ¨¡å‹åˆå§‹åŒ–
        
        åˆå§‹åŒ–è¿‡ç¨‹:
            1. è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
            2. è®¾ç½®é»˜è®¤æ¨¡å‹åç§°
            3. åˆå§‹åŒ–DashScope SDK
        """
        super().__init__(cfg)
        self.model = self.model or 'qwen-max'  # é»˜è®¤ä½¿ç”¨qwen-max
        initialize_dashscope(cfg)  # åˆå§‹åŒ–DashScopeé…ç½®
    
    def _chat_stream(
        self,
        messages: List[Message],
        delta_stream: bool,
        generate_cfg: dict,
    ) -> Iterator[List[Message]]:
        """æµå¼èŠå¤©å®ç°
        
        å¤„ç†æµç¨‹:
            1. æ¶ˆæ¯æ ¼å¼è½¬æ¢
            2. å¤„ç†ç»­å†™æ¨¡å¼ï¼ˆpartial=Trueï¼‰
            3. è°ƒç”¨DashScope Generation API
            4. æ ¹æ®streamæ¨¡å¼è¿”å›ä¸åŒæ ¼å¼
        
        å‚æ•°è¯´æ˜:
            messages: è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            delta_stream: æ˜¯å¦å¢é‡æµå¼è¾“å‡º
            generate_cfg: ç”Ÿæˆé…ç½®å‚æ•°
        """
        # 1. æ¶ˆæ¯æ ¼å¼è½¬æ¢
        messages = [msg.model_dump() for msg in messages]
        
        # 2. å¤„ç†ç»­å†™æ¨¡å¼
        if messages[-1]['role'] == ASSISTANT:
            messages[-1]['partial'] = True
        
        # 3. è½¬æ¢ä¸ºOpenAIæ ¼å¼ï¼ˆDashScopeå…¼å®¹ï¼‰
        messages = self._conv_qwen_agent_messages_to_oai(messages)
        
        # 4. è°ƒè¯•æ—¥å¿—
        logger.debug(f'LLM Input: \n{pformat(messages, indent=2)}')
        logger.debug(f'LLM Input generate_cfg: \n{generate_cfg}')
        
        # 5. è°ƒç”¨DashScope API
        response = dashscope.Generation.call(
            self.model,
            messages=messages,
            result_format='message',
            stream=True,
            **generate_cfg
        )
        
        # 6. æ ¹æ®æµå¼æ¨¡å¼è¿”å›
        if delta_stream:
            return self._delta_stream_output(response)
        else:
            return self._full_stream_output(response)
    
    def _chat_no_stream(
        self,
        messages: List[Message],
        generate_cfg: dict,
    ) -> List[Message]:
        """éæµå¼èŠå¤©å®ç°
        
        å¤„ç†æµç¨‹:
            1. æ¶ˆæ¯æ ¼å¼è½¬æ¢å’Œé¢„å¤„ç†
            2. è°ƒç”¨DashScopeéæµå¼API
            3. ç»“æœè§£æå’Œé”™è¯¯å¤„ç†
            4. è¿”å›æ ‡å‡†Messageæ ¼å¼
        """
        # 1. æ¶ˆæ¯æ ¼å¼è½¬æ¢
        messages = [msg.model_dump() for msg in messages]
        if messages[-1]['role'] == ASSISTANT:
            messages[-1]['partial'] = True
        messages = self._conv_qwen_agent_messages_to_oai(messages)
        
        # 2. è°ƒè¯•æ—¥å¿—
        logger.debug(f'LLM Input: \n{pformat(messages, indent=2)}')
        
        # 3. è°ƒç”¨DashScope API
        response = dashscope.Generation.call(
            self.model,
            messages=messages,
            result_format='message',
            stream=False,
            **generate_cfg
        )
        
        # 4. ç»“æœå¤„ç†
        if response.status_code == HTTPStatus.OK:
            return [
                Message(
                    role=ASSISTANT,
                    content=response.output.choices[0].message.content,
                    reasoning_content=response.output.choices[0].message.get('reasoning_content', ''),
                    extra={'model_service_info': response}
                )
            ]
        else:
            raise ModelServiceError(
                code=response.code,
                message=response.message,
                extra={'model_service_info': response}
            )
    
    @staticmethod
    def _delta_stream_output(response) -> Iterator[List[Message]]:
        """å¢é‡æµå¼è¾“å‡ºå¤„ç†
        
        ç‰¹ç‚¹:
            - æ¯æ¬¡è¿”å›æ–°å¢çš„å†…å®¹ç‰‡æ®µ
            - é€‚ç”¨äºå®æ—¶æ˜¾ç¤ºåœºæ™¯
            - éœ€è¦å®¢æˆ·ç«¯è‡ªè¡Œæ‹¼æ¥å®Œæ•´å†…å®¹
        """
        for chunk in response:
            if chunk.status_code == HTTPStatus.OK:
                choice = chunk.output.choices[0]
                yield [
                    Message(
                        role=ASSISTANT,
                        content=choice.delta.get('content', ''),
                        reasoning_content=choice.delta.get('reasoning_content', ''),
                        extra={'model_service_info': chunk}
                    )
                ]
            else:
                raise ModelServiceError(
                    code=chunk.code,
                    message=chunk.message,
                    extra={'model_service_info': chunk}
                )
    
    @staticmethod
    def _full_stream_output(response) -> Iterator[List[Message]]:
        """å…¨é‡æµå¼è¾“å‡ºå¤„ç†
        
        ç‰¹ç‚¹:
            - æ¯æ¬¡è¿”å›å®Œæ•´çš„ç´¯ç§¯å†…å®¹
            - é€‚ç”¨äºé€æ­¥æ„å»ºå“åº”çš„åœºæ™¯
            - å®¢æˆ·ç«¯æ— éœ€æ‹¼æ¥ï¼Œç›´æ¥ä½¿ç”¨æœ€æ–°ç»“æœ
        """
        for chunk in response:
            if chunk.status_code == HTTPStatus.OK:
                choice = chunk.output.choices[0]
                yield [
                    Message(
                        role=ASSISTANT,
                        content=choice.message.get('content', ''),
                        reasoning_content=choice.message.get('reasoning_content', ''),
                        extra={'model_service_info': chunk}
                    )
                ]
            else:
                raise ModelServiceError(
                    code=chunk.code,
                    message=chunk.message,
                    extra={'model_service_info': chunk}
                )
```

### 2. TextChatAtOAI - OpenAIå…¼å®¹å®ç°

```python
@register_llm('oai')
class TextChatAtOAI(BaseFnCallModel):
    """OpenAIå…¼å®¹çš„æ–‡æœ¬èŠå¤©æ¨¡å‹
    
    ç‰¹ç‚¹:
        1. å…¼å®¹OpenAI APIæ ¼å¼
        2. æ”¯æŒå¤šç§OpenAIå…¼å®¹æœåŠ¡ï¼ˆvLLMã€Ollamaç­‰ï¼‰
        3. è‡ªé€‚åº”OpenAI SDKç‰ˆæœ¬
        4. æ”¯æŒé¢å¤–å‚æ•°é€ä¼ 
    
    å…¼å®¹çš„æœåŠ¡:
        - OpenAIå®˜æ–¹API
        - vLLMéƒ¨ç½²çš„æœåŠ¡
        - Ollamaæœ¬åœ°æœåŠ¡
        - å…¶ä»–OpenAIå…¼å®¹API
    """
    
    def __init__(self, cfg: Optional[Dict] = None):
        """OpenAIå…¼å®¹æ¨¡å‹åˆå§‹åŒ–
        
        åˆå§‹åŒ–è¿‡ç¨‹:
            1. è®¾ç½®é»˜è®¤æ¨¡å‹å’Œé…ç½®
            2. å¤„ç†APIé…ç½®ï¼ˆbase_urlã€api_keyï¼‰
            3. é€‚é…ä¸åŒOpenAI SDKç‰ˆæœ¬
            4. åˆ›å»ºè°ƒç”¨å‡½æ•°
        """
        super().__init__(cfg)
        self.model = self.model or 'gpt-4o-mini'
        cfg = cfg or {}
        
        # 1. APIé…ç½®å¤„ç†
        api_base = cfg.get('api_base') or cfg.get('base_url') or cfg.get('model_server')
        api_base = (api_base or '').strip()
        
        api_key = cfg.get('api_key') or os.getenv('OPENAI_API_KEY')
        api_key = (api_key or 'EMPTY').strip()
        
        # 2. SDKç‰ˆæœ¬é€‚é…
        if openai.__version__.startswith('0.'):
            # OpenAI SDK v0.xç‰ˆæœ¬
            if api_base:
                openai.api_base = api_base
            if api_key:
                openai.api_key = api_key
            self._complete_create = openai.Completion.create
            self._chat_complete_create = openai.ChatCompletion.create
        else:
            # OpenAI SDK v1.xç‰ˆæœ¬
            api_kwargs = {}
            if api_base:
                api_kwargs['base_url'] = api_base
            if api_key:
                api_kwargs['api_key'] = api_key
            
            # 3. åˆ›å»ºå…¼å®¹çš„è°ƒç”¨å‡½æ•°
            def _chat_complete_create(*args, **kwargs):
                """èŠå¤©å®ŒæˆAPIè°ƒç”¨åŒ…è£…å™¨
                
                åŠŸèƒ½:
                    1. å¤„ç†v1 APIä¸æ”¯æŒçš„å‚æ•°
                    2. å°†ç‰¹æ®Šå‚æ•°ç§»åˆ°extra_bodyä¸­
                    3. å¤„ç†è¶…æ—¶å‚æ•°åå˜åŒ–
                """
                # OpenAI API v1ä¸å…è®¸æŸäº›å‚æ•°ï¼Œå¿…é¡»é€šè¿‡extra_bodyä¼ é€’
                extra_params = ['top_k', 'repetition_penalty']
                if any((k in kwargs) for k in extra_params):
                    kwargs['extra_body'] = copy.deepcopy(kwargs.get('extra_body', {}))
                    for k in extra_params:
                        if k in kwargs:
                            kwargs['extra_body'][k] = kwargs.pop(k)
                
                # å¤„ç†è¶…æ—¶å‚æ•°åå˜åŒ–
                if 'request_timeout' in kwargs:
                    kwargs['timeout'] = kwargs.pop('request_timeout')
                
                # åˆ›å»ºå®¢æˆ·ç«¯å¹¶è°ƒç”¨
                client = openai.OpenAI(**api_kwargs)
                return client.chat.completions.create(*args, **kwargs)
            
            def _complete_create(*args, **kwargs):
                """å®ŒæˆAPIè°ƒç”¨åŒ…è£…å™¨"""
                # åŒæ ·çš„å‚æ•°å¤„ç†é€»è¾‘
                extra_params = ['top_k', 'repetition_penalty']
                if any((k in kwargs) for k in extra_params):
                    kwargs['extra_body'] = copy.deepcopy(kwargs.get('extra_body', {}))
                    for k in extra_params:
                        if k in kwargs:
                            kwargs['extra_body'][k] = kwargs.pop(k)
                
                if 'request_timeout' in kwargs:
                    kwargs['timeout'] = kwargs.pop('request_timeout')
                
                client = openai.OpenAI(**api_kwargs)
                return client.completions.create(*args, **kwargs)
            
            self._complete_create = _complete_create
            self._chat_complete_create = _chat_complete_create
    
    def _chat_stream(
        self,
        messages: List[Message],
        delta_stream: bool,
        generate_cfg: dict,
    ) -> Iterator[List[Message]]:
        """OpenAIæµå¼èŠå¤©å®ç°"""
        # æ¶ˆæ¯æ ¼å¼è½¬æ¢
        messages = [msg.model_dump() for msg in messages]
        messages = self._conv_qwen_agent_messages_to_oai(messages)
        
        # è°ƒè¯•æ—¥å¿—
        logger.debug(f'LLM Input: \n{pformat(messages, indent=2)}')
        logger.debug(f'LLM Input generate_cfg: \n{generate_cfg}')
        
        try:
            # è°ƒç”¨OpenAI API
            response = self._chat_complete_create(
                model=self.model,
                messages=messages,
                stream=True,
                **generate_cfg
            )
            
            # æ ¹æ®æµå¼æ¨¡å¼å¤„ç†å“åº”
            if delta_stream:
                return self._delta_stream_output(response)
            else:
                return self._full_stream_output(response)
                
        except OpenAIError as ex:
            raise ModelServiceError(exception=ex)
    
    def _chat_no_stream(
        self,
        messages: List[Message],
        generate_cfg: dict,
    ) -> List[Message]:
        """OpenAIéæµå¼èŠå¤©å®ç°"""
        # æ¶ˆæ¯æ ¼å¼è½¬æ¢
        messages = [msg.model_dump() for msg in messages]
        messages = self._conv_qwen_agent_messages_to_oai(messages)
        
        logger.debug(f'LLM Input: \n{pformat(messages, indent=2)}')
        
        try:
            # è°ƒç”¨OpenAI API
            response = self._chat_complete_create(
                model=self.model,
                messages=messages,
                stream=False,
                **generate_cfg
            )
            
            # è§£æå“åº”
            choice = response.choices[0]
            message = choice.message
            
            return [
                Message(
                    role=ASSISTANT,
                    content=message.content or '',
                    function_call=FunctionCall(
                        name=message.function_call.name,
                        arguments=message.function_call.arguments,
                    ) if message.function_call else None,
                    extra={'model_service_info': response}
                )
            ]
            
        except OpenAIError as ex:
            raise ModelServiceError(exception=ex)
```

## ğŸ”„ LLMæ¨¡å—å·¥ä½œæµç¨‹

### å®Œæ•´çš„LLMè°ƒç”¨æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant A as Agent
    participant LLM as BaseChatModel
    participant Impl as å…·ä½“å®ç°(QwenDS/OAI)
    participant API as å¤–éƒ¨APIæœåŠ¡
    participant Cache as ç¼“å­˜ç³»ç»Ÿ
    
    A->>LLM: chat(messages, functions, stream=True)
    
    Note over LLM: 1. è¾“å…¥å¤„ç†
    LLM->>LLM: æ ¼å¼ç»Ÿä¸€åŒ–
    LLM->>LLM: è¯­è¨€æ£€æµ‹
    LLM->>LLM: é…ç½®åˆå¹¶
    
    Note over LLM: 2. ç¼“å­˜æ£€æŸ¥
    LLM->>Cache: æŸ¥è¯¢ç¼“å­˜
    alt ç¼“å­˜å‘½ä¸­
        Cache-->>LLM: è¿”å›ç¼“å­˜ç»“æœ
        LLM-->>A: è¿”å›ç¼“å­˜å“åº”
    else ç¼“å­˜æœªå‘½ä¸­
        Note over LLM: 3. æ¶ˆæ¯é¢„å¤„ç†
        LLM->>LLM: æˆªæ–­é•¿è¾“å…¥
        LLM->>LLM: æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        LLM->>LLM: å‡½æ•°è°ƒç”¨é¢„å¤„ç†
        
        Note over LLM: 4. è°ƒç”¨å…·ä½“å®ç°
        alt æµå¼æ¨¡å¼
            LLM->>Impl: _chat_stream(messages, delta_stream, generate_cfg)
        else éæµå¼æ¨¡å¼
            LLM->>Impl: _chat_no_stream(messages, generate_cfg)
        end
        
        Note over Impl: 5. APIè°ƒç”¨
        Impl->>API: å‘é€è¯·æ±‚
        
        loop æµå¼å“åº”
            API-->>Impl: å“åº”chunk
            Impl->>Impl: æ ¼å¼åŒ–å“åº”
            Impl-->>LLM: Messageå¯¹è±¡
            
            Note over LLM: 6. åå¤„ç†
            LLM->>LLM: å‡½æ•°è°ƒç”¨åå¤„ç†
            LLM->>LLM: åœç”¨è¯å¤„ç†
            LLM-->>A: æµå¼è¿”å›
        end
        
        Note over LLM: 7. ç¼“å­˜ç»“æœ
        LLM->>Cache: å†™å…¥ç¼“å­˜
    end
```

### é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

```mermaid
graph TD
    A[å¼€å§‹APIè°ƒç”¨] --> B[æ‰§è¡Œæ¨¡å‹æœåŠ¡]
    B --> C{è°ƒç”¨æˆåŠŸ?}
    
    C -->|æˆåŠŸ| D[è¿”å›ç»“æœ]
    
    C -->|å¤±è´¥| E[æ£€æŸ¥é”™è¯¯ç±»å‹]
    E --> F{å¯é‡è¯•é”™è¯¯?}
    
    F -->|å¦| G[æŠ›å‡ºå¼‚å¸¸]
    G --> H[ç»“æŸ]
    
    F -->|æ˜¯| I{è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°?}
    I -->|æ˜¯| G
    I -->|å¦| J[ç­‰å¾…é€€é¿æ—¶é—´]
    J --> K[å¢åŠ é‡è¯•è®¡æ•°]
    K --> B
    
    D --> L[å†™å…¥ç¼“å­˜]
    L --> M[ç»“æŸ]
    
    style A fill:#e1f5fe
    style M fill:#f3e5f5
    style H fill:#ffebee
    style G fill:#ffebee
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. ç¼“å­˜æœºåˆ¶

```python
# ç¼“å­˜keyç”Ÿæˆç­–ç•¥
def generate_cache_key(messages, functions, extra_generate_cfg):
    """ç”Ÿæˆç¼“å­˜é”®
    
    ç¼“å­˜ç­–ç•¥:
        1. åŸºäºå®Œæ•´çš„è¾“å…¥å‚æ•°ç”Ÿæˆå”¯ä¸€é”®
        2. ä½¿ç”¨JSONåºåˆ—åŒ–ç¡®ä¿ä¸€è‡´æ€§
        3. æ’åºé”®åç¡®ä¿ç›¸åŒå‚æ•°äº§ç”Ÿç›¸åŒé”®
    """
    cache_data = {
        'messages': messages,
        'functions': functions,
        'extra_generate_cfg': extra_generate_cfg
    }
    return json_dumps_compact(cache_data, sort_keys=True)

# ç¼“å­˜ä½¿ç”¨ç¤ºä¾‹
if self.cache is not None:
    cache_key = generate_cache_key(messages, functions, extra_generate_cfg)
    cached_result = self.cache.get(cache_key)
    if cached_result:
        return json.loads(cached_result)
```

### 2. è¾“å…¥é•¿åº¦ç®¡ç†

```python
def _truncate_input_messages_roughly(messages: List[Message], max_tokens: int) -> List[Message]:
    """æ™ºèƒ½è¾“å…¥æˆªæ–­ç­–ç•¥
    
    æˆªæ–­åŸåˆ™:
        1. ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼ˆæœ€é‡è¦ï¼‰
        2. ä¿ç•™æœ€æ–°çš„ç”¨æˆ·-åŠ©æ‰‹å¯¹è¯è½®æ¬¡
        3. ä¼˜å…ˆæˆªæ–­å‡½æ•°è°ƒç”¨ç»“æœï¼ˆå ç”¨ç©ºé—´å¤§ï¼‰
        4. ä¿æŒå¯¹è¯çš„è¿è´¯æ€§
    
    æˆªæ–­æ­¥éª¤:
        1. è®¡ç®—æ‰€æœ‰æ¶ˆæ¯çš„tokenæ•°é‡
        2. æŒ‰è½®æ¬¡ç»„ç»‡æ¶ˆæ¯
        3. ä»æ—§åˆ°æ–°é€æ­¥ç§»é™¤æ¶ˆæ¯
        4. ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦é™åˆ¶
    """
    # å®ç°æ™ºèƒ½æˆªæ–­é€»è¾‘
    # è¯¦ç»†å®ç°è§base.pyä¸­çš„_truncate_input_messages_roughlyå‡½æ•°
```

### 3. æµå¼è¾“å‡ºä¼˜åŒ–

```python
class StreamOptimizer:
    """æµå¼è¾“å‡ºä¼˜åŒ–å™¨"""
    
    @staticmethod
    def optimize_stream_output(response_iterator):
        """ä¼˜åŒ–æµå¼è¾“å‡º
        
        ä¼˜åŒ–ç­–ç•¥:
            1. æ‰¹é‡å¤„ç†å°chunkï¼Œå‡å°‘ç½‘ç»œå¾€è¿”
            2. é¢„æµ‹æ€§ç¼“å†²ï¼Œæå‰å‡†å¤‡ä¸‹ä¸€æ‰¹å†…å®¹
            3. è‡ªé€‚åº”å»¶è¿Ÿï¼Œåœ¨é€Ÿåº¦å’Œä½“éªŒé—´å¹³è¡¡
        """
        buffer = []
        buffer_size = 0
        max_buffer_size = 1024  # 1KBç¼“å†²
        
        for chunk in response_iterator:
            buffer.append(chunk)
            buffer_size += len(str(chunk))
            
            # è¾¾åˆ°ç¼“å†²å¤§å°æˆ–é‡åˆ°å®Œæ•´å¥å­æ—¶è¾“å‡º
            if (buffer_size >= max_buffer_size or 
                any('ã€‚' in str(c) or '.' in str(c) for c in buffer)):
                yield buffer
                buffer = []
                buffer_size = 0
        
        # è¾“å‡ºå‰©ä½™å†…å®¹
        if buffer:
            yield buffer
```

## ğŸ¯ LLMæ¨¡å—æ€»ç»“

### è®¾è®¡ä¼˜åŠ¿

1. **ç»Ÿä¸€æŠ½è±¡**: BaseChatModelæä¾›äº†ç»Ÿä¸€çš„æ¨¡å‹æ¥å£ï¼Œå±è”½åº•å±‚å·®å¼‚
2. **å¤šæœåŠ¡æ”¯æŒ**: æ”¯æŒDashScopeã€OpenAIã€Azureç­‰å¤šç§æ¨¡å‹æœåŠ¡
3. **å‡½æ•°è°ƒç”¨**: å®Œæ•´çš„å‡½æ•°è°ƒç”¨æ”¯æŒï¼ŒåŒ…æ‹¬å¹¶è¡Œè°ƒç”¨å’Œæ€ç»´é“¾
4. **æµå¼å¤„ç†**: åŸç”Ÿæ”¯æŒæµå¼è¾“å‡ºï¼Œæå‡ç”¨æˆ·ä½“éªŒ
5. **é”™è¯¯æ¢å¤**: å®Œå–„çš„é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
6. **æ€§èƒ½ä¼˜åŒ–**: ç¼“å­˜ã€æˆªæ–­ã€æ‰¹å¤„ç†ç­‰å¤šç§ä¼˜åŒ–ç­–ç•¥

### æ ¸å¿ƒç‰¹æ€§

1. **æ¨¡å‹æœåŠ¡æŠ½è±¡**: ç»Ÿä¸€çš„APIè°ƒç”¨æ¥å£
2. **é…ç½®ç®¡ç†**: çµæ´»çš„å‚æ•°é…ç½®å’Œåˆå¹¶æœºåˆ¶
3. **æ¶ˆæ¯å¤„ç†**: å®Œæ•´çš„æ¶ˆæ¯é¢„åå¤„ç†pipeline
4. **å¤šæ¨¡æ€æ”¯æŒ**: æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§è¾“å…¥
5. **ç‰ˆæœ¬å…¼å®¹**: é€‚é…ä¸åŒç‰ˆæœ¬çš„ç¬¬ä¸‰æ–¹SDK

### æ‰©å±•å»ºè®®

1. **æ¨¡å‹æœåŠ¡æ‰©å±•**: æ”¯æŒæ›´å¤šçš„æ¨¡å‹æœåŠ¡æä¾›å•†
2. **æ€§èƒ½ç›‘æ§**: å¢åŠ è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡å’Œç›‘æ§
3. **æ‰¹é‡å¤„ç†**: æ”¯æŒæ‰¹é‡è¯·æ±‚å¤„ç†æå‡æ•ˆç‡
4. **æ¨¡å‹ç®¡ç†**: å¢åŠ æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å’ŒA/Bæµ‹è¯•
5. **æˆæœ¬æ§åˆ¶**: å¢åŠ tokenä½¿ç”¨ç»Ÿè®¡å’Œæˆæœ¬æ§åˆ¶

---

*æœ¬LLMæ¨¡å—åˆ†ææ–‡æ¡£åŸºäºQwen-Agent v0.0.30ç‰ˆæœ¬ï¼Œè¯¦ç»†æè¿°äº†æ¨¡å—çš„æ¶æ„è®¾è®¡å’Œå®ç°åŸç†ã€‚*
