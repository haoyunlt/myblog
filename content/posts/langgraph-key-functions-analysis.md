---
title: "LangGraphå…³é”®å‡½æ•°æ·±åº¦è§£æï¼šæ ¸å¿ƒç®—æ³•ä¸å®ç°ç»†èŠ‚"
date: 2025-07-20T13:00:00+08:00
draft: false
featured: true
series: "langgraph-architecture"
tags: ["LangGraph", "å…³é”®å‡½æ•°", "ç®—æ³•åˆ†æ", "æºç è§£æ", "å®ç°ç»†èŠ‚"]
categories: ["langgraph", "AIæ¡†æ¶"]
author: "tommie blog"
description: "æ·±å…¥åˆ†æLangGraphæ¡†æ¶çš„å…³é”®å‡½æ•°å®ç°ï¼ŒåŒ…å«æ ¸å¿ƒç®—æ³•ã€æ•°æ®ç»“æ„å’Œä¼˜åŒ–æŠ€å·§"
showToc: true
tocOpen: true
showReadingTime: true
showWordCount: true
weight: 250
slug: "langgraph-key-functions-analysis"
---

## æ¦‚è¿°

æœ¬æ–‡æ·±å…¥åˆ†æLangGraphæ¡†æ¶ä¸­çš„å…³é”®å‡½æ•°å®ç°ï¼Œä»æ ¸å¿ƒç®—æ³•åˆ°å…·ä½“çš„ä»£ç å®ç°ï¼Œè¯¦ç»†è§£ææ¯ä¸ªå…³é”®å‡½æ•°çš„è®¾è®¡æ€è·¯ã€å®ç°ç»†èŠ‚å’Œä¼˜åŒ–æŠ€å·§ã€‚é€šè¿‡æºç çº§åˆ«çš„åˆ†æï¼Œå¸®åŠ©å¼€å‘è€…æ·±å…¥ç†è§£LangGraphçš„å†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚

<!--more-->

## 1. å›¾ç¼–è¯‘æ ¸å¿ƒå‡½æ•°

### 1.1 StateGraph.compile() - å›¾ç¼–è¯‘ä¸»å‡½æ•°

```python
# æ–‡ä»¶ï¼šlanggraph/graph/state.py
def compile(
    self,
    checkpointer: Optional[BaseCheckpointSaver] = None,
    *,
    store: Optional[BaseStore] = None,
    interrupt_before: Optional[Union[All, List[str]]] = None,
    interrupt_after: Optional[Union[All, List[str]]] = None,
    debug: bool = False,
) -> CompiledStateGraph:
    """ç¼–è¯‘çŠ¶æ€å›¾ä¸ºå¯æ‰§è¡Œå¯¹è±¡
    
    è¿™æ˜¯LangGraphæœ€æ ¸å¿ƒçš„å‡½æ•°ä¹‹ä¸€ï¼Œè´Ÿè´£å°†å£°æ˜å¼çš„å›¾å®šä¹‰è½¬æ¢ä¸º
    å¯æ‰§è¡Œçš„Pregelå¼•æ“ã€‚ç¼–è¯‘è¿‡ç¨‹åŒ…æ‹¬ï¼š
    1. å›¾ç»“æ„éªŒè¯å’Œä¼˜åŒ–
    2. é€šé“ç³»ç»Ÿåˆ›å»º
    3. èŠ‚ç‚¹ç¼–è¯‘å’ŒåŒ…è£…
    4. Pregelå¼•æ“æ„å»º
    5. æ‰§è¡Œç¯å¢ƒé…ç½®
    
    Args:
        checkpointer: æ£€æŸ¥ç‚¹ä¿å­˜å™¨ï¼Œç”¨äºçŠ¶æ€æŒä¹…åŒ–
        store: å­˜å‚¨æ¥å£ï¼Œç”¨äºå¤–éƒ¨æ•°æ®è®¿é—®
        interrupt_before: åœ¨è¿™äº›èŠ‚ç‚¹å‰ä¸­æ–­æ‰§è¡Œ
        interrupt_after: åœ¨è¿™äº›èŠ‚ç‚¹åä¸­æ–­æ‰§è¡Œ
        debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        
    Returns:
        CompiledStateGraph: ç¼–è¯‘åçš„å¯æ‰§è¡Œå›¾å¯¹è±¡
        
    Raises:
        ValueError: å›¾ç»“æ„æ— æ•ˆæ—¶
        CompilationError: ç¼–è¯‘è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶
    """
    if self._compiled:
        raise ValueError("Graph is already compiled")
    
    # === ç¬¬ä¸€é˜¶æ®µï¼šå›¾ç»“æ„éªŒè¯ ===
    self._validate_graph_structure()
    
    # === ç¬¬äºŒé˜¶æ®µï¼šä¸­æ–­é…ç½®å¤„ç† ===
    interrupt_before_nodes = self._process_interrupt_config(interrupt_before)
    interrupt_after_nodes = self._process_interrupt_config(interrupt_after)
    
    # === ç¬¬ä¸‰é˜¶æ®µï¼šé€šé“ç³»ç»Ÿåˆ›å»º ===
    channels = self._create_channel_system()
    
    # === ç¬¬å››é˜¶æ®µï¼šèŠ‚ç‚¹ç¼–è¯‘ ===
    compiled_nodes = self._compile_nodes_with_optimization()
    
    # === ç¬¬äº”é˜¶æ®µï¼šè¾¹å’Œåˆ†æ”¯å¤„ç† ===
    compiled_edges = self._compile_edges_and_branches()
    
    # === ç¬¬å…­é˜¶æ®µï¼šPregelå¼•æ“æ„å»º ===
    pregel = Pregel(
        nodes=compiled_nodes,
        channels=channels,
        input_channels=list(channels.keys()),
        output_channels=list(channels.keys()),
        stream_channels=list(channels.keys()),
        checkpointer=checkpointer,
        store=store,
        interrupt_before=interrupt_before_nodes,
        interrupt_after=interrupt_after_nodes,
        debug=debug,
        step_timeout=getattr(self, 'step_timeout', None),
        retry_policy=getattr(self, 'retry_policy', None),
    )
    
    # === ç¬¬ä¸ƒé˜¶æ®µï¼šç¼–è¯‘å®Œæˆæ ‡è®° ===
    self._compiled = True
    self._compilation_timestamp = time.time()
    
    return CompiledStateGraph(pregel)

def _validate_graph_structure(self) -> None:
    """éªŒè¯å›¾ç»“æ„çš„å®Œæ•´æ€§å’Œæ­£ç¡®æ€§
    
    è¿™ä¸ªå‡½æ•°æ‰§è¡Œå…¨é¢çš„å›¾ç»“æ„éªŒè¯ï¼Œç¡®ä¿å›¾åœ¨ç¼–è¯‘å‰æ˜¯æœ‰æ•ˆçš„ï¼š
    1. åŸºæœ¬ç»“æ„æ£€æŸ¥
    2. è¿é€šæ€§åˆ†æ
    3. å¾ªç¯æ£€æµ‹
    4. æ­»é”åˆ†æ
    5. ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥
    
    Raises:
        ValueError: å›¾ç»“æ„æ— æ•ˆæ—¶
    """
    # 1. åŸºæœ¬ç»“æ„æ£€æŸ¥
    if not self.nodes:
        raise ValueError("Graph must have at least one node")
    
    # 2. å…¥å£ç‚¹æ£€æŸ¥
    entry_nodes = self._find_entry_nodes()
    if not entry_nodes:
        raise ValueError("Graph must have at least one entry point")
    
    # 3. è¿é€šæ€§åˆ†æ
    reachable_nodes = self._analyze_reachability(entry_nodes)
    unreachable_nodes = set(self.nodes.keys()) - reachable_nodes
    if unreachable_nodes:
        logger.warning(f"Unreachable nodes detected: {unreachable_nodes}")
    
    # 4. å¾ªç¯æ£€æµ‹
    cycles = self._detect_cycles()
    if cycles:
        # åŒºåˆ†æœ‰å®³å¾ªç¯å’Œæœ‰ç›Šå¾ªç¯
        harmful_cycles = self._filter_harmful_cycles(cycles)
        if harmful_cycles:
            raise ValueError(f"Harmful cycles detected: {harmful_cycles}")
    
    # 5. æ­»é”åˆ†æ
    potential_deadlocks = self._analyze_deadlocks()
    if potential_deadlocks:
        raise ValueError(f"Potential deadlocks detected: {potential_deadlocks}")
    
    # 6. ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥
    type_errors = self._check_type_consistency()
    if type_errors:
        raise ValueError(f"Type consistency errors: {type_errors}")

def _find_entry_nodes(self) -> Set[str]:
    """æŸ¥æ‰¾å›¾çš„å…¥å£èŠ‚ç‚¹
    
    å…¥å£èŠ‚ç‚¹æ˜¯æ²¡æœ‰å‰é©±èŠ‚ç‚¹æˆ–æ˜¾å¼æ ‡è®°ä¸ºå…¥å£çš„èŠ‚ç‚¹ã€‚
    è¿™ä¸ªå‡½æ•°ä½¿ç”¨å›¾è®ºç®—æ³•æ¥è¯†åˆ«æ‰€æœ‰å¯èƒ½çš„å…¥å£ç‚¹ã€‚
    
    Returns:
        Set[str]: å…¥å£èŠ‚ç‚¹é›†åˆ
        
    ç®—æ³•ï¼š
    1. æ”¶é›†æ‰€æœ‰æœ‰å‰é©±çš„èŠ‚ç‚¹
    2. å‰©ä½™èŠ‚ç‚¹å³ä¸ºæ½œåœ¨å…¥å£èŠ‚ç‚¹
    3. æ£€æŸ¥æ˜¾å¼å…¥å£ç‚¹è®¾ç½®
    4. éªŒè¯å…¥å£ç‚¹çš„æœ‰æ•ˆæ€§
    """
    # æ”¶é›†æ‰€æœ‰æœ‰å‰é©±çš„èŠ‚ç‚¹
    nodes_with_predecessors = set()
    
    # ä»è¾¹æ”¶é›†å‰é©±ä¿¡æ¯
    for start_node, end_node in self.edges:
        if end_node != END:
            nodes_with_predecessors.add(end_node)
    
    # ä»åˆ†æ”¯æ”¶é›†å‰é©±ä¿¡æ¯
    for start_node, branch in self.branches.items():
        for target_node in branch.path_map.values():
            if target_node != END:
                nodes_with_predecessors.add(target_node)
    
    # æ‰¾å‡ºæ²¡æœ‰å‰é©±çš„èŠ‚ç‚¹
    entry_candidates = set(self.nodes.keys()) - nodes_with_predecessors
    
    # å¤„ç†æ˜¾å¼å…¥å£ç‚¹
    if self.entry_point:
        if self.entry_point not in self.nodes:
            raise ValueError(f"Explicit entry point '{self.entry_point}' not found")
        entry_candidates.add(self.entry_point)
    
    return entry_candidates

def _analyze_reachability(self, entry_nodes: Set[str]) -> Set[str]:
    """åˆ†æå›¾çš„å¯è¾¾æ€§
    
    ä½¿ç”¨æ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰ç®—æ³•åˆ†æä»å…¥å£èŠ‚ç‚¹å¯ä»¥åˆ°è¾¾çš„æ‰€æœ‰èŠ‚ç‚¹ã€‚
    è¿™æœ‰åŠ©äºè¯†åˆ«å­¤ç«‹çš„èŠ‚ç‚¹å’Œä¸å¯è¾¾çš„ä»£ç è·¯å¾„ã€‚
    
    Args:
        entry_nodes: å…¥å£èŠ‚ç‚¹é›†åˆ
        
    Returns:
        Set[str]: å¯è¾¾èŠ‚ç‚¹é›†åˆ
        
    ç®—æ³•ï¼š
    1. ä»æ¯ä¸ªå…¥å£èŠ‚ç‚¹å¼€å§‹DFS
    2. éå†æ‰€æœ‰å¯èƒ½çš„è·¯å¾„
    3. å¤„ç†æ¡ä»¶åˆ†æ”¯
    4. è®°å½•è®¿é—®è¿‡çš„èŠ‚ç‚¹
    """
    reachable = set()
    visited = set()
    
    def dfs(node: str):
        """æ·±åº¦ä¼˜å…ˆæœç´¢å®ç°"""
        if node in visited or node == END:
            return
        
        visited.add(node)
        reachable.add(node)
        
        # éå†ç›´æ¥è¾¹
        for start, end in self.edges:
            if start == node:
                dfs(end)
        
        # éå†æ¡ä»¶åˆ†æ”¯
        if node in self.branches:
            branch = self.branches[node]
            for target in branch.path_map.values():
                dfs(target)
            if branch.then:
                dfs(branch.then)
    
    # ä»æ‰€æœ‰å…¥å£èŠ‚ç‚¹å¼€å§‹æœç´¢
    for entry_node in entry_nodes:
        dfs(entry_node)
    
    return reachable

def _detect_cycles(self) -> List[List[str]]:
    """æ£€æµ‹å›¾ä¸­çš„å¾ªç¯
    
    ä½¿ç”¨æ”¹è¿›çš„æ·±åº¦ä¼˜å…ˆæœç´¢ç®—æ³•æ£€æµ‹å›¾ä¸­çš„æ‰€æœ‰å¾ªç¯ã€‚
    è¿™ä¸ªå®ç°èƒ½å¤Ÿæ‰¾åˆ°æ‰€æœ‰å¼ºè¿é€šåˆ†é‡å’Œç®€å•å¾ªç¯ã€‚
    
    Returns:
        List[List[str]]: æ£€æµ‹åˆ°çš„å¾ªç¯åˆ—è¡¨ï¼Œæ¯ä¸ªå¾ªç¯æ˜¯èŠ‚ç‚¹åˆ—è¡¨
        
    ç®—æ³•ï¼š
    1. ä½¿ç”¨ä¸‰è‰²æ ‡è®°æ³•è¿›è¡ŒDFS
    2. ç™½è‰²ï¼šæœªè®¿é—®
    3. ç°è‰²ï¼šæ­£åœ¨è®¿é—®ï¼ˆåœ¨å½“å‰è·¯å¾„ä¸Šï¼‰
    4. é»‘è‰²ï¼šå·²å®Œæˆè®¿é—®
    5. å½“é‡åˆ°ç°è‰²èŠ‚ç‚¹æ—¶ï¼Œå‘ç°å¾ªç¯
    """
    WHITE, GRAY, BLACK = 0, 1, 2
    colors = {node: WHITE for node in self.nodes}
    cycles = []
    current_path = []
    
    def dfs_cycle_detection(node: str) -> bool:
        """DFSå¾ªç¯æ£€æµ‹å®ç°"""
        if colors[node] == GRAY:
            # å‘ç°å¾ªç¯
            cycle_start = current_path.index(node)
            cycle = current_path[cycle_start:] + [node]
            cycles.append(cycle)
            return True
        
        if colors[node] == BLACK:
            return False
        
        # æ ‡è®°ä¸ºæ­£åœ¨è®¿é—®
        colors[node] = GRAY
        current_path.append(node)
        
        # è®¿é—®æ‰€æœ‰é‚»å±…
        neighbors = self._get_node_neighbors(node)
        for neighbor in neighbors:
            if neighbor != END:
                dfs_cycle_detection(neighbor)
        
        # æ ‡è®°ä¸ºå·²å®Œæˆ
        colors[node] = BLACK
        current_path.pop()
        return False
    
    # å¯¹æ‰€æœ‰èŠ‚ç‚¹è¿›è¡ŒDFS
    for node in self.nodes:
        if colors[node] == WHITE:
            dfs_cycle_detection(node)
    
    return cycles

def _get_node_neighbors(self, node: str) -> List[str]:
    """è·å–èŠ‚ç‚¹çš„æ‰€æœ‰é‚»å±…èŠ‚ç‚¹
    
    Args:
        node: èŠ‚ç‚¹åç§°
        
    Returns:
        List[str]: é‚»å±…èŠ‚ç‚¹åˆ—è¡¨
    """
    neighbors = []
    
    # ä»ç›´æ¥è¾¹è·å–é‚»å±…
    for start, end in self.edges:
        if start == node:
            neighbors.append(end)
    
    # ä»æ¡ä»¶åˆ†æ”¯è·å–é‚»å±…
    if node in self.branches:
        branch = self.branches[node]
        neighbors.extend(branch.path_map.values())
        if branch.then:
            neighbors.append(branch.then)
    
    return neighbors
```

### 1.2 _create_channel_system() - é€šé“ç³»ç»Ÿåˆ›å»º

```python
def _create_channel_system(self) -> Dict[str, BaseChannel]:
    """åˆ›å»ºé€šé“ç³»ç»Ÿ
    
    é€šé“ç³»ç»Ÿæ˜¯LangGraphçŠ¶æ€ç®¡ç†çš„æ ¸å¿ƒï¼Œè´Ÿè´£ï¼š
    1. çŠ¶æ€æ•°æ®çš„å­˜å‚¨å’Œä¼ é€’
    2. çŠ¶æ€æ›´æ–°çš„èšåˆå’Œåˆå¹¶
    3. ç‰ˆæœ¬æ§åˆ¶å’Œå˜æ›´è¿½è¸ª
    4. ç±»å‹å®‰å…¨å’Œæ•°æ®éªŒè¯
    
    Returns:
        Dict[str, BaseChannel]: é€šé“ååˆ°é€šé“å¯¹è±¡çš„æ˜ å°„
        
    è®¾è®¡åŸåˆ™ï¼š
    - æ¯ä¸ªçŠ¶æ€å­—æ®µå¯¹åº”ä¸€ä¸ªé€šé“
    - é€šé“ç±»å‹æ ¹æ®å­—æ®µç‰¹æ€§è‡ªåŠ¨é€‰æ‹©
    - æ”¯æŒè‡ªå®šä¹‰reducerå‡½æ•°
    - æä¾›é»˜è®¤å€¼å’Œç±»å‹éªŒè¯
    """
    channels = {}
    
    # åŸºäºçŠ¶æ€æ¨¡å¼åˆ›å»ºé€šé“
    if hasattr(self.state_schema, '__annotations__'):
        for field_name, field_spec in self._channel_specs.items():
            channel = self._create_channel_for_field(field_name, field_spec)
            channels[field_name] = channel
    else:
        # é»˜è®¤æ ¹é€šé“ï¼ˆç”¨äºéç»“æ„åŒ–çŠ¶æ€ï¼‰
        channels["__root__"] = LastValue(self.state_schema)
    
    # æ·»åŠ ç³»ç»Ÿé€šé“
    channels.update(self._create_system_channels())
    
    # éªŒè¯é€šé“é…ç½®
    self._validate_channel_configuration(channels)
    
    return channels

def _create_channel_for_field(
    self, 
    field_name: str, 
    field_spec: ChannelSpec
) -> BaseChannel:
    """ä¸ºçŠ¶æ€å­—æ®µåˆ›å»ºé€šé“
    
    æ ¹æ®å­—æ®µçš„ç‰¹æ€§é€‰æ‹©æœ€é€‚åˆçš„é€šé“ç±»å‹ï¼š
    - æœ‰reducerå‡½æ•°ï¼šä½¿ç”¨BinaryOperatorAggregate
    - åˆ—è¡¨ç±»å‹ï¼šä½¿ç”¨Topicé€šé“
    - ç®€å•ç±»å‹ï¼šä½¿ç”¨LastValueé€šé“
    - é›†åˆç±»å‹ï¼šä½¿ç”¨ç‰¹æ®Šçš„é›†åˆé€šé“
    
    Args:
        field_name: å­—æ®µåç§°
        field_spec: å­—æ®µè§„æ ¼
        
    Returns:
        BaseChannel: åˆ›å»ºçš„é€šé“å¯¹è±¡
    """
    field_type = field_spec.type
    reducer = field_spec.reducer
    default_value = field_spec.default
    
    if reducer:
        # æœ‰reducerå‡½æ•°çš„å­—æ®µä½¿ç”¨BinaryOperatorAggregate
        return BinaryOperatorAggregate(
            typ=field_type,
            operator=reducer,
            default=default_value
        )
    elif self._is_list_type(field_type):
        # åˆ—è¡¨ç±»å‹ä½¿ç”¨Topicé€šé“ï¼ˆæ”¯æŒæ¶ˆæ¯ç´¯ç§¯ï¼‰
        return Topic(
            typ=field_type,
            accumulate=True,
            unique=False,
            default=default_value or []
        )
    elif self._is_set_type(field_type):
        # é›†åˆç±»å‹ä½¿ç”¨å»é‡çš„Topicé€šé“
        return Topic(
            typ=field_type,
            accumulate=True,
            unique=True,
            default=default_value or set()
        )
    elif self._is_dict_type(field_type):
        # å­—å…¸ç±»å‹ä½¿ç”¨ç‰¹æ®Šçš„å­—å…¸åˆå¹¶é€šé“
        return DictMergeChannel(
            typ=field_type,
            default=default_value or {}
        )
    else:
        # é»˜è®¤ä½¿ç”¨LastValueé€šé“
        return LastValue(
            typ=field_type,
            default=default_value
        )

def _create_system_channels(self) -> Dict[str, BaseChannel]:
    """åˆ›å»ºç³»ç»Ÿé€šé“
    
    ç³»ç»Ÿé€šé“ç”¨äºæ¡†æ¶å†…éƒ¨çš„çŠ¶æ€ç®¡ç†å’Œæ§åˆ¶æµï¼š
    - __pregel_loop: å¾ªç¯è®¡æ•°å™¨
    - __pregel_step: æ­¥éª¤è®¡æ•°å™¨  
    - __pregel_task: å½“å‰ä»»åŠ¡ä¿¡æ¯
    - __pregel_resume: æ¢å¤æ ‡è®°
    
    Returns:
        Dict[str, BaseChannel]: ç³»ç»Ÿé€šé“æ˜ å°„
    """
    system_channels = {}
    
    # å¾ªç¯è®¡æ•°å™¨é€šé“
    system_channels["__pregel_loop"] = LastValue(
        typ=int,
        default=0
    )
    
    # æ­¥éª¤è®¡æ•°å™¨é€šé“
    system_channels["__pregel_step"] = LastValue(
        typ=int,
        default=0
    )
    
    # ä»»åŠ¡ä¿¡æ¯é€šé“
    system_channels["__pregel_task"] = LastValue(
        typ=Optional[str],
        default=None
    )
    
    # æ¢å¤æ ‡è®°é€šé“
    system_channels["__pregel_resume"] = LastValue(
        typ=bool,
        default=False
    )
    
    return system_channels

def _is_list_type(self, field_type: Any) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºåˆ—è¡¨ç±»å‹"""
    if hasattr(field_type, '__origin__'):
        return field_type.__origin__ in (list, List)
    return field_type in (list, List)

def _is_set_type(self, field_type: Any) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºé›†åˆç±»å‹"""
    if hasattr(field_type, '__origin__'):
        return field_type.__origin__ in (set, Set)
    return field_type in (set, Set)

def _is_dict_type(self, field_type: Any) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºå­—å…¸ç±»å‹"""
    if hasattr(field_type, '__origin__'):
        return field_type.__origin__ in (dict, Dict)
    return field_type in (dict, Dict)
```

## 2. Pregelæ‰§è¡Œæ ¸å¿ƒå‡½æ•°

### 2.1 Pregel._execute_main_loop() - ä¸»æ‰§è¡Œå¾ªç¯

```python
# æ–‡ä»¶ï¼šlanggraph/pregel/__init__.py
def _execute_main_loop(
    self,
    context: ExecutionContext,
    stream_mode: StreamMode,
    output_keys: Optional[Union[str, Sequence[str]]]
) -> Iterator[Union[dict, Any]]:
    """æ‰§è¡Œä¸»å¾ªç¯ - Pregelå¼•æ“çš„æ ¸å¿ƒ
    
    è¿™æ˜¯Pregelæ‰§è¡Œå¼•æ“çš„å¿ƒè„ï¼Œå®ç°äº†BSPï¼ˆBulk Synchronous Parallelï¼‰
    æ‰§è¡Œæ¨¡å‹ã€‚æ¯ä¸ªè¶…æ­¥åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š
    1. è®¡åˆ’é˜¶æ®µï¼šç¡®å®šæ´»è·ƒä»»åŠ¡
    2. æ‰§è¡Œé˜¶æ®µï¼šå¹¶è¡Œæ‰§è¡Œä»»åŠ¡
    3. åŒæ­¥é˜¶æ®µï¼šæ›´æ–°çŠ¶æ€å’Œæ£€æŸ¥ç‚¹
    
    Args:
        context: æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ŒåŒ…å«çŠ¶æ€å’Œé…ç½®
        stream_mode: æµæ¨¡å¼ï¼Œæ§åˆ¶è¾“å‡ºæ ¼å¼
        output_keys: è¾“å‡ºé”®è¿‡æ»¤
        
    Yields:
        Union[dict, Any]: æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä¸­é—´ç»“æœ
        
    BSPæ¨¡å‹çš„ä¼˜åŠ¿ï¼š
    - ç¡®ä¿çŠ¶æ€ä¸€è‡´æ€§
    - æ”¯æŒå¹¶è¡Œæ‰§è¡Œ
    - ç®€åŒ–é”™è¯¯å¤„ç†
    - ä¾¿äºæ£€æŸ¥ç‚¹ä¿å­˜
    """
    try:
        # è¾“å‡ºåˆå§‹çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if stream_mode == "values":
            initial_output = self._extract_output_values(context.checkpoint, output_keys)
            if initial_output:
                yield initial_output
        
        # === ä¸»æ‰§è¡Œå¾ªç¯ ===
        while True:
            # === è¶…æ­¥å¼€å§‹ ===
            superstep_start_time = time.time()
            
            # === é˜¶æ®µ1ï¼šè®¡åˆ’é˜¶æ®µ ===
            planning_start = time.time()
            tasks = self._task_scheduler.plan_execution_step(context)
            planning_duration = time.time() - planning_start
            
            if not tasks:
                # æ²¡æœ‰æ›´å¤šä»»åŠ¡ï¼Œæ‰§è¡Œå®Œæˆ
                context.stop_reason = StopReason.COMPLETED
                if self.debug:
                    print(f"ğŸ æ‰§è¡Œå®Œæˆï¼Œæ€»å…± {context.step} æ­¥")
                break
            
            if self.debug:
                print(f"ğŸ“‹ æ­¥éª¤ {context.step}: è®¡åˆ’æ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡")
                for task in tasks:
                    print(f"  - {task.name} (ä¼˜å…ˆçº§: {task.priority})")
            
            # === é˜¶æ®µ2ï¼šä¸­æ–­æ£€æŸ¥ï¼ˆæ‰§è¡Œå‰ï¼‰===
            if self._should_interrupt_before(tasks, context):
                context.stop_reason = StopReason.INTERRUPT_BEFORE
                interrupt_output = self._create_interrupt_output(context, tasks, "before")
                if self.debug:
                    print(f"â¸ï¸  æ‰§è¡Œå‰ä¸­æ–­: {[t.name for t in tasks]}")
                yield interrupt_output
                break
            
            # === é˜¶æ®µ3ï¼šæ‰§è¡Œé˜¶æ®µ ===
            execution_start = time.time()
            step_results = self._execute_superstep(tasks, context)
            execution_duration = time.time() - execution_start
            
            # === é˜¶æ®µ4ï¼šåŒæ­¥é˜¶æ®µ ===
            sync_start = time.time()
            self._synchronize_state_updates(step_results, context)
            sync_duration = time.time() - sync_start
            
            # === é˜¶æ®µ5ï¼šæ£€æŸ¥ç‚¹ä¿å­˜ ===
            checkpoint_start = time.time()
            if self.checkpointer:
                self._save_checkpoint_with_retry(context, step_results)
            checkpoint_duration = time.time() - checkpoint_start
            
            # === é˜¶æ®µ6ï¼šä¸­æ–­æ£€æŸ¥ï¼ˆæ‰§è¡Œåï¼‰===
            if self._should_interrupt_after(tasks, context):
                context.stop_reason = StopReason.INTERRUPT_AFTER
                interrupt_output = self._create_interrupt_output(context, tasks, "after")
                if self.debug:
                    print(f"â¸ï¸  æ‰§è¡Œåä¸­æ–­: {[t.name for t in tasks]}")
                yield interrupt_output
                break
            
            # === é˜¶æ®µ7ï¼šè¾“å‡ºç”Ÿæˆ ===
            output_start = time.time()
            step_output = self._generate_step_output(
                context, step_results, stream_mode, output_keys
            )
            output_duration = time.time() - output_start
            
            if step_output:
                yield step_output
            
            # === è¶…æ­¥å®Œæˆç»Ÿè®¡ ===
            superstep_duration = time.time() - superstep_start_time
            
            if self._stats:
                self._stats.record_superstep(
                    step=context.step,
                    tasks_count=len(tasks),
                    planning_time=planning_duration,
                    execution_time=execution_duration,
                    sync_time=sync_duration,
                    checkpoint_time=checkpoint_duration,
                    output_time=output_duration,
                    total_time=superstep_duration,
                    success_count=sum(1 for r in step_results.values() 
                                    if not isinstance(r, PregelTaskError)),
                    error_count=sum(1 for r in step_results.values() 
                                  if isinstance(r, PregelTaskError))
                )
            
            if self.debug:
                print(f"â±ï¸  æ­¥éª¤ {context.step} å®Œæˆ: {superstep_duration:.3f}s "
                      f"(è®¡åˆ’: {planning_duration:.3f}s, "
                      f"æ‰§è¡Œ: {execution_duration:.3f}s, "
                      f"åŒæ­¥: {sync_duration:.3f}s)")
            
            # === æ­¥éª¤é€’å¢ ===
            context.step += 1
            
            # === æ‰§è¡Œé™åˆ¶æ£€æŸ¥ ===
            if self._should_stop_execution(context):
                context.stop_reason = StopReason.LIMIT_REACHED
                if self.debug:
                    print(f"ğŸ›‘ è¾¾åˆ°æ‰§è¡Œé™åˆ¶ï¼Œåœæ­¢æ‰§è¡Œ")
                break
    
    except Exception as e:
        context.exception = e
        context.stop_reason = StopReason.ERROR
        
        if self.debug:
            print(f"ğŸ’¥ æ‰§è¡Œé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        
        if context.debug:
            error_output = self._create_error_output(context, e)
            yield error_output
        
        raise
    
    finally:
        # æ¸…ç†æ‰§è¡Œä¸Šä¸‹æ–‡
        self._cleanup_execution_context(context)
        
        if self.debug:
            print(f"ğŸ§¹ æ‰§è¡Œä¸Šä¸‹æ–‡å·²æ¸…ç†")

def _execute_superstep(
    self,
    tasks: List[PregelTask],
    context: ExecutionContext
) -> Dict[str, Any]:
    """æ‰§è¡Œè¶…æ­¥ä¸­çš„æ‰€æœ‰ä»»åŠ¡
    
    è¿™ä¸ªå‡½æ•°å®ç°äº†BSPæ¨¡å‹çš„æ‰§è¡Œé˜¶æ®µï¼Œæ”¯æŒï¼š
    1. å¹¶è¡Œä»»åŠ¡æ‰§è¡Œ
    2. é”™è¯¯éš”ç¦»å’Œå¤„ç†
    3. è¶…æ—¶æ§åˆ¶
    4. èµ„æºç®¡ç†
    5. æ€§èƒ½ç›‘æ§
    
    Args:
        tasks: å¾…æ‰§è¡Œä»»åŠ¡åˆ—è¡¨
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        
    Returns:
        Dict[str, Any]: ä»»åŠ¡ååˆ°æ‰§è¡Œç»“æœçš„æ˜ å°„
        
    å¹¶è¡Œç­–ç•¥ï¼š
    - å•ä»»åŠ¡ï¼šç›´æ¥æ‰§è¡Œ
    - å¤šä»»åŠ¡ï¼šä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
    - èµ„æºé™åˆ¶ï¼šæ§åˆ¶å¹¶å‘æ•°é‡
    - é”™è¯¯éš”ç¦»ï¼šå•ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡
    """
    if not tasks:
        return {}
    
    if len(tasks) == 1:
        # å•ä»»åŠ¡ä¼˜åŒ–è·¯å¾„
        task = tasks[0]
        result = self._execute_single_task_with_monitoring(task, context)
        return {task.name: result}
    else:
        # å¤šä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ
        return self._execute_parallel_tasks_with_optimization(tasks, context)

def _execute_single_task_with_monitoring(
    self,
    task: PregelTask,
    context: ExecutionContext
) -> Any:
    """æ‰§è¡Œå•ä¸ªä»»åŠ¡ï¼ˆå¸¦ç›‘æ§ï¼‰
    
    Args:
        task: è¦æ‰§è¡Œçš„ä»»åŠ¡
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        
    Returns:
        Any: ä»»åŠ¡æ‰§è¡Œç»“æœæˆ–é”™è¯¯å¯¹è±¡
        
    æ‰§è¡Œæµç¨‹ï¼š
    1. é¢„æ‰§è¡Œæ£€æŸ¥
    2. èµ„æºåˆ†é…
    3. ä»»åŠ¡æ‰§è¡Œ
    4. ç»“æœéªŒè¯
    5. èµ„æºé‡Šæ”¾
    6. ç»Ÿè®¡è®°å½•
    """
    task_start_time = time.time()
    
    try:
        # é¢„æ‰§è¡Œæ£€æŸ¥
        self._pre_execution_check(task, context)
        
        # èµ„æºåˆ†é…
        resources = self._allocate_task_resources(task)
        
        try:
            # æ‰§è¡Œä»»åŠ¡
            if self.step_timeout:
                # å¸¦è¶…æ—¶çš„æ‰§è¡Œ
                result = self._execute_with_timeout(task, context, self.step_timeout)
            else:
                # æ™®é€šæ‰§è¡Œ
                result = self._invoke_task_action(task, context)
            
            # ç»“æœéªŒè¯
            validated_result = self._validate_task_result(task, result)
            
            # è®°å½•æˆåŠŸç»Ÿè®¡
            if self._stats:
                duration = time.time() - task_start_time
                self._stats.record_task_success(
                    task.name, duration, self._estimate_result_size(validated_result)
                )
            
            return validated_result
            
        finally:
            # é‡Šæ”¾èµ„æº
            self._release_task_resources(task, resources)
    
    except Exception as e:
        # é”™è¯¯å¤„ç†
        duration = time.time() - task_start_time
        
        if self._stats:
            self._stats.record_task_error(task.name, duration, str(e))
        
        # é‡è¯•é€»è¾‘
        if self._should_retry_task(task, e):
            task.retry_count += 1
            if self.debug:
                print(f"ğŸ”„ é‡è¯•ä»»åŠ¡ {task.name} (ç¬¬ {task.retry_count} æ¬¡)")
            
            # æŒ‡æ•°é€€é¿
            retry_delay = min(2 ** task.retry_count, 60)  # æœ€å¤§60ç§’
            time.sleep(retry_delay)
            
            return self._execute_single_task_with_monitoring(task, context)
        
        # åŒ…è£…ä¸ºä»»åŠ¡é”™è¯¯
        return PregelTaskError(
            task_name=task.name,
            error=e,
            retry_count=task.retry_count,
            task_id=task.id
        )

def _execute_parallel_tasks_with_optimization(
    self,
    tasks: List[PregelTask],
    context: ExecutionContext
) -> Dict[str, Any]:
    """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡ï¼ˆå¸¦ä¼˜åŒ–ï¼‰
    
    Args:
        tasks: ä»»åŠ¡åˆ—è¡¨
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        
    Returns:
        Dict[str, Any]: ä»»åŠ¡æ‰§è¡Œç»“æœæ˜ å°„
        
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. æ™ºèƒ½çº¿ç¨‹æ± å¤§å°è°ƒæ•´
    2. ä»»åŠ¡ä¼˜å…ˆçº§æ’åº
    3. èµ„æºæ„ŸçŸ¥è°ƒåº¦
    4. é”™è¯¯å¿«é€Ÿå¤±è´¥
    5. å†…å­˜ä½¿ç”¨ä¼˜åŒ–
    """
    import concurrent.futures
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    # è®¡ç®—æœ€ä¼˜çº¿ç¨‹æ± å¤§å°
    optimal_workers = self._calculate_optimal_worker_count(tasks, context)
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºä»»åŠ¡
    sorted_tasks = sorted(tasks, key=lambda t: t.priority, reverse=True)
    
    results = {}
    
    with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {
            executor.submit(self._execute_single_task_with_monitoring, task, context): task
            for task in sorted_tasks
        }
        
        # æ”¶é›†ç»“æœ
        completed_count = 0
        total_count = len(tasks)
        
        for future in as_completed(future_to_task):
            task = future_to_task[future]
            completed_count += 1
            
            try:
                result = future.result()
                results[task.name] = result
                
                if self.debug:
                    print(f"âœ… ä»»åŠ¡ {task.name} å®Œæˆ ({completed_count}/{total_count})")
                
            except Exception as e:
                # ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºå¼‚å¸¸å·²åœ¨å•ä»»åŠ¡æ‰§è¡Œä¸­å¤„ç†ï¼‰
                error_result = PregelTaskError(
                    task_name=task.name,
                    error=e,
                    retry_count=0,
                    task_id=task.id
                )
                results[task.name] = error_result
                
                if self.debug:
                    print(f"âŒ ä»»åŠ¡ {task.name} å¼‚å¸¸: {e}")
    
    return results

def _calculate_optimal_worker_count(
    self,
    tasks: List[PregelTask],
    context: ExecutionContext
) -> int:
    """è®¡ç®—æœ€ä¼˜å·¥ä½œçº¿ç¨‹æ•°é‡
    
    Args:
        tasks: ä»»åŠ¡åˆ—è¡¨
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        
    Returns:
        int: æœ€ä¼˜çº¿ç¨‹æ•°é‡
        
    è®¡ç®—ç­–ç•¥ï¼š
    1. åŸºäºCPUæ ¸å¿ƒæ•°
    2. è€ƒè™‘ä»»åŠ¡ç±»å‹ï¼ˆCPUå¯†é›† vs IOå¯†é›†ï¼‰
    3. å†…å­˜é™åˆ¶
    4. ç³»ç»Ÿè´Ÿè½½
    """
    import os
    import psutil
    
    # åŸºç¡€çº¿ç¨‹æ•°ï¼ˆCPUæ ¸å¿ƒæ•°ï¼‰
    cpu_count = os.cpu_count() or 4
    
    # åˆ†æä»»åŠ¡ç±»å‹
    io_intensive_count = sum(1 for task in tasks if self._is_io_intensive_task(task))
    cpu_intensive_count = len(tasks) - io_intensive_count
    
    # è®¡ç®—å»ºè®®çº¿ç¨‹æ•°
    if io_intensive_count > cpu_intensive_count:
        # IOå¯†é›†å‹ä»»åŠ¡å å¤šæ•°ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤šçº¿ç¨‹
        suggested_workers = min(len(tasks), cpu_count * 4)
    else:
        # CPUå¯†é›†å‹ä»»åŠ¡å å¤šæ•°ï¼Œé™åˆ¶çº¿ç¨‹æ•°
        suggested_workers = min(len(tasks), cpu_count)
    
    # è€ƒè™‘å†…å­˜é™åˆ¶
    available_memory = psutil.virtual_memory().available
    estimated_memory_per_task = 100 * 1024 * 1024  # 100MB per task
    memory_limited_workers = max(1, available_memory // estimated_memory_per_task)
    
    # å–æœ€å°å€¼ä½œä¸ºæœ€ç»ˆç»“æœ
    optimal_workers = min(suggested_workers, memory_limited_workers, 20)  # æœ€å¤§20ä¸ªçº¿ç¨‹
    
    if self.debug:
        print(f"ğŸ§µ ä½¿ç”¨ {optimal_workers} ä¸ªå·¥ä½œçº¿ç¨‹æ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡")
    
    return optimal_workers

def _is_io_intensive_task(self, task: PregelTask) -> bool:
    """åˆ¤æ–­ä»»åŠ¡æ˜¯å¦ä¸ºIOå¯†é›†å‹
    
    Args:
        task: ä»»åŠ¡å¯¹è±¡
        
    Returns:
        bool: æ˜¯å¦ä¸ºIOå¯†é›†å‹ä»»åŠ¡
        
    åˆ¤æ–­ä¾æ®ï¼š
    1. ä»»åŠ¡å…ƒæ•°æ®æ ‡è®°
    2. èŠ‚ç‚¹ç±»å‹åˆ†æ
    3. å†å²æ‰§è¡Œæ¨¡å¼
    """
    # æ£€æŸ¥ä»»åŠ¡å…ƒæ•°æ®
    if task.node.metadata.get("task_type") == "io_intensive":
        return True
    
    # æ£€æŸ¥èŠ‚ç‚¹ç±»å‹
    node_name = task.name.lower()
    io_keywords = ["http", "api", "request", "fetch", "download", "upload", "database", "db"]
    
    if any(keyword in node_name for keyword in io_keywords):
        return True
    
    # é»˜è®¤å‡è®¾ä¸ºCPUå¯†é›†å‹
    return False
```

### 2.2 _synchronize_state_updates() - çŠ¶æ€åŒæ­¥

```python
def _synchronize_state_updates(
    self,
    step_results: Dict[str, Any],
    context: ExecutionContext
) -> None:
    """åŒæ­¥çŠ¶æ€æ›´æ–°
    
    è¿™æ˜¯BSPæ¨¡å‹åŒæ­¥é˜¶æ®µçš„æ ¸å¿ƒå®ç°ï¼Œè´Ÿè´£ï¼š
    1. æ”¶é›†æ‰€æœ‰ä»»åŠ¡çš„çŠ¶æ€æ›´æ–°
    2. è§£å†³æ›´æ–°å†²çª
    3. åº”ç”¨çŠ¶æ€å˜æ›´
    4. æ›´æ–°ç‰ˆæœ¬ä¿¡æ¯
    5. è§¦å‘çŠ¶æ€å˜æ›´äº‹ä»¶
    
    Args:
        step_results: æ­¥éª¤æ‰§è¡Œç»“æœ
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        
    åŒæ­¥ç­–ç•¥ï¼š
    - åŸå­æ€§ï¼šæ‰€æœ‰æ›´æ–°è¦ä¹ˆå…¨éƒ¨æˆåŠŸï¼Œè¦ä¹ˆå…¨éƒ¨å¤±è´¥
    - ä¸€è‡´æ€§ï¼šç¡®ä¿çŠ¶æ€çš„ä¸€è‡´æ€§çº¦æŸ
    - éš”ç¦»æ€§ï¼šä¸åŒçº¿ç¨‹çš„æ›´æ–°äº’ä¸å¹²æ‰°
    - æŒä¹…æ€§ï¼šæ›´æ–°åçš„çŠ¶æ€å¯ä»¥æŒä¹…åŒ–
    """
    sync_start_time = time.time()
    
    try:
        # === ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†çŠ¶æ€æ›´æ–° ===
        all_updates = self._collect_state_updates(step_results, context)
        
        if not all_updates:
            # æ²¡æœ‰çŠ¶æ€æ›´æ–°ï¼Œç›´æ¥è¿”å›
            return
        
        # === ç¬¬äºŒé˜¶æ®µï¼šå†²çªæ£€æµ‹å’Œè§£å†³ ===
        resolved_updates = self._resolve_update_conflicts(all_updates, context)
        
        # === ç¬¬ä¸‰é˜¶æ®µï¼šéªŒè¯æ›´æ–° ===
        validated_updates = self._validate_state_updates(resolved_updates, context)
        
        # === ç¬¬å››é˜¶æ®µï¼šåº”ç”¨æ›´æ–° ===
        self._apply_state_updates(validated_updates, context)
        
        # === ç¬¬äº”é˜¶æ®µï¼šæ›´æ–°ç‰ˆæœ¬ä¿¡æ¯ ===
        self._update_channel_versions(validated_updates, context)
        
        # === ç¬¬å…­é˜¶æ®µï¼šè§¦å‘äº‹ä»¶ ===
        self._trigger_state_change_events(validated_updates, context)
        
        # è®°å½•åŒæ­¥ç»Ÿè®¡
        if self._stats:
            sync_duration = time.time() - sync_start_time
            self._stats.record_sync_operation(
                updates_count=len(validated_updates),
                duration=sync_duration,
                success=True
            )
        
        if self.debug:
            print(f"ğŸ”„ çŠ¶æ€åŒæ­¥å®Œæˆ: {len(validated_updates)} ä¸ªæ›´æ–°")
    
    except Exception as e:
        # åŒæ­¥å¤±è´¥ï¼Œè®°å½•é”™è¯¯
        if self._stats:
            sync_duration = time.time() - sync_start_time
            self._stats.record_sync_operation(
                updates_count=len(all_updates) if 'all_updates' in locals() else 0,
                duration=sync_duration,
                success=False
            )
        
        if self.debug:
            print(f"ğŸ’¥ çŠ¶æ€åŒæ­¥å¤±è´¥: {e}")
        
        raise SynchronizationError(f"State synchronization failed: {e}") from e

def _collect_state_updates(
    self,
    step_results: Dict[str, Any],
    context: ExecutionContext
) -> List[StateUpdate]:
    """æ”¶é›†çŠ¶æ€æ›´æ–°
    
    Args:
        step_results: æ­¥éª¤æ‰§è¡Œç»“æœ
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        
    Returns:
        List[StateUpdate]: çŠ¶æ€æ›´æ–°åˆ—è¡¨
        
    æ”¶é›†ç­–ç•¥ï¼š
    1. éå†æ‰€æœ‰ä»»åŠ¡ç»“æœ
    2. æå–çŠ¶æ€æ›´æ–°
    3. æ ‡è®°æ›´æ–°æ¥æº
    4. éªŒè¯æ›´æ–°æ ¼å¼
    """
    updates = []
    
    for task_name, result in step_results.items():
        # è·³è¿‡é”™è¯¯ç»“æœ
        if isinstance(result, PregelTaskError):
            continue
        
        # æå–çŠ¶æ€æ›´æ–°
        task_updates = self._extract_updates_from_result(task_name, result, context)
        updates.extend(task_updates)
    
    return updates

def _extract_updates_from_result(
    self,
    task_name: str,
    result: Any,
    context: ExecutionContext
) -> List[StateUpdate]:
    """ä»ä»»åŠ¡ç»“æœä¸­æå–çŠ¶æ€æ›´æ–°
    
    Args:
        task_name: ä»»åŠ¡åç§°
        result: ä»»åŠ¡ç»“æœ
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        
    Returns:
        List[StateUpdate]: æå–çš„çŠ¶æ€æ›´æ–°åˆ—è¡¨
        
    æå–è§„åˆ™ï¼š
    1. å­—å…¸ç»“æœï¼šæ¯ä¸ªé”®å€¼å¯¹æ˜¯ä¸€ä¸ªæ›´æ–°
    2. å¯¹è±¡ç»“æœï¼šæ ¹æ®ç±»å‹è½¬æ¢ä¸ºå­—å…¸
    3. ç®€å•å€¼ï¼šæ›´æ–°åˆ°é»˜è®¤é€šé“
    4. Noneç»“æœï¼šæ— æ›´æ–°
    """
    updates = []
    
    if result is None:
        # æ— æ›´æ–°
        return updates
    
    if isinstance(result, dict):
        # å­—å…¸ç»“æœï¼šæ¯ä¸ªé”®å€¼å¯¹æ˜¯ä¸€ä¸ªæ›´æ–°
        for channel_name, value in result.items():
            if channel_name in self.channels:
                update = StateUpdate(
                    channel=channel_name,
                    value=value,
                    source_task=task_name,
                    timestamp=time.time(),
                    step=context.step
                )
                updates.append(update)
            else:
                logger.warning(f"Unknown channel '{channel_name}' in task '{task_name}' result")
    
    elif hasattr(result, '__dict__'):
        # å¯¹è±¡ç»“æœï¼šè½¬æ¢ä¸ºå­—å…¸
        result_dict = result.__dict__
        for channel_name, value in result_dict.items():
            if channel_name in self.channels:
                update = StateUpdate(
                    channel=channel_name,
                    value=value,
                    source_task=task_name,
                    timestamp=time.time(),
                    step=context.step
                )
                updates.append(update)
    
    else:
        # ç®€å•å€¼ï¼šæ›´æ–°åˆ°é»˜è®¤é€šé“æˆ–æ ¹é€šé“
        default_channel = self._get_default_output_channel(task_name)
        if default_channel:
            update = StateUpdate(
                channel=default_channel,
                value=result,
                source_task=task_name,
                timestamp=time.time(),
                step=context.step
            )
            updates.append(update)
    
    return updates

def _resolve_update_conflicts(
    self,
    updates: List[StateUpdate],
    context: ExecutionContext
) -> List[StateUpdate]:
    """è§£å†³æ›´æ–°å†²çª
    
    Args:
        updates: åŸå§‹æ›´æ–°åˆ—è¡¨
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        
    Returns:
        List[StateUpdate]: è§£å†³å†²çªåçš„æ›´æ–°åˆ—è¡¨
        
    å†²çªè§£å†³ç­–ç•¥ï¼š
    1. åŒä¸€é€šé“çš„å¤šä¸ªæ›´æ–°ï¼šä½¿ç”¨é€šé“çš„reducerå‡½æ•°
    2. æ— reducerå‡½æ•°ï¼šä½¿ç”¨æœ€åæ›´æ–°
    3. æ—¶é—´æˆ³æ’åºï¼šç¡®ä¿æ›´æ–°é¡ºåº
    4. ä¼˜å…ˆçº§è€ƒè™‘ï¼šé«˜ä¼˜å…ˆçº§ä»»åŠ¡ä¼˜å…ˆ
    """
    if not updates:
        return updates
    
    # æŒ‰é€šé“åˆ†ç»„æ›´æ–°
    updates_by_channel = defaultdict(list)
    for update in updates:
        updates_by_channel[update.channel].append(update)
    
    resolved_updates = []
    
    for channel_name, channel_updates in updates_by_channel.items():
        if len(channel_updates) == 1:
            # å•ä¸ªæ›´æ–°ï¼Œæ— å†²çª
            resolved_updates.append(channel_updates[0])
        else:
            # å¤šä¸ªæ›´æ–°ï¼Œéœ€è¦è§£å†³å†²çª
            resolved_update = self._resolve_channel_conflicts(
                channel_name, channel_updates, context
            )
            resolved_updates.append(resolved_update)
    
    return resolved_updates

def _resolve_channel_conflicts(
    self,
    channel_name: str,
    updates: List[StateUpdate],
    context: ExecutionContext
) -> StateUpdate:
    """è§£å†³ç‰¹å®šé€šé“çš„æ›´æ–°å†²çª
    
    Args:
        channel_name: é€šé“åç§°
        updates: è¯¥é€šé“çš„æ›´æ–°åˆ—è¡¨
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        
    Returns:
        StateUpdate: è§£å†³å†²çªåçš„æ›´æ–°
    """
    channel = self.channels[channel_name]
    
    # æŒ‰æ—¶é—´æˆ³æ’åº
    sorted_updates = sorted(updates, key=lambda u: u.timestamp)
    
    if hasattr(channel, 'operator') and channel.operator:
        # ä½¿ç”¨é€šé“çš„reducerå‡½æ•°
        current_value = context.checkpoint.get("channel_values", {}).get(channel_name)
        
        for update in sorted_updates:
            if current_value is None:
                current_value = update.value
            else:
                current_value = channel.operator(current_value, update.value)
        
        # åˆ›å»ºåˆå¹¶åçš„æ›´æ–°
        merged_update = StateUpdate(
            channel=channel_name,
            value=current_value,
            source_task=f"merged({','.join(u.source_task for u in updates)})",
            timestamp=sorted_updates[-1].timestamp,
            step=context.step
        )
        
        return merged_update
    
    else:
        # ä½¿ç”¨æœ€åæ›´æ–°ï¼ˆLastValueè¯­ä¹‰ï¼‰
        return sorted_updates[-1]
```

## 3. æ£€æŸ¥ç‚¹ä¿å­˜æ ¸å¿ƒå‡½æ•°

### 3.1 PostgresCheckpointSaver.put() - æ£€æŸ¥ç‚¹ä¿å­˜

```python
# æ–‡ä»¶ï¼šlanggraph/checkpoint/postgres/base.py
def put(
    self,
    config: RunnableConfig,
    checkpoint: Checkpoint,
    metadata: CheckpointMetadata,
    new_versions: ChannelVersions,
) -> RunnableConfig:
    """ä¿å­˜æ£€æŸ¥ç‚¹åˆ°PostgreSQL
    
    è¿™æ˜¯æ£€æŸ¥ç‚¹ç³»ç»Ÿçš„æ ¸å¿ƒå‡½æ•°ï¼Œè´Ÿè´£å°†æ‰§è¡ŒçŠ¶æ€æŒä¹…åŒ–åˆ°æ•°æ®åº“ã€‚
    å®ç°äº†ACIDç‰¹æ€§ï¼Œç¡®ä¿æ•°æ®çš„ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚
    
    Args:
        config: è¿è¡Œé…ç½®ï¼ŒåŒ…å«thread_idç­‰æ ‡è¯†ä¿¡æ¯
        checkpoint: æ£€æŸ¥ç‚¹æ•°æ®ï¼ŒåŒ…å«å®Œæ•´çš„æ‰§è¡ŒçŠ¶æ€
        metadata: æ£€æŸ¥ç‚¹å…ƒæ•°æ®ï¼ŒåŒ…å«æ­¥éª¤ä¿¡æ¯å’Œæ¥æº
        new_versions: æ–°çš„é€šé“ç‰ˆæœ¬ä¿¡æ¯
        
    Returns:
        RunnableConfig: æ›´æ–°åçš„é…ç½®ï¼ŒåŒ…å«æ–°çš„checkpoint_id
        
    å®ç°ç‰¹æ€§ï¼š
    1. åŸå­æ€§æ“ä½œï¼šä½¿ç”¨æ•°æ®åº“äº‹åŠ¡ç¡®ä¿ä¸€è‡´æ€§
    2. å†²çªå¤„ç†ï¼šæ”¯æŒå¹¶å‘å†™å…¥çš„å†²çªè§£å†³
    3. æ•°æ®å‹ç¼©ï¼šå¤§å‹æ£€æŸ¥ç‚¹è‡ªåŠ¨å‹ç¼©
    4. æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡æ“ä½œå’Œè¿æ¥æ± 
    5. é”™è¯¯æ¢å¤ï¼šå¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•
    
    Raises:
        CheckpointStorageError: å­˜å‚¨æ“ä½œå¤±è´¥æ—¶
        CheckpointSerializationError: åºåˆ—åŒ–å¤±è´¥æ—¶
    """
    operation_start_time = time.time()
    
    try:
        # === ç¬¬ä¸€é˜¶æ®µï¼šå‚æ•°è§£æå’ŒéªŒè¯ ===
        thread_id = config["configurable"]["thread_id"]
        checkpoint_ns = config["configurable"].get("checkpoint_ns", "")
        checkpoint_id = checkpoint["id"]
        parent_checkpoint_id = metadata.get("parents", {}).get(checkpoint_ns)
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not thread_id:
            raise ValueError("thread_id is required")
        if not checkpoint_id:
            raise ValueError("checkpoint_id is required")
        
        # === ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®åºåˆ—åŒ– ===
        serialization_start = time.time()
        
        try:
            # åºåˆ—åŒ–æ£€æŸ¥ç‚¹æ•°æ®
            checkpoint_data = self.serde.dumps(checkpoint)
            metadata_data = self.serde.dumps(metadata)
            
            # æ£€æŸ¥æ•°æ®å¤§å°å¹¶è€ƒè™‘å‹ç¼©
            if len(checkpoint_data) > self.compression_threshold:
                checkpoint_data = self._compress_data(checkpoint_data)
                metadata["compressed"] = True
            
        except Exception as e:
            raise CheckpointSerializationError(f"Failed to serialize checkpoint: {e}") from e
        
        serialization_duration = time.time() - serialization_start
        
        # === ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®åº“æ“ä½œ ===
        db_start = time.time()
        
        with self._cursor() as cur:
            try:
                # å¼€å§‹äº‹åŠ¡ï¼ˆå¦‚æœä¸åœ¨äº‹åŠ¡ä¸­ï¼‰
                if not self._in_transaction(cur):
                    cur.execute("BEGIN")
                
                # æ‰§è¡ŒUPSERTæ“ä½œ
                cur.execute(
                    """
                    INSERT INTO checkpoints 
                    (thread_id, checkpoint_ns, checkpoint_id, parent_checkpoint_id, 
                     type, checkpoint, metadata, created_at, updated_at)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                    ON CONFLICT (thread_id, checkpoint_ns, checkpoint_id) 
                    DO UPDATE SET 
                        checkpoint = EXCLUDED.checkpoint,
                        metadata = EXCLUDED.metadata,
                        updated_at = CURRENT_TIMESTAMP,
                        parent_checkpoint_id = EXCLUDED.parent_checkpoint_id
                    RETURNING created_at, updated_at
                    """,
                    (
                        thread_id,
                        checkpoint_ns,
                        checkpoint_id,
                        parent_checkpoint_id,
                        "checkpoint",  # ç±»å‹æ ‡è¯†
                        checkpoint_data,
                        metadata_data,
                    ),
                )
                
                # è·å–æ—¶é—´æˆ³ä¿¡æ¯
                result = cur.fetchone()
                created_at = result["created_at"] if result else None
                
                # æ›´æ–°ç‰ˆæœ¬ä¿¡æ¯è¡¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if new_versions:
                    self._update_channel_versions(cur, thread_id, checkpoint_ns, 
                                                checkpoint_id, new_versions)
                
                # æäº¤äº‹åŠ¡
                if not self._in_transaction(cur):
                    cur.execute("COMMIT")
                
                # åŒæ­¥Pipelineï¼ˆå¦‚æœä½¿ç”¨ï¼‰
                if self.pipe:
                    self.pipe.sync()
                
            except Exception as e:
                # å›æ»šäº‹åŠ¡
                if not self._in_transaction(cur):
                    cur.execute("ROLLBACK")
                raise CheckpointStorageError(f"Database operation failed: {e}") from e
        
        db_duration = time.time() - db_start
        
        # === ç¬¬å››é˜¶æ®µï¼šç¼“å­˜æ›´æ–° ===
        if self._cache:
            cache_key = self._make_cache_key(thread_id, checkpoint_ns, checkpoint_id)
            checkpoint_tuple = CheckpointTuple(
                config=config,
                checkpoint=checkpoint,
                metadata=metadata,
                parent_config=None,  # å»¶è¿ŸåŠ è½½
                pending_writes=None,  # å»¶è¿ŸåŠ è½½
            )
            self._cache.put(cache_key, checkpoint_tuple)
        
        # === ç¬¬äº”é˜¶æ®µï¼šç»Ÿè®¡è®°å½• ===
        total_duration = time.time() - operation_start_time
        
        if self._stats:
            self._stats.record_put_operation(
                thread_id=thread_id,
                checkpoint_size=len(checkpoint_data),
                serialization_time=serialization_duration,
                db_time=db_duration,
                total_time=total_duration,
                success=True
            )
        
        # === ç¬¬å…­é˜¶æ®µï¼šæ„å»ºè¿”å›é…ç½® ===
        updated_config = {
            **config,
            "configurable": {
                **config["configurable"],
                "checkpoint_id": checkpoint_id,
                "checkpoint_ts": created_at.isoformat() if created_at else None,
            }
        }
        
        if self.debug:
            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {thread_id}/{checkpoint_id} "
                  f"({len(checkpoint_data)} bytes, {total_duration:.3f}s)")
        
        return updated_config
    
    except Exception as e:
        # è®°å½•é”™è¯¯ç»Ÿè®¡
        total_duration = time.time() - operation_start_time
        
        if self._stats:
            self._stats.record_put_operation(
                thread_id=config["configurable"].get("thread_id", "unknown"),
                checkpoint_size=0,
                serialization_time=0,
                db_time=0,
                total_time=total_duration,
                success=False
            )
        
        logger.error(f"Failed to save checkpoint: {e}")
        raise

def _compress_data(self, data: bytes) -> bytes:
    """å‹ç¼©æ•°æ®
    
    Args:
        data: åŸå§‹æ•°æ®
        
    Returns:
        bytes: å‹ç¼©åçš„æ•°æ®
        
    å‹ç¼©ç­–ç•¥ï¼š
    1. ä½¿ç”¨zlibå‹ç¼©ç®—æ³•
    2. è‡ªé€‚åº”å‹ç¼©çº§åˆ«
    3. å‹ç¼©ç‡æ£€æŸ¥
    4. æ·»åŠ å‹ç¼©æ ‡è®°
    """
    import zlib
    
    # å°è¯•ä¸åŒçš„å‹ç¼©çº§åˆ«
    best_compressed = data
    best_ratio = 1.0
    
    for level in [1, 6, 9]:  # å¿«é€Ÿã€å¹³è¡¡ã€æœ€ä½³
        try:
            compressed = zlib.compress(data, level)
            ratio = len(compressed) / len(data)
            
            if ratio < best_ratio:
                best_compressed = compressed
                best_ratio = ratio
                
        except Exception:
            continue
    
    # åªæœ‰åœ¨å‹ç¼©ç‡è¶³å¤Ÿå¥½æ—¶æ‰ä½¿ç”¨å‹ç¼©æ•°æ®
    if best_ratio < 0.8:  # è‡³å°‘å‹ç¼©20%
        # æ·»åŠ å‹ç¼©æ ‡è®°
        return b'\x01' + best_compressed
    else:
        return data

def _update_channel_versions(
    self,
    cur: Cursor,
    thread_id: str,
    checkpoint_ns: str,
    checkpoint_id: str,
    new_versions: ChannelVersions
) -> None:
    """æ›´æ–°é€šé“ç‰ˆæœ¬ä¿¡æ¯
    
    Args:
        cur: æ•°æ®åº“æ¸¸æ ‡
        thread_id: çº¿ç¨‹ID
        checkpoint_ns: æ£€æŸ¥ç‚¹å‘½åç©ºé—´
        checkpoint_id: æ£€æŸ¥ç‚¹ID
        new_versions: æ–°ç‰ˆæœ¬ä¿¡æ¯
    """
    if not new_versions:
        return
    
    # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
    version_data = []
    for channel_name, version in new_versions.items():
        version_data.append((
            thread_id,
            checkpoint_ns,
            checkpoint_id,
            channel_name,
            str(version),
            time.time()
        ))
    
    if version_data:
        # æ‰¹é‡æ’å…¥ç‰ˆæœ¬ä¿¡æ¯
        cur.executemany(
            """
            INSERT INTO channel_versions 
            (thread_id, checkpoint_ns, checkpoint_id, channel_name, version, updated_at)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (thread_id, checkpoint_ns, checkpoint_id, channel_name)
            DO UPDATE SET 
                version = EXCLUDED.version,
                updated_at = EXCLUDED.updated_at
            """,
            version_data
        )
```

### 3.2 PostgresCheckpointSaver.list() - æ£€æŸ¥ç‚¹åˆ—è¡¨æŸ¥è¯¢

```python
def list(
    self,
    config: Optional[RunnableConfig],
    *,
    filter: Optional[Dict[str, Any]] = None,
    before: Optional[RunnableConfig] = None,
    limit: Optional[int] = None,
) -> Iterator[CheckpointTuple]:
    """åˆ—å‡ºæ£€æŸ¥ç‚¹çš„PostgreSQLå®ç°
    
    è¿™æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„æ£€æŸ¥ç‚¹æŸ¥è¯¢å‡½æ•°ï¼Œæ”¯æŒï¼š
    1. å¤æ‚çš„è¿‡æ»¤æ¡ä»¶
    2. åˆ†é¡µæŸ¥è¯¢
    3. æ—¶é—´èŒƒå›´æŸ¥è¯¢
    4. æµå¼ç»“æœå¤„ç†
    5. æŸ¥è¯¢ä¼˜åŒ–
    
    Args:
        config: åŸºç¡€é…ç½®ï¼ŒåŒ…å«thread_id
        filter: è¿‡æ»¤æ¡ä»¶å­—å…¸ï¼Œæ”¯æŒå…ƒæ•°æ®å­—æ®µè¿‡æ»¤
        before: è·å–æ­¤é…ç½®ä¹‹å‰çš„æ£€æŸ¥ç‚¹
        limit: é™åˆ¶è¿”å›æ•°é‡
        
    Yields:
        CheckpointTuple: åŒ¹é…çš„æ£€æŸ¥ç‚¹å…ƒç»„
        
    æŸ¥è¯¢ä¼˜åŒ–ï¼š
    1. ç´¢å¼•ä¼˜åŒ–ï¼šä½¿ç”¨å¤åˆç´¢å¼•åŠ é€ŸæŸ¥è¯¢
    2. åˆ†é¡µä¼˜åŒ–ï¼šä½¿ç”¨æ¸¸æ ‡åˆ†é¡µé¿å…OFFSETæ€§èƒ½é—®é¢˜
    3. ç¼“å­˜åˆ©ç”¨ï¼šä¼˜å…ˆä»ç¼“å­˜è·å–çƒ­ç‚¹æ•°æ®
    4. è¿æ¥å¤ç”¨ï¼šå¤ç”¨æ•°æ®åº“è¿æ¥å‡å°‘å¼€é”€
    
    Raises:
        ValueError: å‚æ•°æ— æ•ˆæ—¶
        CheckpointQueryError: æŸ¥è¯¢æ‰§è¡Œå¤±è´¥æ—¶
    """
    if config is None:
        return
    
    query_start_time = time.time()
    
    try:
        # === ç¬¬ä¸€é˜¶æ®µï¼šå‚æ•°è§£æå’ŒéªŒè¯ ===
        thread_id = config["configurable"]["thread_id"]
        checkpoint_ns = config["configurable"].get("checkpoint_ns", "")
        
        if not thread_id:
            raise ValueError("thread_id is required")
        
        # === ç¬¬äºŒé˜¶æ®µï¼šæ„å»ºæŸ¥è¯¢ ===
        query_builder = self._create_query_builder()
        
        # åŸºç¡€æŸ¥è¯¢
        query_builder.select([
            "checkpoint", "metadata", "checkpoint_id", 
            "parent_checkpoint_id", "created_at", "updated_at"
        ])
        query_builder.from_table("checkpoints")
        query_builder.where("thread_id = %s", thread_id)
        query_builder.where("checkpoint_ns = %s", checkpoint_ns)
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if filter:
            self._apply_filter_conditions(query_builder, filter)
        
        # åº”ç”¨æ—¶é—´èŒƒå›´æ¡ä»¶
        if before:
            before_ts = self._extract_timestamp_from_config(before)
            if before_ts:
                query_builder.where("created_at < %s", before_ts)
        
        # æ’åºå’Œé™åˆ¶
        query_builder.order_by("created_at DESC")
        if limit is not None:
            query_builder.limit(limit)
        
        # æ„å»ºæœ€ç»ˆæŸ¥è¯¢
        query, params = query_builder.build()
        
        # === ç¬¬ä¸‰é˜¶æ®µï¼šæ‰§è¡ŒæŸ¥è¯¢ ===
        with self._cursor() as cur:
            cur.execute(query, params)
            
            # === ç¬¬å››é˜¶æ®µï¼šæµå¼å¤„ç†ç»“æœ ===
            processed_count = 0
            
            for row in cur:
                try:
                    # ååºåˆ—åŒ–æ•°æ®
                    checkpoint = self._deserialize_checkpoint(row["checkpoint"])
                    metadata = self._deserialize_metadata(row["metadata"])
                    
                    # æ„å»ºé…ç½®
                    current_config = self._build_checkpoint_config(
                        config, row["checkpoint_id"], row["created_at"]
                    )
                    
                    # æ„å»ºçˆ¶é…ç½®
                    parent_config = None
                    if row["parent_checkpoint_id"]:
                        parent_config = self._build_checkpoint_config(
                            config, row["parent_checkpoint_id"], None
                        )
                    
                    # è·å–å¾…å†™å…¥æ“ä½œï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
                    pending_writes = None  # å»¶è¿ŸåŠ è½½ä»¥æé«˜æ€§èƒ½
                    
                    # æ„å»ºæ£€æŸ¥ç‚¹å…ƒç»„
                    checkpoint_tuple = CheckpointTuple(
                        config=current_config,
                        checkpoint=checkpoint,
                        metadata=metadata,
                        parent_config=parent_config,
                        pending_writes=pending_writes,
                    )
                    
                    yield checkpoint_tuple
                    processed_count += 1
                    
                except Exception as e:
                    logger.error(f"Failed to process checkpoint row: {e}")
                    continue
        
        # === ç¬¬äº”é˜¶æ®µï¼šç»Ÿè®¡è®°å½• ===
        query_duration = time.time() - query_start_time
        
        if self._stats:
            self._stats.record_list_operation(
                thread_id=thread_id,
                filter_conditions=len(filter) if filter else 0,
                results_count=processed_count,
                duration=query_duration,
                success=True
            )
        
        if self.debug:
            print(f"ğŸ“‹ æ£€æŸ¥ç‚¹æŸ¥è¯¢å®Œæˆ: {processed_count} ä¸ªç»“æœ ({query_duration:.3f}s)")
    
    except Exception as e:
        # è®°å½•é”™è¯¯ç»Ÿè®¡
        query_duration = time.time() - query_start_time
        
        if self._stats:
            self._stats.record_list_operation(
                thread_id=config["configurable"].get("thread_id", "unknown"),
                filter_conditions=len(filter) if filter else 0,
                results_count=0,
                duration=query_duration,
                success=False
            )
        
        logger.error(f"Failed to list checkpoints: {e}")
        raise CheckpointQueryError(f"Query execution failed: {e}") from e

def _create_query_builder(self) -> QueryBuilder:
    """åˆ›å»ºæŸ¥è¯¢æ„å»ºå™¨
    
    Returns:
        QueryBuilder: æŸ¥è¯¢æ„å»ºå™¨å®ä¾‹
    """
    return QueryBuilder()

def _apply_filter_conditions(
    self,
    query_builder: QueryBuilder,
    filter: Dict[str, Any]
) -> None:
    """åº”ç”¨è¿‡æ»¤æ¡ä»¶
    
    Args:
        query_builder: æŸ¥è¯¢æ„å»ºå™¨
        filter: è¿‡æ»¤æ¡ä»¶å­—å…¸
        
    æ”¯æŒçš„è¿‡æ»¤æ¡ä»¶ï¼š
    1. source: æ£€æŸ¥ç‚¹æ¥æº
    2. step: æ­¥éª¤ç¼–å·
    3. è‡ªå®šä¹‰å…ƒæ•°æ®å­—æ®µ
    4. æ—¶é—´èŒƒå›´
    5. ç±»å‹è¿‡æ»¤
    """
    for key, value in filter.items():
        if key == "source":
            # æ¥æºè¿‡æ»¤
            query_builder.where("metadata->>'source' = %s", value)
        
        elif key == "step":
            # æ­¥éª¤è¿‡æ»¤
            if isinstance(value, int):
                query_builder.where("(metadata->>'step')::int = %s", value)
            elif isinstance(value, dict):
                # èŒƒå›´æŸ¥è¯¢
                if "gte" in value:
                    query_builder.where("(metadata->>'step')::int >= %s", value["gte"])
                if "lte" in value:
                    query_builder.where("(metadata->>'step')::int <= %s", value["lte"])
                if "gt" in value:
                    query_builder.where("(metadata->>'step')::int > %s", value["gt"])
                if "lt" in value:
                    query_builder.where("(metadata->>'step')::int < %s", value["lt"])
        
        elif key == "created_after":
            # åˆ›å»ºæ—¶é—´è¿‡æ»¤
            query_builder.where("created_at > %s", value)
        
        elif key == "created_before":
            # åˆ›å»ºæ—¶é—´è¿‡æ»¤
            query_builder.where("created_at < %s", value)
        
        elif key.startswith("metadata."):
            # å…ƒæ•°æ®å­—æ®µè¿‡æ»¤
            field_name = key[9:]  # å»æ‰ "metadata." å‰ç¼€
            query_builder.where(f"metadata->>%s = %s", field_name, str(value))
        
        else:
            # é€šç”¨å…ƒæ•°æ®è¿‡æ»¤
            query_builder.where(f"metadata->>%s = %s", key, str(value))

class QueryBuilder:
    """SQLæŸ¥è¯¢æ„å»ºå™¨
    
    æä¾›æµç•…çš„APIæ¥æ„å»ºå¤æ‚çš„SQLæŸ¥è¯¢ï¼Œæ”¯æŒï¼š
    1. åŠ¨æ€æ¡ä»¶æ„å»º
    2. å‚æ•°åŒ–æŸ¥è¯¢
    3. SQLæ³¨å…¥é˜²æŠ¤
    4. æŸ¥è¯¢ä¼˜åŒ–æç¤º
    """
    
    def __init__(self):
        self._select_fields = []
        self._from_table = None
        self._where_conditions = []
        self._order_by_fields = []
        self._limit_count = None
        self._params = []
    
    def select(self, fields: List[str]) -> "QueryBuilder":
        """è®¾ç½®SELECTå­—æ®µ"""
        self._select_fields.extend(fields)
        return self
    
    def from_table(self, table: str) -> "QueryBuilder":
        """è®¾ç½®FROMè¡¨"""
        self._from_table = table
        return self
    
    def where(self, condition: str, *params) -> "QueryBuilder":
        """æ·»åŠ WHEREæ¡ä»¶"""
        self._where_conditions.append(condition)
        self._params.extend(params)
        return self
    
    def order_by(self, field: str) -> "QueryBuilder":
        """æ·»åŠ ORDER BYå­—æ®µ"""
        self._order_by_fields.append(field)
        return self
    
    def limit(self, count: int) -> "QueryBuilder":
        """è®¾ç½®LIMIT"""
        self._limit_count = count
        return self
    
    def build(self) -> Tuple[str, List[Any]]:
        """æ„å»ºæœ€ç»ˆæŸ¥è¯¢"""
        if not self._select_fields or not self._from_table:
            raise ValueError("SELECT and FROM are required")
        
        # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
        query_parts = []
        
        # SELECTå­å¥
        query_parts.append(f"SELECT {', '.join(self._select_fields)}")
        
        # FROMå­å¥
        query_parts.append(f"FROM {self._from_table}")
        
        # WHEREå­å¥
        if self._where_conditions:
            query_parts.append(f"WHERE {' AND '.join(self._where_conditions)}")
        
        # ORDER BYå­å¥
        if self._order_by_fields:
            query_parts.append(f"ORDER BY {', '.join(self._order_by_fields)}")
        
        # LIMITå­å¥
        if self._limit_count is not None:
            query_parts.append(f"LIMIT {self._limit_count}")
        
        query = " ".join(query_parts)
        return query, self._params
```

## 4. é€šé“ç³»ç»Ÿæ ¸å¿ƒå‡½æ•°

### 4.1 BinaryOperatorAggregate.update() - çŠ¶æ€èšåˆ

```python
# æ–‡ä»¶ï¼šlanggraph/channels/binop.py
class BinaryOperatorAggregate(BaseChannel[Value, Update, Value]):
    """äºŒå…ƒæ“ä½œç¬¦èšåˆé€šé“
    
    è¿™æ˜¯LangGraphçŠ¶æ€ç®¡ç†çš„æ ¸å¿ƒé€šé“ç±»å‹ï¼Œæ”¯æŒï¼š
    1. è‡ªå®šä¹‰èšåˆå‡½æ•°ï¼ˆreducerï¼‰
    2. å¢é‡çŠ¶æ€æ›´æ–°
    3. ç±»å‹å®‰å…¨çš„æ“ä½œ
    4. å¹¶å‘æ›´æ–°æ”¯æŒ
    5. çŠ¶æ€ç‰ˆæœ¬ç®¡ç†
    
    å¸¸ç”¨åœºæ™¯ï¼š
    - æ¶ˆæ¯åˆ—è¡¨ç´¯ç§¯ï¼ˆadd_messagesï¼‰
    - æ•°å€¼ç´¯åŠ ï¼ˆoperator.addï¼‰
    - é›†åˆåˆå¹¶ï¼ˆset.unionï¼‰
    - å­—å…¸æ›´æ–°ï¼ˆdict.updateï¼‰
    """
    
    def __init__(
        self,
        typ: Type[Value],
        operator: BinaryOperator[Value, Update],
        *,
        default: Optional[Value] = None,
    ):
        """åˆå§‹åŒ–äºŒå…ƒæ“ä½œç¬¦èšåˆé€šé“
        
        Args:
            typ: å€¼ç±»å‹
            operator: äºŒå…ƒæ“ä½œç¬¦å‡½æ•°
            default: é»˜è®¤å€¼
        """
        self.typ = typ
        self.operator = operator
        self.default = default
        self._value = default
        self._version = 0
        self._lock = threading.RLock()
    
    def update(self, values: Sequence[Update]) -> bool:
        """æ›´æ–°é€šé“å€¼
        
        è¿™æ˜¯çŠ¶æ€èšåˆçš„æ ¸å¿ƒå‡½æ•°ï¼Œå®ç°äº†çº¿ç¨‹å®‰å…¨çš„çŠ¶æ€æ›´æ–°ï¼š
        1. åŸå­æ€§æ“ä½œï¼šç¡®ä¿æ›´æ–°çš„åŸå­æ€§
        2. ç±»å‹éªŒè¯ï¼šéªŒè¯æ›´æ–°å€¼çš„ç±»å‹
        3. èšåˆè®¡ç®—ï¼šä½¿ç”¨operatorå‡½æ•°èšåˆå¤šä¸ªæ›´æ–°
        4. ç‰ˆæœ¬ç®¡ç†ï¼šè‡ªåŠ¨é€’å¢ç‰ˆæœ¬å·
        5. å˜æ›´æ£€æµ‹ï¼šæ£€æµ‹å€¼æ˜¯å¦çœŸæ­£å‘ç”Ÿå˜åŒ–
        
        Args:
            values: æ›´æ–°å€¼åºåˆ—
            
        Returns:
            bool: å€¼æ˜¯å¦å‘ç”Ÿäº†å˜åŒ–
            
        ç®—æ³•æµç¨‹ï¼š
        1. è·å–é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
        2. éªŒè¯è¾“å…¥å€¼
        3. åº”ç”¨èšåˆæ“ä½œ
        4. æ£€æµ‹å˜æ›´
        5. æ›´æ–°ç‰ˆæœ¬
        6. è¿”å›å˜æ›´çŠ¶æ€
        """
        if not values:
            return False
        
        with self._lock:
            # è®°å½•åŸå§‹å€¼ç”¨äºå˜æ›´æ£€æµ‹
            original_value = self._value
            
            # è·å–å½“å‰å€¼
            current_value = self._value if self._value is not None else self.default
            
            # åº”ç”¨æ‰€æœ‰æ›´æ–°
            for update_value in values:
                try:
                    # ç±»å‹éªŒè¯
                    validated_update = self._validate_update_value(update_value)
                    
                    # åº”ç”¨æ“ä½œç¬¦
                    if current_value is None:
                        current_value = validated_update
                    else:
                        current_value = self._apply_operator(current_value, validated_update)
                
                except Exception as e:
                    logger.error(f"Failed to apply update {update_value}: {e}")
                    continue
            
            # æ£€æµ‹å˜æ›´
            changed = self._detect_value_change(original_value, current_value)
            
            if changed:
                # æ›´æ–°å€¼å’Œç‰ˆæœ¬
                self._value = current_value
                self._version += 1
                
                if self.debug:
                    print(f"ğŸ”„ é€šé“æ›´æ–°: {self.name} v{self._version}")
            
            return changed
    
    def _validate_update_value(self, value: Update) -> Update:
        """éªŒè¯æ›´æ–°å€¼
        
        Args:
            value: å¾…éªŒè¯çš„æ›´æ–°å€¼
            
        Returns:
            Update: éªŒè¯åçš„æ›´æ–°å€¼
            
        Raises:
            TypeError: ç±»å‹ä¸åŒ¹é…æ—¶
            ValueError: å€¼æ— æ•ˆæ—¶
        """
        # åŸºæœ¬ç±»å‹æ£€æŸ¥
        if not self._is_compatible_type(value):
            raise TypeError(f"Update value type {type(value)} is not compatible with {self.typ}")
        
        # è‡ªå®šä¹‰éªŒè¯é€»è¾‘
        if hasattr(self, 'validator') and self.validator:
            validated_value = self.validator(value)
            if validated_value is None:
                raise ValueError(f"Update value {value} failed validation")
            return validated_value
        
        return value
    
    def _apply_operator(self, current: Value, update: Update) -> Value:
        """åº”ç”¨æ“ä½œç¬¦
        
        Args:
            current: å½“å‰å€¼
            update: æ›´æ–°å€¼
            
        Returns:
            Value: æ“ä½œåçš„æ–°å€¼
            
        é”™è¯¯å¤„ç†ï¼š
        1. æ“ä½œç¬¦å¼‚å¸¸æ•è·
        2. ç±»å‹è½¬æ¢å°è¯•
        3. é™çº§ç­–ç•¥åº”ç”¨
        4. é”™è¯¯æ—¥å¿—è®°å½•
        """
        try:
            # ç›´æ¥åº”ç”¨æ“ä½œç¬¦
            result = self.operator(current, update)
            
            # ç»“æœç±»å‹æ£€æŸ¥
            if not self._is_compatible_type(result):
                logger.warning(f"Operator result type {type(result)} may not be compatible")
            
            return result
        
        except TypeError as e:
            # ç±»å‹é”™è¯¯ï¼Œå°è¯•ç±»å‹è½¬æ¢
            try:
                converted_update = self._try_type_conversion(update, type(current))
                result = self.operator(current, converted_update)
                logger.info(f"Applied type conversion for update: {type(update)} -> {type(converted_update)}")
                return result
            except Exception:
                logger.error(f"Operator failed with type error: {e}")
                raise
        
        except Exception as e:
            # å…¶ä»–æ“ä½œç¬¦é”™è¯¯
            logger.error(f"Operator failed: {e}")
            
            # å°è¯•é™çº§ç­–ç•¥
            if hasattr(self, 'fallback_operator') and self.fallback_operator:
                try:
                    result = self.fallback_operator(current, update)
                    logger.info(f"Applied fallback operator successfully")
                    return result
                except Exception:
                    pass
            
            raise
    
    def _detect_value_change(self, old_value: Value, new_value: Value) -> bool:
        """æ£€æµ‹å€¼å˜æ›´
        
        Args:
            old_value: æ—§å€¼
            new_value: æ–°å€¼
            
        Returns:
            bool: æ˜¯å¦å‘ç”Ÿå˜æ›´
            
        å˜æ›´æ£€æµ‹ç­–ç•¥ï¼š
        1. å¼•ç”¨ç›¸ç­‰æ€§æ£€æŸ¥
        2. å€¼ç›¸ç­‰æ€§æ£€æŸ¥
        3. æ·±åº¦æ¯”è¾ƒï¼ˆå¯¹äºå¤æ‚å¯¹è±¡ï¼‰
        4. è‡ªå®šä¹‰æ¯”è¾ƒå‡½æ•°
        """
        # å¼•ç”¨ç›¸ç­‰æ€§æ£€æŸ¥ï¼ˆæœ€å¿«ï¼‰
        if old_value is new_value:
            return False
        
        # Noneå€¼ç‰¹æ®Šå¤„ç†
        if old_value is None or new_value is None:
            return old_value != new_value
        
        # å€¼ç›¸ç­‰æ€§æ£€æŸ¥
        try:
            if old_value == new_value:
                return False
        except Exception:
            # æ¯”è¾ƒæ“ä½œå¤±è´¥ï¼Œå‡è®¾å‘ç”Ÿäº†å˜æ›´
            pass
        
        # å¯¹äºå¤æ‚å¯¹è±¡ï¼Œå°è¯•æ·±åº¦æ¯”è¾ƒ
        if hasattr(old_value, '__dict__') and hasattr(new_value, '__dict__'):
            try:
                return old_value.__dict__ != new_value.__dict__
            except Exception:
                pass
        
        # å¯¹äºåˆ—è¡¨å’Œå­—å…¸ï¼Œä½¿ç”¨å†…å®¹æ¯”è¾ƒ
        if isinstance(old_value, (list, dict)) and isinstance(new_value, (list, dict)):
            try:
                return old_value != new_value
            except Exception:
                pass
        
        # é»˜è®¤å‡è®¾å‘ç”Ÿäº†å˜æ›´
        return True
    
    def _try_type_conversion(self, value: Any, target_type: Type) -> Any:
        """å°è¯•ç±»å‹è½¬æ¢
        
        Args:
            value: å¾…è½¬æ¢çš„å€¼
            target_type: ç›®æ ‡ç±»å‹
            
        Returns:
            Any: è½¬æ¢åçš„å€¼
            
        Raises:
            TypeError: æ— æ³•è½¬æ¢æ—¶
        """
        # å¸¸è§ç±»å‹è½¬æ¢
        if target_type == str:
            return str(value)
        elif target_type == int:
            return int(value)
        elif target_type == float:
            return float(value)
        elif target_type == list and hasattr(value, '__iter__'):
            return list(value)
        elif target_type == dict and hasattr(value, 'items'):
            return dict(value)
        
        # å°è¯•ç›´æ¥æ„é€ 
        try:
            return target_type(value)
        except Exception:
            raise TypeError(f"Cannot convert {type(value)} to {target_type}")
    
    def get(self) -> Value:
        """è·å–å½“å‰å€¼
        
        Returns:
            Value: å½“å‰é€šé“å€¼
        """
        with self._lock:
            return self._value if self._value is not None else self.default
    
    def checkpoint(self) -> Value:
        """åˆ›å»ºæ£€æŸ¥ç‚¹
        
        Returns:
            Value: æ£€æŸ¥ç‚¹å€¼ï¼ˆæ·±æ‹·è´ï¼‰
        """
        with self._lock:
            current_value = self._value if self._value is not None else self.default
            return self._deep_copy_value(current_value)
    
    def _deep_copy_value(self, value: Value) -> Value:
        """æ·±æ‹·è´å€¼
        
        Args:
            value: å¾…æ‹·è´çš„å€¼
            
        Returns:
            Value: æ‹·è´åçš„å€¼
        """
        import copy
        
        try:
            return copy.deepcopy(value)
        except Exception:
            # æ·±æ‹·è´å¤±è´¥ï¼Œå°è¯•æµ…æ‹·è´
            try:
                return copy.copy(value)
            except Exception:
                # æ‹·è´å¤±è´¥ï¼Œè¿”å›åŸå€¼ï¼ˆé£é™©æ“ä½œï¼‰
                logger.warning(f"Failed to copy value of type {type(value)}")
                return value
```

## 5. æ€»ç»“

é€šè¿‡æ·±å…¥åˆ†æLangGraphçš„å…³é”®å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼š

### 5.1 è®¾è®¡ç²¾é«“

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¯ä¸ªå‡½æ•°èŒè´£å•ä¸€ï¼Œæ¥å£æ¸…æ™°
2. **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ¢å¤æœºåˆ¶
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šå¤šå±‚æ¬¡çš„æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
4. **ç±»å‹å®‰å…¨**ï¼šå¹¿æ³›ä½¿ç”¨ç±»å‹æ³¨è§£å’Œè¿è¡Œæ—¶æ£€æŸ¥

### 5.2 æ ¸å¿ƒç®—æ³•

1. **BSPæ‰§è¡Œæ¨¡å‹**ï¼šç¡®ä¿çŠ¶æ€ä¸€è‡´æ€§çš„å¹¶è¡Œæ‰§è¡Œ
2. **å›¾ç¼–è¯‘ä¼˜åŒ–**ï¼šå£°æ˜å¼åˆ°æ‰§è¡Œå¼çš„é«˜æ•ˆè½¬æ¢
3. **çŠ¶æ€èšåˆç®—æ³•**ï¼šçµæ´»çš„çŠ¶æ€æ›´æ–°å’Œåˆå¹¶æœºåˆ¶
4. **æ£€æŸ¥ç‚¹ç®—æ³•**ï¼šå¯é çš„çŠ¶æ€æŒä¹…åŒ–å’Œæ¢å¤

### 5.3 æŠ€æœ¯äº®ç‚¹

1. **å¹¶å‘æ§åˆ¶**ï¼šçº¿ç¨‹å®‰å…¨çš„çŠ¶æ€ç®¡ç†
2. **èµ„æºç®¡ç†**ï¼šæ™ºèƒ½çš„èµ„æºåˆ†é…å’Œå›æ”¶
3. **ç¼“å­˜ä¼˜åŒ–**ï¼šå¤šå±‚æ¬¡çš„ç¼“å­˜ç­–ç•¥
4. **ç›‘æ§é›†æˆ**ï¼šå…¨é¢çš„æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡

è¿™äº›å…³é”®å‡½æ•°çš„ç²¾å¿ƒè®¾è®¡å’Œå®ç°ï¼Œä¸ºLangGraphæä¾›äº†å¼ºå¤§è€Œå¯é çš„æŠ€æœ¯åŸºç¡€ï¼Œä½¿å…¶èƒ½å¤Ÿæ”¯æŒå¤æ‚çš„å¤šæ™ºèƒ½ä½“åº”ç”¨åœºæ™¯ã€‚

---

---

tommie blog
