---
title: "æ·±å…¥Difyæ¨¡å‹è¿è¡Œæ—¶ï¼šå¤šæ¨¡å‹ç»Ÿä¸€æ¥å£ä¸è´Ÿè½½å‡è¡¡æ¶æ„"
date: 2025-06-09T15:00:00+08:00
draft: false
featured: true
series: "dify-architecture"
tags: ["Dify", "æ¨¡å‹è¿è¡Œæ—¶", "å¤šæ¨¡å‹æ”¯æŒ", "è´Ÿè½½å‡è¡¡", "æ¨¡å‹æä¾›è€…", "AIæ¨¡å‹ç®¡ç†"]
categories: ["dify", "æ¨¡å‹è¿è¡Œæ—¶æ¶æ„"]
author: "Difyæ¶æ„åˆ†æ"
description: "æ·±åº¦è§£æDifyæ¨¡å‹è¿è¡Œæ—¶æ¨¡å—çš„å®Œæ•´æ¶æ„ï¼ŒåŒ…å«40+æ¨¡å‹æä¾›è€…ã€6ç§æ¨¡å‹ç±»å‹ã€è´Ÿè½½å‡è¡¡å’Œç»Ÿä¸€è°ƒç”¨æ¥å£çš„è®¾è®¡ä¸å®ç°"
toc: true
tocOpen: true
showReadingTime: true
showWordCount: true
weight: 40
slug: "dify-model-runtime-module"
---

## æ¦‚è¿°

Difyçš„æ¨¡å‹è¿è¡Œæ—¶æ¨¡å—ï¼ˆ`core/model_runtime/`ï¼‰æ˜¯å¹³å°çš„å¤šæ¨¡å‹ç»Ÿä¸€ç®¡ç†å¼•æ“ï¼Œä¸ºä¸Šå±‚åº”ç”¨æä¾›äº†ç»Ÿä¸€çš„æ¨¡å‹è°ƒç”¨æ¥å£ã€‚è¯¥æ¨¡å—æ”¯æŒ40+ä¸ªä¸»æµAIæ¨¡å‹æä¾›è€…ã€6ç§ä¸åŒç±»å‹çš„AIæ¨¡å‹ï¼Œå¹¶å®ç°äº†æ™ºèƒ½è´Ÿè½½å‡è¡¡ã€å‡­æ®ç®¡ç†å’Œé”™è¯¯å¤„ç†æœºåˆ¶ã€‚

**æŠ€æœ¯æ ˆä¸æ¶æ„ç‰¹ç‚¹**ï¼š
Difyæ¨¡å‹è¿è¡Œæ—¶é‡‡ç”¨äº†**Python/Flask/PostgreSQL**çš„ç»å…¸æŠ€æœ¯æ ˆï¼š
- **Flaskæ¡†æ¶**ï¼šè½»é‡çº§Webæ¡†æ¶ï¼Œæ”¯æŒå¿«é€ŸAPIå¼€å‘å’Œæ¨¡å—åŒ–æ‰©å±•
- **PostgreSQLæ•°æ®åº“**ï¼šä¼ä¸šçº§å…³ç³»æ•°æ®åº“ï¼Œæ”¯æŒJSONå­—æ®µå’Œå¤æ‚æŸ¥è¯¢
- **Redisç¼“å­˜**ï¼šé«˜æ€§èƒ½ç¼“å­˜å±‚ï¼Œç”¨äºä¼šè¯ç®¡ç†å’Œè´Ÿè½½å‡è¡¡çŠ¶æ€
- **Celeryä»»åŠ¡é˜Ÿåˆ—**ï¼šå¼‚æ­¥ä»»åŠ¡å¤„ç†ï¼Œæ”¯æŒå¤§è§„æ¨¡å¹¶å‘å’Œé•¿æ—¶é—´è¿è¡Œä»»åŠ¡

**æ¨¡å‹é€‚é…å™¨æ¨¡å¼**ï¼š
Difyé€šè¿‡**é€‚é…å™¨æ¨¡å¼**å®ç°å¯¹40+æ¨¡å‹æä¾›è€…çš„ç»Ÿä¸€æ”¯æŒï¼š
```python
# æ¨¡å‹æä¾›è€…é€‚é…å™¨ç¤ºä¾‹
class ModelProviderAdapter:
    """æ¨¡å‹æä¾›è€…é€‚é…å™¨åŸºç±»"""
    
    def __init__(self, provider_config: dict):
        self.provider_config = provider_config
        self.rate_limiter = self._init_rate_limiter()
        self.credential_manager = self._init_credentials()
    
    def invoke_model(self, prompt: str, **kwargs) -> ModelResponse:
        """ç»Ÿä¸€çš„æ¨¡å‹è°ƒç”¨æ¥å£"""
        # 1. å‡­æ®éªŒè¯
        self._validate_credentials()
        
        # 2. é€Ÿç‡æ§åˆ¶
        self._check_rate_limits()
        
        # 3. å‚æ•°é€‚é…
        adapted_params = self._adapt_parameters(**kwargs)
        
        # 4. æ¨¡å‹è°ƒç”¨
        return self._call_provider_api(prompt, adapted_params)
```

ã€æ ¸å¿ƒç»„ä»¶å’Œå…³é”®å®ç°ç»†èŠ‚ã€‚

<!--more-->

## 1. æ¨¡å‹è¿è¡Œæ—¶æ•´ä½“æ¶æ„

### 1.1 ä¸‰å±‚æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "å·¥å‚å±‚ Factory Layer"
        ModelProviderFactory[ModelProviderFactory æ¨¡å‹æä¾›è€…å·¥å‚]
        ModelManager[ModelManager æ¨¡å‹ç®¡ç†å™¨]
        ProviderManager[ProviderManager æä¾›è€…ç®¡ç†å™¨]
    end
    
    subgraph "æä¾›è€…å±‚ Provider Layer"
        BaseProvider[BaseProvider åŸºç¡€æä¾›è€…]
        
        subgraph "ä¸»æµäº‘æœåŠ¡æä¾›è€…"
            OpenAI[OpenAI Provider]
            Anthropic[Anthropic Provider]
            Google[Google Provider]
            Azure[Azure OpenAI Provider]
            AWS[AWS Bedrock Provider]
        end
        
        subgraph "å›½å†…AIæä¾›è€…"
            ZhipuAI[æ™ºè°±AI Provider]
            Tongyi[é€šä¹‰åƒé—® Provider]
            Wenxin[æ–‡å¿ƒä¸€è¨€ Provider]
            Moonshot[æœˆä¹‹æš—é¢ Provider]
            Minimax[Minimax Provider]
        end
        
        subgraph "å¼€æº/æœ¬åœ°éƒ¨ç½²"
            Ollama[Ollama Provider]
            LocalAI[LocalAI Provider]
            Xinference[Xinference Provider]
            HuggingFace[HuggingFace Provider]
        end
    end
    
    subgraph "æ¨¡å‹å±‚ Model Layer"
        BaseModel[BaseModel åŸºç¡€æ¨¡å‹]
        
        subgraph "6ç§æ¨¡å‹ç±»å‹"
            LLM[LargeLanguageModel å¤§è¯­è¨€æ¨¡å‹]
            Embedding[TextEmbeddingModel åµŒå…¥æ¨¡å‹]
            Rerank[RerankModel é‡æ’åºæ¨¡å‹]
            TTS[TTSModel è¯­éŸ³åˆæˆæ¨¡å‹]
            STT[Speech2TextModel è¯­éŸ³è½¬æ–‡å­—æ¨¡å‹]
            Moderation[ModerationModel å†…å®¹å®¡æ ¸æ¨¡å‹]
        end
    end
    
    subgraph "æ”¯æ’‘æœåŠ¡å±‚"
        LoadBalancer[LBModelManager è´Ÿè½½å‡è¡¡ç®¡ç†å™¨]
        CredentialManager[å‡­æ®ç®¡ç†å™¨]
        CallbackSystem[å›è°ƒç³»ç»Ÿ]
        ErrorHandler[é”™è¯¯å¤„ç†å™¨]
    end
    
    ModelProviderFactory --> BaseProvider
    ModelManager --> ModelInstance[ModelInstance æ¨¡å‹å®ä¾‹]
    
    BaseProvider --> OpenAI
    BaseProvider --> ZhipuAI
    BaseProvider --> Ollama
    
    BaseModel --> LLM
    BaseModel --> Embedding
    BaseModel --> Rerank
    
    OpenAI --> LLM
    Anthropic --> LLM
    Google --> Embedding
    
    ModelInstance --> LoadBalancer
    ModelInstance --> CredentialManager
    
    style ModelProviderFactory fill:#e3f2fd
    style BaseProvider fill:#e8f5e8
    style BaseModel fill:#fff3e0
    style LoadBalancer fill:#fce4ec
```

### 1.2 æ¨¡å‹è°ƒç”¨æµç¨‹

```mermaid
sequenceDiagram
    participant App as åº”ç”¨å±‚
    participant ModelManager as æ¨¡å‹ç®¡ç†å™¨
    participant ModelInstance as æ¨¡å‹å®ä¾‹
    participant LBManager as è´Ÿè½½å‡è¡¡ç®¡ç†å™¨
    participant Provider as æ¨¡å‹æä¾›è€…
    participant AIModel as AIæ¨¡å‹æœåŠ¡
    
    Note over App,AIModel: æ¨¡å‹è°ƒç”¨å®Œæ•´æµç¨‹
    
    App->>ModelManager: è·å–æ¨¡å‹å®ä¾‹
    ModelManager->>ModelManager: æŸ¥æ‰¾æä¾›è€…é…ç½®
    ModelManager->>ModelInstance: åˆ›å»ºæ¨¡å‹å®ä¾‹
    
    ModelInstance->>ModelInstance: åŠ è½½å‡­æ®å’Œé…ç½®
    ModelInstance->>LBManager: åˆå§‹åŒ–è´Ÿè½½å‡è¡¡
    
    App->>ModelInstance: è°ƒç”¨æ¨¡å‹æ¨ç†
    ModelInstance->>LBManager: è·å–ä¸‹ä¸€ä¸ªå¯ç”¨é…ç½®
    LBManager-->>ModelInstance: è¿”å›è½®è¯¢é…ç½®
    
    loop é‡è¯•æœºåˆ¶
        ModelInstance->>Provider: æ‰§è¡Œæ¨¡å‹è°ƒç”¨
        Provider->>AIModel: å‘é€APIè¯·æ±‚
        
        alt è°ƒç”¨æˆåŠŸ
            AIModel-->>Provider: è¿”å›ç»“æœ
            Provider-->>ModelInstance: è¿”å›å¤„ç†ç»“æœ
            ModelInstance-->>App: è¿”å›æœ€ç»ˆç»“æœ
        else é€Ÿç‡é™åˆ¶
            AIModel-->>Provider: è¿”å›429é”™è¯¯
            Provider-->>ModelInstance: æŠ›å‡ºé€Ÿç‡é™åˆ¶å¼‚å¸¸
            ModelInstance->>LBManager: æ ‡è®°é…ç½®å†·å´
            LBManager->>LBManager: é€‰æ‹©ä¸‹ä¸€ä¸ªé…ç½®
        else è®¤è¯å¤±è´¥
            AIModel-->>Provider: è¿”å›401é”™è¯¯
            Provider-->>ModelInstance: æŠ›å‡ºè®¤è¯å¼‚å¸¸
            ModelInstance->>LBManager: æ ‡è®°é…ç½®å†·å´
        else è¿æ¥å¤±è´¥
            AIModel-->>Provider: è¿”å›è¿æ¥é”™è¯¯
            Provider-->>ModelInstance: æŠ›å‡ºè¿æ¥å¼‚å¸¸
            ModelInstance->>LBManager: æ ‡è®°é…ç½®å†·å´
        end
    end
```

## 2. æ¨¡å‹ç®¡ç†å™¨æ ¸å¿ƒå®ç°

### 2.1 ModelManagerç»Ÿä¸€ç®¡ç†

```python
class ModelManager:
    """
    æ¨¡å‹ç®¡ç†å™¨
    æä¾›ç»Ÿä¸€çš„æ¨¡å‹å®ä¾‹è·å–å’Œç®¡ç†æ¥å£
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        åˆ›å»ºæä¾›è€…ç®¡ç†å™¨å®ä¾‹
        """
        self._provider_manager = ProviderManager()
        
        # æ¨¡å‹å®ä¾‹ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
        self._model_instance_cache = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self._performance_stats = ModelPerformanceStats()

    def get_model_instance(
        self, 
        tenant_id: str, 
        provider: str, 
        model_type: ModelType, 
        model: str
    ) -> ModelInstance:
        """
        è·å–æ¨¡å‹å®ä¾‹
        è¿™æ˜¯æ¨¡å‹è°ƒç”¨çš„ä¸»è¦å…¥å£ç‚¹
        
        Args:
            tenant_id: ç§Ÿæˆ·IDï¼Œç”¨äºéš”ç¦»ä¸åŒç§Ÿæˆ·çš„æ¨¡å‹é…ç½®
            provider: æ¨¡å‹æä¾›è€…åç§°ï¼ˆå¦‚openaiã€anthropicç­‰ï¼‰
            model_type: æ¨¡å‹ç±»å‹æšä¸¾ï¼ˆLLMã€Embeddingç­‰ï¼‰
            model: å…·ä½“æ¨¡å‹åç§°ï¼ˆå¦‚gpt-4ã€claude-3ç­‰ï¼‰
            
        Returns:
            ModelInstance: é…ç½®å¥½çš„æ¨¡å‹å®ä¾‹
            
        Raises:
            ProviderTokenNotInitError: æä¾›è€…å‡­æ®æœªåˆå§‹åŒ–
            ModelNotFoundError: æ¨¡å‹ä¸å­˜åœ¨
        """
        # å¦‚æœæä¾›è€…ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
        if not provider:
            return self.get_default_model_instance(tenant_id, model_type)

        # è·å–æä¾›è€…æ¨¡å‹åŒ…
        provider_model_bundle = self._provider_manager.get_provider_model_bundle(
            tenant_id=tenant_id,
            provider=provider,
            model_type=model_type
        )

        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model_instance = ModelInstance(provider_model_bundle, model)
        
        # è®°å½•æ€§èƒ½ç»Ÿè®¡
        self._performance_stats.record_model_request(
            provider=provider,
            model_type=model_type,
            model=model
        )
        
        return model_instance

    def get_default_model_instance(
        self, 
        tenant_id: str, 
        model_type: ModelType
    ) -> ModelInstance:
        """
        è·å–é»˜è®¤æ¨¡å‹å®ä¾‹
        å½“æœªæŒ‡å®šç‰¹å®šæä¾›è€…æ—¶ï¼Œè¿”å›é…ç½®çš„é»˜è®¤æ¨¡å‹
        
        Args:
            tenant_id: ç§Ÿæˆ·ID
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            ModelInstance: é»˜è®¤æ¨¡å‹å®ä¾‹
            
        Raises:
            ProviderTokenNotInitError: é»˜è®¤æ¨¡å‹æœªé…ç½®
        """
        # è·å–é»˜è®¤æ¨¡å‹å®ä½“
        default_model_entity = self._provider_manager.get_default_model(
            tenant_id=tenant_id, 
            model_type=model_type
        )

        if not default_model_entity:
            raise ProviderTokenNotInitError(f"æ¨¡å‹ç±»å‹ {model_type} çš„é»˜è®¤æ¨¡å‹æœªæ‰¾åˆ°")

        # è¿”å›é»˜è®¤æ¨¡å‹å®ä¾‹
        return self.get_model_instance(
            tenant_id=tenant_id,
            provider=default_model_entity.provider.provider,
            model_type=model_type,
            model=default_model_entity.model,
        )

    def get_default_provider_model_name(
        self, 
        tenant_id: str, 
        model_type: ModelType
    ) -> tuple[str | None, str | None]:
        """
        è·å–é»˜è®¤æä¾›è€…å’Œæ¨¡å‹åç§°
        è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„æä¾›è€…å’Œè¯¥æä¾›è€…ä¸‹çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
        
        Args:
            tenant_id: ç§Ÿæˆ·ID
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            tuple[str | None, str | None]: (æä¾›è€…åç§°, æ¨¡å‹åç§°)
        """
        return self._provider_manager.get_first_provider_first_model(tenant_id, model_type)

    def validate_model_availability(
        self,
        tenant_id: str,
        provider: str,
        model_type: ModelType,
        model: str
    ) -> bool:
        """
        éªŒè¯æ¨¡å‹å¯ç”¨æ€§
        æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ä¸”é…ç½®æ­£ç¡®
        
        Args:
            tenant_id: ç§Ÿæˆ·ID
            provider: æä¾›è€…åç§°
            model_type: æ¨¡å‹ç±»å‹
            model: æ¨¡å‹åç§°
            
        Returns:
            bool: æ¨¡å‹æ˜¯å¦å¯ç”¨
        """
        try:
            model_instance = self.get_model_instance(
                tenant_id=tenant_id,
                provider=provider,
                model_type=model_type,
                model=model
            )
            
            # å¯ä»¥è¿›ä¸€æ­¥è¿›è¡Œå¥åº·æ£€æŸ¥
            # ä¾‹å¦‚è°ƒç”¨ç®€å•çš„æµ‹è¯•è¯·æ±‚
            return True
            
        except Exception as e:
            logger.warning(f"æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥å¤±è´¥: {provider}/{model}: {e}")
            return False

    def get_available_models(
        self,
        tenant_id: str,
        model_type: ModelType
    ) -> list[dict[str, Any]]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        è¿”å›æ‰€æœ‰é…ç½®æ­£ç¡®çš„æ¨¡å‹ä¿¡æ¯
        
        Args:
            tenant_id: ç§Ÿæˆ·ID
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            list[dict[str, Any]]: å¯ç”¨æ¨¡å‹åˆ—è¡¨
        """
        available_models = []
        
        # è·å–æ‰€æœ‰æä¾›è€…
        providers = self._provider_manager.get_providers(tenant_id)
        
        for provider in providers:
            try:
                # è·å–æä¾›è€…æ”¯æŒçš„æ¨¡å‹
                provider_models = self._provider_manager.get_provider_models(
                    tenant_id=tenant_id,
                    provider=provider.provider,
                    model_type=model_type
                )
                
                for model in provider_models:
                    # æ£€æŸ¥æ¨¡å‹é…ç½®çŠ¶æ€
                    if self.validate_model_availability(
                        tenant_id=tenant_id,
                        provider=provider.provider,
                        model_type=model_type,
                        model=model.model
                    ):
                        available_models.append({
                            "provider": provider.provider,
                            "provider_label": provider.label,
                            "model": model.model,
                            "model_label": model.label,
                            "model_type": model_type.value,
                            "features": model.features or [],
                            "parameter_rules": model.parameter_rules or [],
                        })
                        
            except Exception as e:
                logger.warning(f"è·å–æä¾›è€… {provider.provider} çš„æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
                continue
        
        return available_models

class ModelPerformanceStats:
    """æ¨¡å‹æ€§èƒ½ç»Ÿè®¡"""
    
    def __init__(self):
        self.request_counts = {}
        self.error_counts = {}
        self.response_times = {}
        
    def record_model_request(
        self, 
        provider: str, 
        model_type: ModelType, 
        model: str
    ):
        """è®°å½•æ¨¡å‹è¯·æ±‚"""
        key = f"{provider}:{model_type.value}:{model}"
        self.request_counts[key] = self.request_counts.get(key, 0) + 1
    
    def record_model_error(
        self, 
        provider: str, 
        model_type: ModelType, 
        model: str, 
        error_type: str
    ):
        """è®°å½•æ¨¡å‹é”™è¯¯"""
        key = f"{provider}:{model_type.value}:{model}:{error_type}"
        self.error_counts[key] = self.error_counts.get(key, 0) + 1
    
    def get_stats(self) -> dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "request_counts": self.request_counts,
            "error_counts": self.error_counts,
            "response_times": self.response_times
        }
```

### 2.2 ModelInstanceæ¨¡å‹å®ä¾‹

ModelInstanceæ˜¯æ¨¡å‹è°ƒç”¨çš„æ ¸å¿ƒæŠ½è±¡ï¼Œæä¾›äº†ç»Ÿä¸€çš„æ¨¡å‹è°ƒç”¨æ¥å£ã€‚å®ƒå°è£…äº†å•ä¸ªæ¨¡å‹çš„å®Œæ•´è°ƒç”¨é€»è¾‘ï¼ŒåŒ…æ‹¬å‡­æ®ç®¡ç†ã€è´Ÿè½½å‡è¡¡å’Œé”™è¯¯å¤„ç†ã€‚

**ModelInstanceæ ¸å¿ƒåŠŸèƒ½**ï¼š
- ğŸ”‘ **å‡­æ®ç®¡ç†**ï¼šè‡ªåŠ¨è·å–å’ŒéªŒè¯æ¨¡å‹è®¿é—®å‡­æ®
- âš–ï¸ **è´Ÿè½½å‡è¡¡**ï¼šæ”¯æŒå¤šé…ç½®é—´çš„æ™ºèƒ½è´Ÿè½½å‡è¡¡  
- ğŸ”„ **é”™è¯¯é‡è¯•**ï¼šå¯¹ç½‘ç»œã€é™æµç­‰é”™è¯¯è¿›è¡Œæ™ºèƒ½é‡è¯•
- ğŸ“Š **ç»Ÿä¸€æ¥å£**ï¼šä¸º6ç§æ¨¡å‹ç±»å‹æä¾›ä¸€è‡´çš„è°ƒç”¨æ–¹å¼
- ğŸ¯ **ç±»å‹å®‰å…¨**ï¼šé€šè¿‡æ–¹æ³•é‡è½½ç¡®ä¿ç±»å‹å®‰å…¨

```python
class ModelInstance:
    """
    æ¨¡å‹å®ä¾‹ç±»
    å°è£…äº†å•ä¸ªæ¨¡å‹çš„å®Œæ•´è°ƒç”¨é€»è¾‘ï¼ŒåŒ…æ‹¬å‡­æ®ç®¡ç†ã€è´Ÿè½½å‡è¡¡å’Œé”™è¯¯å¤„ç†
    """

    def __init__(self, provider_model_bundle: ProviderModelBundle, model: str):
        """
        åˆå§‹åŒ–æ¨¡å‹å®ä¾‹
        
        Args:
            provider_model_bundle: æä¾›è€…æ¨¡å‹åŒ…ï¼ŒåŒ…å«æä¾›è€…é…ç½®å’Œæ¨¡å‹ç±»å‹å®ä¾‹
            model: å…·ä½“çš„æ¨¡å‹åç§°
        """
        # åŸºç¡€å±æ€§
        self.provider_model_bundle = provider_model_bundle
        self.model = model
        self.provider = provider_model_bundle.configuration.provider.provider
        
        # è·å–æ¨¡å‹å‡­æ®
        self.credentials = self._fetch_credentials_from_bundle(provider_model_bundle, model)
        
        # æ¨¡å‹ç±»å‹å®ä¾‹ï¼ˆå®é™…çš„æ¨¡å‹è°ƒç”¨å®ç°ï¼‰
        self.model_type_instance = provider_model_bundle.model_type_instance
        
        # åˆå§‹åŒ–è´Ÿè½½å‡è¡¡ç®¡ç†å™¨ï¼ˆå¦‚æœé…ç½®äº†è´Ÿè½½å‡è¡¡ï¼‰
        self.load_balancing_manager = self._get_load_balancing_manager(
            configuration=provider_model_bundle.configuration,
            model_type=provider_model_bundle.model_type_instance.model_type,
            model=model,
            credentials=self.credentials,
        )

    @staticmethod
    def _fetch_credentials_from_bundle(
        provider_model_bundle: ProviderModelBundle, 
        model: str
    ) -> dict[str, Any]:
        """
        ä»æä¾›è€…æ¨¡å‹åŒ…ä¸­è·å–å‡­æ®
        
        Args:
            provider_model_bundle: æä¾›è€…æ¨¡å‹åŒ…
            model: æ¨¡å‹åç§°
            
        Returns:
            dict[str, Any]: æ¨¡å‹å‡­æ®å­—å…¸
            
        Raises:
            ProviderTokenNotInitError: å‡­æ®æœªåˆå§‹åŒ–
        """
        configuration = provider_model_bundle.configuration
        model_type = provider_model_bundle.model_type_instance.model_type
        
        # è·å–å½“å‰æ¨¡å‹çš„å‡­æ®
        credentials = configuration.get_current_credentials(
            model_type=model_type, 
            model=model
        )

        if credentials is None:
            raise ProviderTokenNotInitError(f"æ¨¡å‹ {model} çš„å‡­æ®æœªåˆå§‹åŒ–")

        return credentials

    @staticmethod
    def _get_load_balancing_manager(
        configuration: ProviderConfiguration,
        model_type: ModelType,
        model: str,
        credentials: dict
    ) -> Optional["LBModelManager"]:
        """
        è·å–è´Ÿè½½å‡è¡¡ç®¡ç†å™¨
        å¦‚æœé…ç½®äº†è´Ÿè½½å‡è¡¡ï¼Œåˆ›å»ºç›¸åº”çš„ç®¡ç†å™¨å®ä¾‹
        
        Args:
            configuration: æä¾›è€…é…ç½®
            model_type: æ¨¡å‹ç±»å‹
            model: æ¨¡å‹åç§°
            credentials: åŸºç¡€å‡­æ®
            
        Returns:
            Optional[LBModelManager]: è´Ÿè½½å‡è¡¡ç®¡ç†å™¨ï¼ˆå¦‚æœé…ç½®äº†è´Ÿè½½å‡è¡¡ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºè‡ªå®šä¹‰æä¾›è€…ä¸”é…ç½®äº†æ¨¡å‹è®¾ç½®
        if (configuration.model_settings and 
            configuration.using_provider_type == ProviderType.CUSTOM):
            
            # æŸ¥æ‰¾å½“å‰æ¨¡å‹çš„è®¾ç½®
            current_model_setting = None
            for model_setting in configuration.model_settings:
                if (model_setting.model_type == model_type and 
                    model_setting.model == model):
                    current_model_setting = model_setting
                    break

            # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†è´Ÿè½½å‡è¡¡
            if (current_model_setting and 
                current_model_setting.load_balancing_configs):
                
                # åˆ›å»ºè´Ÿè½½å‡è¡¡ç®¡ç†å™¨
                lb_model_manager = LBModelManager(
                    tenant_id=configuration.tenant_id,
                    provider=configuration.provider.provider,
                    model_type=model_type,
                    model=model,
                    load_balancing_configs=current_model_setting.load_balancing_configs,
                    managed_credentials=(
                        credentials if configuration.custom_configuration.provider 
                        else None
                    ),
                )

                return lb_model_manager

        return None

    # LLMæ¨¡å‹è°ƒç”¨æ–¹æ³•ï¼ˆæ”¯æŒæ–¹æ³•é‡è½½ï¼‰
    @overload
    def invoke_llm(
        self,
        prompt_messages: Sequence[PromptMessage],
        model_parameters: Optional[dict] = None,
        tools: Sequence[PromptMessageTool] | None = None,
        stop: Optional[list[str]] = None,
        stream: Literal[True] = True,
        user: Optional[str] = None,
        callbacks: Optional[list[Callback]] = None,
    ) -> Generator: ...

    @overload
    def invoke_llm(
        self,
        prompt_messages: list[PromptMessage],
        model_parameters: Optional[dict] = None,
        tools: Sequence[PromptMessageTool] | None = None,
        stop: Optional[list[str]] = None,
        stream: Literal[False] = False,
        user: Optional[str] = None,
        callbacks: Optional[list[Callback]] = None,
    ) -> LLMResult: ...

    def invoke_llm(
        self,
        prompt_messages: Sequence[PromptMessage],
        model_parameters: Optional[dict] = None,
        tools: Sequence[PromptMessageTool] | None = None,
        stop: Optional[Sequence[str]] = None,
        stream: bool = True,
        user: Optional[str] = None,
        callbacks: Optional[list[Callback]] = None,
    ) -> Union[LLMResult, Generator]:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹
        æ”¯æŒæµå¼å’Œéæµå¼è¾“å‡ºï¼Œè‡ªåŠ¨å¤„ç†è´Ÿè½½å‡è¡¡å’Œé”™è¯¯é‡è¯•
        
        Args:
            prompt_messages: æç¤ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç³»ç»Ÿã€ç”¨æˆ·ã€åŠ©æ‰‹æ¶ˆæ¯
            model_parameters: æ¨¡å‹å‚æ•°ï¼Œå¦‚æ¸©åº¦ã€æœ€å¤§ä»¤ç‰Œæ•°ç­‰
            tools: å·¥å…·åˆ—è¡¨ï¼Œç”¨äºå‡½æ•°è°ƒç”¨
            stop: åœæ­¢è¯åˆ—è¡¨
            stream: æ˜¯å¦æµå¼è¾“å‡º
            user: å”¯ä¸€ç”¨æˆ·æ ‡è¯†ï¼Œç”¨äºæ¨¡å‹æä¾›è€…çš„ä½¿ç”¨ç»Ÿè®¡
            callbacks: å›è°ƒå‡½æ•°åˆ—è¡¨ï¼Œç”¨äºç›‘æ§å’Œæ—¥å¿—è®°å½•
            
        Returns:
            Union[LLMResult, Generator]: 
                - éæµå¼ï¼šLLMResultå¯¹è±¡
                - æµå¼ï¼šGeneratorç”Ÿæˆå™¨ï¼Œäº§ç”ŸLLMResultChunkå¯¹è±¡
                
        Raises:
            Exception: æ¨¡å‹ç±»å‹ä¸åŒ¹é…æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # éªŒè¯æ¨¡å‹ç±»å‹
        if not isinstance(self.model_type_instance, LargeLanguageModel):
            raise Exception("æ¨¡å‹ç±»å‹å®ä¾‹ä¸æ˜¯LargeLanguageModel")
        
        # ä½¿ç”¨è½®è¯¢è°ƒç”¨æ‰§è¡Œå®é™…çš„æ¨¡å‹è°ƒç”¨
        return cast(
            Union[LLMResult, Generator],
            self._round_robin_invoke(
                function=self.model_type_instance.invoke,
                model=self.model,
                credentials=self.credentials,
                prompt_messages=prompt_messages,
                model_parameters=model_parameters,
                tools=tools,
                stop=stop,
                stream=stream,
                user=user,
                callbacks=callbacks,
            ),
        )

    def get_llm_num_tokens(
        self, 
        prompt_messages: Sequence[PromptMessage], 
        tools: Optional[Sequence[PromptMessageTool]] = None
    ) -> int:
        """
        è·å–LLMä»¤ç‰Œæ•°é‡
        è®¡ç®—æç¤ºæ¶ˆæ¯å’Œå·¥å…·è°ƒç”¨çš„æ€»ä»¤ç‰Œæ•°
        
        Args:
            prompt_messages: æç¤ºæ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·åˆ—è¡¨ï¼ˆç”¨äºå‡½æ•°è°ƒç”¨ï¼‰
            
        Returns:
            int: ä»¤ç‰Œæ€»æ•°
            
        Raises:
            Exception: æ¨¡å‹ç±»å‹ä¸åŒ¹é…æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        if not isinstance(self.model_type_instance, LargeLanguageModel):
            raise Exception("æ¨¡å‹ç±»å‹å®ä¾‹ä¸æ˜¯LargeLanguageModel")
        
        return cast(
            int,
            self._round_robin_invoke(
                function=self.model_type_instance.get_num_tokens,
                model=self.model,
                credentials=self.credentials,
                prompt_messages=prompt_messages,
                tools=tools,
            ),
        )

    def invoke_text_embedding(
        self, 
        texts: list[str], 
        user: Optional[str] = None, 
        input_type: EmbeddingInputType = EmbeddingInputType.DOCUMENT
    ) -> TextEmbeddingResult:
        """
        è°ƒç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹
        å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
        
        Args:
            texts: å¾…åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
            user: ç”¨æˆ·æ ‡è¯†
            input_type: è¾“å…¥ç±»å‹ï¼Œå½±å“åµŒå…¥ä¼˜åŒ–ç­–ç•¥
            
        Returns:
            TextEmbeddingResult: åµŒå…¥ç»“æœï¼ŒåŒ…å«å‘é‡åˆ—è¡¨å’Œä½¿ç”¨ç»Ÿè®¡
            
        Raises:
            Exception: æ¨¡å‹ç±»å‹ä¸åŒ¹é…æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        if not isinstance(self.model_type_instance, TextEmbeddingModel):
            raise Exception("æ¨¡å‹ç±»å‹å®ä¾‹ä¸æ˜¯TextEmbeddingModel")
        
        return cast(
            TextEmbeddingResult,
            self._round_robin_invoke(
                function=self.model_type_instance.invoke,
                model=self.model,
                credentials=self.credentials,
                texts=texts,
                user=user,
                input_type=input_type,
            ),
        )

    def get_text_embedding_num_tokens(self, texts: list[str]) -> list[int]:
        """
        è·å–æ–‡æœ¬åµŒå…¥ä»¤ç‰Œæ•°é‡
        è®¡ç®—æ¯ä¸ªæ–‡æœ¬çš„ä»¤ç‰Œæ•°é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            list[int]: æ¯ä¸ªæ–‡æœ¬çš„ä»¤ç‰Œæ•°é‡åˆ—è¡¨
        """
        if not isinstance(self.model_type_instance, TextEmbeddingModel):
            raise Exception("æ¨¡å‹ç±»å‹å®ä¾‹ä¸æ˜¯TextEmbeddingModel")
        
        return cast(
            list[int],
            self._round_robin_invoke(
                function=self.model_type_instance.get_num_tokens,
                model=self.model,
                credentials=self.credentials,
                texts=texts,
            ),
        )

    def invoke_rerank(
        self,
        query: str,
        docs: list[str],
        score_threshold: Optional[float] = None,
        top_n: Optional[int] = None,
        user: Optional[str] = None,
    ) -> RerankResult:
        """
        è°ƒç”¨é‡æ’åºæ¨¡å‹
        å¯¹æ–‡æ¡£åˆ—è¡¨æŒ‰ç›¸å…³æ€§é‡æ–°æ’åº
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            docs: å¾…é‡æ’åºçš„æ–‡æ¡£åˆ—è¡¨
            score_threshold: åˆ†æ•°é˜ˆå€¼ï¼Œè¿‡æ»¤ä½ç›¸å…³æ€§æ–‡æ¡£
            top_n: è¿”å›å‰Nä¸ªç»“æœ
            user: ç”¨æˆ·æ ‡è¯†
            
        Returns:
            RerankResult: é‡æ’åºç»“æœ
        """
        if not isinstance(self.model_type_instance, RerankModel):
            raise Exception("æ¨¡å‹ç±»å‹å®ä¾‹ä¸æ˜¯RerankModel")
        
        return cast(
            RerankResult,
            self._round_robin_invoke(
                function=self.model_type_instance.invoke,
                model=self.model,
                credentials=self.credentials,
                query=query,
                docs=docs,
                score_threshold=score_threshold,
                top_n=top_n,
                user=user,
            ),
        )

    def invoke_moderation(self, text: str, user: Optional[str] = None) -> bool:
        """
        è°ƒç”¨å†…å®¹å®¡æ ¸æ¨¡å‹
        æ£€æŸ¥æ–‡æœ¬å†…å®¹æ˜¯å¦åˆè§„
        
        Args:
            text: å¾…å®¡æ ¸çš„æ–‡æœ¬
            user: ç”¨æˆ·æ ‡è¯†
            
        Returns:
            bool: Trueè¡¨ç¤ºå†…å®¹æœ‰é—®é¢˜ï¼ŒFalseè¡¨ç¤ºå†…å®¹å®‰å…¨
        """
        if not isinstance(self.model_type_instance, ModerationModel):
            raise Exception("æ¨¡å‹ç±»å‹å®ä¾‹ä¸æ˜¯ModerationModel")
        
        return cast(
            bool,
            self._round_robin_invoke(
                function=self.model_type_instance.invoke,
                model=self.model,
                credentials=self.credentials,
                text=text,
                user=user,
            ),
        )

    def invoke_speech2text(self, file: IO[bytes], user: Optional[str] = None) -> str:
        """
        è°ƒç”¨è¯­éŸ³è½¬æ–‡å­—æ¨¡å‹
        å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæ–‡æœ¬
        
        Args:
            file: éŸ³é¢‘æ–‡ä»¶äºŒè¿›åˆ¶æµ
            user: ç”¨æˆ·æ ‡è¯†
            
        Returns:
            str: è½¬æ¢åçš„æ–‡æœ¬å†…å®¹
        """
        if not isinstance(self.model_type_instance, Speech2TextModel):
            raise Exception("æ¨¡å‹ç±»å‹å®ä¾‹ä¸æ˜¯Speech2TextModel")
        
        return cast(
            str,
            self._round_robin_invoke(
                function=self.model_type_instance.invoke,
                model=self.model,
                credentials=self.credentials,
                file=file,
                user=user,
            ),
        )

    def invoke_tts(
        self, 
        content_text: str, 
        tenant_id: str, 
        voice: str, 
        user: Optional[str] = None
    ) -> Iterable[bytes]:
        """
        è°ƒç”¨æ–‡å­—è½¬è¯­éŸ³æ¨¡å‹
        å°†æ–‡æœ¬è½¬æ¢ä¸ºéŸ³é¢‘æµ
        
        Args:
            content_text: å¾…è½¬æ¢çš„æ–‡æœ¬å†…å®¹
            tenant_id: ç§Ÿæˆ·ID
            voice: è¯­éŸ³éŸ³è‰²
            user: ç”¨æˆ·æ ‡è¯†
            
        Returns:
            Iterable[bytes]: éŸ³é¢‘æ•°æ®æµ
        """
        if not isinstance(self.model_type_instance, TTSModel):
            raise Exception("æ¨¡å‹ç±»å‹å®ä¾‹ä¸æ˜¯TTSModel")
        
        return cast(
            Iterable[bytes],
            self._round_robin_invoke(
                function=self.model_type_instance.invoke,
                model=self.model,
                credentials=self.credentials,
                content_text=content_text,
                user=user,
                tenant_id=tenant_id,
                voice=voice,
            ),
        )

    def _round_robin_invoke(self, function: Callable[..., Any], *args, **kwargs):
        """
        è½®è¯¢è°ƒç”¨æ–¹æ³•
        å®ç°è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»çš„æ ¸å¿ƒé€»è¾‘
        
        Args:
            function: è¦è°ƒç”¨çš„å‡½æ•°
            *args: ä½ç½®å‚æ•°
            **kwargs: å…³é”®å­—å‚æ•°
            
        Returns:
            è°ƒç”¨ç»“æœ
            
        Raises:
            ProviderTokenNotInitError: æ‰€æœ‰å‡­æ®éƒ½ä¸å¯ç”¨
            æœ€åä¸€ä¸ªå¼‚å¸¸: å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        """
        # å¦‚æœæ²¡æœ‰è´Ÿè½½å‡è¡¡é…ç½®ï¼Œç›´æ¥è°ƒç”¨
        if not self.load_balancing_manager:
            return function(*args, **kwargs)

        last_exception: Union[
            InvokeRateLimitError, 
            InvokeAuthorizationError, 
            InvokeConnectionError, 
            None
        ] = None

        # è´Ÿè½½å‡è¡¡è½®è¯¢å¾ªç¯
        while True:
            # è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„è´Ÿè½½å‡è¡¡é…ç½®
            lb_config = self.load_balancing_manager.fetch_next()
            
            if not lb_config:
                # æ²¡æœ‰å¯ç”¨é…ç½®
                if not last_exception:
                    raise ProviderTokenNotInitError("æ¨¡å‹å‡­æ®æœªåˆå§‹åŒ–")
                else:
                    raise last_exception

            # é¢å¤–çš„ç­–ç•¥åˆè§„æ€§æ£€æŸ¥ï¼ˆä½œä¸ºfetch_nextçš„åå¤‡ï¼‰
            try:
                from core.helper.credential_utils import check_credential_policy_compliance
                
                if lb_config.credential_id:
                    check_credential_policy_compliance(
                        credential_id=lb_config.credential_id,
                        provider=self.provider,
                        credential_type=PluginCredentialType.MODEL,
                    )
                    
            except Exception as e:
                logger.warning(
                    f"è´Ÿè½½å‡è¡¡é…ç½® {lb_config.id} ç­–ç•¥åˆè§„æ€§æ£€æŸ¥å¤±è´¥: {e}"
                )
                # å°†è¯¥é…ç½®æ ‡è®°ä¸ºå†·å´çŠ¶æ€
                self.load_balancing_manager.cooldown(lb_config, expire=60)
                continue

            try:
                # ä½¿ç”¨è´Ÿè½½å‡è¡¡é…ç½®çš„å‡­æ®è°ƒç”¨å‡½æ•°
                if "credentials" in kwargs:
                    del kwargs["credentials"]
                
                return function(*args, **kwargs, credentials=lb_config.credentials)
                
            except InvokeRateLimitError as e:
                # é€Ÿç‡é™åˆ¶é”™è¯¯ï¼šå°†é…ç½®æ ‡è®°ä¸ºå†·å´60ç§’
                self.load_balancing_manager.cooldown(lb_config, expire=60)
                last_exception = e
                continue
                
            except (InvokeAuthorizationError, InvokeConnectionError) as e:
                # è®¤è¯æˆ–è¿æ¥é”™è¯¯ï¼šå°†é…ç½®æ ‡è®°ä¸ºå†·å´10ç§’
                self.load_balancing_manager.cooldown(lb_config, expire=10)
                last_exception = e
                continue
                
            except Exception as e:
                # å…¶ä»–å¼‚å¸¸ï¼šç›´æ¥æŠ›å‡ºï¼Œä¸è¿›è¡Œé‡è¯•
                raise e

    def get_tts_voices(self, language: Optional[str] = None):
        """
        è·å–TTSæ¨¡å‹æ”¯æŒçš„è¯­éŸ³åˆ—è¡¨
        
        Args:
            language: å¯é€‰çš„è¯­è¨€é™åˆ¶
            
        Returns:
            TTSæ¨¡å‹æ”¯æŒçš„è¯­éŸ³åˆ—è¡¨
        """
        if not isinstance(self.model_type_instance, TTSModel):
            raise Exception("æ¨¡å‹ç±»å‹å®ä¾‹ä¸æ˜¯TTSModel")
        
        return self.model_type_instance.get_tts_model_voices(
            model=self.model, 
            credentials=self.credentials, 
            language=language
        )
```

**ModelInstanceæ–¹æ³•åŠŸèƒ½æ€»è§ˆ**ï¼š

| æ–¹æ³•ç±»åˆ« | æ–¹æ³•åç§° | åŠŸèƒ½æè¿° | å‚æ•°è¦ç‚¹ |
|---------|---------|---------|---------|
| **LLMè°ƒç”¨** | `invoke_llm()` | è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ | æ”¯æŒæµå¼/éæµå¼ï¼Œå·¥å…·è°ƒç”¨ï¼Œåœæ­¢è¯ |
| | `get_llm_num_tokens()` | è®¡ç®—LLMä»¤ç‰Œæ•° | ç”¨äºæˆæœ¬é¢„ä¼°å’Œä¸Šä¸‹æ–‡ç®¡ç† |
| **åµŒå…¥æ¨¡å‹** | `invoke_text_embedding()` | è°ƒç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹ | æ”¯æŒæ–‡æ¡£/æŸ¥è¯¢ä¸¤ç§è¾“å…¥ç±»å‹ |
| | `get_text_embedding_num_tokens()` | è®¡ç®—åµŒå…¥ä»¤ç‰Œæ•° | è¿”å›æ¯ä¸ªæ–‡æœ¬çš„ä»¤ç‰Œæ•°é‡ |
| **é‡æ’åº** | `invoke_rerank()` | è°ƒç”¨é‡æ’åºæ¨¡å‹ | æ”¯æŒåˆ†æ•°é˜ˆå€¼å’ŒTop-Nè¿‡æ»¤ |
| **è¯­éŸ³å¤„ç†** | `invoke_speech2text()` | è¯­éŸ³è½¬æ–‡å­— | æ”¯æŒéŸ³é¢‘æ–‡ä»¶è½¬å½• |
| | `invoke_tts()` | æ–‡å­—è½¬è¯­éŸ³ | æ”¯æŒå¤šç§éŸ³è‰²å’Œè¯­éŸ³åˆæˆ |
| | `get_tts_voices()` | è·å–TTSéŸ³è‰²åˆ—è¡¨ | å¯æŒ‰è¯­è¨€ç­›é€‰å¯ç”¨éŸ³è‰² |
| **å†…å®¹å®‰å…¨** | `invoke_moderation()` | å†…å®¹å®¡æ ¸ | æ£€æµ‹æ–‡æœ¬å†…å®¹å®‰å…¨æ€§ |
| **æ ¸å¿ƒæœºåˆ¶** | `_round_robin_invoke()` | è½®è¯¢è°ƒç”¨æ ¸å¿ƒ | è´Ÿè½½å‡è¡¡+é”™è¯¯å¤„ç†+é‡è¯•é€»è¾‘ |

**è´Ÿè½½å‡è¡¡ä¸å®¹é”™æœºåˆ¶**ï¼š
```python
# ModelInstance çš„æ ¸å¿ƒè°ƒç”¨æµç¨‹
def _round_robin_invoke(self, function, *args, **kwargs):
    """
    å®ç°æ™ºèƒ½çš„è´Ÿè½½å‡è¡¡è°ƒç”¨ï¼š
    1. ä» LBModelManager è·å–ä¸‹ä¸€ä¸ªå¯ç”¨é…ç½®
    2. éªŒè¯é…ç½®çš„åˆè§„æ€§ï¼ˆç­–ç•¥æ£€æŸ¥ï¼‰
    3. ä½¿ç”¨é…ç½®çš„å‡­æ®è°ƒç”¨ç›®æ ‡å‡½æ•°
    4. æ ¹æ®é”™è¯¯ç±»å‹å†³å®šé‡è¯•ç­–ç•¥ï¼š
       - 429 é™æµé”™è¯¯ï¼šå†·å´ 60 ç§’ï¼Œç»§ç»­è½®è¯¢
       - è®¤è¯/è¿æ¥é”™è¯¯ï¼šå†·å´ 10 ç§’ï¼Œç»§ç»­è½®è¯¢  
       - å…¶ä»–é”™è¯¯ï¼šç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
    """
```

## 3. è´Ÿè½½å‡è¡¡ç®¡ç†å™¨

### 3.1 LBModelManageræ™ºèƒ½è´Ÿè½½å‡è¡¡

```python
class LBModelManager:
    """
    è´Ÿè½½å‡è¡¡æ¨¡å‹ç®¡ç†å™¨
    å®ç°å¤šä¸ªæ¨¡å‹é…ç½®ä¹‹é—´çš„æ™ºèƒ½è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»
    """

    def __init__(
        self,
        tenant_id: str,
        provider: str,
        model_type: ModelType,
        model: str,
        load_balancing_configs: list[ModelLoadBalancingConfiguration],
        managed_credentials: Optional[dict] = None,
    ):
        """
        åˆå§‹åŒ–è´Ÿè½½å‡è¡¡ç®¡ç†å™¨
        
        Args:
            tenant_id: ç§Ÿæˆ·ID
            provider: æä¾›è€…åç§°
            model_type: æ¨¡å‹ç±»å‹
            model: æ¨¡å‹åç§°
            load_balancing_configs: è´Ÿè½½å‡è¡¡é…ç½®åˆ—è¡¨
            managed_credentials: æ‰˜ç®¡å‡­æ®ï¼ˆç”¨äº__inherit__é…ç½®ï¼‰
        """
        self._tenant_id = tenant_id
        self._provider = provider
        self._model_type = model_type
        self._model = model
        self._load_balancing_configs = load_balancing_configs

        # å¤„ç†ç»§æ‰¿å‡­æ®çš„é…ç½®
        for load_balancing_config in self._load_balancing_configs[:]:
            if load_balancing_config.name == "__inherit__":
                if not managed_credentials:
                    # å¦‚æœæ²¡æœ‰æä¾›æ‰˜ç®¡å‡­æ®ï¼Œç§»é™¤ç»§æ‰¿é…ç½®
                    self._load_balancing_configs.remove(load_balancing_config)
                else:
                    # ä½¿ç”¨æ‰˜ç®¡å‡­æ®
                    load_balancing_config.credentials = managed_credentials

    def fetch_next(self) -> Optional[ModelLoadBalancingConfiguration]:
        """
        è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„è´Ÿè½½å‡è¡¡é…ç½®
        ä½¿ç”¨è½®è¯¢ç­–ç•¥å®ç°è´Ÿè½½å‡è¡¡
        
        Returns:
            Optional[ModelLoadBalancingConfiguration]: ä¸‹ä¸€ä¸ªå¯ç”¨é…ç½®ï¼ŒNoneè¡¨ç¤ºæ— å¯ç”¨é…ç½®
        """
        # æ„å»ºRedisç¼“å­˜é”®ï¼Œç”¨äºè®°å½•è½®è¯¢ç´¢å¼•
        cache_key = "model_lb_index:{}:{}:{}:{}".format(
            self._tenant_id, self._provider, self._model_type.value, self._model
        )

        cooldown_configs = []  # è®°å½•å¤„äºå†·å´çŠ¶æ€çš„é…ç½®
        max_index = len(self._load_balancing_configs)

        while True:
            # åŸå­æ€§åœ°å¢åŠ è½®è¯¢ç´¢å¼•
            current_index = redis_client.incr(cache_key)
            current_index = cast(int, current_index)
            
            # é˜²æ­¢ç´¢å¼•è¿‡å¤§ï¼Œé‡ç½®ä¸º1
            if current_index >= 10000000:
                current_index = 1
                redis_client.set(cache_key, current_index)

            # è®¾ç½®ç¼“å­˜è¿‡æœŸæ—¶é—´
            redis_client.expire(cache_key, 3600)
            
            # è®¡ç®—å®é™…é…ç½®ç´¢å¼•
            if current_index > max_index:
                current_index = current_index % max_index

            real_index = current_index - 1
            if real_index >= max_index:
                real_index = 0

            # è·å–å½“å‰é…ç½®
            config: ModelLoadBalancingConfiguration = self._load_balancing_configs[real_index]

            # æ£€æŸ¥é…ç½®æ˜¯å¦åœ¨å†·å´æœŸ
            if self.in_cooldown(config):
                cooldown_configs.append(config)
                
                # å¦‚æœæ‰€æœ‰é…ç½®éƒ½åœ¨å†·å´æœŸï¼Œè¿”å›None
                if len(cooldown_configs) >= len(self._load_balancing_configs):
                    return None

                continue  # å°è¯•ä¸‹ä¸€ä¸ªé…ç½®

            # æ£€æŸ¥ç­–ç•¥åˆè§„æ€§
            try:
                from core.helper.credential_utils import check_credential_policy_compliance

                if config.credential_id:
                    check_credential_policy_compliance(
                        credential_id=config.credential_id,
                        provider=self._provider,
                        credential_type=PluginCredentialType.MODEL,
                    )
                    
            except Exception as e:
                logger.warning(f"è´Ÿè½½å‡è¡¡é…ç½® {config.id} ç­–ç•¥åˆè§„æ€§æ£€æŸ¥å¤±è´¥: {e}")
                cooldown_configs.append(config)
                
                if len(cooldown_configs) >= len(self._load_balancing_configs):
                    return None
                continue

            # è°ƒè¯•æ—¥å¿—
            if dify_config.DEBUG:
                logger.info(
                    f"""æ¨¡å‹è´Ÿè½½å‡è¡¡é€‰æ‹©:
ID: {config.id}
åç§°: {config.name}
ç§Ÿæˆ·ID: {self._tenant_id}
æä¾›è€…: {self._provider}
æ¨¡å‹ç±»å‹: {self._model_type.value}
æ¨¡å‹: {self._model}"""
                )

            return config

    def cooldown(self, config: ModelLoadBalancingConfiguration, expire: int = 60):
        """
        å°†è´Ÿè½½å‡è¡¡é…ç½®æ ‡è®°ä¸ºå†·å´çŠ¶æ€
        ç”¨äºæš‚æ—¶å±è”½æœ‰é—®é¢˜çš„é…ç½®
        
        Args:
            config: è¦å†·å´çš„é…ç½®
            expire: å†·å´æ—¶é—´ï¼ˆç§’ï¼‰
        """
        cooldown_cache_key = "model_lb_index:cooldown:{}:{}:{}:{}:{}".format(
            self._tenant_id, self._provider, self._model_type.value, self._model, config.id
        )

        # è®¾ç½®å†·å´æ ‡è®°ï¼Œå¸¦è¿‡æœŸæ—¶é—´
        redis_client.setex(cooldown_cache_key, expire, "true")
        
        logger.info(f"é…ç½® {config.id} å·²è¿›å…¥å†·å´æœŸ {expire} ç§’")

    def in_cooldown(self, config: ModelLoadBalancingConfiguration) -> bool:
        """
        æ£€æŸ¥é…ç½®æ˜¯å¦åœ¨å†·å´æœŸ
        
        Args:
            config: è¦æ£€æŸ¥çš„é…ç½®
            
        Returns:
            bool: æ˜¯å¦åœ¨å†·å´æœŸ
        """
        cooldown_cache_key = "model_lb_index:cooldown:{}:{}:{}:{}:{}".format(
            self._tenant_id, self._provider, self._model_type.value, self._model, config.id
        )

        return bool(redis_client.exists(cooldown_cache_key))

    @staticmethod
    def get_config_in_cooldown_and_ttl(
        tenant_id: str, 
        provider: str, 
        model_type: ModelType, 
        model: str, 
        config_id: str
    ) -> tuple[bool, int]:
        """
        è·å–é…ç½®çš„å†·å´çŠ¶æ€å’Œå‰©ä½™æ—¶é—´
        
        Args:
            tenant_id: ç§Ÿæˆ·ID
            provider: æä¾›è€…åç§°
            model_type: æ¨¡å‹ç±»å‹
            model: æ¨¡å‹åç§°
            config_id: é…ç½®ID
            
        Returns:
            tuple[bool, int]: (æ˜¯å¦åœ¨å†·å´æœŸ, å‰©ä½™å†·å´æ—¶é—´ç§’æ•°)
        """
        cooldown_cache_key = "model_lb_index:cooldown:{}:{}:{}:{}:{}".format(
            tenant_id, provider, model_type.value, model, config_id
        )

        ttl = redis_client.ttl(cooldown_cache_key)
        
        if ttl == -2:  # é”®ä¸å­˜åœ¨
            return False, 0

        ttl = cast(int, ttl)
        return True, ttl

    def get_active_configs(self) -> list[ModelLoadBalancingConfiguration]:
        """
        è·å–å½“å‰æ´»è·ƒçš„é…ç½®åˆ—è¡¨
        æ’é™¤å¤„äºå†·å´æœŸçš„é…ç½®
        
        Returns:
            list[ModelLoadBalancingConfiguration]: æ´»è·ƒé…ç½®åˆ—è¡¨
        """
        active_configs = []
        
        for config in self._load_balancing_configs:
            if not self.in_cooldown(config):
                active_configs.append(config)
        
        return active_configs

    def get_load_balancing_status(self) -> dict[str, Any]:
        """
        è·å–è´Ÿè½½å‡è¡¡çŠ¶æ€
        è¿”å›æ‰€æœ‰é…ç½®çš„çŠ¶æ€ä¿¡æ¯
        
        Returns:
            dict[str, Any]: è´Ÿè½½å‡è¡¡çŠ¶æ€ä¿¡æ¯
        """
        status_info = {
            "total_configs": len(self._load_balancing_configs),
            "active_configs": 0,
            "cooldown_configs": 0,
            "config_details": []
        }
        
        for config in self._load_balancing_configs:
            is_cooldown, ttl = self.get_config_in_cooldown_and_ttl(
                self._tenant_id, self._provider, self._model_type, 
                self._model, config.id
            )
            
            config_detail = {
                "id": config.id,
                "name": config.name,
                "is_cooldown": is_cooldown,
                "cooldown_ttl": ttl,
                "weight": getattr(config, 'weight', 1)
            }
            
            status_info["config_details"].append(config_detail)
            
            if is_cooldown:
                status_info["cooldown_configs"] += 1
            else:
                status_info["active_configs"] += 1
        
        return status_info

class ModelLoadBalancingConfiguration(BaseModel):
    """
    æ¨¡å‹è´Ÿè½½å‡è¡¡é…ç½®
    å®šä¹‰å•ä¸ªè´Ÿè½½å‡è¡¡é…ç½®çš„æ‰€æœ‰ä¿¡æ¯
    """
    
    # é…ç½®å”¯ä¸€ID
    id: str
    
    # é…ç½®åç§°
    name: str
    
    # æ¨¡å‹å‡­æ®
    credentials: dict[str, Any]
    
    # å‡­æ®IDï¼ˆç”¨äºç­–ç•¥åˆè§„æ€§æ£€æŸ¥ï¼‰
    credential_id: Optional[str] = None
    
    # æƒé‡ï¼ˆç”¨äºåŠ æƒè½®è¯¢ï¼‰
    weight: int = 1
    
    # ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
    priority: int = 0
    
    # æ˜¯å¦å¯ç”¨
    enabled: bool = True
    
    # é…ç½®å…ƒæ•°æ®
    metadata: dict[str, Any] = Field(default_factory=dict)
```

## 4. æ¨¡å‹æä¾›è€…å·¥å‚

### 4.1 ModelProviderFactoryæä¾›è€…ç®¡ç†

```python
class ModelProviderFactory:
    """
    æ¨¡å‹æä¾›è€…å·¥å‚
    è´Ÿè´£ç®¡ç†å’Œåˆ›å»ºå„ç§æ¨¡å‹æä¾›è€…å®ä¾‹
    """

    def __init__(self, tenant_id: str):
        """
        åˆå§‹åŒ–æ¨¡å‹æä¾›è€…å·¥å‚
        
        Args:
            tenant_id: ç§Ÿæˆ·IDï¼Œç”¨äºè·å–ç§Ÿæˆ·ç‰¹å®šçš„é…ç½®
        """
        self.tenant_id = tenant_id
        self.plugin_model_manager = PluginModelClient()
        
        # æä¾›è€…ç¼“å­˜
        self._provider_cache = {}
        self._cache_lock = Lock()

    def get_providers(self) -> Sequence[ProviderEntity]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹æä¾›è€…
        è¿”å›æ”¯æŒçš„æ‰€æœ‰æä¾›è€…åˆ—è¡¨ï¼ŒåŒ…æ‹¬äº‘æœåŠ¡ã€å¼€æºå’Œæ’ä»¶æä¾›è€…
        
        Returns:
            Sequence[ProviderEntity]: æä¾›è€…å®ä½“åˆ—è¡¨
        """
        # ä»æ’ä»¶æœåŠ¡å™¨è·å–æä¾›è€…ï¼ˆåŒ…å«å†…ç½®å’Œæ’ä»¶æä¾›è€…ï¼‰
        plugin_providers = self.get_plugin_model_providers()
        
        # æå–æä¾›è€…å£°æ˜
        return [provider.declaration for provider in plugin_providers]

    def get_plugin_model_providers(self) -> Sequence[PluginModelProviderEntity]:
        """
        è·å–æ‰€æœ‰æ’ä»¶æ¨¡å‹æä¾›è€…
        åŒ…å«å†…ç½®æä¾›è€…å’Œç¬¬ä¸‰æ–¹æ’ä»¶æä¾›è€…
        
        Returns:
            Sequence[PluginModelProviderEntity]: æ’ä»¶æ¨¡å‹æä¾›è€…åˆ—è¡¨
        """
        # ä½¿ç”¨ä¸Šä¸‹æ–‡å˜é‡è¿›è¡Œçº¿ç¨‹å®‰å…¨çš„ç¼“å­˜
        try:
            contexts.plugin_model_providers.get()
        except LookupError:
            contexts.plugin_model_providers.set(None)
            contexts.plugin_model_providers_lock.set(Lock())

        with contexts.plugin_model_providers_lock.get():
            plugin_model_providers = contexts.plugin_model_providers.get()
            
            # æ£€æŸ¥ç¼“å­˜
            if plugin_model_providers is not None:
                return plugin_model_providers

            # ç¼“å­˜ä¸ºç©ºï¼Œé‡æ–°åŠ è½½
            plugin_model_providers = []
            contexts.plugin_model_providers.set(plugin_model_providers)

            # ä»æ’ä»¶æœåŠ¡å™¨è·å–æ¨¡å‹æä¾›è€…
            plugin_providers = self.plugin_model_manager.fetch_model_providers(self.tenant_id)

            for provider in plugin_providers:
                # ä¸ºæ’ä»¶æä¾›è€…æ·»åŠ å‘½åç©ºé—´å‰ç¼€
                provider.declaration.provider = (
                    provider.plugin_id + "/" + provider.declaration.provider
                )
                plugin_model_providers.append(provider)

            return plugin_model_providers

    def get_provider_schema(self, provider: str) -> ProviderEntity:
        """
        è·å–æä¾›è€…schema
        è¿”å›æä¾›è€…çš„é…ç½®è§„åˆ™å’Œæ”¯æŒçš„æ¨¡å‹ä¿¡æ¯
        
        Args:
            provider: æä¾›è€…åç§°
            
        Returns:
            ProviderEntity: æä¾›è€…å®ä½“ï¼ŒåŒ…å«é…ç½®è§„åˆ™
            
        Raises:
            ValueError: æä¾›è€…ä¸å­˜åœ¨
        """
        providers = self.get_providers()
        
        for provider_entity in providers:
            if provider_entity.provider == provider:
                return provider_entity
        
        raise ValueError(f"æä¾›è€… {provider} ä¸å­˜åœ¨")

    def get_provider_instance(
        self, 
        provider: str, 
        model_type: ModelType
    ) -> BaseProvider:
        """
        è·å–æä¾›è€…å®ä¾‹
        æ ¹æ®æä¾›è€…åç§°å’Œæ¨¡å‹ç±»å‹åˆ›å»ºæä¾›è€…å®ä¾‹
        
        Args:
            provider: æä¾›è€…åç§°
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            BaseProvider: æä¾›è€…å®ä¾‹
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{provider}:{model_type.value}"
        if cache_key in self._provider_cache:
            return self._provider_cache[cache_key]

        # åˆ›å»ºæ–°çš„æä¾›è€…å®ä¾‹
        provider_instance = self._create_provider_instance(provider, model_type)
        
        # ç¼“å­˜å®ä¾‹
        with self._cache_lock:
            self._provider_cache[cache_key] = provider_instance
        
        return provider_instance

    def _create_provider_instance(
        self, 
        provider: str, 
        model_type: ModelType
    ) -> BaseProvider:
        """
        åˆ›å»ºæä¾›è€…å®ä¾‹
        æ ¹æ®æä¾›è€…åç§°åŠ¨æ€å¯¼å…¥å’Œåˆ›å»ºç›¸åº”çš„æä¾›è€…ç±»
        
        Args:
            provider: æä¾›è€…åç§°
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            BaseProvider: åˆ›å»ºçš„æä¾›è€…å®ä¾‹
        """
        # å¤„ç†æ’ä»¶æä¾›è€…ï¼ˆåŒ…å«å‘½åç©ºé—´ï¼‰
        if "/" in provider:
            plugin_id, provider_name = provider.split("/", 1)
            return self._create_plugin_provider_instance(
                plugin_id=plugin_id,
                provider_name=provider_name,
                model_type=model_type
            )
        
        # å†…ç½®æä¾›è€…æ˜ å°„
        provider_mapping = {
            "openai": "core.model_runtime.model_providers.openai",
            "anthropic": "core.model_runtime.model_providers.anthropic", 
            "google": "core.model_runtime.model_providers.google",
            "azure_openai": "core.model_runtime.model_providers.azure_openai",
            "zhipuai": "core.model_runtime.model_providers.zhipuai",
            "tongyi": "core.model_runtime.model_providers.tongyi",
            "wenxin": "core.model_runtime.model_providers.wenxin",
            "moonshot": "core.model_runtime.model_providers.moonshot",
            "ollama": "core.model_runtime.model_providers.ollama",
            # ... æ›´å¤šæä¾›è€…æ˜ å°„
        }
        
        provider_module_path = provider_mapping.get(provider)
        if not provider_module_path:
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›è€…: {provider}")
        
        # åŠ¨æ€å¯¼å…¥æä¾›è€…æ¨¡å—
        try:
            provider_module = importlib.import_module(provider_module_path)
            provider_class = getattr(provider_module, f"{provider.title()}Provider")
            
            return provider_class(tenant_id=self.tenant_id)
            
        except (ImportError, AttributeError) as e:
            raise ValueError(f"æ— æ³•åŠ è½½æä¾›è€… {provider}: {e}")

    def _create_plugin_provider_instance(
        self,
        plugin_id: str,
        provider_name: str,
        model_type: ModelType
    ) -> BaseProvider:
        """
        åˆ›å»ºæ’ä»¶æä¾›è€…å®ä¾‹
        
        Args:
            plugin_id: æ’ä»¶ID
            provider_name: æä¾›è€…åç§°
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            BaseProvider: æ’ä»¶æä¾›è€…å®ä¾‹
        """
        # é€šè¿‡æ’ä»¶ç®¡ç†å™¨åˆ›å»ºæä¾›è€…å®ä¾‹
        plugin_provider = self.plugin_model_manager.create_provider_instance(
            plugin_id=plugin_id,
            provider_name=provider_name,
            model_type=model_type,
            tenant_id=self.tenant_id
        )
        
        return plugin_provider

    def validate_provider_credentials(
        self,
        provider: str,
        model_type: ModelType,
        credentials: dict[str, Any]
    ) -> tuple[bool, Optional[str]]:
        """
        éªŒè¯æä¾›è€…å‡­æ®
        æ£€æŸ¥æä¾›çš„å‡­æ®æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            provider: æä¾›è€…åç§°
            model_type: æ¨¡å‹ç±»å‹
            credentials: å‡­æ®å­—å…¸
            
        Returns:
            tuple[bool, Optional[str]]: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # è·å–æä¾›è€…å®ä¾‹
            provider_instance = self.get_provider_instance(provider, model_type)
            
            # éªŒè¯å‡­æ®
            provider_instance.validate_provider_credentials(credentials)
            
            return True, None
            
        except Exception as e:
            error_message = f"å‡­æ®éªŒè¯å¤±è´¥: {str(e)}"
            logger.warning(error_message)
            return False, error_message

    def get_provider_supported_models(
        self,
        provider: str,
        model_type: ModelType
    ) -> list[dict[str, Any]]:
        """
        è·å–æä¾›è€…æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
        
        Args:
            provider: æä¾›è€…åç§°
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            list[dict[str, Any]]: æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
        """
        try:
            provider_instance = self.get_provider_instance(provider, model_type)
            
            # è·å–é¢„å®šä¹‰æ¨¡å‹
            predefined_models = provider_instance.get_predefined_models(model_type)
            
            # è·å–è¿œç¨‹æ¨¡å‹ï¼ˆå¦‚æœæ”¯æŒï¼‰
            remote_models = []
            if hasattr(provider_instance, 'get_remote_models'):
                try:
                    remote_models = provider_instance.get_remote_models(model_type)
                except Exception as e:
                    logger.warning(f"è·å–è¿œç¨‹æ¨¡å‹å¤±è´¥: {e}")
            
            # åˆå¹¶æ¨¡å‹åˆ—è¡¨
            all_models = predefined_models + remote_models
            
            return [
                {
                    "model": model.model,
                    "label": model.label,
                    "model_type": model_type.value,
                    "features": model.features or [],
                    "parameter_rules": model.parameter_rules or [],
                    "pricing": model.pricing or {},
                    "deprecated": getattr(model, 'deprecated', False)
                }
                for model in all_models
            ]
            
        except Exception as e:
            logger.exception(f"è·å–æä¾›è€… {provider} æ”¯æŒçš„æ¨¡å‹å¤±è´¥: {e}")
            return []
```

## 5. å¤šæ¨¡å‹ç±»å‹æ”¯æŒ

### 5.1 å…­ç§æ¨¡å‹ç±»å‹æ¶æ„

Difyæ”¯æŒ6ç§ä¸åŒç±»å‹çš„AIæ¨¡å‹ï¼Œæ¯ç§æ¨¡å‹éƒ½æœ‰ä¸“é—¨çš„æ¥å£å’Œå®ç°ã€‚Difyçš„æ¨¡å‹ç±»å‹è®¾è®¡å……åˆ†è€ƒè™‘äº†**AIåº”ç”¨çš„å…¨ç”Ÿå‘½å‘¨æœŸéœ€æ±‚**ï¼š

**æ¨¡å‹ç±»å‹è®¾è®¡çš„æˆ˜ç•¥è€ƒè™‘**ï¼š
```python
# åŸºäºAIåº”ç”¨åœºæ™¯çš„æ¨¡å‹ç±»å‹è§„åˆ’
AI_APPLICATION_MODEL_MATRIX = {
    "å¯¹è¯åº”ç”¨åœºæ™¯": {
        "æ ¸å¿ƒæ¨¡å‹": "LLM",
        "å¢å¼ºæ¨¡å‹": ["Embedding", "Moderation"], 
        "å¯é€‰æ¨¡å‹": ["TTS", "STT"],
        "å…¸å‹æ¶æ„": "LLM + Embedding(RAG) + Moderation(å®‰å…¨)"
    },
    
    "çŸ¥è¯†é—®ç­”åœºæ™¯": {
        "æ ¸å¿ƒæ¨¡å‹": "LLM", 
        "å¢å¼ºæ¨¡å‹": ["Embedding", "Rerank"],
        "å¯é€‰æ¨¡å‹": ["Moderation"],
        "å…¸å‹æ¶æ„": "LLM + Embedding(å‘é‡åŒ–) + Rerank(é‡æ’åº)"
    },
    
    "å†…å®¹åˆ›ä½œåœºæ™¯": {
        "æ ¸å¿ƒæ¨¡å‹": "LLM",
        "å¢å¼ºæ¨¡å‹": ["Moderation"],
        "å¯é€‰æ¨¡å‹": ["TTS", "Embedding"],
        "å…¸å‹æ¶æ„": "LLM + Moderation(å†…å®¹å®‰å…¨)"
    },
    
    "å¤šæ¨¡æ€åº”ç”¨åœºæ™¯": {
        "æ ¸å¿ƒæ¨¡å‹": "LLM",
        "å¢å¼ºæ¨¡å‹": ["Embedding", "STT", "TTS"],
        "å¯é€‰æ¨¡å‹": ["Moderation", "Rerank"],
        "å…¸å‹æ¶æ„": "LLM + STT(è¯­éŸ³è¾“å…¥) + TTS(è¯­éŸ³è¾“å‡º) + Embedding(å¤šæ¨¡æ€æ£€ç´¢)"
    },
    
    "ä¼ä¸šçº§çŸ¥è¯†ç®¡ç†": {
        "æ ¸å¿ƒæ¨¡å‹": "LLM",
        "å¢å¼ºæ¨¡å‹": ["Embedding", "Rerank", "Moderation"],
        "å¯é€‰æ¨¡å‹": ["STT", "TTS"],
        "å…¸å‹æ¶æ„": "LLM + å…¨å¥—æ¨¡å‹æ”¯æŒ(å®Œæ•´çš„ä¼ä¸šçº§èƒ½åŠ›)"
    }
}

# 40+æ¨¡å‹æä¾›è€…çš„ç”Ÿæ€ä½åˆ†æ
MODEL_PROVIDER_ECOSYSTEM = {
    "å…¨çƒå¤´éƒ¨å‚å•†": {
        "openai": {
            "ä¼˜åŠ¿": ["æŠ€æœ¯é¢†å…ˆ", "ç”Ÿæ€å®Œå–„", "APIç¨³å®š"],
            "åŠ£åŠ¿": ["æˆæœ¬è¾ƒé«˜", "æ•°æ®å‡ºå¢ƒ", "APIé™åˆ¶"],
            "é€‚ç”¨åœºæ™¯": ["é«˜ç«¯åº”ç”¨", "åŸå‹éªŒè¯", "æŠ€æœ¯æ ‡æ†"],
            "æ¨èæ¨¡å‹": ["gpt-4", "text-embedding-3-large", "whisper-1"]
        },
        "anthropic": {
            "ä¼˜åŠ¿": ["å®‰å…¨æ€§å¼º", "æ¨ç†èƒ½åŠ›ä¼˜", "é•¿ä¸Šä¸‹æ–‡"],
            "åŠ£åŠ¿": ["æ¨¡å‹ç§ç±»å°‘", "ä»·æ ¼åé«˜", "å¯ç”¨æ€§é™åˆ¶"],
            "é€‚ç”¨åœºæ™¯": ["ä¼ä¸šåº”ç”¨", "å®‰å…¨æ•æ„Ÿ", "é•¿æ–‡æœ¬å¤„ç†"],
            "æ¨èæ¨¡å‹": ["claude-3-opus", "claude-3-haiku"]
        },
        "google": {
            "ä¼˜åŠ¿": ["å¤šæ¨¡æ€å¼º", "å…è´¹é¢åº¦", "æŠ€æœ¯åˆ›æ–°"],
            "åŠ£åŠ¿": ["APIç¨³å®šæ€§", "ä¸­æ–‡æ”¯æŒå¼±", "å•†ä¸šåŒ–ç¨‹åº¦"],
            "é€‚ç”¨åœºæ™¯": ["å¤šæ¨¡æ€åº”ç”¨", "åˆ›æ–°è¯•éªŒ", "æˆæœ¬æ§åˆ¶"],
            "æ¨èæ¨¡å‹": ["gemini-pro", "embedding-001"]
        }
    },
    
    "ä¸­å›½æœ¬åœŸå‚å•†": {
        "zhipuai": {
            "ä¼˜åŠ¿": ["ä¸­æ–‡ä¼˜åŒ–", "æœ¬åœŸåŒ–æœåŠ¡", "åˆè§„æ€§å¼º"],
            "åŠ£åŠ¿": ["å›½é™…åŒ–ç¨‹åº¦", "æŠ€æœ¯ä»£é™…", "ç”Ÿæ€å»ºè®¾"],
            "é€‚ç”¨åœºæ™¯": ["ä¸­æ–‡åº”ç”¨", "åˆè§„è¦æ±‚", "æœ¬åœŸéƒ¨ç½²"],
            "æ¨èæ¨¡å‹": ["glm-4", "embedding-2"]
        },
        "tongyi": {
            "ä¼˜åŠ¿": ["é˜¿é‡Œç”Ÿæ€", "ä¼ä¸šçº§æœåŠ¡", "ä¸­æ–‡ä¼˜åŒ–"],
            "åŠ£åŠ¿": ["å¼€æ”¾ç¨‹åº¦", "æˆæœ¬ç»“æ„", "æŠ€æœ¯ä¾èµ–"],
            "é€‚ç”¨åœºæ™¯": ["é˜¿é‡Œäº‘ç”Ÿæ€", "ä¼ä¸šå®¢æˆ·", "ç”µå•†åœºæ™¯"],
            "æ¨èæ¨¡å‹": ["qwen-max", "text-embedding-v1"]
        },
        "moonshot": {
            "ä¼˜åŠ¿": ["é•¿ä¸Šä¸‹æ–‡", "æ€§ä»·æ¯”é«˜", "å“åº”é€Ÿåº¦"],
            "åŠ£åŠ¿": ["æ¨¡å‹æ•°é‡", "ç¨³å®šæ€§", "åŠŸèƒ½å®Œæ•´æ€§"],
            "é€‚ç”¨åœºæ™¯": ["é•¿æ–‡æ¡£å¤„ç†", "æˆæœ¬æ•æ„Ÿ", "å¿«é€Ÿè¿­ä»£"],
            "æ¨èæ¨¡å‹": ["moonshot-v1-128k"]
        }
    },
    
    "å¼€æºæœ¬åœ°åŒ–æ–¹æ¡ˆ": {
        "ollama": {
            "ä¼˜åŠ¿": ["å®Œå…¨æœ¬åœ°", "é›¶æˆæœ¬", "æ•°æ®å®‰å…¨"],
            "åŠ£åŠ¿": ["æ€§èƒ½è¦æ±‚", "æ¨¡å‹è´¨é‡", "ç»´æŠ¤æˆæœ¬"],
            "é€‚ç”¨åœºæ™¯": ["æ•°æ®æ•æ„Ÿ", "ç¦»çº¿ç¯å¢ƒ", "å¼€å‘æµ‹è¯•"],
            "æ¨èæ¨¡å‹": ["llama2", "code-llama", "mistral"]
        },
        "xinference": {
            "ä¼˜åŠ¿": ["æ¨¡å‹ä¸°å¯Œ", "éƒ¨ç½²çµæ´»", "æ€§èƒ½ä¼˜åŒ–"],
            "åŠ£åŠ¿": ["è¿ç»´å¤æ‚", "èµ„æºè¦æ±‚", "æŠ€æœ¯é—¨æ§›"],
            "é€‚ç”¨åœºæ™¯": ["æ··åˆéƒ¨ç½²", "æ¨¡å‹å¯¹æ¯”", "æ€§èƒ½è°ƒä¼˜"],
            "æ¨èæ¨¡å‹": ["chatglm", "baichuan", "internlm"]
        }
    }
}
```

**æ¨¡å‹æä¾›è€…é€‰æ‹©çš„å†³ç­–çŸ©é˜µ**ï¼š
æ ¹æ®å®é™…éƒ¨ç½²ç»éªŒï¼Œä¸åŒä¸šåŠ¡åœºæ™¯ä¸‹çš„æœ€ä¼˜é€‰æ‹©ï¼š

- **åˆåˆ›ä¼ä¸š**ï¼šOpenAI(å¿«é€ŸéªŒè¯) + é˜¿é‡Œäº‘é€šä¹‰åƒé—®(ä¸­æ–‡ä¼˜åŒ–) + Ollama(æˆæœ¬æ§åˆ¶)
- **ä¸­å‹ä¼ä¸š**ï¼šClaude(å®‰å…¨æ€§) + æ™ºè°±AI(ä¸­æ–‡ä¸“ä¸š) + æœ¬åœ°éƒ¨ç½²(æ•°æ®å®‰å…¨)
- **å¤§å‹ä¼ä¸š**ï¼šå¤šæä¾›è€…æ··åˆ(é£é™©åˆ†æ•£) + ç§æœ‰åŒ–éƒ¨ç½²(åˆè§„è¦æ±‚) + æˆæœ¬ä¼˜åŒ–ç­–ç•¥
- **æ”¿åºœæœºæ„**ï¼šçº¯å›½äº§åŒ–æ–¹æ¡ˆ + ç§æœ‰äº‘éƒ¨ç½² + ä¸¥æ ¼çš„å®‰å…¨å®¡è®¡

```python
class ModelType(Enum):
    """
    æ¨¡å‹ç±»å‹æšä¸¾
    å®šä¹‰Difyæ”¯æŒçš„æ‰€æœ‰AIæ¨¡å‹ç±»å‹
    """
    
    # å¤§è¯­è¨€æ¨¡å‹ - æ–‡æœ¬ç”Ÿæˆå’Œå¯¹è¯
    LLM = "llm"
    
    # æ–‡æœ¬åµŒå…¥æ¨¡å‹ - å‘é‡åŒ–æ–‡æœ¬
    TEXT_EMBEDDING = "text-embedding"
    
    # é‡æ’åºæ¨¡å‹ - æ–‡æ¡£ç›¸å…³æ€§é‡æ’åº
    RERANK = "rerank"
    
    # è¯­éŸ³è½¬æ–‡å­—æ¨¡å‹ - éŸ³é¢‘è½¬å½•
    SPEECH2TEXT = "speech2text"
    
    # æ–‡å­—è½¬è¯­éŸ³æ¨¡å‹ - è¯­éŸ³åˆæˆ
    TTS = "tts"
    
    # å†…å®¹å®¡æ ¸æ¨¡å‹ - å†…å®¹å®‰å…¨æ£€æµ‹
    MODERATION = "moderation"

# æ¨¡å‹ç±»å‹ç‰¹æ€§å¯¹æ¯”
model_type_features = {
    "llm": {
        "description": "å¤§è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒæ–‡æœ¬ç”Ÿæˆã€å¯¹è¯å’Œæ¨ç†",
        "primary_function": "text_generation",
        "input_types": ["text", "image", "audio"],
        "output_types": ["text", "function_calls"],
        "key_features": [
            "å¤šè½®å¯¹è¯", "å‡½æ•°è°ƒç”¨", "ä»£ç ç”Ÿæˆ", 
            "å†…å®¹åˆ›ä½œ", "æ¨ç†é—®ç­”", "å¤šæ¨¡æ€ç†è§£"
        ],
        "providers": [
            "openai", "anthropic", "google", "azure_openai",
            "zhipuai", "tongyi", "wenxin", "moonshot", "ollama"
        ],
        "typical_models": [
            "gpt-4", "claude-3", "gemini-pro", 
            "qwen-plus", "ernie-4.0", "moonshot-v1"
        ]
    },
    "text-embedding": {
        "description": "æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡è¡¨ç¤º",
        "primary_function": "text_vectorization", 
        "input_types": ["text"],
        "output_types": ["vector"],
        "key_features": [
            "è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—", "æ–‡æ¡£æ£€ç´¢", "èšç±»åˆ†æ",
            "æ¨èç³»ç»Ÿ", "å¼‚å¸¸æ£€æµ‹"
        ],
        "providers": [
            "openai", "google", "cohere", "jina",
            "zhipuai", "tongyi", "bge", "m3e"
        ],
        "typical_models": [
            "text-embedding-3-large", "embedding-001",
            "embed-multilingual-v3.0", "bge-large-zh"
        ]
    },
    "rerank": {
        "description": "é‡æ’åºæ¨¡å‹ï¼Œå¯¹æ£€ç´¢ç»“æœæŒ‰ç›¸å…³æ€§é‡æ–°æ’åº",
        "primary_function": "relevance_ranking",
        "input_types": ["query_document_pairs"],
        "output_types": ["relevance_scores"],
        "key_features": [
            "æ£€ç´¢ç»“æœä¼˜åŒ–", "ç›¸å…³æ€§è¯„åˆ†", "æ’åºç®—æ³•",
            "å¤šè¯­è¨€æ”¯æŒ", "è·¨åŸŸæ³›åŒ–"
        ],
        "providers": [
            "cohere", "jina", "voyage", "xinference",
            "bge", "bce"
        ],
        "typical_models": [
            "rerank-english-v3.0", "jina-reranker-v1-base",
            "voyage-rerank-lite", "bge-reranker-large"
        ]
    },
    "speech2text": {
        "description": "è¯­éŸ³è½¬æ–‡å­—æ¨¡å‹ï¼Œå°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬",
        "primary_function": "audio_transcription",
        "input_types": ["audio"],
        "output_types": ["text"],
        "key_features": [
            "å¤šè¯­è¨€è¯†åˆ«", "å®æ—¶è½¬å½•", "æ ‡ç‚¹ç¬¦å·",
            "è¯´è¯äººè¯†åˆ«", "å™ªå£°å¤„ç†"
        ],
        "providers": [
            "openai", "azure_openai", "google",
            "alibaba", "baidu", "iflytek"
        ],
        "typical_models": [
            "whisper-1", "whisper-large-v3",
            "speech-recognition-v1", "asr-v1"
        ]
    },
    "tts": {
        "description": "æ–‡å­—è½¬è¯­éŸ³æ¨¡å‹ï¼Œå°†æ–‡æœ¬åˆæˆä¸ºè¯­éŸ³",
        "primary_function": "speech_synthesis",
        "input_types": ["text"],
        "output_types": ["audio"],
        "key_features": [
            "å¤šéŸ³è‰²é€‰æ‹©", "æƒ…æ„Ÿè¡¨è¾¾", "è¯­é€Ÿæ§åˆ¶",
            "å¤šè¯­è¨€åˆæˆ", "é«˜ä¿çœŸéŸ³è´¨"
        ],
        "providers": [
            "openai", "azure_openai", "google",
            "alibaba", "baidu", "iflytek", "elevenlabs"
        ],
        "typical_models": [
            "tts-1", "tts-1-hd", "neural-voice",
            "speech-synthesis-v1"
        ]
    },
    "moderation": {
        "description": "å†…å®¹å®¡æ ¸æ¨¡å‹ï¼Œæ£€æµ‹æ–‡æœ¬å†…å®¹çš„å®‰å…¨æ€§",
        "primary_function": "content_safety_detection",
        "input_types": ["text"],
        "output_types": ["safety_scores"],
        "key_features": [
            "æœ‰å®³å†…å®¹æ£€æµ‹", "æ•æ„Ÿä¿¡æ¯è¯†åˆ«", "å¤šç»´åº¦è¯„åˆ†",
            "å®æ—¶å®¡æ ¸", "åˆè§„æ€§æ£€æŸ¥"
        ],
        "providers": [
            "openai", "azure_openai", "google",
            "alibaba", "baidu", "tencent"
        ],
        "typical_models": [
            "text-moderation-latest", "content-safety-v1",
            "moderation-api"
        ]
    }
}

class LargeLanguageModel(AIModel):
    """
    å¤§è¯­è¨€æ¨¡å‹åŸºç±»
    å®šä¹‰æ‰€æœ‰LLMæä¾›è€…çš„é€šç”¨æ¥å£
    """
    
    model_type: ModelType = ModelType.LLM

    @abstractmethod
    def invoke(
        self,
        model: str,
        credentials: dict[str, Any],
        prompt_messages: list[PromptMessage],
        model_parameters: Optional[dict] = None,
        tools: Optional[list[PromptMessageTool]] = None,
        stop: Optional[list[str]] = None,
        stream: bool = True,
        user: Optional[str] = None,
        callbacks: Optional[list[Callback]] = None,
    ) -> Union[LLMResult, Generator[LLMResultChunk, None, None]]:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹
        å­ç±»å¿…é¡»å®ç°çš„æ ¸å¿ƒè°ƒç”¨æ–¹æ³•
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            prompt_messages: æç¤ºæ¶ˆæ¯åˆ—è¡¨
            model_parameters: æ¨¡å‹å‚æ•°ï¼ˆæ¸©åº¦ã€æœ€å¤§ä»¤ç‰Œç­‰ï¼‰
            tools: å·¥å…·åˆ—è¡¨ï¼ˆç”¨äºå‡½æ•°è°ƒç”¨ï¼‰
            stop: åœæ­¢è¯åˆ—è¡¨
            stream: æ˜¯å¦æµå¼è¾“å‡º
            user: ç”¨æˆ·æ ‡è¯†
            callbacks: å›è°ƒå‡½æ•°åˆ—è¡¨
            
        Returns:
            Union[LLMResult, Generator]: LLMè°ƒç”¨ç»“æœ
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°invokeæ–¹æ³•")

    @abstractmethod
    def get_num_tokens(
        self,
        model: str,
        credentials: dict[str, Any],
        prompt_messages: list[PromptMessage],
        tools: Optional[list[PromptMessageTool]] = None,
    ) -> int:
        """
        è·å–æç¤ºæ¶ˆæ¯çš„ä»¤ç‰Œæ•°é‡
        ç”¨äºæˆæœ¬è®¡ç®—å’Œä¸Šä¸‹æ–‡é•¿åº¦ç®¡ç†
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            prompt_messages: æç¤ºæ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·åˆ—è¡¨
            
        Returns:
            int: ä»¤ç‰Œæ€»æ•°
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°get_num_tokensæ–¹æ³•")

    def validate_credentials(
        self, 
        model: str, 
        credentials: dict[str, Any]
    ) -> bool:
        """
        éªŒè¯æ¨¡å‹å‡­æ®
        æ£€æŸ¥æä¾›çš„å‡­æ®æ˜¯å¦èƒ½å¤ŸæˆåŠŸè°ƒç”¨æ¨¡å‹
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: å‡­æ®å­—å…¸
            
        Returns:
            bool: å‡­æ®æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # ä½¿ç”¨ç®€å•çš„æµ‹è¯•æç¤ºéªŒè¯å‡­æ®
            test_messages = [
                UserPromptMessage(content="Hello")
            ]
            
            # è°ƒç”¨æ¨¡å‹ï¼ˆéæµå¼ï¼Œæœ€å°å‚æ•°ï¼‰
            result = self.invoke(
                model=model,
                credentials=credentials,
                prompt_messages=test_messages,
                model_parameters={"max_tokens": 1, "temperature": 0},
                stream=False,
                user="system_validation"
            )
            
            return True
            
        except Exception as e:
            logger.warning(f"å‡­æ®éªŒè¯å¤±è´¥: {e}")
            return False

    def get_model_schema(
        self, 
        model: str, 
        credentials: dict[str, Any]
    ) -> Optional[AIModelEntity]:
        """
        è·å–æ¨¡å‹schema
        è¿”å›æ¨¡å‹çš„èƒ½åŠ›ã€å‚æ•°è§„åˆ™å’Œé™åˆ¶ä¿¡æ¯
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            
        Returns:
            Optional[AIModelEntity]: æ¨¡å‹å®ä½“ï¼ŒåŒ…å«å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯
        """
        # é¦–å…ˆä»é¢„å®šä¹‰æ¨¡å‹ä¸­æŸ¥æ‰¾
        predefined_models = self.get_predefined_models()
        
        for predefined_model in predefined_models:
            if predefined_model.model == model:
                return predefined_model
        
        # å¦‚æœæ”¯æŒè¿œç¨‹æ¨¡å‹ï¼Œå°è¯•è·å–
        if hasattr(self, 'get_remote_models'):
            try:
                remote_models = self.get_remote_models(credentials)
                for remote_model in remote_models:
                    if remote_model.model == model:
                        return remote_model
            except Exception as e:
                logger.warning(f"è·å–è¿œç¨‹æ¨¡å‹schemaå¤±è´¥: {e}")
        
        return None

    def calculate_cost(
        self,
        model: str,
        prompt_tokens: int,
        completion_tokens: int
    ) -> float:
        """
        è®¡ç®—è°ƒç”¨æˆæœ¬
        æ ¹æ®ä»¤ç‰Œæ•°é‡å’Œæ¨¡å‹å®šä»·è®¡ç®—æˆæœ¬
        
        Args:
            model: æ¨¡å‹åç§°
            prompt_tokens: æç¤ºä»¤ç‰Œæ•°
            completion_tokens: å®Œæˆä»¤ç‰Œæ•°
            
        Returns:
            float: è°ƒç”¨æˆæœ¬ï¼ˆç¾å…ƒï¼‰
        """
        model_schema = self.get_model_schema(model, {})
        if not model_schema or not model_schema.pricing:
            return 0.0
        
        pricing = model_schema.pricing
        
        # è®¡ç®—æç¤ºæˆæœ¬
        prompt_cost = (prompt_tokens / 1000000) * pricing.input_price
        
        # è®¡ç®—å®Œæˆæˆæœ¬
        completion_cost = (completion_tokens / 1000000) * pricing.output_price
        
        return prompt_cost + completion_cost

class TextEmbeddingModel(AIModel):
    """
    æ–‡æœ¬åµŒå…¥æ¨¡å‹åŸºç±»
    å®šä¹‰æ–‡æœ¬å‘é‡åŒ–çš„é€šç”¨æ¥å£
    """
    
    model_type: ModelType = ModelType.TEXT_EMBEDDING

    @abstractmethod
    def invoke(
        self,
        model: str,
        credentials: dict[str, Any],
        texts: list[str],
        user: Optional[str] = None,
        input_type: EmbeddingInputType = EmbeddingInputType.DOCUMENT,
    ) -> TextEmbeddingResult:
        """
        è°ƒç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            texts: å¾…åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
            user: ç”¨æˆ·æ ‡è¯†
            input_type: è¾“å…¥ç±»å‹ï¼Œå½±å“åµŒå…¥ä¼˜åŒ–ç­–ç•¥
            
        Returns:
            TextEmbeddingResult: åµŒå…¥ç»“æœ
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°invokeæ–¹æ³•")

    @abstractmethod
    def get_num_tokens(
        self, 
        model: str, 
        credentials: dict[str, Any], 
        texts: list[str]
    ) -> list[int]:
        """
        è·å–æ–‡æœ¬ä»¤ç‰Œæ•°é‡
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            list[int]: æ¯ä¸ªæ–‡æœ¬çš„ä»¤ç‰Œæ•°é‡
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°get_num_tokensæ–¹æ³•")

    def batch_embed_with_optimization(
        self,
        model: str,
        credentials: dict[str, Any],
        texts: list[str],
        batch_size: int = 100,
        user: Optional[str] = None,
        input_type: EmbeddingInputType = EmbeddingInputType.DOCUMENT,
    ) -> TextEmbeddingResult:
        """
        æ‰¹é‡åµŒå…¥ä¼˜åŒ–
        å¯¹å¤§é‡æ–‡æœ¬è¿›è¡Œåˆ†æ‰¹å¤„ç†ï¼Œé¿å…APIé™åˆ¶
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            texts: å¾…åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            user: ç”¨æˆ·æ ‡è¯†
            input_type: è¾“å…¥ç±»å‹
            
        Returns:
            TextEmbeddingResult: åˆå¹¶çš„åµŒå…¥ç»“æœ
        """
        if len(texts) <= batch_size:
            # å°äºæ‰¹æ¬¡å¤§å°ï¼Œç›´æ¥è°ƒç”¨
            return self.invoke(
                model=model,
                credentials=credentials,
                texts=texts,
                user=user,
                input_type=input_type
            )

        # åˆ†æ‰¹å¤„ç†
        all_embeddings = []
        total_tokens = 0
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            batch_result = self.invoke(
                model=model,
                credentials=credentials,
                texts=batch_texts,
                user=user,
                input_type=input_type
            )
            
            all_embeddings.extend(batch_result.embeddings)
            total_tokens += batch_result.usage.total_tokens

        # åˆå¹¶ç»“æœ
        return TextEmbeddingResult(
            embeddings=all_embeddings,
            usage=EmbeddingUsage(
                tokens=total_tokens,
                total_price=self._calculate_embedding_cost(model, total_tokens)
            )
        )

class RerankModel(AIModel):
    """
    é‡æ’åºæ¨¡å‹åŸºç±»
    å¯¹æ£€ç´¢ç»“æœè¿›è¡Œç›¸å…³æ€§é‡æ–°æ’åº
    """
    
    model_type: ModelType = ModelType.RERANK

    @abstractmethod
    def invoke(
        self,
        model: str,
        credentials: dict[str, Any],
        query: str,
        docs: list[str],
        score_threshold: Optional[float] = None,
        top_n: Optional[int] = None,
        user: Optional[str] = None,
    ) -> RerankResult:
        """
        è°ƒç”¨é‡æ’åºæ¨¡å‹
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            query: æœç´¢æŸ¥è¯¢
            docs: å¾…é‡æ’åºçš„æ–‡æ¡£åˆ—è¡¨
            score_threshold: ç›¸å…³æ€§åˆ†æ•°é˜ˆå€¼
            top_n: è¿”å›å‰Nä¸ªç»“æœ
            user: ç”¨æˆ·æ ‡è¯†
            
        Returns:
            RerankResult: é‡æ’åºç»“æœ
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°invokeæ–¹æ³•")

    def batch_rerank(
        self,
        model: str,
        credentials: dict[str, Any],
        queries: list[str],
        docs_list: list[list[str]],
        score_threshold: Optional[float] = None,
        top_n: Optional[int] = None,
        user: Optional[str] = None,
    ) -> list[RerankResult]:
        """
        æ‰¹é‡é‡æ’åº
        åŒæ—¶å¤„ç†å¤šä¸ªæŸ¥è¯¢çš„é‡æ’åºä»»åŠ¡
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            queries: æŸ¥è¯¢åˆ—è¡¨
            docs_list: å¯¹åº”çš„æ–‡æ¡£åˆ—è¡¨
            score_threshold: åˆ†æ•°é˜ˆå€¼
            top_n: è¿”å›æ•°é‡
            user: ç”¨æˆ·æ ‡è¯†
            
        Returns:
            list[RerankResult]: é‡æ’åºç»“æœåˆ—è¡¨
        """
        results = []
        
        for query, docs in zip(queries, docs_list):
            try:
                result = self.invoke(
                    model=model,
                    credentials=credentials,
                    query=query,
                    docs=docs,
                    score_threshold=score_threshold,
                    top_n=top_n,
                    user=user
                )
                results.append(result)
                
            except Exception as e:
                logger.warning(f"é‡æ’åºå¤±è´¥ - æŸ¥è¯¢: {query[:50]}..., é”™è¯¯: {e}")
                # è¿”å›ç©ºç»“æœä½œä¸ºå¤‡é€‰
                results.append(RerankResult(docs=[]))
        
        return results

class TTSModel(AIModel):
    """
    æ–‡å­—è½¬è¯­éŸ³æ¨¡å‹åŸºç±»
    æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆçš„é€šç”¨æ¥å£
    """
    
    model_type: ModelType = ModelType.TTS

    @abstractmethod
    def invoke(
        self,
        model: str,
        credentials: dict[str, Any],
        content_text: str,
        user: Optional[str] = None,
        tenant_id: Optional[str] = None,
        voice: Optional[str] = None,
    ) -> Iterable[bytes]:
        """
        è°ƒç”¨æ–‡å­—è½¬è¯­éŸ³æ¨¡å‹
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            content_text: å¾…åˆæˆçš„æ–‡æœ¬å†…å®¹
            user: ç”¨æˆ·æ ‡è¯†
            tenant_id: ç§Ÿæˆ·ID
            voice: è¯­éŸ³éŸ³è‰²
            
        Returns:
            Iterable[bytes]: éŸ³é¢‘æ•°æ®æµ
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°invokeæ–¹æ³•")

    @abstractmethod
    def get_tts_model_voices(
        self, 
        model: str, 
        credentials: dict[str, Any], 
        language: Optional[str] = None
    ) -> list[dict]:
        """
        è·å–TTSæ¨¡å‹æ”¯æŒçš„è¯­éŸ³åˆ—è¡¨
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            language: å¯é€‰çš„è¯­è¨€è¿‡æ»¤
            
        Returns:
            list[dict]: è¯­éŸ³åˆ—è¡¨ï¼ŒåŒ…å«éŸ³è‰²ä¿¡æ¯
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°get_tts_model_voicesæ–¹æ³•")

    def synthesize_long_text(
        self,
        model: str,
        credentials: dict[str, Any],
        content_text: str,
        voice: str,
        max_chunk_length: int = 1000,
        user: Optional[str] = None,
        tenant_id: Optional[str] = None,
    ) -> Iterable[bytes]:
        """
        é•¿æ–‡æœ¬è¯­éŸ³åˆæˆ
        å°†é•¿æ–‡æœ¬åˆ†å—è¿›è¡Œè¯­éŸ³åˆæˆï¼Œé¿å…é•¿åº¦é™åˆ¶
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            content_text: é•¿æ–‡æœ¬å†…å®¹
            voice: è¯­éŸ³éŸ³è‰²
            max_chunk_length: æœ€å¤§å—é•¿åº¦
            user: ç”¨æˆ·æ ‡è¯†
            tenant_id: ç§Ÿæˆ·ID
            
        Yields:
            bytes: éŸ³é¢‘æ•°æ®å—
        """
        # æ™ºèƒ½åˆ†å—ï¼Œé¿å…åœ¨å¥å­ä¸­é—´æ–­å¼€
        text_chunks = self._smart_split_text(content_text, max_chunk_length)
        
        for chunk in text_chunks:
            if chunk.strip():
                try:
                    # åˆæˆå½“å‰å—
                    audio_stream = self.invoke(
                        model=model,
                        credentials=credentials,
                        content_text=chunk,
                        user=user,
                        tenant_id=tenant_id,
                        voice=voice
                    )
                    
                    # è¾“å‡ºéŸ³é¢‘æ•°æ®
                    for audio_chunk in audio_stream:
                        yield audio_chunk
                        
                except Exception as e:
                    logger.warning(f"æ–‡æœ¬å—åˆæˆå¤±è´¥: {e}")
                    continue

    def _smart_split_text(self, text: str, max_length: int) -> list[str]:
        """
        æ™ºèƒ½æ–‡æœ¬åˆ†å—
        åœ¨å¥å­è¾¹ç•Œå¤„åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            max_length: æœ€å¤§å—é•¿åº¦
            
        Returns:
            list[str]: åˆ†å‰²åçš„æ–‡æœ¬å—
        """
        if len(text) <= max_length:
            return [text]
        
        chunks = []
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        
        current_chunk = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥æ·»åŠ åˆ°å½“å‰å—
            if len(current_chunk + sentence) <= max_length:
                current_chunk += sentence + "ã€‚"
            else:
                # å½“å‰å—å·²æ»¡ï¼Œå¼€å§‹æ–°å—
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence + "ã€‚"
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

class ModerationModel(AIModel):
    """
    å†…å®¹å®¡æ ¸æ¨¡å‹åŸºç±»
    æ£€æµ‹æ–‡æœ¬å†…å®¹çš„å®‰å…¨æ€§å’Œåˆè§„æ€§
    """
    
    model_type: ModelType = ModelType.MODERATION

    @abstractmethod
    def invoke(
        self,
        model: str,
        credentials: dict[str, Any],
        text: str,
        user: Optional[str] = None,
    ) -> bool:
        """
        è°ƒç”¨å†…å®¹å®¡æ ¸æ¨¡å‹
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            text: å¾…å®¡æ ¸çš„æ–‡æœ¬
            user: ç”¨æˆ·æ ‡è¯†
            
        Returns:
            bool: Trueè¡¨ç¤ºå†…å®¹æœ‰é—®é¢˜ï¼ŒFalseè¡¨ç¤ºå†…å®¹å®‰å…¨
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°invokeæ–¹æ³•")

    def batch_moderate(
        self,
        model: str,
        credentials: dict[str, Any],
        texts: list[str],
        user: Optional[str] = None,
    ) -> list[bool]:
        """
        æ‰¹é‡å†…å®¹å®¡æ ¸
        åŒæ—¶å®¡æ ¸å¤šä¸ªæ–‡æœ¬
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            texts: å¾…å®¡æ ¸çš„æ–‡æœ¬åˆ—è¡¨
            user: ç”¨æˆ·æ ‡è¯†
            
        Returns:
            list[bool]: æ¯ä¸ªæ–‡æœ¬çš„å®¡æ ¸ç»“æœ
        """
        results = []
        
        for text in texts:
            try:
                result = self.invoke(
                    model=model,
                    credentials=credentials,
                    text=text,
                    user=user
                )
                results.append(result)
                
            except Exception as e:
                logger.warning(f"æ–‡æœ¬å®¡æ ¸å¤±è´¥: {e}")
                # å‡ºé”™æ—¶æ ‡è®°ä¸ºæœ‰é—®é¢˜ï¼Œç¡®ä¿å®‰å…¨
                results.append(True)
        
        return results

    def get_moderation_details(
        self,
        model: str,
        credentials: dict[str, Any],
        text: str,
        user: Optional[str] = None,
    ) -> dict[str, Any]:
        """
        è·å–è¯¦ç»†çš„å®¡æ ¸ä¿¡æ¯
        è¿”å›å„ä¸ªç»´åº¦çš„å®¡æ ¸åˆ†æ•°å’ŒåŸå› 
        
        Args:
            model: æ¨¡å‹åç§°
            credentials: æ¨¡å‹å‡­æ®
            text: å¾…å®¡æ ¸æ–‡æœ¬
            user: ç”¨æˆ·æ ‡è¯†
            
        Returns:
            dict[str, Any]: è¯¦ç»†å®¡æ ¸ä¿¡æ¯
        """
        # åŸºç¡€å®ç°ï¼Œå­ç±»å¯ä»¥è¦†ç›–ä»¥æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯
        is_flagged = self.invoke(model, credentials, text, user)
        
        return {
            "flagged": is_flagged,
            "categories": {},  # å­ç±»åº”è¯¥æä¾›å…·ä½“çš„åˆ†ç±»ä¿¡æ¯
            "scores": {},      # å­ç±»åº”è¯¥æä¾›å…·ä½“çš„åˆ†æ•°ä¿¡æ¯
        }
```

### 5.3 æ¨¡å‹è·¯ç”±ä¸ç†”æ–­ç­–ç•¥

**å¤šç»´åº¦è·¯ç”±**ï¼šä»·æ ¼ã€å»¶è¿Ÿã€å¯é æ€§ã€é…é¢ã€åŒºåŸŸåˆè§„ã€‚

```python
class SmartModelRouter:
    def select(self, candidates: list[ProviderModel], context: dict) -> ProviderModel:
        # è¯„åˆ† = w1*æˆæœ¬ + w2*å»¶è¿Ÿ + w3*(1-é”™è¯¯ç‡) + w4*é…é¢å æ¯”
        for c in candidates:
            c.score = (
                0.3*c.cost_per_1k + 0.3*c.p95_latency + 0.3*(1-c.error_rate) + 0.1*c.quota_ratio
            )
        return min(candidates, key=lambda x: x.score)
```

**ç†”æ–­é™çº§**ï¼š
- å¿«é€Ÿå¤±è´¥ â†’ å¤‡ç”¨åŒæ¡£æ¨¡å‹ â†’ ä½æˆæœ¬æ¨¡å‹ â†’ è§„åˆ™å›ç­”
- æŒ‰æä¾›å•†ç»´åº¦å¥åº·åº¦è¯„åˆ†ï¼ˆæ»‘åŠ¨çª—å£é”™è¯¯ç‡/è¶…æ—¶ç‡ï¼‰

**æˆæœ¬/å»¶è¿Ÿå‚è€ƒçŸ©é˜µï¼ˆç¤ºä¾‹ï¼‰**ï¼š
- é«˜ç«¯ï¼šGPT-4 / Claude-Opusï¼ˆé«˜æˆæœ¬/é«˜è´¨é‡/ä¸­å»¶è¿Ÿï¼‰
- ä¸­ç«¯ï¼šClaude-Haiku / Qwen-Plusï¼ˆä¸­æˆæœ¬/ä¸­è´¨é‡/ä½å»¶è¿Ÿï¼‰
- ä½ç«¯ï¼šGPT-3.5 / Qwen-Turboï¼ˆä½æˆæœ¬/ä¸­è´¨é‡/ä½å»¶è¿Ÿï¼‰

## 6. é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶

### 6.1 æ™ºèƒ½é”™è¯¯å¤„ç†

```python
class ModelInvokeErrorHandler:
    """
    æ¨¡å‹è°ƒç”¨é”™è¯¯å¤„ç†å™¨
    æä¾›æ™ºèƒ½çš„é”™è¯¯åˆ†ç±»ã€é‡è¯•å’Œæ•…éšœè½¬ç§»æœºåˆ¶
    """
    
    # å¯é‡è¯•çš„é”™è¯¯ç±»å‹
    RETRYABLE_ERRORS = {
        InvokeRateLimitError: {"max_retries": 5, "backoff_factor": 2},
        InvokeConnectionError: {"max_retries": 3, "backoff_factor": 1.5},
        InvokeServerError: {"max_retries": 2, "backoff_factor": 1},
    }
    
    # ä¸å¯é‡è¯•çš„é”™è¯¯ç±»å‹
    NON_RETRYABLE_ERRORS = {
        InvokeAuthorizationError,  # è®¤è¯é”™è¯¯
        InvokeQuotaExceededError,  # é…é¢è¶…å‡º
        InvokeValidationError,     # å‚æ•°éªŒè¯é”™è¯¯
    }

    def __init__(self, model_instance: ModelInstance):
        """
        åˆå§‹åŒ–é”™è¯¯å¤„ç†å™¨
        
        Args:
            model_instance: æ¨¡å‹å®ä¾‹
        """
        self.model_instance = model_instance
        self.retry_counts = {}

    def handle_invoke_error(
        self,
        error: Exception,
        function: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        å¤„ç†æ¨¡å‹è°ƒç”¨é”™è¯¯
        æ ¹æ®é”™è¯¯ç±»å‹å†³å®šé‡è¯•ã€æ•…éšœè½¬ç§»æˆ–ç›´æ¥æŠ›å‡º
        
        Args:
            error: æ•è·çš„å¼‚å¸¸
            function: è°ƒç”¨çš„å‡½æ•°
            *args: å‡½æ•°å‚æ•°
            **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°
            
        Returns:
            Any: é‡è¯•æˆåŠŸçš„ç»“æœ
            
        Raises:
            Exception: æœ€ç»ˆå¤±è´¥æ—¶æŠ›å‡ºåŸå§‹å¼‚å¸¸
        """
        error_type = type(error)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¯é‡è¯•é”™è¯¯
        if error_type in self.NON_RETRYABLE_ERRORS:
            logger.error(f"ä¸å¯é‡è¯•é”™è¯¯: {error}")
            raise error

        if error_type not in self.RETRYABLE_ERRORS:
            logger.error(f"æœªçŸ¥é”™è¯¯ç±»å‹: {error}")
            raise error

        # è·å–é‡è¯•é…ç½®
        retry_config = self.RETRYABLE_ERRORS[error_type]
        max_retries = retry_config["max_retries"]
        backoff_factor = retry_config["backoff_factor"]

        # è·å–å½“å‰é‡è¯•æ¬¡æ•°
        error_key = f"{function.__name__}:{error_type.__name__}"
        current_retries = self.retry_counts.get(error_key, 0)

        if current_retries >= max_retries:
            logger.error(f"é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ {max_retries}: {error}")
            raise error

        # æ‰§è¡Œé‡è¯•
        self.retry_counts[error_key] = current_retries + 1
        
        # è®¡ç®—é€€é¿æ—¶é—´
        backoff_time = backoff_factor ** current_retries
        
        logger.info(
            f"é‡è¯•è°ƒç”¨ ({current_retries + 1}/{max_retries})ï¼Œ"
            f"ç­‰å¾… {backoff_time} ç§’: {error}"
        )
        
        time.sleep(backoff_time)
        
        try:
            # é‡æ–°æ‰§è¡Œè°ƒç”¨
            result = function(*args, **kwargs)
            
            # é‡è¯•æˆåŠŸï¼Œé‡ç½®è®¡æ•°
            if error_key in self.retry_counts:
                del self.retry_counts[error_key]
            
            return result
            
        except Exception as retry_error:
            # é‡è¯•ä»ç„¶å¤±è´¥ï¼Œé€’å½’å¤„ç†
            return self.handle_invoke_error(retry_error, function, *args, **kwargs)

class ModelHealthChecker:
    """
    æ¨¡å‹å¥åº·æ£€æŸ¥å™¨
    å®šæœŸæ£€æŸ¥æ¨¡å‹çš„å¯ç”¨æ€§å’Œæ€§èƒ½
    """
    
    def __init__(self, model_manager: ModelManager):
        """
        åˆå§‹åŒ–å¥åº·æ£€æŸ¥å™¨
        
        Args:
            model_manager: æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
        """
        self.model_manager = model_manager
        self.health_status = {}
        self.performance_metrics = {}

    def check_model_health(
        self,
        tenant_id: str,
        provider: str,
        model_type: ModelType,
        model: str
    ) -> dict[str, Any]:
        """
        æ£€æŸ¥å•ä¸ªæ¨¡å‹çš„å¥åº·çŠ¶å†µ
        
        Args:
            tenant_id: ç§Ÿæˆ·ID
            provider: æä¾›è€…
            model_type: æ¨¡å‹ç±»å‹
            model: æ¨¡å‹åç§°
            
        Returns:
            dict[str, Any]: å¥åº·çŠ¶å†µæŠ¥å‘Š
        """
        health_report = {
            "model_key": f"{provider}:{model_type.value}:{model}",
            "is_healthy": False,
            "response_time": None,
            "error": None,
            "last_check": time.time()
        }
        
        try:
            start_time = time.time()
            
            # è·å–æ¨¡å‹å®ä¾‹
            model_instance = self.model_manager.get_model_instance(
                tenant_id=tenant_id,
                provider=provider,
                model_type=model_type,
                model=model
            )
            
            # æ‰§è¡Œå¥åº·æ£€æŸ¥è°ƒç”¨
            if model_type == ModelType.LLM:
                self._check_llm_health(model_instance)
            elif model_type == ModelType.TEXT_EMBEDDING:
                self._check_embedding_health(model_instance)
            elif model_type == ModelType.RERANK:
                self._check_rerank_health(model_instance)
            # ... å…¶ä»–æ¨¡å‹ç±»å‹çš„å¥åº·æ£€æŸ¥
            
            response_time = time.time() - start_time
            
            health_report.update({
                "is_healthy": True,
                "response_time": response_time
            })
            
        except Exception as e:
            health_report.update({
                "is_healthy": False,
                "error": str(e)
            })
        
        # æ›´æ–°å¥åº·çŠ¶æ€ç¼“å­˜
        self.health_status[health_report["model_key"]] = health_report
        
        return health_report

    def _check_llm_health(self, model_instance: ModelInstance):
        """æ£€æŸ¥LLMæ¨¡å‹å¥åº·çŠ¶å†µ"""
        test_messages = [UserPromptMessage(content="Hello")]
        
        result = model_instance.invoke_llm(
            prompt_messages=test_messages,
            model_parameters={"max_tokens": 1, "temperature": 0},
            stream=False,
            user="health_check"
        )
        
        if not result or not result.message:
            raise Exception("LLMå¥åº·æ£€æŸ¥å¤±è´¥ï¼šæ— è¿”å›å†…å®¹")

    def _check_embedding_health(self, model_instance: ModelInstance):
        """æ£€æŸ¥åµŒå…¥æ¨¡å‹å¥åº·çŠ¶å†µ"""
        test_texts = ["å¥åº·æ£€æŸ¥æµ‹è¯•æ–‡æœ¬"]
        
        result = model_instance.invoke_text_embedding(
            texts=test_texts,
            user="health_check"
        )
        
        if not result or not result.embeddings:
            raise Exception("åµŒå…¥æ¨¡å‹å¥åº·æ£€æŸ¥å¤±è´¥ï¼šæ— è¿”å›å‘é‡")

    def _check_rerank_health(self, model_instance: ModelInstance):
        """æ£€æŸ¥é‡æ’åºæ¨¡å‹å¥åº·çŠ¶å†µ"""
        test_query = "æµ‹è¯•æŸ¥è¯¢"
        test_docs = ["æµ‹è¯•æ–‡æ¡£1", "æµ‹è¯•æ–‡æ¡£2"]
        
        result = model_instance.invoke_rerank(
            query=test_query,
            docs=test_docs,
            user="health_check"
        )
        
        if not result or not result.docs:
            raise Exception("é‡æ’åºæ¨¡å‹å¥åº·æ£€æŸ¥å¤±è´¥ï¼šæ— è¿”å›ç»“æœ")

    def get_overall_health_status(self) -> dict[str, Any]:
        """
        è·å–æ•´ä½“å¥åº·çŠ¶å†µ
        
        Returns:
            dict[str, Any]: æ•´ä½“å¥åº·çŠ¶å†µç»Ÿè®¡
        """
        total_models = len(self.health_status)
        healthy_models = sum(
            1 for status in self.health_status.values()
            if status["is_healthy"]
        )
        
        return {
            "total_models": total_models,
            "healthy_models": healthy_models,
            "unhealthy_models": total_models - healthy_models,
            "health_rate": healthy_models / total_models if total_models > 0 else 0,
            "last_check": max(
                (status["last_check"] for status in self.health_status.values()),
                default=0
            )
        }
```

## 7. æ€»ç»“

### 7.1 æ¨¡å‹è¿è¡Œæ—¶æ ¸å¿ƒç‰¹ç‚¹

Difyæ¨¡å‹è¿è¡Œæ—¶æ¨¡å—çš„è®¾è®¡ä½“ç°äº†ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **ç»Ÿä¸€æ¥å£**ï¼šä¸º6ç§æ¨¡å‹ç±»å‹æä¾›ä¸€è‡´çš„è°ƒç”¨æ¥å£
2. **å¤šæä¾›è€…æ”¯æŒ**ï¼šæ”¯æŒ40+ä¸ªä¸»æµAIæ¨¡å‹æä¾›è€…
3. **æ™ºèƒ½è´Ÿè½½å‡è¡¡**ï¼šè‡ªåŠ¨æ•…éšœè½¬ç§»å’Œè´Ÿè½½åˆ†å‘
4. **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„é‡è¯•æœºåˆ¶å’Œé”™è¯¯åˆ†ç±»
5. **æ€§èƒ½ç›‘æ§**ï¼šå…¨é¢çš„æ€§èƒ½æŒ‡æ ‡å’Œå¥åº·æ£€æŸ¥

### 7.2 æŠ€æœ¯ä¼˜åŠ¿

1. **æ¨ªå‘æ‰©å±•æ€§**ï¼šæ–°æä¾›è€…å’Œæ¨¡å‹ç±»å‹æ˜“äºæ·»åŠ 
2. **é«˜å¯ç”¨æ€§**ï¼šå¤šé‡æ•…éšœè½¬ç§»ä¿éšœæœåŠ¡ç¨³å®š
3. **æˆæœ¬ä¼˜åŒ–**ï¼šæ™ºèƒ½çš„ä»¤ç‰Œè®¡ç®—å’Œæˆæœ¬æ§åˆ¶
4. **æ’ä»¶ç”Ÿæ€**ï¼šæ”¯æŒç¬¬ä¸‰æ–¹æ’ä»¶æ‰©å±•
5. **ä¼ä¸šçº§ç‰¹æ€§**ï¼šå®Œæ•´çš„ç§Ÿæˆ·éš”ç¦»å’Œæƒé™ç®¡ç†

## 8. å…³é”®å‡½æ•°æ ¸å¿ƒä»£ç ä¸è¯´æ˜

ä»¥ä¸‹æ‘˜å½•èšç„¦ç»Ÿä¸€è°ƒç”¨ä¸è´Ÿè½½å‡è¡¡çš„å…³é”®è·¯å¾„ï¼Œä¾¿äºä¸æºç äº¤å‰å¯¹ç…§ã€‚

```python
class ModelManager:
    def get_model_instance(self, tenant_id: str, provider: str, model_type: ModelType, model: str) -> ModelInstance:
        """è¿”å›å°è£…å¥½å‡­æ®/ç±»å‹å®ä¾‹/è´Ÿè½½å‡è¡¡å™¨çš„ `ModelInstance`ã€‚æ‰¾ä¸åˆ°é…ç½®å°†æŠ›å‡ºè¯­ä¹‰åŒ–å¼‚å¸¸ã€‚"""

class ModelInstance:
    def invoke_llm(self, prompt_messages: Sequence[PromptMessage], model_parameters: dict | None = None,
                   tools: Sequence[PromptMessageTool] | None = None, stop: Sequence[str] | None = None,
                   stream: bool = True, user: str | None = None, callbacks: list[Callback] | None = None) -> Union[LLMResult, Generator]:
        """ç»Ÿä¸€çš„ LLM è°ƒç”¨å…¥å£ï¼›å†…éƒ¨é€šè¿‡ `_round_robin_invoke` è¿›è¡Œè´Ÿè½½å‡è¡¡ä¸é”™è¯¯å¤„ç†ã€‚"""

    def _round_robin_invoke(self, function: Callable[..., Any], *args, **kwargs):
        """è½®è¯¢å–ä¸‹ä¸€ä¸ªå¯ç”¨é…ç½®å¹¶è°ƒç”¨ï¼›å¯¹é™æµ/è®¤è¯/è¿æ¥ç­‰é”™è¯¯åšå†·å´ä¸é‡è¯•ï¼Œå…¶ä»–å¼‚å¸¸ç›´æŠ›ã€‚"""

class LBModelManager:
    def fetch_next(self) -> Optional[ModelLoadBalancingConfiguration]:
        """è¿”å›æœªå¤„äºå†·å´æœŸçš„ä¸‹ä¸€ä¸ªé…ç½®ï¼›åŸºäº Redis ç»´æŠ¤è½®è¯¢ç´¢å¼•ä¸å†·å´çŠ¶æ€ã€‚"""

class ModelProviderFactory:
    def get_provider_instance(self, provider: str, model_type: ModelType) -> BaseProvider:
        """æŒ‰ provider + model_type åŠ¨æ€åˆ›å»º/ç¼“å­˜ Provider å®ä¾‹ï¼›æ”¯æŒæ’ä»¶å‘½åç©ºé—´ã€‚"""
```

### 8.1 è¦ç‚¹è¯´æ˜

- ç»Ÿä¸€å…¥å£ï¼šä¸Šå±‚ä»…ä¾èµ– `ModelManager`/`ModelInstance`ï¼Œå±è”½ Provider ç»†èŠ‚ã€‚
- è´Ÿè½½å‡è¡¡ï¼š`_round_robin_invoke` ä¸ `LBModelManager` åä½œï¼Œæ”¯æŒå†·å´ä¸è½®è¯¢ã€‚
- é”™è¯¯æ²»ç†ï¼šåŒºåˆ†å¯é‡è¯•ï¼ˆ429/ç½‘ç»œï¼‰ä¸ä¸å¯é‡è¯•ï¼ˆè®¤è¯/å‚æ•°ï¼‰è·¯å¾„ã€‚
- å¯æ‹“å±•æ€§ï¼š`ModelProviderFactory` åŠ¨æ€åŠ è½½ä¸ç¼“å­˜ Provider å®ä¾‹ã€‚

## 9. å…³é”®å‡½æ•°è°ƒç”¨é“¾ï¼ˆæŒ‰èŒè´£ï¼‰

```text
è°ƒç”¨å‘èµ·(åº”ç”¨/æœåŠ¡)
  â†’ ModelManager.get_model_instance
    â†’ ProviderManager.get_provider_model_bundle
    â†’ ModelInstance.__init__(å‡­æ®æå– + LB åˆå§‹åŒ–)
  â†’ ModelInstance.invoke_* (llm/embedding/rerank/...)
    â†’ ModelInstance._round_robin_invoke
      â†’ LBModelManager.fetch_next â†’ åˆè§„æ£€æŸ¥/å†·å´åˆ¤æ–­
      â†’ BaseProvider(ModelType).invoke(..., credentials=...)
      â†’ æˆåŠŸè¿”å› æˆ– æ•è·é”™è¯¯ â†’ å†·å´/é‡è¯•/ç›´æŠ›
```

## 10. ç»Ÿä¸€æ—¶åºå›¾ï¼ˆç²¾ç®€ç‰ˆï¼‰

```mermaid
sequenceDiagram
    participant Caller as è°ƒç”¨æ–¹
    participant MM as ModelManager
    participant MI as ModelInstance
    participant LB as LBModelManager
    participant Prov as Provider(LLM/Embedding/...)

    Caller->>MM: get_model_instance(provider, model_type, model)
    MM-->>Caller: ModelInstance
    Caller->>MI: invoke_xxx(..., stream?)
    MI->>LB: fetch_next()
    LB-->>MI: é…ç½®(å«å‡­æ®/æƒé‡/å†·å´)
    MI->>Prov: invoke(credentials=...)
    alt æˆåŠŸ
        Prov-->>MI: ç»“æœ/Chunk
        MI-->>Caller: LLMResult/Generator
    else å¯é‡è¯•é”™è¯¯(429/ç½‘ç»œ)
        MI->>LB: cooldown(config)
        MI->>LB: fetch_next() é‡è¯•
    else ä¸å¯é‡è¯•(è®¤è¯/å‚æ•°)
        MI-->>Caller: å¼‚å¸¸
    end
```

## 11. å…³é”®ç»“æ„ä¸ç»§æ‰¿å…³ç³»ï¼ˆç±»å›¾ï¼‰

```mermaid
classDiagram
    class ModelManager
    class ModelInstance
    class LBModelManager
    class ModelProviderFactory
    class BaseProvider
    class LargeLanguageModel
    class TextEmbeddingModel
    class RerankModel
    class TTSModel
    class Speech2TextModel
    class ModerationModel

    ModelManager --> ModelInstance
    ModelInstance --> LBModelManager
    ModelInstance --> BaseProvider : uses
    ModelProviderFactory --> BaseProvider : create
    BaseProvider <|-- LargeLanguageModel
    BaseProvider <|-- TextEmbeddingModel
    BaseProvider <|-- RerankModel
    BaseProvider <|-- TTSModel
    BaseProvider <|-- Speech2TextModel
    BaseProvider <|-- ModerationModel
```
