---
title: "Kubernetes Controller Manager è¯¦ç»†æºç å‰–æ"
date: 2025-09-28T00:47:16+08:00
draft: false
tags: ['å®¹å™¨ç¼–æ’', 'DevOps', 'Go', 'æºç åˆ†æ', 'Kubernetes']
categories: ['å®¹å™¨ç¼–æ’']
description: "Kubernetes Controller Manager è¯¦ç»†æºç å‰–æçš„æ·±å…¥æŠ€æœ¯åˆ†ææ–‡æ¡£"
keywords: ['å®¹å™¨ç¼–æ’', 'DevOps', 'Go', 'æºç åˆ†æ', 'Kubernetes']
author: "æŠ€æœ¯åˆ†æå¸ˆ"
weight: 1
---

## ğŸ“š æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£æ·±å…¥åˆ†æ Kubernetes Controller Manager çš„æ¶æ„è®¾è®¡ã€æºç å®ç°å’Œæ§åˆ¶å™¨æ¨¡å¼ã€‚Controller Manager æ˜¯ Kubernetes é›†ç¾¤çš„"å¤§è„‘"ï¼Œè´Ÿè´£è¿è¡Œå„ç§æ§åˆ¶å™¨ï¼Œå®ç°å£°æ˜å¼ç®¡ç†å’Œè‡ªåŠ¨åŒ–è¿ç»´ã€‚

## ğŸ—ï¸ Controller Manager æ•´ä½“æ¶æ„

### 1.1 æ§åˆ¶å™¨ç®¡ç†å™¨æ¶æ„

```mermaid
graph TB
    subgraph "kube-controller-manager æ¶æ„"
        subgraph "ç®¡ç†å±‚ (Management Layer)"
            CM[Controller Manager]
            LE[Leader Election<br/>é¢†å¯¼é€‰ä¸¾]
            HM[Health Manager<br/>å¥åº·ç®¡ç†]
            MM[Metrics Manager<br/>æŒ‡æ ‡ç®¡ç†]
        end
        
        subgraph "æ ¸å¿ƒæ§åˆ¶å™¨ (Core Controllers)"
            RC[Replication Controller<br/>å‰¯æœ¬æ§åˆ¶å™¨]
            RS[ReplicaSet Controller<br/>å‰¯æœ¬é›†æ§åˆ¶å™¨]
            DC[Deployment Controller<br/>éƒ¨ç½²æ§åˆ¶å™¨]
            SC[Service Controller<br/>æœåŠ¡æ§åˆ¶å™¨]
            EC[Endpoint Controller<br/>ç«¯ç‚¹æ§åˆ¶å™¨]
            NC[Node Controller<br/>èŠ‚ç‚¹æ§åˆ¶å™¨]
        end
        
        subgraph "å·¥ä½œè´Ÿè½½æ§åˆ¶å™¨ (Workload Controllers)"
            JC[Job Controller<br/>ä»»åŠ¡æ§åˆ¶å™¨]
            CJC[CronJob Controller<br/>å®šæ—¶ä»»åŠ¡æ§åˆ¶å™¨]
            STC[StatefulSet Controller<br/>æœ‰çŠ¶æ€é›†æ§åˆ¶å™¨]
            DSC[DaemonSet Controller<br/>å®ˆæŠ¤è¿›ç¨‹é›†æ§åˆ¶å™¨]
        end
        
        subgraph "èµ„æºæ§åˆ¶å™¨ (Resource Controllers)"
            NSC[Namespace Controller<br/>å‘½åç©ºé—´æ§åˆ¶å™¨]
            SAC[ServiceAccount Controller<br/>æœåŠ¡è´¦æˆ·æ§åˆ¶å™¨]
            RQC[ResourceQuota Controller<br/>èµ„æºé…é¢æ§åˆ¶å™¨]
            GCC[GarbageCollector Controller<br/>åƒåœ¾æ”¶é›†æ§åˆ¶å™¨]
        end
        
        subgraph "åŸºç¡€è®¾æ–½ (Infrastructure)"
            INFORMER[Shared Informers<br/>å…±äº«é€šçŸ¥å™¨]
            QUEUE[Work Queues<br/>å·¥ä½œé˜Ÿåˆ—]
            CLIENT[Kubernetes Client<br/>K8s å®¢æˆ·ç«¯]
            CACHE[Local Cache<br/>æœ¬åœ°ç¼“å­˜]
        end
    end
    
    %% ç®¡ç†å…³ç³»
    CM --> LE
    CM --> HM
    CM --> MM
    
    %% æ§åˆ¶å™¨å…³ç³»
    CM --> RC
    CM --> RS
    CM --> DC
    CM --> SC
    CM --> EC
    CM --> NC
    CM --> JC
    CM --> CJC
    CM --> STC
    CM --> DSC
    CM --> NSC
    CM --> SAC
    CM --> RQC
    CM --> GCC
    
    %% åŸºç¡€è®¾æ–½å…³ç³»
    RC --> INFORMER
    RS --> INFORMER
    DC --> INFORMER
    SC --> INFORMER
    EC --> INFORMER
    NC --> INFORMER
    JC --> INFORMER
    CJC --> INFORMER
    STC --> INFORMER
    DSC --> INFORMER
    NSC --> INFORMER
    SAC --> INFORMER
    RQC --> INFORMER
    GCC --> INFORMER
    
    INFORMER --> QUEUE
    INFORMER --> CLIENT
    INFORMER --> CACHE
    
    %% æ ·å¼å®šä¹‰
    classDef management fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef core fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef workload fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef resource fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef infrastructure fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    
    class CM,LE,HM,MM management
    class RC,RS,DC,SC,EC,NC core
    class JC,CJC,STC,DSC workload
    class NSC,SAC,RQC,GCC resource
    class INFORMER,QUEUE,CLIENT,CACHE infrastructure
```

### 1.2 æ§åˆ¶å™¨æ¨¡å¼æ ¸å¿ƒåŸç†

```mermaid
graph LR
    subgraph "æ§åˆ¶å™¨æ¨¡å¼ (Controller Pattern)"
        subgraph "è§‚å¯Ÿé˜¶æ®µ (Observe)"
            WATCH[Watch API Server<br/>ç›‘å¬ API å˜åŒ–]
            CACHE_UPDATE[Update Local Cache<br/>æ›´æ–°æœ¬åœ°ç¼“å­˜]
            EVENT[Generate Events<br/>ç”Ÿæˆäº‹ä»¶]
        end
        
        subgraph "åˆ†æé˜¶æ®µ (Analyze)"
            CURRENT[Current State<br/>å½“å‰çŠ¶æ€]
            DESIRED[Desired State<br/>æœŸæœ›çŠ¶æ€]
            DIFF[Calculate Diff<br/>è®¡ç®—å·®å¼‚]
        end
        
        subgraph "æ‰§è¡Œé˜¶æ®µ (Act)"
            PLAN[Create Action Plan<br/>åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’]
            EXECUTE[Execute Actions<br/>æ‰§è¡Œæ“ä½œ]
            UPDATE[Update Status<br/>æ›´æ–°çŠ¶æ€]
        end
        
        subgraph "åé¦ˆé˜¶æ®µ (Feedback)"
            MONITOR[Monitor Results<br/>ç›‘æ§ç»“æœ]
            RETRY[Retry on Failure<br/>å¤±è´¥é‡è¯•]
            METRICS[Record Metrics<br/>è®°å½•æŒ‡æ ‡]
        end
    end
    
    %% æ§åˆ¶å¾ªç¯
    WATCH --> CACHE_UPDATE
    CACHE_UPDATE --> EVENT
    EVENT --> CURRENT
    CURRENT --> DESIRED
    DESIRED --> DIFF
    DIFF --> PLAN
    PLAN --> EXECUTE
    EXECUTE --> UPDATE
    UPDATE --> MONITOR
    MONITOR --> RETRY
    RETRY --> METRICS
    METRICS --> WATCH
    
    %% æ ·å¼å®šä¹‰
    classDef observe fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
    classDef analyze fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
    classDef act fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    classDef feedback fill:#fce4ec,stroke:#e91e63,stroke-width:2px
    
    class WATCH,CACHE_UPDATE,EVENT observe
    class CURRENT,DESIRED,DIFF analyze
    class PLAN,EXECUTE,UPDATE act
    class MONITOR,RETRY,METRICS feedback
```

## ğŸš€ å¯åŠ¨æµç¨‹è¯¦ç»†åˆ†æ

### 2.1 Controller Manager å¯åŠ¨å…¥å£

```go
// cmd/kube-controller-manager/controller-manager.go
/*
Controller Manager ä¸»å…¥å£æ–‡ä»¶
è´Ÿè´£åˆå§‹åŒ–å’Œå¯åŠ¨æ‰€æœ‰æ§åˆ¶å™¨

ä¸»è¦èŒè´£ï¼š
1. åˆ›å»º Cobra å‘½ä»¤å¯¹è±¡
2. è§£æå‘½ä»¤è¡Œå‚æ•°
3. å¯åŠ¨æ§åˆ¶å™¨ç®¡ç†å™¨
*/
package main

import (
    "os"
    _ "time/tzdata" // ä¸º CronJob æ—¶åŒºæ”¯æŒ

    "k8s.io/component-base/cli"
    _ "k8s.io/component-base/logs/json/register"          // JSON æ—¥å¿—æ ¼å¼æ³¨å†Œ
    _ "k8s.io/component-base/metrics/prometheus/clientgo" // Prometheus å®¢æˆ·ç«¯æ’ä»¶
    _ "k8s.io/component-base/metrics/prometheus/version"  // ç‰ˆæœ¬æŒ‡æ ‡æ³¨å†Œ
    "k8s.io/kubernetes/cmd/kube-controller-manager/app"
)

/*
main å‡½æ•°æ˜¯ Controller Manager çš„ç¨‹åºå…¥å£ç‚¹

æ‰§è¡Œæµç¨‹ï¼š
1. åˆ›å»ºæ§åˆ¶å™¨ç®¡ç†å™¨å‘½ä»¤å¯¹è±¡
2. é€šè¿‡ CLI æ¡†æ¶æ‰§è¡Œå‘½ä»¤
3. æ ¹æ®æ‰§è¡Œç»“æœé€€å‡ºç¨‹åº

è¿”å›å€¼ï¼š
- ç¨‹åºé€€å‡ºç ï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼Œé 0 è¡¨ç¤ºå¤±è´¥ï¼‰
*/
func main() {
    // åˆ›å»ºæ§åˆ¶å™¨ç®¡ç†å™¨å‘½ä»¤å¯¹è±¡
    command := app.NewControllerManagerCommand()
    
    // æ‰§è¡Œå‘½ä»¤ï¼Œå¯åŠ¨æ§åˆ¶å™¨ç®¡ç†å™¨
    code := cli.Run(command)
    
    // æ ¹æ®æ‰§è¡Œç»“æœé€€å‡ºç¨‹åº
    os.Exit(code)
}
```

### 2.2 å‘½ä»¤åˆ›å»ºå’Œé…ç½®

```go
// cmd/kube-controller-manager/app/controllermanager.go
/*
NewControllerManagerCommand åˆ›å»ºæ§åˆ¶å™¨ç®¡ç†å™¨çš„ Cobra å‘½ä»¤å¯¹è±¡

åŠŸèƒ½è¯´æ˜ï¼š
1. åˆå§‹åŒ–æ§åˆ¶å™¨ç®¡ç†å™¨é€‰é¡¹
2. è®¾ç½®å‘½ä»¤è¡Œå‚æ•°å’Œæ ‡å¿—
3. å®šä¹‰å‘½ä»¤æ‰§è¡Œé€»è¾‘
4. é…ç½®æ§åˆ¶å™¨å¯åŠ¨å‚æ•°

è¿”å›å€¼ï¼š
- *cobra.Command: é…ç½®å®Œæ•´çš„å‘½ä»¤å¯¹è±¡
*/
func NewControllerManagerCommand() *cobra.Command {
    // åˆ›å»ºæ§åˆ¶å™¨ç®¡ç†å™¨é€‰é¡¹
    s, err := options.NewKubeControllerManagerOptions()
    if err != nil {
        klog.Background().Error(err, "æ— æ³•åˆå§‹åŒ–å‘½ä»¤é€‰é¡¹")
        klog.FlushAndExit(klog.ExitFlushTimeout, 1)
    }

    // åˆ›å»º Cobra å‘½ä»¤å¯¹è±¡
    cmd := &cobra.Command{
        Use: kubeControllerManager,
        Long: `Kubernetes æ§åˆ¶å™¨ç®¡ç†å™¨æ˜¯ä¸€ä¸ªå®ˆæŠ¤è¿›ç¨‹ï¼Œå®ƒåµŒå…¥äº†
Kubernetes é™„å¸¦çš„æ ¸å¿ƒæ§åˆ¶å¾ªç¯ã€‚åœ¨æœºå™¨äººå’Œè‡ªåŠ¨åŒ–åº”ç”¨ä¸­ï¼Œ
æ§åˆ¶å¾ªç¯æ˜¯ä¸€ä¸ªè°ƒèŠ‚ç³»ç»ŸçŠ¶æ€çš„éç»ˆæ­¢å¾ªç¯ã€‚åœ¨ Kubernetes ä¸­ï¼Œ
æ§åˆ¶å™¨æ˜¯ä¸€ä¸ªæ§åˆ¶å¾ªç¯ï¼Œå®ƒé€šè¿‡ apiserver ç›‘è§†é›†ç¾¤çš„å…±äº«çŠ¶æ€ï¼Œ
å¹¶è¿›è¡Œæ›´æ”¹ï¼Œè¯•å›¾å°†å½“å‰çŠ¶æ€ç§»å‘æœŸæœ›çŠ¶æ€ã€‚
ä»Šå¤©éš Kubernetes ä¸€èµ·æä¾›çš„æ§åˆ¶å™¨ç¤ºä¾‹åŒ…æ‹¬å‰¯æœ¬æ§åˆ¶å™¨ã€
ç«¯ç‚¹æ§åˆ¶å™¨ã€å‘½åç©ºé—´æ§åˆ¶å™¨å’ŒæœåŠ¡è´¦æˆ·æ§åˆ¶å™¨ã€‚`,
        
        // æŒä¹…åŒ–é¢„è¿è¡Œé’©å­
        PersistentPreRunE: func(*cobra.Command, []string) error {
            // é™é»˜ client-go è­¦å‘Š
            // kube-controller-manager é€šç”¨åœ°ç›‘è§† APIï¼ˆåŒ…æ‹¬å·²å¼ƒç”¨çš„ APIï¼‰ï¼Œ
            // CI ç¡®ä¿å®ƒèƒ½å¤Ÿæ­£ç¡®åœ°ä¸åŒ¹é…çš„ kube-apiserver ç‰ˆæœ¬ä¸€èµ·å·¥ä½œ
            restclient.SetDefaultWarningHandler(restclient.NoWarnings{})
            
            // ç¡®ä¿åœ¨ RunE ä¹‹å‰è®¾ç½®ç‰¹æ€§é—¨æ§
            return s.ComponentGlobalsRegistry.Set()
        },
        
        // ä¸»å‘½ä»¤æ‰§è¡Œé€»è¾‘
        RunE: func(cmd *cobra.Command, args []string) error {
            // æ£€æŸ¥å¹¶æ‰“å°ç‰ˆæœ¬ä¿¡æ¯ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
            verflag.PrintAndExitIfRequested()

            // å°½æ—©æ¿€æ´»æ—¥å¿—è®°å½•ï¼Œç„¶åæ˜¾ç¤ºæœ€ç»ˆæ—¥å¿—é…ç½®çš„æ ‡å¿—
            if err := logsapi.ValidateAndApply(s.Logs, utilfeature.DefaultFeatureGate); err != nil {
                return err
            }
            cliflag.PrintFlags(cmd.Flags())

            // åˆ›å»ºä¸Šä¸‹æ–‡
            ctx := context.Background()
            
            // åˆ›å»ºé…ç½®
            c, err := s.Config(ctx, KnownControllers(), ControllersDisabledByDefault(), ControllerAliases())
            if err != nil {
                return err
            }

            // æ·»åŠ ç‰¹æ€§å¯ç”¨æŒ‡æ ‡
            fg := s.ComponentGlobalsRegistry.FeatureGateFor(basecompatibility.DefaultKubeComponent)
            fg.(featuregate.MutableFeatureGate).AddMetrics()
            
            // æ·»åŠ ç»„ä»¶ç‰ˆæœ¬æŒ‡æ ‡
            s.ComponentGlobalsRegistry.AddMetrics()
            
            // è¿è¡Œæ§åˆ¶å™¨ç®¡ç†å™¨
            return Run(ctx, c.Complete())
        },
        
        // å‚æ•°éªŒè¯å‡½æ•°
        Args: func(cmd *cobra.Command, args []string) error {
            for _, arg := range args {
                if len(arg) > 0 {
                    return fmt.Errorf("%q ä¸æ¥å—ä»»ä½•å‚æ•°ï¼Œå¾—åˆ° %q", cmd.CommandPath(), args)
                }
            }
            return nil
        },
    }

    // è®¾ç½®å‘½ä»¤æ ‡å¿—
    fs := cmd.Flags()
    namedFlagSets := s.Flags(KnownControllers(), ControllersDisabledByDefault(), ControllerAliases())
    
    // æ·»åŠ ç‰ˆæœ¬æ ‡å¿—
    verflag.AddFlags(namedFlagSets.FlagSet("global"))
    
    // æ·»åŠ å…¨å±€æ ‡å¿—
    globalflag.AddGlobalFlags(namedFlagSets.FlagSet("global"), cmd.Name(), logs.SkipLoggingConfigurationFlags())
    
    // å°†æ‰€æœ‰å‘½åæ ‡å¿—é›†æ·»åŠ åˆ°å‘½ä»¤ä¸­
    for _, f := range namedFlagSets.FlagSets {
        fs.AddFlagSet(f)
    }

    // è®¾ç½®ä½¿ç”¨è¯´æ˜å’Œå¸®åŠ©å‡½æ•°
    cols, _, _ := term.TerminalSize(cmd.OutOrStdout())
    cliflag.SetUsageAndHelpFunc(cmd, namedFlagSets, cols)

    return cmd
}
```

### 2.3 æ§åˆ¶å™¨ç®¡ç†å™¨è¿è¡Œæµç¨‹

```mermaid
sequenceDiagram
    participant Main as ä¸»ç¨‹åº
    participant Config as é…ç½®ç®¡ç†
    participant LE as é¢†å¯¼é€‰ä¸¾
    participant CM as æ§åˆ¶å™¨ç®¡ç†å™¨
    participant Controllers as å„ä¸ªæ§åˆ¶å™¨
    participant Informers as å…±äº«é€šçŸ¥å™¨
    participant API as API Server

    Note over Main,API: Controller Manager å¯åŠ¨æ—¶åºå›¾

    Main->>+Config: 1. åˆ›å»ºé…ç½®å¯¹è±¡
    Config->>Config: è§£æå‘½ä»¤è¡Œå‚æ•°
    Config->>Config: éªŒè¯é…ç½®æœ‰æ•ˆæ€§
    Config-->>-Main: è¿”å›å®Œæ•´é…ç½®

    Main->>+LE: 2. å¯åŠ¨é¢†å¯¼é€‰ä¸¾
    LE->>API: åˆ›å»ºæˆ–æ›´æ–°ç§Ÿçº¦å¯¹è±¡
    LE->>LE: ç«äº‰é¢†å¯¼æƒ
    LE-->>-Main: è·å¾—é¢†å¯¼æƒ

    Main->>+CM: 3. å¯åŠ¨æ§åˆ¶å™¨ç®¡ç†å™¨
    CM->>+Informers: 4. å¯åŠ¨å…±äº«é€šçŸ¥å™¨
    Informers->>API: å»ºç«‹ Watch è¿æ¥
    Informers->>Informers: åŒæ­¥æœ¬åœ°ç¼“å­˜
    Informers-->>-CM: é€šçŸ¥å™¨å°±ç»ª

    CM->>+Controllers: 5. å¯åŠ¨å„ä¸ªæ§åˆ¶å™¨
    
    par å¹¶è¡Œå¯åŠ¨æ§åˆ¶å™¨
        Controllers->>Controllers: å¯åŠ¨ ReplicaSet Controller
        Controllers->>Controllers: å¯åŠ¨ Deployment Controller
        Controllers->>Controllers: å¯åŠ¨ Service Controller
        Controllers->>Controllers: å¯åŠ¨ Node Controller
        Controllers->>Controllers: å¯åŠ¨å…¶ä»–æ§åˆ¶å™¨
    end
    
    Controllers-->>-CM: æ‰€æœ‰æ§åˆ¶å™¨å¯åŠ¨å®Œæˆ

    loop æ§åˆ¶å¾ªç¯
        API->>Informers: 6. å‘é€èµ„æºå˜æ›´äº‹ä»¶
        Informers->>Controllers: 7. é€šçŸ¥ç›¸å…³æ§åˆ¶å™¨
        Controllers->>Controllers: 8. å¤„ç†äº‹ä»¶
        Controllers->>API: 9. æ‰§è¡Œå¿…è¦çš„ API æ“ä½œ
        Controllers->>Controllers: 10. æ›´æ–°æœ¬åœ°çŠ¶æ€
    end

    Note over Main,API: æ§åˆ¶å™¨æŒç»­è¿è¡Œï¼Œå¤„ç†é›†ç¾¤çŠ¶æ€å˜åŒ–
```

```go
/*
Run è¿è¡Œæ§åˆ¶å™¨ç®¡ç†å™¨çš„ä¸»å‡½æ•°

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼Œç”¨äºæ§åˆ¶ç”Ÿå‘½å‘¨æœŸ
- c: å®Œæ•´çš„æ§åˆ¶å™¨ç®¡ç†å™¨é…ç½®

è¿”å›å€¼ï¼š
- error: è¿è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯

æ‰§è¡Œæµç¨‹ï¼š
1. è®¾ç½®å¥åº·æ£€æŸ¥å’ŒæŒ‡æ ‡æœåŠ¡
2. å¯åŠ¨å…±äº«é€šçŸ¥å™¨å·¥å‚
3. å¯åŠ¨å„ä¸ªæ§åˆ¶å™¨
4. è¿è¡Œé¢†å¯¼é€‰ä¸¾
5. ç­‰å¾…åœæ­¢ä¿¡å·
*/
func Run(ctx context.Context, c *config.CompletedConfig) error {
    // è®°å½•ç‰ˆæœ¬ä¿¡æ¯
    klog.InfoS("ç‰ˆæœ¬", "version", utilversion.Get())

    // 1. è®¾ç½®å¥åº·æ£€æŸ¥å’ŒæŒ‡æ ‡æœåŠ¡
    if c.ComponentConfig.Generic.LeaderElection.LeaderElect {
        // å¦‚æœå¯ç”¨é¢†å¯¼é€‰ä¸¾ï¼Œè®¾ç½®å¥åº·æ£€æŸ¥
        c.ComponentConfig.Generic.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{
            OnStartedLeading: func(ctx context.Context) {
                klog.InfoS("å¼€å§‹é¢†å¯¼")
                run(ctx, c)
            },
            OnStoppedLeading: func() {
                klog.ErrorS(nil, "é¢†å¯¼é€‰ä¸¾å¤±è´¥")
                klog.FlushAndExit(klog.ExitFlushTimeout, 1)
            },
        }
        
        // å¯åŠ¨é¢†å¯¼é€‰ä¸¾
        leaderElector, err := leaderelection.NewLeaderElector(*c.ComponentConfig.Generic.LeaderElection)
        if err != nil {
            return err
        }
        
        leaderElector.Run(ctx)
        return nil
    }

    // 2. å¦‚æœæœªå¯ç”¨é¢†å¯¼é€‰ä¸¾ï¼Œç›´æ¥è¿è¡Œ
    run(ctx, c)
    return nil
}

/*
run å®é™…è¿è¡Œæ§åˆ¶å™¨çš„å‡½æ•°

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- c: å®Œæ•´çš„æ§åˆ¶å™¨ç®¡ç†å™¨é…ç½®

å·¥ä½œæµç¨‹ï¼š
1. å¯åŠ¨ HTTP æœåŠ¡å™¨ï¼ˆå¥åº·æ£€æŸ¥ã€æŒ‡æ ‡ç­‰ï¼‰
2. å¯åŠ¨å…±äº«é€šçŸ¥å™¨å·¥å‚
3. å¯åŠ¨å„ä¸ªæ§åˆ¶å™¨
4. ç­‰å¾…åœæ­¢ä¿¡å·
*/
func run(ctx context.Context, c *config.CompletedConfig) {
    // 1. å¯åŠ¨ HTTP æœåŠ¡å™¨
    if c.SecureServing != nil {
        handler := buildHandlerChain(newBaseHandler(&c.ComponentConfig, c.VersionedClient), c.Authentication.Authenticator, c.Authorization.Authorizer)
        
        // å¯åŠ¨å®‰å…¨æœåŠ¡å™¨
        if _, _, err := c.SecureServing.Serve(handler, 0, ctx.Done()); err != nil {
            klog.ErrorS(err, "å¯åŠ¨å®‰å…¨æœåŠ¡å™¨å¤±è´¥")
            return
        }
    }

    // 2. å¯åŠ¨å…±äº«é€šçŸ¥å™¨å·¥å‚
    c.InformerFactory.Start(ctx.Done())
    c.ObjectOrMetadataInformerFactory.Start(ctx.Done())
    
    // ç­‰å¾…ç¼“å­˜åŒæ­¥
    c.InformerFactory.WaitForCacheSync(ctx.Done())
    c.ObjectOrMetadataInformerFactory.WaitForCacheSync(ctx.Done())

    // 3. å¯åŠ¨å„ä¸ªæ§åˆ¶å™¨
    controllerContext := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done())
    
    if err := StartControllers(ctx, controllerContext, NewControllerInitializers(), unsecuredMux, healthzHandler); err != nil {
        klog.ErrorS(err, "å¯åŠ¨æ§åˆ¶å™¨å¤±è´¥")
        return
    }

    // 4. ç­‰å¾…åœæ­¢ä¿¡å·
    <-ctx.Done()
    klog.InfoS("æ§åˆ¶å™¨ç®¡ç†å™¨æ­£åœ¨å…³é—­")
}
```

## ğŸ¯ æ ¸å¿ƒæ§åˆ¶å™¨è¯¦ç»†åˆ†æ

### 3.1 ReplicaSet æ§åˆ¶å™¨

#### 3.1.1 ReplicaSet æ§åˆ¶å™¨æ¶æ„

```mermaid
graph TB
    subgraph "ReplicaSet Controller æ¶æ„"
        subgraph "æ§åˆ¶å™¨æ ¸å¿ƒ (Controller Core)"
            RSC[ReplicaSet Controller<br/>å‰¯æœ¬é›†æ§åˆ¶å™¨]
            SYNC[Sync Handler<br/>åŒæ­¥å¤„ç†å™¨]
            QUEUE[Work Queue<br/>å·¥ä½œé˜Ÿåˆ—]
        end
        
        subgraph "èµ„æºç®¡ç† (Resource Management)"
            POD_CONTROL[Pod Control<br/>Pod æ§åˆ¶æ¥å£]
            RS_LISTER[ReplicaSet Lister<br/>å‰¯æœ¬é›†åˆ—è¡¨å™¨]
            POD_LISTER[Pod Lister<br/>Pod åˆ—è¡¨å™¨]
        end
        
        subgraph "äº‹ä»¶å¤„ç† (Event Handling)"
            RS_HANDLER[ReplicaSet Handler<br/>å‰¯æœ¬é›†äº‹ä»¶å¤„ç†å™¨]
            POD_HANDLER[Pod Handler<br/>Pod äº‹ä»¶å¤„ç†å™¨]
            EVENT_RECORDER[Event Recorder<br/>äº‹ä»¶è®°å½•å™¨]
        end
        
        subgraph "æ‰©ç¼©å®¹é€»è¾‘ (Scaling Logic)"
            SCALE_UP[Scale Up<br/>æ‰©å®¹é€»è¾‘]
            SCALE_DOWN[Scale Down<br/>ç¼©å®¹é€»è¾‘]
            POD_CREATION[Pod Creation<br/>Pod åˆ›å»º]
            POD_DELETION[Pod Deletion<br/>Pod åˆ é™¤]
        end
    end
    
    %% æ§åˆ¶æµ
    RSC --> SYNC
    SYNC --> QUEUE
    
    %% èµ„æºç®¡ç†
    RSC --> POD_CONTROL
    RSC --> RS_LISTER
    RSC --> POD_LISTER
    
    %% äº‹ä»¶å¤„ç†
    RS_HANDLER --> QUEUE
    POD_HANDLER --> QUEUE
    RSC --> EVENT_RECORDER
    
    %% æ‰©ç¼©å®¹é€»è¾‘
    SYNC --> SCALE_UP
    SYNC --> SCALE_DOWN
    SCALE_UP --> POD_CREATION
    SCALE_DOWN --> POD_DELETION
    POD_CREATION --> POD_CONTROL
    POD_DELETION --> POD_CONTROL
    
    %% æ ·å¼å®šä¹‰
    classDef core fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef resource fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef event fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef scaling fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    
    class RSC,SYNC,QUEUE core
    class POD_CONTROL,RS_LISTER,POD_LISTER resource
    class RS_HANDLER,POD_HANDLER,EVENT_RECORDER event
    class SCALE_UP,SCALE_DOWN,POD_CREATION,POD_DELETION scaling
```

#### 3.1.2 ReplicaSet æ§åˆ¶å™¨æºç å®ç°

```go
// pkg/controller/replicaset/replica_set.go
/*
ReplicaSetController è´Ÿè´£åŒæ­¥å­˜å‚¨åœ¨ç³»ç»Ÿä¸­çš„ ReplicaSet å¯¹è±¡ä¸å®é™…è¿è¡Œçš„ Pod

ä¸»è¦åŠŸèƒ½ï¼š
1. ç›‘æ§ ReplicaSet å’Œ Pod çš„å˜åŒ–
2. ç¡®ä¿å®é™…è¿è¡Œçš„ Pod æ•°é‡ä¸æœŸæœ›çš„å‰¯æœ¬æ•°ä¸€è‡´
3. å¤„ç† Pod çš„åˆ›å»ºå’Œåˆ é™¤
4. ç®¡ç† Pod çš„æ‰€æœ‰æƒå…³ç³»
*/

/*
ReplicaSetController ç»“æ„ä½“å®šä¹‰

å­—æ®µè¯´æ˜ï¼š
- GroupVersionKind: æ§åˆ¶å™¨ç±»å‹æ ‡è¯†
- kubeClient: Kubernetes å®¢æˆ·ç«¯
- podControl: Pod æ§åˆ¶æ¥å£
- burstReplicas: çªå‘å‰¯æœ¬æ•°é™åˆ¶
- syncHandler: åŒæ­¥å¤„ç†å‡½æ•°
- expectations: æœŸæœ›ç®¡ç†å™¨
- rsLister: ReplicaSet åˆ—è¡¨å™¨
- podLister: Pod åˆ—è¡¨å™¨
- queue: å·¥ä½œé˜Ÿåˆ—
*/
type ReplicaSetController struct {
    // GroupVersionKind è¡¨ç¤ºæ§åˆ¶å™¨ç±»å‹
    // æ­¤ç»“æ„çš„ä¸åŒå®ä¾‹å¯èƒ½å¤„ç†ä¸åŒçš„ GVK
    // ä¾‹å¦‚ï¼Œæ­¤ç»“æ„å¯ä»¥ï¼ˆé€šè¿‡é€‚é…å™¨ï¼‰ç”¨äºå¤„ç† ReplicationController
    schema.GroupVersionKind

    // Kubernetes å®¢æˆ·ç«¯
    kubeClient clientset.Interface
    
    // Pod æ§åˆ¶æ¥å£ï¼Œç”¨äºåˆ›å»ºå’Œåˆ é™¤ Pod
    podControl controller.PodControlInterface
    
    // podIndexer å…è®¸é€šè¿‡ ControllerRef UID æŸ¥æ‰¾ Pod
    podIndexer cache.Indexer
    
    // äº‹ä»¶å¹¿æ’­å™¨
    eventBroadcaster record.EventBroadcaster

    // ReplicaSet åœ¨åˆ›å»º/åˆ é™¤è¿™ä¹ˆå¤šå‰¯æœ¬åæš‚æ—¶æŒ‚èµ·
    // åœ¨è§‚å¯Ÿåˆ°å®ƒä»¬çš„ watch äº‹ä»¶åæ¢å¤æ­£å¸¸æ“ä½œ
    burstReplicas int
    
    // å…è®¸æ³¨å…¥ syncReplicaSet ç”¨äºæµ‹è¯•
    syncHandler func(ctx context.Context, rsKey string) error

    // ç”¨äºå•å…ƒæµ‹è¯•
    enqueueReplicaSet func(rs *apps.ReplicaSet)

    // rsLister å¯ä»¥ä»å…±äº«é€šçŸ¥å™¨çš„å­˜å‚¨ä¸­åˆ—å‡º/è·å– ReplicaSet
    rsLister appslisters.ReplicaSetLister
    
    // podLister å¯ä»¥ä»å…±äº«é€šçŸ¥å™¨çš„å­˜å‚¨ä¸­åˆ—å‡º/è·å– Pod
    podLister corelisters.PodLister

    // rsListerSynced å¦‚æœ ReplicaSet å­˜å‚¨è‡³å°‘åŒæ­¥è¿‡ä¸€æ¬¡åˆ™è¿”å› true
    rsListerSynced cache.InformerSynced
    
    // podListerSynced å¦‚æœ Pod å­˜å‚¨è‡³å°‘åŒæ­¥è¿‡ä¸€æ¬¡åˆ™è¿”å› true
    podListerSynced cache.InformerSynced

    // æœŸæœ›ç®¡ç†å™¨ï¼Œç”¨äºè·Ÿè¸ªæ§åˆ¶å™¨çš„æœŸæœ›çŠ¶æ€
    expectations *controller.UIDTrackingControllerExpectations

    // éœ€è¦åŒæ­¥çš„ ReplicaSet
    queue workqueue.TypedRateLimitingInterface[string]
}

/*
NewReplicaSetController åˆ›å»ºæ–°çš„ ReplicaSetController

å‚æ•°ï¼š
- rsInformer: ReplicaSet é€šçŸ¥å™¨
- podInformer: Pod é€šçŸ¥å™¨
- kubeClient: Kubernetes å®¢æˆ·ç«¯
- burstReplicas: çªå‘å‰¯æœ¬æ•°é™åˆ¶

è¿”å›å€¼ï¼š
- *ReplicaSetController: æ–°åˆ›å»ºçš„æ§åˆ¶å™¨å®ä¾‹

åˆå§‹åŒ–æµç¨‹ï¼š
1. åˆ›å»ºæ§åˆ¶å™¨å®ä¾‹
2. è®¾ç½®äº‹ä»¶å¤„ç†å™¨
3. é…ç½®å·¥ä½œé˜Ÿåˆ—
4. åˆå§‹åŒ–æœŸæœ›ç®¡ç†å™¨
*/
func NewReplicaSetController(rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int) *ReplicaSetController {
    // åˆ›å»ºäº‹ä»¶å¹¿æ’­å™¨
    eventBroadcaster := record.NewBroadcaster()
    eventBroadcaster.StartStructuredLogging(0)
    eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: kubeClient.CoreV1().Events("")})
    
    // å¦‚æœæŒ‡æ ‡æœªæ³¨å†Œï¼Œåˆ™æ³¨å†ŒæŒ‡æ ‡
    if kubeClient != nil && kubeClient.Discovery().RESTClient().GetRateLimiter() != nil {
        ratelimiter.RegisterMetricAndTrackRateLimiterUsage("replicaset_controller", kubeClient.Discovery().RESTClient().GetRateLimiter())
    }

    // åˆ›å»ºæ§åˆ¶å™¨å®ä¾‹
    rsc := &ReplicaSetController{
        GroupVersionKind: apps.SchemeGroupVersion.WithKind("ReplicaSet"),
        kubeClient:       kubeClient,
        podControl:       controller.RealPodControl{KubeClient: kubeClient, Recorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: "replicaset-controller"})},
        burstReplicas:    burstReplicas,
        expectations:     controller.NewUIDTrackingControllerExpectations(controller.NewControllerExpectations()),
        queue:            workqueue.NewTypedRateLimitingQueueWithConfig(workqueue.DefaultTypedControllerRateLimiter[string](), workqueue.TypedRateLimitingQueueConfig[string]{Name: "replicaset"}),
        eventBroadcaster: eventBroadcaster,
    }

    // è®¾ç½®åŒæ­¥å¤„ç†å‡½æ•°
    rsc.syncHandler = rsc.syncReplicaSet

    // è®¾ç½® ReplicaSet äº‹ä»¶å¤„ç†å™¨
    rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
        AddFunc: func(obj interface{}) {
            rsc.addRS(obj)
        },
        UpdateFunc: func(oldObj, newObj interface{}) {
            rsc.updateRS(oldObj, newObj)
        },
        DeleteFunc: func(obj interface{}) {
            rsc.deleteRS(obj)
        },
    })
    rsc.rsLister = rsInformer.Lister()
    rsc.rsListerSynced = rsInformer.Informer().HasSynced

    // è®¾ç½® Pod äº‹ä»¶å¤„ç†å™¨
    podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
        AddFunc: func(obj interface{}) {
            rsc.addPod(obj)
        },
        UpdateFunc: func(oldObj, newObj interface{}) {
            rsc.updatePod(oldObj, newObj)
        },
        DeleteFunc: func(obj interface{}) {
            rsc.deletePod(obj)
        },
    })
    rsc.podLister = podInformer.Lister()
    rsc.podListerSynced = podInformer.Informer().HasSynced

    return rsc
}

/*
Run å¼€å§‹è¿è¡Œæ§åˆ¶å™¨

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- workers: å·¥ä½œåç¨‹æ•°é‡

å·¥ä½œæµç¨‹ï¼š
1. ç­‰å¾…ç¼“å­˜åŒæ­¥
2. å¯åŠ¨å·¥ä½œåç¨‹
3. ç­‰å¾…åœæ­¢ä¿¡å·
4. æ¸…ç†èµ„æº
*/
func (rsc *ReplicaSetController) Run(ctx context.Context, workers int) {
    defer utilruntime.HandleCrash()
    defer rsc.queue.ShutDown()

    klog.InfoS("å¯åŠ¨æ§åˆ¶å™¨", "controller", "replicaset")
    defer klog.InfoS("å…³é—­æ§åˆ¶å™¨", "controller", "replicaset")

    // ç­‰å¾…ç¼“å­˜åŒæ­¥
    if !cache.WaitForNamedCacheSync("replicaset", ctx.Done(), rsc.rsListerSynced, rsc.podListerSynced) {
        return
    }

    // å¯åŠ¨å·¥ä½œåç¨‹
    for i := 0; i < workers; i++ {
        go wait.UntilWithContext(ctx, rsc.worker, time.Second)
    }

    <-ctx.Done()
}

/*
worker å·¥ä½œåç¨‹å‡½æ•°

å·¥ä½œæµç¨‹ï¼š
1. ä»é˜Ÿåˆ—è·å–å·¥ä½œé¡¹
2. å¤„ç†å·¥ä½œé¡¹
3. å¤„ç†ç»“æœï¼ˆé‡è¯•æˆ–å®Œæˆï¼‰
*/
func (rsc *ReplicaSetController) worker(ctx context.Context) {
    for rsc.processNextWorkItem(ctx) {
    }
}

func (rsc *ReplicaSetController) processNextWorkItem(ctx context.Context) bool {
    // ä»é˜Ÿåˆ—è·å–ä¸‹ä¸€ä¸ªå·¥ä½œé¡¹
    key, quit := rsc.queue.Get()
    if quit {
        return false
    }
    defer rsc.queue.Done(key)

    // å¤„ç†å·¥ä½œé¡¹
    err := rsc.syncHandler(ctx, key)
    if err == nil {
        // å¤„ç†æˆåŠŸï¼Œä»é˜Ÿåˆ—ä¸­ç§»é™¤
        rsc.queue.Forget(key)
        return true
    }

    // å¤„ç†å¤±è´¥ï¼Œé‡æ–°å…¥é˜Ÿ
    utilruntime.HandleError(fmt.Errorf("åŒæ­¥ %q æ—¶å‡ºé”™: %v", key, err))
    rsc.queue.AddRateLimited(key)

    return true
}

/*
syncReplicaSet åŒæ­¥ ReplicaSet çš„æ ¸å¿ƒé€»è¾‘

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- key: ReplicaSet çš„é”®ï¼ˆnamespace/nameï¼‰

è¿”å›å€¼ï¼š
- error: åŒæ­¥è¿‡ç¨‹ä¸­çš„é”™è¯¯

åŒæ­¥æµç¨‹ï¼š
1. è§£æé”®è·å–å‘½åç©ºé—´å’Œåç§°
2. è·å– ReplicaSet å¯¹è±¡
3. è·å–ç›¸å…³çš„ Pod åˆ—è¡¨
4. è®¡ç®—éœ€è¦åˆ›å»ºæˆ–åˆ é™¤çš„ Pod æ•°é‡
5. æ‰§è¡Œæ‰©ç¼©å®¹æ“ä½œ
6. æ›´æ–° ReplicaSet çŠ¶æ€
*/
func (rsc *ReplicaSetController) syncReplicaSet(ctx context.Context, key string) error {
    startTime := time.Now()
    defer func() {
        klog.V(4).InfoS("å®ŒæˆåŒæ­¥ ReplicaSet", "key", key, "duration", time.Since(startTime))
    }()

    // 1. è§£æé”®
    namespace, name, err := cache.SplitMetaNamespaceKey(key)
    if err != nil {
        return err
    }

    // 2. è·å– ReplicaSet å¯¹è±¡
    rs, err := rsc.rsLister.ReplicaSets(namespace).Get(name)
    if apierrors.IsNotFound(err) {
        klog.V(4).InfoS("ReplicaSet å·²è¢«åˆ é™¤", "key", key)
        rsc.expectations.DeleteExpectations(key)
        return nil
    }
    if err != nil {
        return err
    }

    // 3. æ£€æŸ¥æœŸæœ›çŠ¶æ€
    rsNeedsSync := rsc.expectations.SatisfiedExpectations(key)
    selector, err := metav1.LabelSelectorAsSelector(rs.Spec.Selector)
    if err != nil {
        utilruntime.HandleError(fmt.Errorf("å°† ReplicaSet %v é€‰æ‹©å™¨è½¬æ¢ä¸ºé€‰æ‹©å™¨æ—¶å‡ºé”™: %v", rs.Name, err))
        return nil
    }

    // 4. è·å–ç›¸å…³çš„ Pod åˆ—è¡¨
    allPods, err := rsc.podLister.Pods(rs.Namespace).List(labels.Everything())
    if err != nil {
        return err
    }
    
    // è¿‡æ»¤å‡ºåŒ¹é…çš„ Pod
    filteredPods := controller.FilterActivePods(allPods)
    filteredPods, err = rsc.claimPods(ctx, rs, selector, filteredPods)
    if err != nil {
        return err
    }

    var manageReplicasErr error
    if rsNeedsSync && rs.DeletionTimestamp == nil {
        // 5. æ‰§è¡Œå‰¯æœ¬ç®¡ç†
        manageReplicasErr = rsc.manageReplicas(ctx, filteredPods, rs)
    }

    // 6. æ›´æ–° ReplicaSet çŠ¶æ€
    rs = rs.DeepCopy()
    newStatus := calculateStatus(rs, filteredPods, manageReplicasErr)

    // å§‹ç»ˆæ›´æ–°çŠ¶æ€ï¼Œå› ä¸ºå³ä½¿åœ¨é”™è¯¯æƒ…å†µä¸‹ï¼ŒPod è®¡æ•°ä¹Ÿå¯èƒ½å·²æ›´æ”¹
    updatedRS, err := updateReplicaSetStatus(rsc.kubeClient.AppsV1().ReplicaSets(rs.Namespace), rs, newStatus)
    if err != nil {
        // å¤šæ¬¡é‡è¯•å¤±è´¥ä¸åº”è¯¥æˆä¸ºè‡´å‘½é”™è¯¯
        utilruntime.HandleError(fmt.Errorf("æ— æ³•æ›´æ–° ReplicaSet %s/%s çš„çŠ¶æ€: %v", rs.Namespace, rs.Name, err))
        return manageReplicasErr
    }

    // å¦‚æœçŠ¶æ€æ›´æ–°æˆåŠŸä½†å‰¯æœ¬ç®¡ç†å¤±è´¥ï¼Œé‡æ–°æ’é˜Ÿ
    if manageReplicasErr == nil && updatedRS.Spec.MinReadySeconds > 0 && updatedRS.Status.ReadyReplicas == *(updatedRS.Spec.Replicas) && updatedRS.Status.AvailableReplicas != *(updatedRS.Spec.Replicas) {
        rsc.enqueueReplicaSetAfter(updatedRS, time.Duration(updatedRS.Spec.MinReadySeconds)*time.Second)
    }
    return manageReplicasErr
}

/*
manageReplicas ç®¡ç†å‰¯æœ¬æ•°é‡çš„æ ¸å¿ƒé€»è¾‘

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- filteredPods: è¿‡æ»¤åçš„ Pod åˆ—è¡¨
- rs: ReplicaSet å¯¹è±¡

è¿”å›å€¼ï¼š
- error: ç®¡ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯

ç®¡ç†æµç¨‹ï¼š
1. è®¡ç®—å½“å‰å‰¯æœ¬æ•°å’ŒæœŸæœ›å‰¯æœ¬æ•°çš„å·®å¼‚
2. å¦‚æœéœ€è¦æ‰©å®¹ï¼Œåˆ›å»ºæ–°çš„ Pod
3. å¦‚æœéœ€è¦ç¼©å®¹ï¼Œåˆ é™¤å¤šä½™çš„ Pod
4. å¤„ç†çªå‘é™åˆ¶å’Œæ‰¹é‡æ“ä½œ
*/
func (rsc *ReplicaSetController) manageReplicas(ctx context.Context, filteredPods []*v1.Pod, rs *apps.ReplicaSet) error {
    // è®¡ç®—å·®å¼‚
    diff := len(filteredPods) - int(*(rs.Spec.Replicas))
    rsKey := controller.KeyFunc(rs)
    
    if diff < 0 {
        // éœ€è¦æ‰©å®¹
        diff *= -1
        if diff > rsc.burstReplicas {
            diff = rsc.burstReplicas
        }
        
        // è®¾ç½®æœŸæœ›
        rsc.expectations.ExpectCreations(rsKey, diff)
        
        klog.V(2).InfoS("åˆ›å»º Pod è¿‡å¤š", "replicaSet", klog.KObj(rs), "need", diff)
        
        // æ‰¹é‡åˆ›å»º Pod
        successfulCreations, err := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() error {
            err := rsc.podControl.CreatePods(ctx, rs.Namespace, &rs.Spec.Template, rs, metav1.NewControllerRef(rs, rsc.GroupVersionKind))
            if err != nil {
                if apierrors.HasStatusCause(err, v1.NamespaceTerminatingCause) {
                    // å¦‚æœå‘½åç©ºé—´æ­£åœ¨ç»ˆæ­¢ï¼Œä¸è¦é‡è¯•
                    return nil
                }
            }
            return err
        })
        
        // å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œè°ƒæ•´æœŸæœ›
        if skippedPods := diff - successfulCreations; skippedPods > 0 {
            klog.V(2).InfoS("æ…¢å¯åŠ¨å¤±è´¥ï¼Œè·³è¿‡åˆ›å»º Pod", "skippedPods", skippedPods, "replicaSet", klog.KObj(rs))
            for i := 0; i < skippedPods; i++ {
                rsc.expectations.CreationObserved(rsKey)
            }
        }
        return err
        
    } else if diff > 0 {
        // éœ€è¦ç¼©å®¹
        if diff > rsc.burstReplicas {
            diff = rsc.burstReplicas
        }
        
        klog.V(2).InfoS("åˆ é™¤ Pod è¿‡å¤š", "replicaSet", klog.KObj(rs), "need", diff)
        
        // é€‰æ‹©è¦åˆ é™¤çš„ Pod
        podsToDelete := getPodsToDelete(filteredPods, diff)
        
        // è®¾ç½®æœŸæœ›
        rsc.expectations.ExpectDeletions(rsKey, getPodKeys(podsToDelete))
        
        errCh := make(chan error, diff)
        var wg sync.WaitGroup
        wg.Add(diff)
        
        // å¹¶è¡Œåˆ é™¤ Pod
        for _, pod := range podsToDelete {
            go func(targetPod *v1.Pod) {
                defer wg.Done()
                if err := rsc.podControl.DeletePod(ctx, rs.Namespace, targetPod.Name, rs); err != nil {
                    // å‡å°‘æœŸæœ›è®¡æ•°
                    podKey := controller.PodKey(targetPod)
                    klog.V(2).InfoS("åˆ é™¤ Pod å¤±è´¥", "pod", podKey, "replicaSet", klog.KObj(rs), "error", err)
                    rsc.expectations.DeletionObserved(rsKey, podKey)
                    errCh <- err
                }
            }(pod)
        }
        wg.Wait()
        
        select {
        case err := <-errCh:
            // æ‰€æœ‰é”™è¯¯éƒ½å·²è®°å½•åœ¨ä¸Šé¢
            // ä¸è¦é‡æ–°è®°å½•ï¼Œåªè¿”å›ç¬¬ä¸€ä¸ªé”™è¯¯
            if err != nil {
                return err
            }
        default:
        }
    }

    return nil
}
```

### 3.2 Deployment æ§åˆ¶å™¨

#### 3.2.1 Deployment æ§åˆ¶å™¨æ¶æ„

```mermaid
graph TB
    subgraph "Deployment Controller æ¶æ„"
        subgraph "æ§åˆ¶å™¨æ ¸å¿ƒ (Controller Core)"
            DC[Deployment Controller<br/>éƒ¨ç½²æ§åˆ¶å™¨]
            SYNC[Sync Handler<br/>åŒæ­¥å¤„ç†å™¨]
            QUEUE[Work Queue<br/>å·¥ä½œé˜Ÿåˆ—]
        end
        
        subgraph "èµ„æºç®¡ç† (Resource Management)"
            RS_CONTROL[ReplicaSet Control<br/>å‰¯æœ¬é›†æ§åˆ¶æ¥å£]
            D_LISTER[Deployment Lister<br/>éƒ¨ç½²åˆ—è¡¨å™¨]
            RS_LISTER[ReplicaSet Lister<br/>å‰¯æœ¬é›†åˆ—è¡¨å™¨]
            POD_LISTER[Pod Lister<br/>Pod åˆ—è¡¨å™¨]
        end
        
        subgraph "éƒ¨ç½²ç­–ç•¥ (Deployment Strategies)"
            ROLLING[Rolling Update<br/>æ»šåŠ¨æ›´æ–°]
            RECREATE[Recreate<br/>é‡æ–°åˆ›å»º]
            ROLLBACK[Rollback<br/>å›æ»š]
            PAUSE[Pause/Resume<br/>æš‚åœ/æ¢å¤]
        end
        
        subgraph "çŠ¶æ€ç®¡ç† (Status Management)"
            PROGRESS[Progress Tracking<br/>è¿›åº¦è·Ÿè¸ª]
            CONDITIONS[Condition Management<br/>æ¡ä»¶ç®¡ç†]
            REVISION[Revision Control<br/>ç‰ˆæœ¬æ§åˆ¶]
        end
    end
    
    %% æ§åˆ¶æµ
    DC --> SYNC
    SYNC --> QUEUE
    
    %% èµ„æºç®¡ç†
    DC --> RS_CONTROL
    DC --> D_LISTER
    DC --> RS_LISTER
    DC --> POD_LISTER
    
    %% éƒ¨ç½²ç­–ç•¥
    SYNC --> ROLLING
    SYNC --> RECREATE
    SYNC --> ROLLBACK
    SYNC --> PAUSE
    
    %% çŠ¶æ€ç®¡ç†
    SYNC --> PROGRESS
    SYNC --> CONDITIONS
    SYNC --> REVISION
    
    %% æ ·å¼å®šä¹‰
    classDef core fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef resource fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef strategy fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef status fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    
    class DC,SYNC,QUEUE core
    class RS_CONTROL,D_LISTER,RS_LISTER,POD_LISTER resource
    class ROLLING,RECREATE,ROLLBACK,PAUSE strategy
    class PROGRESS,CONDITIONS,REVISION status
```

#### 3.2.2 Deployment æ»šåŠ¨æ›´æ–°æµç¨‹

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant DC as Deployment Controller
    participant OldRS as æ—§ ReplicaSet
    participant NewRS as æ–° ReplicaSet
    participant Pods as Pod å®ä¾‹

    Note over User,Pods: Deployment æ»šåŠ¨æ›´æ–°æµç¨‹

    User->>+DC: 1. æ›´æ–° Deployment é•œåƒ
    DC->>DC: 2. æ£€æµ‹åˆ° Deployment å˜æ›´
    DC->>DC: 3. è®¡ç®—æ»šåŠ¨æ›´æ–°å‚æ•°
    
    DC->>+NewRS: 4. åˆ›å»ºæ–° ReplicaSet
    NewRS-->>-DC: æ–° ReplicaSet åˆ›å»ºæˆåŠŸ
    
    loop æ»šåŠ¨æ›´æ–°å¾ªç¯
        DC->>+NewRS: 5. å¢åŠ æ–° ReplicaSet å‰¯æœ¬æ•°
        NewRS->>+Pods: 6. åˆ›å»ºæ–° Pod
        Pods-->>-NewRS: æ–° Pod å¯åŠ¨æˆåŠŸ
        NewRS-->>-DC: æ–°å‰¯æœ¬å°±ç»ª
        
        DC->>DC: 7. æ£€æŸ¥æ–° Pod å°±ç»ªçŠ¶æ€
        
        alt æ–° Pod å°±ç»ª
            DC->>+OldRS: 8. å‡å°‘æ—§ ReplicaSet å‰¯æœ¬æ•°
            OldRS->>+Pods: 9. åˆ é™¤æ—§ Pod
            Pods-->>-OldRS: æ—§ Pod åˆ é™¤æˆåŠŸ
            OldRS-->>-DC: æ—§å‰¯æœ¬ç¼©å‡å®Œæˆ
        else æ–° Pod æœªå°±ç»ª
            DC->>DC: ç­‰å¾…æ–° Pod å°±ç»ª
        end
        
        DC->>DC: 10. æ£€æŸ¥æ›´æ–°è¿›åº¦
        
        alt æ›´æ–°å®Œæˆ
            DC->>DC: æ ‡è®° Deployment ä¸ºæˆåŠŸ
        else ç»§ç»­æ›´æ–°
            Note over DC: ç»§ç»­ä¸‹ä¸€è½®æ»šåŠ¨æ›´æ–°
        end
    end
    
    DC->>User: 11. æ»šåŠ¨æ›´æ–°å®Œæˆ
    
    Note over User,Pods: æ‰€æœ‰æ—§ Pod è¢«æ–° Pod æ›¿æ¢
```

#### 3.2.3 Deployment æ§åˆ¶å™¨æºç å®ç°

```go
// pkg/controller/deployment/deployment_controller.go
/*
DeploymentController è´Ÿè´£åŒæ­¥å­˜å‚¨åœ¨ç³»ç»Ÿä¸­çš„ Deployment å¯¹è±¡ä¸å®é™…è¿è¡Œçš„å‰¯æœ¬é›†å’Œ Pod

ä¸»è¦åŠŸèƒ½ï¼š
1. ç›‘æ§ Deploymentã€ReplicaSet å’Œ Pod çš„å˜åŒ–
2. å®ç°æ»šåŠ¨æ›´æ–°å’Œé‡æ–°åˆ›å»ºéƒ¨ç½²ç­–ç•¥
3. ç®¡ç†éƒ¨ç½²çš„å›æ»šå’Œæš‚åœ/æ¢å¤
4. è·Ÿè¸ªéƒ¨ç½²è¿›åº¦å’ŒçŠ¶æ€
*/

/*
DeploymentController ç»“æ„ä½“å®šä¹‰

å­—æ®µè¯´æ˜ï¼š
- rsControl: ç”¨äºé‡‡ç”¨/é‡Šæ”¾å‰¯æœ¬é›†çš„æ§åˆ¶æ¥å£
- client: Kubernetes å®¢æˆ·ç«¯
- eventBroadcaster: äº‹ä»¶å¹¿æ’­å™¨
- syncHandler: åŒæ­¥å¤„ç†å‡½æ•°
- dLister: Deployment åˆ—è¡¨å™¨
- rsLister: ReplicaSet åˆ—è¡¨å™¨
- podLister: Pod åˆ—è¡¨å™¨
- queue: å·¥ä½œé˜Ÿåˆ—
*/
type DeploymentController struct {
    // rsControl ç”¨äºé‡‡ç”¨/é‡Šæ”¾å‰¯æœ¬é›†
    rsControl controller.RSControlInterface
    client    clientset.Interface

    // äº‹ä»¶å¹¿æ’­å™¨å’Œè®°å½•å™¨
    eventBroadcaster record.EventBroadcaster
    eventRecorder    record.EventRecorder

    // å…è®¸æ³¨å…¥ syncDeployment ç”¨äºæµ‹è¯•
    syncHandler func(ctx context.Context, dKey string) error
    
    // ç”¨äºå•å…ƒæµ‹è¯•
    enqueueDeployment func(deployment *apps.Deployment)

    // dLister å¯ä»¥ä»å…±äº«é€šçŸ¥å™¨çš„å­˜å‚¨ä¸­åˆ—å‡º/è·å–éƒ¨ç½²
    dLister appslisters.DeploymentLister
    
    // rsLister å¯ä»¥ä»å…±äº«é€šçŸ¥å™¨çš„å­˜å‚¨ä¸­åˆ—å‡º/è·å–å‰¯æœ¬é›†
    rsLister appslisters.ReplicaSetLister
    
    // podLister å¯ä»¥ä»å…±äº«é€šçŸ¥å™¨çš„å­˜å‚¨ä¸­åˆ—å‡º/è·å– Pod
    podLister corelisters.PodLister

    // dListerSynced å¦‚æœ Deployment å­˜å‚¨è‡³å°‘åŒæ­¥è¿‡ä¸€æ¬¡åˆ™è¿”å› true
    dListerSynced cache.InformerSynced
    
    // rsListerSynced å¦‚æœ ReplicaSet å­˜å‚¨è‡³å°‘åŒæ­¥è¿‡ä¸€æ¬¡åˆ™è¿”å› true
    rsListerSynced cache.InformerSynced
    
    // podListerSynced å¦‚æœ Pod å­˜å‚¨è‡³å°‘åŒæ­¥è¿‡ä¸€æ¬¡åˆ™è¿”å› true
    podListerSynced cache.InformerSynced

    // éœ€è¦åŒæ­¥çš„ Deployment
    queue workqueue.TypedRateLimitingInterface[string]
}

/*
NewDeploymentController åˆ›å»ºæ–°çš„ DeploymentController

å‚æ•°ï¼š
- dInformer: Deployment é€šçŸ¥å™¨
- rsInformer: ReplicaSet é€šçŸ¥å™¨
- podInformer: Pod é€šçŸ¥å™¨
- client: Kubernetes å®¢æˆ·ç«¯

è¿”å›å€¼ï¼š
- *DeploymentController: æ–°åˆ›å»ºçš„æ§åˆ¶å™¨å®ä¾‹

åˆå§‹åŒ–æµç¨‹ï¼š
1. åˆ›å»ºæ§åˆ¶å™¨å®ä¾‹
2. è®¾ç½®äº‹ä»¶å¤„ç†å™¨
3. é…ç½®å·¥ä½œé˜Ÿåˆ—
4. åˆå§‹åŒ–å„ç§åˆ—è¡¨å™¨
*/
func NewDeploymentController(dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) {
    // åˆ›å»ºäº‹ä»¶å¹¿æ’­å™¨
    eventBroadcaster := record.NewBroadcaster()
    eventBroadcaster.StartStructuredLogging(0)
    eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: client.CoreV1().Events("")})

    // å¦‚æœæŒ‡æ ‡æœªæ³¨å†Œï¼Œåˆ™æ³¨å†ŒæŒ‡æ ‡
    if client != nil && client.Discovery().RESTClient().GetRateLimiter() != nil {
        ratelimiter.RegisterMetricAndTrackRateLimiterUsage("deployment_controller", client.Discovery().RESTClient().GetRateLimiter())
    }

    // åˆ›å»ºæ§åˆ¶å™¨å®ä¾‹
    dc := &DeploymentController{
        client:           client,
        eventBroadcaster: eventBroadcaster,
        eventRecorder:    eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: "deployment-controller"}),
        queue:            workqueue.NewTypedRateLimitingQueueWithConfig(workqueue.DefaultTypedControllerRateLimiter[string](), workqueue.TypedRateLimitingQueueConfig[string]{Name: "deployment"}),
    }
    
    // è®¾ç½®åŒæ­¥å¤„ç†å‡½æ•°
    dc.syncHandler = dc.syncDeployment
    dc.enqueueDeployment = dc.enqueue

    // è®¾ç½® Deployment äº‹ä»¶å¤„ç†å™¨
    dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
        AddFunc: func(obj interface{}) {
            dc.addDeployment(obj)
        },
        UpdateFunc: func(oldObj, newObj interface{}) {
            dc.updateDeployment(oldObj, newObj)
        },
        DeleteFunc: func(obj interface{}) {
            dc.deleteDeployment(obj)
        },
    })
    dc.dLister = dInformer.Lister()
    dc.dListerSynced = dInformer.Informer().HasSynced

    // è®¾ç½® ReplicaSet äº‹ä»¶å¤„ç†å™¨
    rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
        AddFunc: func(obj interface{}) {
            dc.addReplicaSet(obj)
        },
        UpdateFunc: func(oldObj, newObj interface{}) {
            dc.updateReplicaSet(oldObj, newObj)
        },
        DeleteFunc: func(obj interface{}) {
            dc.deleteReplicaSet(obj)
        },
    })
    dc.rsLister = rsInformer.Lister()
    dc.rsListerSynced = rsInformer.Informer().HasSynced

    // è®¾ç½® Pod äº‹ä»¶å¤„ç†å™¨
    podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
        DeleteFunc: func(obj interface{}) {
            dc.deletePod(obj)
        },
    })
    dc.podLister = podInformer.Lister()
    dc.podListerSynced = podInformer.Informer().HasSynced

    // åˆ›å»ºå‰¯æœ¬é›†æ§åˆ¶æ¥å£
    dc.rsControl = controller.RealRSControl{
        KubeClient: client,
        Recorder:   dc.eventRecorder,
    }

    return dc, nil
}

/*
syncDeployment åŒæ­¥ Deployment çš„æ ¸å¿ƒé€»è¾‘

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- key: Deployment çš„é”®ï¼ˆnamespace/nameï¼‰

è¿”å›å€¼ï¼š
- error: åŒæ­¥è¿‡ç¨‹ä¸­çš„é”™è¯¯

åŒæ­¥æµç¨‹ï¼š
1. è§£æé”®è·å–å‘½åç©ºé—´å’Œåç§°
2. è·å– Deployment å¯¹è±¡
3. è·å–ç›¸å…³çš„ ReplicaSet åˆ—è¡¨
4. æ ¹æ®éƒ¨ç½²ç­–ç•¥æ‰§è¡Œç›¸åº”æ“ä½œ
5. æ›´æ–° Deployment çŠ¶æ€
*/
func (dc *DeploymentController) syncDeployment(ctx context.Context, key string) error {
    startTime := time.Now()
    defer func() {
        klog.V(4).InfoS("å®ŒæˆåŒæ­¥ Deployment", "key", key, "duration", time.Since(startTime))
    }()

    // 1. è§£æé”®
    namespace, name, err := cache.SplitMetaNamespaceKey(key)
    if err != nil {
        return err
    }

    // 2. è·å– Deployment å¯¹è±¡
    deployment, err := dc.dLister.Deployments(namespace).Get(name)
    if errors.IsNotFound(err) {
        klog.V(2).InfoS("Deployment å·²è¢«åˆ é™¤", "key", key)
        return nil
    }
    if err != nil {
        return err
    }

    // 3. æ·±æ‹·è´ Deployment ä»¥é¿å…ä¿®æ”¹ç¼“å­˜
    d := deployment.DeepCopy()

    // 4. è·å–æ‰€æœ‰ç›¸å…³çš„ ReplicaSet
    rsList, err := dc.getReplicaSetsForDeployment(ctx, d)
    if err != nil {
        return err
    }

    // 5. è·å–æ‰€æœ‰ç›¸å…³çš„ Pod
    podMap, err := dc.getPodMapForDeployment(d, rsList)
    if err != nil {
        return err
    }

    // 6. æ£€æŸ¥ Deployment æ˜¯å¦æš‚åœ
    if d.Spec.Paused {
        return dc.sync(ctx, d, rsList)
    }

    // 7. æ£€æŸ¥æ˜¯å¦éœ€è¦å›æ»š
    if getRollbackTo(d) != nil {
        return dc.rollback(ctx, d, rsList)
    }

    // 8. æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©ç¼©å®¹
    scalingEvent, err := dc.isScalingEvent(ctx, d, rsList)
    if err != nil {
        return err
    }
    if scalingEvent {
        return dc.sync(ctx, d, rsList)
    }

    // 9. æ ¹æ®éƒ¨ç½²ç­–ç•¥æ‰§è¡Œæ“ä½œ
    switch d.Spec.Strategy.Type {
    case apps.RecreateDeploymentStrategyType:
        return dc.rolloutRecreate(ctx, d, rsList, podMap)
    case apps.RollingUpdateDeploymentStrategyType:
        return dc.rolloutRolling(ctx, d, rsList)
    }
    return fmt.Errorf("æœªçŸ¥çš„éƒ¨ç½²ç­–ç•¥ç±»å‹: %s", d.Spec.Strategy.Type)
}

/*
rolloutRolling æ‰§è¡Œæ»šåŠ¨æ›´æ–°éƒ¨ç½²ç­–ç•¥

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- d: Deployment å¯¹è±¡
- rsList: ReplicaSet åˆ—è¡¨

è¿”å›å€¼ï¼š
- error: æ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯

æ»šåŠ¨æ›´æ–°æµç¨‹ï¼š
1. è·å–æ–°æ—§ ReplicaSet
2. è®¡ç®—æ»šåŠ¨æ›´æ–°å‚æ•°
3. æ‰§è¡Œæ»šåŠ¨æ›´æ–°é€»è¾‘
4. æ›´æ–°éƒ¨ç½²çŠ¶æ€
*/
func (dc *DeploymentController) rolloutRolling(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error {
    // 1. è·å–æ–° ReplicaSetï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true)
    if err != nil {
        return err
    }
    
    // 2. è·å–æ‰€æœ‰ç›¸å…³çš„ Pod
    allRSs := append(oldRSs, newRS)
    podMap, err := dc.getPodMapForDeployment(d, allRSs)
    if err != nil {
        return err
    }

    // 3. å¦‚æœåªæœ‰ä¸€ä¸ªå‰¯æœ¬é›†ä¸”æ˜¯æ–°çš„ï¼Œç›´æ¥æ‰©å®¹
    if len(oldRSs) == 0 {
        return dc.scaleUpNewReplicaSetForRollingUpdate(ctx, newRS, d)
    }

    // 4. æ‰§è¡Œæ»šåŠ¨æ›´æ–°é€»è¾‘
    if util.DeploymentComplete(d, &d.Status) {
        // éƒ¨ç½²å·²å®Œæˆï¼Œæ¸…ç†æ—§çš„å‰¯æœ¬é›†
        return dc.cleanupDeployment(ctx, oldRSs, d)
    }

    // 5. è®¡ç®—æ»šåŠ¨æ›´æ–°å‚æ•°
    maxUnavailable := deploymentutil.MaxUnavailable(*d)
    maxSurge := deploymentutil.MaxSurge(*d)
    
    // 6. è·å–å½“å‰å¯ç”¨çš„å‰¯æœ¬æ•°
    availablePodCount := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs)
    totalReplicaCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs)
    
    // 7. è®¡ç®—å…è®¸çš„ä¸å¯ç”¨å‰¯æœ¬æ•°
    allowedUnavailable := int32(math.Max(0, float64(*(d.Spec.Replicas)-maxUnavailable)))
    
    // 8. å¦‚æœå½“å‰å¯ç”¨å‰¯æœ¬æ•°å°äºå…è®¸çš„æœ€å°å€¼ï¼Œéœ€è¦æ‰©å®¹æ–°å‰¯æœ¬é›†
    if availablePodCount < allowedUnavailable {
        return dc.scaleUpNewReplicaSetForRollingUpdate(ctx, newRS, d)
    }
    
    // 9. è®¡ç®—å…è®¸çš„æœ€å¤§å‰¯æœ¬æ•°
    allowedSurge := *(d.Spec.Replicas) + maxSurge
    
    // 10. å¦‚æœæ€»å‰¯æœ¬æ•°è¶…è¿‡å…è®¸çš„æœ€å¤§å€¼ï¼Œéœ€è¦ç¼©å®¹æ—§å‰¯æœ¬é›†
    if totalReplicaCount > allowedSurge {
        return dc.scaleDownOldReplicaSetsForRollingUpdate(ctx, oldRSs, d)
    }
    
    // 11. åŒæ—¶è¿›è¡Œæ‰©å®¹å’Œç¼©å®¹
    return dc.reconcileNewReplicaSet(ctx, allRSs, newRS, d)
}

/*
scaleUpNewReplicaSetForRollingUpdate ä¸ºæ»šåŠ¨æ›´æ–°æ‰©å®¹æ–°å‰¯æœ¬é›†

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- newRS: æ–° ReplicaSet
- deployment: Deployment å¯¹è±¡

è¿”å›å€¼ï¼š
- error: æ‰©å®¹è¿‡ç¨‹ä¸­çš„é”™è¯¯

æ‰©å®¹é€»è¾‘ï¼š
1. è®¡ç®—éœ€è¦æ‰©å®¹çš„å‰¯æœ¬æ•°
2. è€ƒè™‘æ»šåŠ¨æ›´æ–°å‚æ•°é™åˆ¶
3. æ‰§è¡Œæ‰©å®¹æ“ä½œ
4. è®°å½•äº‹ä»¶
*/
func (dc *DeploymentController) scaleUpNewReplicaSetForRollingUpdate(ctx context.Context, newRS *apps.ReplicaSet, deployment *apps.Deployment) error {
    if *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) {
        // æ–°å‰¯æœ¬é›†å·²ç»è¾¾åˆ°æœŸæœ›å‰¯æœ¬æ•°
        return nil
    }
    
    // è®¡ç®—æœ€å¤§å¯æ‰©å®¹çš„å‰¯æœ¬æ•°
    maxSurge := deploymentutil.MaxSurge(*deployment)
    currentPodCount := deploymentutil.GetReplicaCountForReplicaSets([]*apps.ReplicaSet{newRS})
    maxTotalPods := *(deployment.Spec.Replicas) + maxSurge
    
    // è®¡ç®—å¯ä»¥æ‰©å®¹çš„å‰¯æœ¬æ•°
    scaleUpCount := maxTotalPods - currentPodCount
    if scaleUpCount <= 0 {
        return nil
    }
    
    // ä¸èƒ½è¶…è¿‡æœŸæœ›çš„å‰¯æœ¬æ•°
    newReplicasCount := int32(math.Min(float64(*(newRS.Spec.Replicas)+scaleUpCount), float64(*(deployment.Spec.Replicas))))
    
    // æ‰§è¡Œæ‰©å®¹
    newRS, err := dc.scaleReplicaSetAndRecordEvent(ctx, newRS, newReplicasCount, deployment)
    if err != nil {
        return err
    }
    
    klog.V(4).InfoS("æ‰©å®¹æ–°å‰¯æœ¬é›†", "replicaSet", klog.KObj(newRS), "oldReplicas", *(newRS.Spec.Replicas), "newReplicas", newReplicasCount)
    return nil
}

/*
scaleDownOldReplicaSetsForRollingUpdate ä¸ºæ»šåŠ¨æ›´æ–°ç¼©å®¹æ—§å‰¯æœ¬é›†

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- oldRSs: æ—§ ReplicaSet åˆ—è¡¨
- deployment: Deployment å¯¹è±¡

è¿”å›å€¼ï¼š
- error: ç¼©å®¹è¿‡ç¨‹ä¸­çš„é”™è¯¯

ç¼©å®¹é€»è¾‘ï¼š
1. è®¡ç®—éœ€è¦ç¼©å®¹çš„æ€»å‰¯æœ¬æ•°
2. æŒ‰ä¼˜å…ˆçº§é€‰æ‹©è¦ç¼©å®¹çš„å‰¯æœ¬é›†
3. æ‰§è¡Œç¼©å®¹æ“ä½œ
4. è®°å½•äº‹ä»¶
*/
func (dc *DeploymentController) scaleDownOldReplicaSetsForRollingUpdate(ctx context.Context, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) error {
    // è®¡ç®—å½“å‰æ—§å‰¯æœ¬é›†çš„æ€»å‰¯æœ¬æ•°
    oldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs)
    if oldPodsCount == 0 {
        // æ²¡æœ‰æ—§å‰¯æœ¬éœ€è¦ç¼©å®¹
        return nil
    }
    
    // è®¡ç®—æœ€å¤§ä¸å¯ç”¨å‰¯æœ¬æ•°
    maxUnavailable := deploymentutil.MaxUnavailable(*deployment)
    minAvailable := *(deployment.Spec.Replicas) - maxUnavailable
    
    // è·å–å½“å‰å¯ç”¨å‰¯æœ¬æ•°
    newRS, err := deploymentutil.GetNewReplicaSet(deployment, dc.rsLister)
    if err != nil {
        return err
    }
    
    allRSs := append(oldRSs, newRS)
    availablePodCount := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs)
    
    // è®¡ç®—å¯ä»¥ç¼©å®¹çš„å‰¯æœ¬æ•°
    if availablePodCount <= minAvailable {
        // ä¸èƒ½å†ç¼©å®¹äº†ï¼Œå¦åˆ™ä¼šè¿åæœ€å°å¯ç”¨å‰¯æœ¬æ•°é™åˆ¶
        return nil
    }
    
    scaleDownCount := availablePodCount - minAvailable
    scaleDownCount = int32(math.Min(float64(scaleDownCount), float64(oldPodsCount)))
    
    // æŒ‰ä¼˜å…ˆçº§æ’åºæ—§å‰¯æœ¬é›†ï¼ˆå‰¯æœ¬æ•°å°‘çš„ä¼˜å…ˆç¼©å®¹ï¼‰
    sort.Sort(controller.ReplicaSetsByCreationTimestamp(oldRSs))
    
    // æ‰§è¡Œç¼©å®¹
    totalScaledDown := int32(0)
    for _, targetRS := range oldRSs {
        if totalScaledDown >= scaleDownCount {
            break
        }
        
        if *(targetRS.Spec.Replicas) == 0 {
            // å‰¯æœ¬é›†å·²ç»ç¼©å®¹åˆ° 0
            continue
        }
        
        // è®¡ç®—è¿™ä¸ªå‰¯æœ¬é›†éœ€è¦ç¼©å®¹çš„æ•°é‡
        scaleDownCountForRS := int32(math.Min(float64(*(targetRS.Spec.Replicas)), float64(scaleDownCount-totalScaledDown)))
        newReplicasCount := *(targetRS.Spec.Replicas) - scaleDownCountForRS
        
        // æ‰§è¡Œç¼©å®¹
        _, err := dc.scaleReplicaSetAndRecordEvent(ctx, targetRS, newReplicasCount, deployment)
        if err != nil {
            return err
        }
        
        totalScaledDown += scaleDownCountForRS
        klog.V(4).InfoS("ç¼©å®¹æ—§å‰¯æœ¬é›†", "replicaSet", klog.KObj(targetRS), "oldReplicas", *(targetRS.Spec.Replicas), "newReplicas", newReplicasCount)
    }
    
    return nil
}
```

## ğŸ“Š æ§åˆ¶å™¨ç›‘æ§å’ŒæŒ‡æ ‡

### 4.1 æ§åˆ¶å™¨æŒ‡æ ‡ä½“ç³»

```yaml
# Controller Manager ç›‘æ§é…ç½®
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: kube-controller-manager
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: kube-controller-manager
  endpoints:
  - port: https
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      serverName: kube-controller-manager
      insecureSkipVerify: false
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    interval: 30s
    path: /metrics
    
---
# æ§åˆ¶å™¨å…³é”®æŒ‡æ ‡å‘Šè­¦è§„åˆ™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kube-controller-manager-alerts
  namespace: kube-system
spec:
  groups:
  - name: kube-controller-manager.rules
    rules:
    # Controller Manager å¯ç”¨æ€§å‘Šè­¦
    - alert: KubeControllerManagerDown
      expr: up{job="kube-controller-manager"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes Controller Manager ä¸å¯ç”¨"
        description: "Controller Manager {{ $labels.instance }} å·²ç»å®•æœºè¶…è¿‡ 5 åˆ†é’Ÿ"
    
    # å·¥ä½œé˜Ÿåˆ—æ·±åº¦å‘Šè­¦
    - alert: KubeControllerManagerWorkQueueDepth
      expr: |
        workqueue_depth{job="kube-controller-manager"} > 100
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Controller Manager å·¥ä½œé˜Ÿåˆ—æ·±åº¦è¿‡é«˜"
        description: "Controller {{ $labels.name }} å·¥ä½œé˜Ÿåˆ—æ·±åº¦ä¸º {{ $value }}"
    
    # å·¥ä½œé˜Ÿåˆ—å»¶è¿Ÿå‘Šè­¦
    - alert: KubeControllerManagerWorkQueueLatency
      expr: |
        histogram_quantile(0.99, sum(rate(workqueue_queue_duration_seconds_bucket{job="kube-controller-manager"}[5m])) by (le, name)) > 60
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Controller Manager å·¥ä½œé˜Ÿåˆ—å»¶è¿Ÿè¿‡é«˜"
        description: "Controller {{ $labels.name }} å·¥ä½œé˜Ÿåˆ— 99% åˆ†ä½å»¶è¿Ÿä¸º {{ $value }} ç§’"
    
    # æ§åˆ¶å™¨åŒæ­¥å¤±è´¥å‘Šè­¦
    - alert: KubeControllerManagerSyncFailures
      expr: |
        increase(controller_runtime_reconcile_errors_total{job="kube-controller-manager"}[5m]) > 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Controller Manager åŒæ­¥å¤±è´¥æ¬¡æ•°è¿‡å¤š"
        description: "Controller {{ $labels.controller }} åœ¨è¿‡å» 5 åˆ†é’Ÿå†…åŒæ­¥å¤±è´¥ {{ $value }} æ¬¡"
    
    # é¢†å¯¼é€‰ä¸¾å¤±è´¥å‘Šè­¦
    - alert: KubeControllerManagerLeaderElectionFailure
      expr: |
        increase(leader_election_master_status{job="kube-controller-manager"}[5m]) == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Controller Manager é¢†å¯¼é€‰ä¸¾å¤±è´¥"
        description: "Controller Manager {{ $labels.instance }} å¤±å»é¢†å¯¼æƒ"
    
    # è¯ä¹¦è¿‡æœŸå‘Šè­¦
    - alert: KubeControllerManagerCertificateExpiration
      expr: |
        apiserver_client_certificate_expiration_seconds{job="kube-controller-manager"} < 7*24*60*60
      for: 0m
      labels:
        severity: warning
      annotations:
        summary: "Controller Manager å®¢æˆ·ç«¯è¯ä¹¦å³å°†è¿‡æœŸ"
        description: "Controller Manager å®¢æˆ·ç«¯è¯ä¹¦å°†åœ¨ 7 å¤©å†…è¿‡æœŸ"
```

### 4.2 æ€§èƒ½ä¼˜åŒ–é…ç½®

```yaml
# Controller Manager é«˜æ€§èƒ½é…ç½®
apiVersion: v1
kind: Pod
metadata:
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - name: kube-controller-manager
    image: k8s.gcr.io/kube-controller-manager:v1.29.0
    command:
    - kube-controller-manager
    
    # åŸºç¡€é…ç½®
    - --bind-address=0.0.0.0
    - --secure-port=10257
    - --port=0
    
    # é¢†å¯¼é€‰ä¸¾é…ç½®
    - --leader-elect=true
    - --leader-elect-lease-duration=15s
    - --leader-elect-renew-deadline=10s
    - --leader-elect-retry-period=2s
    
    # æ§åˆ¶å™¨é…ç½®
    - --controllers=*,bootstrapsigner,tokencleaner
    - --concurrent-deployment-syncs=5          # Deployment æ§åˆ¶å™¨å¹¶å‘æ•°
    - --concurrent-replicaset-syncs=5          # ReplicaSet æ§åˆ¶å™¨å¹¶å‘æ•°
    - --concurrent-service-syncs=1             # Service æ§åˆ¶å™¨å¹¶å‘æ•°
    - --concurrent-namespace-syncs=10          # Namespace æ§åˆ¶å™¨å¹¶å‘æ•°
    - --concurrent-gc-syncs=20                 # åƒåœ¾æ”¶é›†å™¨å¹¶å‘æ•°
    
    # èŠ‚ç‚¹æ§åˆ¶å™¨é…ç½®
    - --node-monitor-period=5s                 # èŠ‚ç‚¹ç›‘æ§å‘¨æœŸ
    - --node-monitor-grace-period=40s          # èŠ‚ç‚¹ç›‘æ§å®½é™æœŸ
    - --pod-eviction-timeout=5m0s              # Pod é©±é€è¶…æ—¶
    - --unhealthy-zone-threshold=0.55          # ä¸å¥åº·åŒºåŸŸé˜ˆå€¼
    - --large-cluster-size-threshold=50        # å¤§é›†ç¾¤å¤§å°é˜ˆå€¼
    - --secondary-node-eviction-rate=0.01      # æ¬¡è¦èŠ‚ç‚¹é©±é€é€Ÿç‡
    
    # èµ„æºé…é¢æ§åˆ¶å™¨é…ç½®
    - --concurrent-resource-quota-syncs=5      # èµ„æºé…é¢æ§åˆ¶å™¨å¹¶å‘æ•°
    
    # æœåŠ¡è´¦æˆ·æ§åˆ¶å™¨é…ç½®
    - --concurrent-serviceaccount-token-syncs=5 # æœåŠ¡è´¦æˆ·ä»¤ç‰Œæ§åˆ¶å™¨å¹¶å‘æ•°
    
    # åƒåœ¾æ”¶é›†å™¨é…ç½®
    - --enable-garbage-collector=true          # å¯ç”¨åƒåœ¾æ”¶é›†å™¨
    
    # æ°´å¹³ Pod è‡ªåŠ¨æ‰©ç¼©å™¨é…ç½®
    - --horizontal-pod-autoscaler-sync-period=15s              # HPA åŒæ­¥å‘¨æœŸ
    - --horizontal-pod-autoscaler-upscale-delay=3m0s           # HPA æ‰©å®¹å»¶è¿Ÿ
    - --horizontal-pod-autoscaler-downscale-delay=5m0s         # HPA ç¼©å®¹å»¶è¿Ÿ
    - --horizontal-pod-autoscaler-downscale-stabilization=5m0s # HPA ç¼©å®¹ç¨³å®šæœŸ
    - --horizontal-pod-autoscaler-cpu-initialization-period=5m0s # HPA CPU åˆå§‹åŒ–å‘¨æœŸ
    - --horizontal-pod-autoscaler-initial-readiness-delay=30s   # HPA åˆå§‹å°±ç»ªå»¶è¿Ÿ
    
    # æŒä¹…å·æ§åˆ¶å™¨é…ç½®
    - --pvclaimbinder-sync-period=15s          # PV ç»‘å®šå™¨åŒæ­¥å‘¨æœŸ
    
    # ç‰¹æ€§é—¨æ§
    - --feature-gates=RemoveSelfLink=false     # ä¿æŒå‘åå…¼å®¹
    
    # æ—¥å¿—é…ç½®
    - --v=2                                    # æ—¥å¿—çº§åˆ«
    - --logtostderr=true                       # è¾“å‡ºåˆ°æ ‡å‡†é”™è¯¯
    
    # è®¤è¯å’Œæˆæƒ
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    
    # TLS é…ç½®
    - --tls-cert-file=/etc/kubernetes/pki/kube-controller-manager.crt
    - --tls-private-key-file=/etc/kubernetes/pki/kube-controller-manager.key
    - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
    
    # èµ„æºé…ç½®
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 2Gi
    
    # å­˜å‚¨å·æŒ‚è½½
    volumeMounts:
    - name: ca-certs
      mountPath: /etc/ssl/certs
      readOnly: true
    - name: etc-ca-certificates
      mountPath: /etc/ca-certificates
      readOnly: true
    - name: k8s-certs
      mountPath: /etc/kubernetes/pki
      readOnly: true
    - name: kubeconfig
      mountPath: /etc/kubernetes/controller-manager.conf
      readOnly: true
      
  # ä¸»æœºç½‘ç»œæ¨¡å¼
  hostNetwork: true
  
  # ä¼˜å…ˆçº§ç±»
  priorityClassName: system-node-critical
  
  # å­˜å‚¨å·å®šä¹‰
  volumes:
  - name: ca-certs
    hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
  - name: etc-ca-certificates
    hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
  - name: k8s-certs
    hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
  - name: kubeconfig
    hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: File
```

## ğŸ“š æ€»ç»“

### æ ¸å¿ƒç‰¹æ€§æ€»ç»“

1. **æ§åˆ¶å™¨æ¨¡å¼**ï¼šå£°æ˜å¼ç®¡ç†å’Œè‡ªåŠ¨åŒ–åè°ƒ
2. **å¤šæ§åˆ¶å™¨æ¶æ„**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼ŒèŒè´£åˆ†ç¦»
3. **äº‹ä»¶é©±åŠ¨**ï¼šåŸºäº Watch æœºåˆ¶çš„å“åº”å¼å¤„ç†
4. **é«˜å¯ç”¨è®¾è®¡**ï¼šé¢†å¯¼é€‰ä¸¾å’Œæ•…éšœè½¬ç§»
5. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒè‡ªå®šä¹‰æ§åˆ¶å™¨å’Œæ’ä»¶

### æœ€ä½³å®è·µå»ºè®®

1. **æ€§èƒ½è°ƒä¼˜**ï¼šåˆç†é…ç½®å¹¶å‘æ•°å’ŒåŒæ­¥å‘¨æœŸ
2. **ç›‘æ§å®Œå–„**ï¼šå»ºç«‹å…¨é¢çš„æŒ‡æ ‡å’Œå‘Šè­¦ä½“ç³»
3. **èµ„æºç®¡ç†**ï¼šåˆç†è®¾ç½®èµ„æºè¯·æ±‚å’Œé™åˆ¶
4. **æ•…éšœå¤„ç†**ï¼šå®ç°ä¼˜é›…çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
5. **ç‰ˆæœ¬å…¼å®¹**ï¼šä¿æŒ API ç‰ˆæœ¬çš„å‘åå…¼å®¹æ€§

é€šè¿‡æ·±å…¥ç†è§£ Controller Manager çš„æ¶æ„å’Œå®ç°ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°è®¾è®¡å’Œå®ç°è‡ªå®šä¹‰æ§åˆ¶å™¨ï¼Œæ„å»ºç¨³å®šå¯é çš„ Kubernetes åº”ç”¨ç³»ç»Ÿã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´09æœˆ27æ—¥  
**é€‚ç”¨ç‰ˆæœ¬**: Kubernetes 1.29+
