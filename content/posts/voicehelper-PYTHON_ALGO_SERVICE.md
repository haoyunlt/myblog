# Pythonç®—æ³•æœåŠ¡è¯¦ç»†åˆ†æ

## ğŸ“‹ æ¨¡å—æ¦‚è¿°

Pythonç®—æ³•æœåŠ¡æ˜¯VoiceHelperç³»ç»Ÿçš„æ ¸å¿ƒAIå¼•æ“ï¼Œè´Ÿè´£RAGæ£€ç´¢ã€GraphRAGæ¨ç†ã€è¯­éŸ³å¤„ç†å’Œæ™ºèƒ½å¯¹è¯ç”Ÿæˆã€‚åŸºäºFastAPIå¼‚æ­¥æ¡†æ¶æ„å»ºï¼Œé›†æˆå¤šç§AIæ¨¡å‹å’Œå‘é‡æ•°æ®åº“ï¼Œæä¾›é«˜æ€§èƒ½çš„æ™ºèƒ½ç®—æ³•æœåŠ¡ã€‚

## ğŸ—ï¸ æœåŠ¡æ¶æ„å›¾

```mermaid
graph TB
    subgraph "Pythonç®—æ³•æœåŠ¡æ¶æ„"
        FASTAPI[FastAPIæœåŠ¡å™¨<br/>å¼‚æ­¥HTTP/WebSocket]
        
        subgraph "æ ¸å¿ƒæœåŠ¡å±‚"
            INGEST[æ–‡æ¡£å…¥åº“æœåŠ¡<br/>IngestService]
            RETRIEVE[æ£€ç´¢æœåŠ¡<br/>RetrieveService]
            VOICE[è¯­éŸ³æœåŠ¡<br/>VoiceService]
            ENHANCED[å¢å¼ºè¯­éŸ³æœåŠ¡<br/>EnhancedVoiceService]
        end
        
        subgraph "AIæ¨¡å‹å±‚"
            BGE[BGEå‘é‡æ¨¡å‹<br/>æ–‡æ¡£åµŒå…¥]
            WHISPER[Whisperæ¨¡å‹<br/>è¯­éŸ³è¯†åˆ«]
            TTS[Edge-TTS<br/>è¯­éŸ³åˆæˆ]
            LLM[å¤§è¯­è¨€æ¨¡å‹<br/>OpenAI/è±†åŒ…/GLM]
        end
        
        subgraph "æ•°æ®å­˜å‚¨å±‚"
            FAISS[FAISSç´¢å¼•<br/>å‘é‡æ£€ç´¢]
            NEO4J[Neo4jå›¾æ•°æ®åº“<br/>çŸ¥è¯†å›¾è°±]
            REDIS[Redisç¼“å­˜<br/>ä¼šè¯ç®¡ç†]
            MINIO[MinIOå­˜å‚¨<br/>æ–‡æ¡£æ–‡ä»¶]
        end
        
        subgraph "æ¨ç†å¼•æ“"
            GRAPHRAG[GraphRAGå¼•æ“<br/>å›¾æ¨ç†]
            REASONING[æ¨ç†æ¨¡å—<br/>å¤šç§æ¨ç†æ¨¡å¼]
            AGENT[Agentç³»ç»Ÿ<br/>å·¥å…·è°ƒç”¨]
        end
    end
    
    FASTAPI --> INGEST
    FASTAPI --> RETRIEVE
    FASTAPI --> VOICE
    FASTAPI --> ENHANCED
    
    RETRIEVE --> BGE
    RETRIEVE --> LLM
    RETRIEVE --> FAISS
    RETRIEVE --> NEO4J
    RETRIEVE --> GRAPHRAG
    
    VOICE --> WHISPER
    VOICE --> TTS
    ENHANCED --> WHISPER
    ENHANCED --> TTS
    
    INGEST --> BGE
    INGEST --> FAISS
    INGEST --> NEO4J
    INGEST --> MINIO
    
    GRAPHRAG --> REASONING
    GRAPHRAG --> AGENT
    
    style FASTAPI fill:#e3f2fd
    style RETRIEVE fill:#f3e5f5
    style GRAPHRAG fill:#e8f5e8
    style LLM fill:#fff3e0
```

## ğŸš€ æ ¸å¿ƒAPIè¯¦ç»†åˆ†æ

### 1. æ–‡æ¡£å…¥åº“API

#### å…¥å£å‡½æ•°è¯¦ç»†è§£æ

**æ–‡ä»¶ä½ç½®**: `algo/app/main.py:ingest_documents`

```python
@app.post("/ingest", response_model=IngestResponse)
async def ingest_documents(
    request: IngestRequest,
    background_tasks: BackgroundTasks,
    http_request: Request
):
    """
    æ–‡æ¡£å…¥åº“æ¥å£ - å¤„ç†æ–‡æ¡£ä¸Šä¼ ã€è§£æã€å‘é‡åŒ–å’Œç´¢å¼•æ„å»º
    
    Args:
        request (IngestRequest): å…¥åº“è¯·æ±‚å¯¹è±¡
            - files: List[IngestFile] æ–‡æ¡£æ–‡ä»¶åˆ—è¡¨
            - collection_name: str é›†åˆåç§°ï¼Œé»˜è®¤'default'
            - chunk_size: int åˆ†å—å¤§å°ï¼Œé»˜è®¤1000å­—ç¬¦
            - chunk_overlap: int åˆ†å—é‡å ï¼Œé»˜è®¤200å­—ç¬¦
            - metadata: dict è‡ªå®šä¹‰å…ƒæ•°æ®
            
        background_tasks (BackgroundTasks): FastAPIåå°ä»»åŠ¡ç®¡ç†å™¨
        http_request (Request): HTTPè¯·æ±‚å¯¹è±¡ï¼Œç”¨äºè·å–å®¢æˆ·ç«¯ä¿¡æ¯
    
    Returns:
        IngestResponse: å…¥åº“å“åº”å¯¹è±¡
            - task_id: str ä»»åŠ¡IDï¼Œç”¨äºæŸ¥è¯¢å¤„ç†çŠ¶æ€
            - status: str åˆå§‹çŠ¶æ€ 'pending'
            - message: str å“åº”æ¶ˆæ¯
    
    Raises:
        VoiceHelperError: è‡ªå®šä¹‰ä¸šåŠ¡å¼‚å¸¸
            - RAG_INVALID_QUERY: æ— æ•ˆçš„è¯·æ±‚å‚æ•°
            - RAG_INDEXING_FAILED: ç´¢å¼•æ„å»ºå¤±è´¥
    
    å¤„ç†æµç¨‹:
        1. å‚æ•°éªŒè¯ - æ£€æŸ¥æ–‡ä»¶åˆ—è¡¨å’Œæ ¼å¼
        2. ä»»åŠ¡åˆ›å»º - ç”Ÿæˆå”¯ä¸€ä»»åŠ¡ID
        3. åå°å¤„ç† - å¼‚æ­¥æ‰§è¡Œå…¥åº“æµç¨‹
        4. çŠ¶æ€å“åº” - è¿”å›ä»»åŠ¡IDä¾›å®¢æˆ·ç«¯æŸ¥è¯¢
    """
    start_time = time.time()
    
    # === ç¬¬ä¸€é˜¶æ®µï¼šè¯·æ±‚æ—¥å¿—å’ŒéªŒè¯ ===
    
    # 1.1 è®°å½•ä¸šåŠ¡æ—¥å¿—
    logger.business("æ–‡æ¡£å…¥åº“è¯·æ±‚", context={
        "files_count": len(request.files) if request.files else 0,
        "collection_name": getattr(request, 'collection_name', 'default'),
        "client_ip": http_request.client.host if http_request.client else "unknown",
    })
    
    try:
        # 1.2 éªŒè¯å¿…è¦å‚æ•°
        if not request.files or len(request.files) == 0:
            raise VoiceHelperError(
                ErrorCode.RAG_INVALID_QUERY, 
                "æ²¡æœ‰æä¾›æ–‡æ¡£æ–‡ä»¶"
            )
        
        # 1.3 éªŒè¯æ–‡ä»¶æ ¼å¼å’Œå¤§å°
        total_size = 0
        for file in request.files:
            if not file.filename or not file.content:
                raise VoiceHelperError(
                    ErrorCode.RAG_INVALID_QUERY,
                    f"æ–‡ä»¶ {file.filename} å†…å®¹ä¸ºç©º"
                )
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            allowed_types = ['.txt', '.pdf', '.docx', '.md', '.html']
            if not any(file.filename.lower().endswith(ext) for ext in allowed_types):
                raise VoiceHelperError(
                    ErrorCode.RAG_INVALID_QUERY,
                    f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.filename}"
                )
            
            total_size += len(file.content.encode('utf-8') if isinstance(file.content, str) else file.content)
        
        # 1.4 æ£€æŸ¥æ€»æ–‡ä»¶å¤§å°é™åˆ¶ (100MB)
        if total_size > 100 * 1024 * 1024:
            raise VoiceHelperError(
                ErrorCode.RAG_INVALID_QUERY,
                f"æ–‡ä»¶æ€»å¤§å°è¶…è¿‡é™åˆ¶: {total_size / 1024 / 1024:.2f}MB"
            )
        
        # === ç¬¬äºŒé˜¶æ®µï¼šä»»åŠ¡åˆ›å»ºå’Œè°ƒåº¦ ===
        
        # 2.1 ç”Ÿæˆå”¯ä¸€ä»»åŠ¡ID
        task_id = ingest_service.generate_task_id()
        
        # 2.2 è®°å½•ä»»åŠ¡åˆ›å»ºæ—¥å¿—
        logger.info(f"ç”Ÿæˆå…¥åº“ä»»åŠ¡ID: {task_id}", context={
            "task_id": task_id,
            "files_count": len(request.files),
            "total_size_mb": total_size / 1024 / 1024,
        })
        
        # 2.3 å°†å…¥åº“ä»»åŠ¡æ·»åŠ åˆ°åå°é˜Ÿåˆ—
        background_tasks.add_task(
            ingest_service.process_ingest_task,
            task_id,
            request
        )
        
        # === ç¬¬ä¸‰é˜¶æ®µï¼šæ€§èƒ½è®°å½•å’Œå“åº” ===
        
        # 3.1 è®°å½•æ€§èƒ½æŒ‡æ ‡
        duration_ms = (time.time() - start_time) * 1000
        logger.performance("æ–‡æ¡£å…¥åº“ä»»åŠ¡åˆ›å»º", duration_ms, context={
            "task_id": task_id,
            "files_count": len(request.files),
        })
        
        # 3.2 è¿”å›ä»»åŠ¡å“åº”
        return IngestResponse(
            task_id=task_id,
            status="pending",
            message=f"å·²åˆ›å»ºå…¥åº“ä»»åŠ¡ï¼Œæ­£åœ¨å¤„ç† {len(request.files)} ä¸ªæ–‡ä»¶"
        )
    
    except VoiceHelperError:
        # é‡æ–°æŠ›å‡ºä¸šåŠ¡å¼‚å¸¸
        raise
    except Exception as e:
        # å¤„ç†æœªé¢„æœŸå¼‚å¸¸
        logger.exception("æ–‡æ¡£å…¥åº“å¤±è´¥", e, context={
            "files_count": len(request.files) if request.files else 0,
        })
        raise VoiceHelperError(
            ErrorCode.RAG_INDEXING_FAILED, 
            f"æ–‡æ¡£å…¥åº“å¤±è´¥: {str(e)}"
        )
```

#### å…¥åº“æœåŠ¡æ ¸å¿ƒå®ç°

**æ–‡ä»¶ä½ç½®**: `algo/core/ingest.py:IngestService`

```python
class IngestService:
    """
    æ–‡æ¡£å…¥åº“æœåŠ¡ - å¤„ç†æ–‡æ¡£è§£æã€å‘é‡åŒ–ã€ç´¢å¼•æ„å»ºå’Œå­˜å‚¨
    
    ä¸»è¦åŠŸèƒ½:
    - æ–‡æ¡£è§£æ: æ”¯æŒå¤šç§æ ¼å¼æ–‡æ¡£çš„æ–‡æœ¬æå–
    - æ™ºèƒ½åˆ†å—: åŸºäºè¯­ä¹‰çš„è‡ªé€‚åº”æ–‡æ¡£åˆ†å—
    - å‘é‡åŒ–: BGEæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ä¸­æ–‡å‘é‡
    - ç´¢å¼•æ„å»º: FAISSé«˜æ€§èƒ½å‘é‡ç´¢å¼•
    - å›¾è°±æ„å»º: å®ä½“æŠ½å–å’Œå…³ç³»è¯†åˆ«
    - å¼‚æ­¥å¤„ç†: åå°ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å…¥åº“æœåŠ¡åŠå…¶ä¾èµ–ç»„ä»¶"""
        self.text_splitter = self._init_text_splitter()      # æ–‡æœ¬åˆ†å—å™¨
        self.embedding_service = self._init_embedding()       # å‘é‡åŒ–æœåŠ¡
        self.vector_store = self._init_vector_store()        # å‘é‡å­˜å‚¨
        self.graph_store = self._init_graph_store()          # å›¾æ•°æ®åº“
        self.document_parser = self._init_parser()           # æ–‡æ¡£è§£æå™¨
        self.task_manager = self._init_task_manager()        # ä»»åŠ¡ç®¡ç†å™¨
        
        # å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—å’ŒçŠ¶æ€ç®¡ç†
        self.task_status: Dict[str, TaskStatus] = {}
        self.processing_queue = asyncio.Queue(maxsize=100)   # é™åˆ¶é˜Ÿåˆ—å¤§å°
        self.worker_pool = []                                # å·¥ä½œçº¿ç¨‹æ± 
        
        # å¯åŠ¨åå°å·¥ä½œçº¿ç¨‹
        self._start_workers()
    
    async def process_ingest_task(self, task_id: str, request: IngestRequest):
        """
        å¤„ç†å…¥åº“ä»»åŠ¡çš„ä¸»è¦æµç¨‹
        
        Args:
            task_id (str): ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
            request (IngestRequest): å…¥åº“è¯·æ±‚å¯¹è±¡
            
        å¤„ç†æ­¥éª¤:
            1. ä»»åŠ¡åˆå§‹åŒ– - è®¾ç½®çŠ¶æ€å’Œè¿›åº¦è·Ÿè¸ª
            2. æ–‡æ¡£è§£æ - æå–æ–‡æœ¬å†…å®¹å’Œå…ƒæ•°æ®
            3. å†…å®¹åˆ†å— - æ™ºèƒ½åˆ†å‰²é•¿æ–‡æ¡£
            4. å‘é‡åŒ– - ç”Ÿæˆæ–‡æ¡£embeddings
            5. ç´¢å¼•æ„å»º - æ›´æ–°FAISSç´¢å¼•
            6. å›¾è°±æ„å»º - æŠ½å–å®ä½“å’Œå…³ç³»
            7. çŠ¶æ€æ›´æ–° - æ›´æ–°ä»»åŠ¡å®ŒæˆçŠ¶æ€
        """
        start_time = time.time()
        
        try:
            # === ç¬¬ä¸€é˜¶æ®µï¼šä»»åŠ¡åˆå§‹åŒ– ===
            
            # 1.1 åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
            self.task_status[task_id] = TaskStatus(
                task_id=task_id,
                status="processing",
                progress=0,
                total_files=len(request.files),
                processed_files=0,
                created_at=datetime.now(),
                updated_at=datetime.now()
            )
            
            logger.info(f"å¼€å§‹å¤„ç†å…¥åº“ä»»åŠ¡: {task_id}", context={
                "task_id": task_id,
                "files_count": len(request.files),
                "collection": getattr(request, 'collection_name', 'default')
            })
            
            # 1.2 åˆ›å»ºæ–‡æ¡£é›†åˆ
            collection_name = getattr(request, 'collection_name', 'default')
            await self._ensure_collection_exists(collection_name)
            
            # === ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡æ–‡æ¡£å¤„ç† ===
            
            all_documents = []
            all_chunks = []
            processed_files = 0
            
            for file_index, file in enumerate(request.files):
                try:
                    # 2.1 æ›´æ–°å¤„ç†è¿›åº¦
                    progress = int((file_index / len(request.files)) * 100)
                    await self._update_task_progress(task_id, progress, f"å¤„ç†æ–‡ä»¶: {file.filename}")
                    
                    # 2.2 è§£æå•ä¸ªæ–‡æ¡£
                    document = await self._parse_single_document(file, collection_name, request.metadata)
                    
                    # 2.3 æ–‡æ¡£åˆ†å—å¤„ç†
                    chunks = await self._split_document_into_chunks(
                        document,
                        chunk_size=getattr(request, 'chunk_size', 1000),
                        chunk_overlap=getattr(request, 'chunk_overlap', 200)
                    )
                    
                    # 2.4 å‘é‡åŒ–å¤„ç†
                    vectorized_chunks = await self._vectorize_chunks(chunks)
                    
                    all_documents.append(document)
                    all_chunks.extend(vectorized_chunks)
                    processed_files += 1
                    
                    # 2.5 è®°å½•æ–‡ä»¶å¤„ç†å®Œæˆ
                    logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {file.filename}", context={
                        "task_id": task_id,
                        "file_index": file_index + 1,
                        "chunks_count": len(chunks),
                        "progress": f"{processed_files}/{len(request.files)}"
                    })
                    
                except Exception as e:
                    logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {file.filename}", context={
                        "task_id": task_id,
                        "error": str(e),
                        "file_index": file_index
                    })
                    # ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶ï¼Œä¸ä¸­æ–­æ•´ä¸ªä»»åŠ¡
                    continue
            
            # === ç¬¬ä¸‰é˜¶æ®µï¼šå‘é‡ç´¢å¼•æ„å»º ===
            
            if all_chunks:
                await self._update_task_progress(task_id, 80, "æ„å»ºå‘é‡ç´¢å¼•...")
                
                # 3.1 æ‰¹é‡æ›´æ–°FAISSç´¢å¼•
                index_update_result = await self._update_vector_index(all_chunks, collection_name)
                
                logger.info("å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ", context={
                    "task_id": task_id,
                    "chunks_indexed": len(all_chunks),
                    "index_size": index_update_result.get("total_vectors", 0)
                })
            
            # === ç¬¬å››é˜¶æ®µï¼šçŸ¥è¯†å›¾è°±æ„å»º ===
            
            if all_documents:
                await self._update_task_progress(task_id, 90, "æ„å»ºçŸ¥è¯†å›¾è°±...")
                
                # 4.1 å®ä½“æŠ½å–å’Œå…³ç³»è¯†åˆ«
                graph_result = await self._build_knowledge_graph(all_documents, collection_name)
                
                logger.info("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ", context={
                    "task_id": task_id,
                    "entities_count": graph_result.get("entities_count", 0),
                    "relations_count": graph_result.get("relations_count", 0)
                })
            
            # === ç¬¬äº”é˜¶æ®µï¼šä»»åŠ¡å®Œæˆå¤„ç† ===
            
            # 5.1 æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºæˆåŠŸ
            total_duration = time.time() - start_time
            
            self.task_status[task_id].status = "completed"
            self.task_status[task_id].progress = 100
            self.task_status[task_id].processed_files = processed_files
            self.task_status[task_id].completed_at = datetime.now()
            self.task_status[task_id].duration_seconds = total_duration
            self.task_status[task_id].result = {
                "documents_processed": len(all_documents),
                "chunks_created": len(all_chunks),
                "vectors_indexed": len(all_chunks),
                "entities_extracted": graph_result.get("entities_count", 0) if all_documents else 0,
                "processing_time_seconds": total_duration
            }
            
            # 5.2 è®°å½•ä»»åŠ¡å®Œæˆæ—¥å¿—
            logger.business("æ–‡æ¡£å…¥åº“ä»»åŠ¡å®Œæˆ", context={
                "task_id": task_id,
                "documents_processed": len(all_documents),
                "chunks_created": len(all_chunks),
                "processing_time_seconds": total_duration,
                "success_rate": f"{processed_files}/{len(request.files)}"
            })
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šæ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            logger.exception(f"å…¥åº“ä»»åŠ¡å¤±è´¥: {task_id}", e)
            
            self.task_status[task_id].status = "failed"
            self.task_status[task_id].error = str(e)
            self.task_status[task_id].completed_at = datetime.now()
            
            raise VoiceHelperError(
                ErrorCode.RAG_INDEXING_FAILED,
                f"å…¥åº“ä»»åŠ¡å¤±è´¥: {str(e)}"
            )
    
    async def _parse_single_document(
        self, 
        file: IngestFile, 
        collection_name: str, 
        metadata: Optional[Dict] = None
    ) -> Document:
        """
        è§£æå•ä¸ªæ–‡æ¡£æ–‡ä»¶
        
        Args:
            file (IngestFile): å¾…è§£æçš„æ–‡ä»¶å¯¹è±¡
            collection_name (str): æ–‡æ¡£é›†åˆåç§°
            metadata (dict): é¢å¤–çš„å…ƒæ•°æ®
            
        Returns:
            Document: è§£æåçš„æ–‡æ¡£å¯¹è±¡
            
        æ”¯æŒæ ¼å¼:
            - TXT: çº¯æ–‡æœ¬æ–‡ä»¶
            - PDF: PDFæ–‡æ¡£ï¼ˆä½¿ç”¨PyPDF2ï¼‰
            - DOCX: Wordæ–‡æ¡£ï¼ˆä½¿ç”¨python-docxï¼‰
            - MD: Markdownæ–‡ä»¶
            - HTML: HTMLç½‘é¡µ
        """
        try:
            # 1. ç¡®å®šæ–‡ä»¶ç±»å‹
            file_extension = file.filename.lower().split('.')[-1]
            
            # 2. æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è§£æå™¨
            if file_extension == 'txt':
                content = await self._parse_txt(file.content)
            elif file_extension == 'pdf':
                content = await self._parse_pdf(file.content)
            elif file_extension == 'docx':
                content = await self._parse_docx(file.content)
            elif file_extension == 'md':
                content = await self._parse_markdown(file.content)
            elif file_extension == 'html':
                content = await self._parse_html(file.content)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")
            
            # 3. å†…å®¹æ¸…æ´—å’Œé¢„å¤„ç†
            cleaned_content = self._clean_text_content(content)
            
            # 4. æ„å»ºæ–‡æ¡£å¯¹è±¡
            document = Document(
                doc_id=f"{collection_name}_{file.filename}_{int(time.time())}",
                title=file.filename,
                content=cleaned_content,
                metadata={
                    "filename": file.filename,
                    "file_type": file_extension,
                    "file_size": len(file.content.encode('utf-8') if isinstance(file.content, str) else file.content),
                    "collection": collection_name,
                    "created_at": datetime.now().isoformat(),
                    "content_length": len(cleaned_content),
                    **(metadata or {})
                }
            )
            
            return document
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£è§£æå¤±è´¥: {file.filename}", error=str(e))
            raise ValueError(f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")
    
    async def _split_document_into_chunks(
        self,
        document: Document,
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ) -> List[DocumentChunk]:
        """
        æ™ºèƒ½æ–‡æ¡£åˆ†å— - åŸºäºè¯­ä¹‰å’Œç»“æ„çš„è‡ªé€‚åº”åˆ†å‰²
        
        Args:
            document (Document): å¾…åˆ†å—çš„æ–‡æ¡£
            chunk_size (int): ç›®æ ‡åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap (int): åˆ†å—é‡å å¤§å°
            
        Returns:
            List[DocumentChunk]: æ–‡æ¡£åˆ†å—åˆ—è¡¨
            
        åˆ†å—ç­–ç•¥:
            1. ç»“æ„åŒ–åˆ†å‰²: æŒ‰æ®µè½ã€ç« èŠ‚ç­‰è‡ªç„¶è¾¹ç•Œ
            2. è¯­ä¹‰ä¿æŒ: ç¡®ä¿è¯­ä¹‰å®Œæ•´æ€§
            3. å¤§å°æ§åˆ¶: æ§åˆ¶åˆ†å—å¤§å°åœ¨åˆç†èŒƒå›´
            4. é‡å è®¾è®¡: ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
        """
        try:
            chunks = []
            content = document.content
            
            # 1. é¢„å¤„ç†ï¼šæŒ‰è‡ªç„¶è¾¹ç•Œåˆ†å‰²
            paragraphs = self._split_by_paragraphs(content)
            
            current_chunk = ""
            current_start = 0
            chunk_index = 0
            
            for para in paragraphs:
                # 2. æ£€æŸ¥å½“å‰åˆ†å—å¤§å°
                if len(current_chunk) + len(para) <= chunk_size:
                    # å¯ä»¥åŠ å…¥å½“å‰åˆ†å—
                    current_chunk += para + "\n\n"
                else:
                    # éœ€è¦åˆ›å»ºæ–°åˆ†å—
                    if current_chunk:
                        chunk = DocumentChunk(
                            chunk_id=f"{document.doc_id}_chunk_{chunk_index}",
                            doc_id=document.doc_id,
                            content=current_chunk.strip(),
                            start_index=current_start,
                            end_index=current_start + len(current_chunk),
                            metadata={
                                "chunk_index": chunk_index,
                                "chunk_type": "paragraph_based",
                                "original_doc_title": document.title,
                                **document.metadata
                            }
                        )
                        chunks.append(chunk)
                        chunk_index += 1
                    
                    # 3. å¤„ç†é‡å é€»è¾‘
                    if chunk_overlap > 0 and chunks:
                        # ä»ä¸Šä¸€ä¸ªåˆ†å—çš„æœ«å°¾å–é‡å å†…å®¹
                        overlap_text = current_chunk[-chunk_overlap:] if len(current_chunk) > chunk_overlap else current_chunk
                        current_chunk = overlap_text + para + "\n\n"
                    else:
                        current_chunk = para + "\n\n"
                    
                    current_start = current_start + len(chunks[-1].content) - chunk_overlap if chunks else 0
            
            # 4. å¤„ç†æœ€åä¸€ä¸ªåˆ†å—
            if current_chunk:
                chunk = DocumentChunk(
                    chunk_id=f"{document.doc_id}_chunk_{chunk_index}",
                    doc_id=document.doc_id,
                    content=current_chunk.strip(),
                    start_index=current_start,
                    end_index=current_start + len(current_chunk),
                    metadata={
                        "chunk_index": chunk_index,
                        "chunk_type": "paragraph_based",
                        "original_doc_title": document.title,
                        **document.metadata
                    }
                )
                chunks.append(chunk)
            
            # 5. åˆ†å—è´¨é‡æ£€æŸ¥
            chunks = self._validate_and_optimize_chunks(chunks)
            
            logger.info(f"æ–‡æ¡£åˆ†å—å®Œæˆ: {document.title}", context={
                "doc_id": document.doc_id,
                "total_chunks": len(chunks),
                "avg_chunk_size": sum(len(c.content) for c in chunks) / len(chunks) if chunks else 0,
                "content_coverage": sum(len(c.content) for c in chunks) / len(document.content) * 100
            })
            
            return chunks
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†å—å¤±è´¥: {document.title}", error=str(e))
            raise ValueError(f"æ–‡æ¡£åˆ†å—å¤±è´¥: {str(e)}")
```

### 2. æ£€ç´¢æŸ¥è¯¢API

#### æµå¼æ£€ç´¢æ ¸å¿ƒå®ç°

**æ–‡ä»¶ä½ç½®**: `algo/core/retrieve.py:RetrieveService.stream_query`

```python
async def stream_query(self, request: QueryRequest) -> AsyncGenerator[str, None]:
    """
    æµå¼æŸ¥è¯¢å¤„ç† - å¤šè·¯å¬å›+èåˆé‡æ’+æµå¼ç”Ÿæˆçš„å®Œæ•´RAGæµç¨‹
    
    Args:
        request (QueryRequest): æŸ¥è¯¢è¯·æ±‚å¯¹è±¡
            - messages: List[Message] å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            - top_k: int è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤10
            - temperature: float LLMç”Ÿæˆæ¸©åº¦ï¼Œé»˜è®¤0.7
            - collection_name: str æ£€ç´¢é›†åˆï¼Œé»˜è®¤'default'
            - retrieval_mode: str æ£€ç´¢æ¨¡å¼ 'hybrid'|'vector'|'graph'
    
    Yields:
        str: NDJSONæ ¼å¼çš„äº‹ä»¶æµï¼ŒåŒ…å«ä»¥ä¸‹äº‹ä»¶ç±»å‹:
            - retrieval_start: {"type": "retrieval_start", "data": {"query_id": "...", "timestamp": 123}}
            - retrieval_progress: {"type": "retrieval_progress", "data": {"stage": "vector", "progress": 0.3}}
            - retrieval_result: {"type": "retrieval_result", "data": {"results": [...], "total": 10}}
            - generation_start: {"type": "generation_start", "data": {"model": "gpt-3.5-turbo"}}
            - generation_chunk: {"type": "generation_chunk", "data": {"text": "..."}}
            - generation_done: {"type": "generation_done", "data": {"full_text": "...", "metrics": {...}}}
            - error: {"type": "error", "data": {"error": "...", "code": "..."}}
    
    æ£€ç´¢æµç¨‹è¯¦è§£:
        1. æŸ¥è¯¢é¢„å¤„ç†: æ„å›¾è¯†åˆ«ã€å…³é”®è¯æå–ã€æŸ¥è¯¢é‡å†™
        2. å¤šè·¯å¬å›: å¹¶è¡Œæ‰§è¡Œå‘é‡æ£€ç´¢ã€BM25æ£€ç´¢ã€å›¾æ¨ç†
        3. ç»“æœèåˆ: å»é‡ã€é‡æ’ã€ç›¸å…³æ€§æ‰“åˆ†
        4. ä¸Šä¸‹æ–‡æ„å»º: æ•´åˆæ£€ç´¢ç»“æœä¸ºLLMæç¤º
        5. æµå¼ç”Ÿæˆ: è°ƒç”¨LLMå¹¶å®æ—¶æµå¼è¿”å›
    """
    query_id = self._generate_query_id()
    start_time = time.time()
    metrics = RetrievalMetrics()
    
    try:
        # === ç¬¬ä¸€é˜¶æ®µï¼šæŸ¥è¯¢é¢„å¤„ç†å’Œåˆå§‹åŒ– ===
        
        # 1.1 å‘é€æ£€ç´¢å¼€å§‹äº‹ä»¶
        yield self._create_event("retrieval_start", {
            "query_id": query_id,
            "timestamp": int(time.time() * 1000),
            "mode": getattr(request, 'retrieval_mode', 'hybrid')
        })
        
        # 1.2 æå–æœ€æ–°ç”¨æˆ·æŸ¥è¯¢
        if not request.messages or len(request.messages) == 0:
            raise ValueError("æ²¡æœ‰æä¾›æŸ¥è¯¢æ¶ˆæ¯")
        
        user_query = request.messages[-1].content
        conversation_history = request.messages[:-1] if len(request.messages) > 1 else []
        
        # 1.3 æŸ¥è¯¢å¢å¼ºå’Œé‡å†™
        enhanced_query = await self._enhance_user_query(user_query, conversation_history)
        
        logger.info("å¼€å§‹æ£€ç´¢æŸ¥è¯¢", context={
            "query_id": query_id,
            "original_query": user_query[:100],
            "enhanced_query": enhanced_query[:100],
            "history_length": len(conversation_history)
        })
        
        # === ç¬¬äºŒé˜¶æ®µï¼šå¤šè·¯å¹¶è¡Œæ£€ç´¢ ===
        
        # 2.1 æ ¹æ®æ£€ç´¢æ¨¡å¼ç¡®å®šç­–ç•¥
        retrieval_mode = getattr(request, 'retrieval_mode', 'hybrid')
        top_k = request.top_k or 10
        
        retrieval_tasks = []
        
        if retrieval_mode in ['hybrid', 'vector']:
            # å‘é‡æ£€ç´¢ä»»åŠ¡
            retrieval_tasks.append(
                self._vector_retrieval(enhanced_query, top_k, query_id)
            )
        
        if retrieval_mode in ['hybrid', 'text']:
            # BM25æ–‡æœ¬æ£€ç´¢ä»»åŠ¡  
            retrieval_tasks.append(
                self._bm25_retrieval(enhanced_query, top_k, query_id)
            )
        
        if retrieval_mode in ['hybrid', 'graph']:
            # GraphRAGå›¾æ¨ç†ä»»åŠ¡
            retrieval_tasks.append(
                self._graph_reasoning_retrieval(enhanced_query, top_k // 2, query_id)
            )
        
        # 2.2 å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ£€ç´¢ä»»åŠ¡
        yield self._create_event("retrieval_progress", {
            "stage": "multi_recall",
            "tasks_count": len(retrieval_tasks),
            "progress": 0.1
        })
        
        # ä½¿ç”¨asyncio.gatherå¹¶å‘æ‰§è¡Œï¼Œè®¾ç½®è¶…æ—¶
        try:
            retrieval_results = await asyncio.wait_for(
                asyncio.gather(*retrieval_tasks, return_exceptions=True),
                timeout=30.0  # 30ç§’è¶…æ—¶
            )
        except asyncio.TimeoutError:
            logger.error("æ£€ç´¢ä»»åŠ¡è¶…æ—¶", context={"query_id": query_id})
            yield self._create_event("error", {
                "error": "æ£€ç´¢è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
                "code": "RETRIEVAL_TIMEOUT"
            })
            return
        
        # 2.3 å¤„ç†æ£€ç´¢ç»“æœ
        vector_results, bm25_results, graph_results = [], [], []
        
        for i, result in enumerate(retrieval_results):
            if isinstance(result, Exception):
                logger.error(f"æ£€ç´¢ä»»åŠ¡ {i} å¤±è´¥", error=str(result))
                continue
            
            if i == 0 and retrieval_mode in ['hybrid', 'vector']:
                vector_results = result
            elif (i == 1 and retrieval_mode == 'hybrid') or (i == 0 and retrieval_mode == 'text'):
                bm25_results = result  
            elif retrieval_mode in ['hybrid', 'graph']:
                graph_results = result
        
        # === ç¬¬ä¸‰é˜¶æ®µï¼šç»“æœèåˆå’Œé‡æ’ ===
        
        yield self._create_event("retrieval_progress", {
            "stage": "fusion_rerank",
            "vector_count": len(vector_results),
            "bm25_count": len(bm25_results),
            "graph_count": len(graph_results),
            "progress": 0.6
        })
        
        # 3.1 å¤šè·¯ç»“æœèåˆ
        fused_results = await self._fuse_retrieval_results(
            vector_results=vector_results,
            bm25_results=bm25_results, 
            graph_results=graph_results,
            original_query=user_query,
            enhanced_query=enhanced_query
        )
        
        # 3.2 æ™ºèƒ½é‡æ’åº
        reranked_results = await self._rerank_results(
            results=fused_results,
            query=enhanced_query,
            top_k=top_k
        )
        
        # 3.3 ç»“æœè´¨é‡è¿‡æ»¤
        filtered_results = self._filter_low_quality_results(
            reranked_results,
            min_score=0.3,  # æœ€å°ç›¸å…³æ€§åˆ†æ•°
            max_results=top_k
        )
        
        # === ç¬¬å››é˜¶æ®µï¼šè¿”å›æ£€ç´¢ç»“æœ ===
        
        # 4.1 æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
        formatted_results = []
        for i, result in enumerate(filtered_results):
            formatted_result = {
                "rank": i + 1,
                "doc_id": result.doc_id,
                "chunk_id": result.chunk_id,
                "title": result.metadata.get("title", ""),
                "content": result.content[:500] + "..." if len(result.content) > 500 else result.content,
                "score": float(result.score),
                "source": result.source,
                "metadata": {
                    k: v for k, v in result.metadata.items() 
                    if k in ["filename", "file_type", "created_at", "section"]
                }
            }
            formatted_results.append(formatted_result)
        
        # 4.2 å‘é€æ£€ç´¢ç»“æœäº‹ä»¶
        retrieval_time = (time.time() - start_time) * 1000
        yield self._create_event("retrieval_result", {
            "results": formatted_results,
            "total_found": len(filtered_results),
            "retrieval_time_ms": retrieval_time,
            "retrieval_modes": retrieval_mode,
            "query_enhancement": {
                "original": user_query[:100],
                "enhanced": enhanced_query[:100] if enhanced_query != user_query else None
            }
        })
        
        # === ç¬¬äº”é˜¶æ®µï¼šLLMä¸Šä¸‹æ–‡æ„å»º ===
        
        # 5.1 æ„å»ºå¢å¼ºæç¤º
        augmented_context = await self._build_augmented_context(
            query=user_query,
            conversation_history=conversation_history,
            retrieval_results=filtered_results[:5],  # ä½¿ç”¨top5ç»“æœ
            enhanced_query=enhanced_query
        )
        
        # 5.2 æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = self._build_system_prompt(retrieval_mode, len(filtered_results))
        
        # === ç¬¬å…­é˜¶æ®µï¼šæµå¼LLMç”Ÿæˆ ===
        
        # 6.1 å‘é€ç”Ÿæˆå¼€å§‹äº‹ä»¶
        model_name = getattr(request, 'model', 'gpt-3.5-turbo')
        yield self._create_event("generation_start", {
            "model": model_name,
            "context_length": len(augmented_context),
            "temperature": request.temperature or 0.7,
            "max_tokens": getattr(request, 'max_tokens', 2048)
        })
        
        # 6.2 æµå¼è°ƒç”¨LLMç”Ÿæˆå›å¤
        full_response = ""
        chunk_count = 0
        generation_start = time.time()
        
        async for chunk in self._stream_llm_response(
            system_prompt=system_prompt,
            augmented_context=augmented_context,
            model=model_name,
            temperature=request.temperature or 0.7,
            max_tokens=getattr(request, 'max_tokens', 2048)
        ):
            if chunk.strip():
                full_response += chunk
                chunk_count += 1
                
                # å‘é€ç”Ÿæˆç‰‡æ®µäº‹ä»¶
                yield self._create_event("generation_chunk", {
                    "text": chunk,
                    "chunk_index": chunk_count
                })
        
        # === ç¬¬ä¸ƒé˜¶æ®µï¼šç”Ÿæˆå®Œæˆå’ŒæŒ‡æ ‡ç»Ÿè®¡ ===
        
        generation_time = (time.time() - generation_start) * 1000
        total_time = (time.time() - start_time) * 1000
        
        # 7.1 è®¡ç®—ç”ŸæˆæŒ‡æ ‡
        generation_metrics = {
            "total_tokens": len(full_response.split()),
            "generation_time_ms": generation_time,
            "tokens_per_second": len(full_response.split()) / (generation_time / 1000) if generation_time > 0 else 0,
            "chunks_generated": chunk_count
        }
        
        # 7.2 è®¡ç®—æ£€ç´¢æŒ‡æ ‡  
        retrieval_metrics = {
            "retrieval_time_ms": retrieval_time,
            "total_results": len(fused_results),
            "filtered_results": len(filtered_results),
            "vector_results": len(vector_results),
            "bm25_results": len(bm25_results),
            "graph_results": len(graph_results)
        }
        
        # 7.3 å‘é€å®Œæˆäº‹ä»¶
        yield self._create_event("generation_done", {
            "full_text": full_response,
            "query_id": query_id,
            "total_time_ms": total_time,
            "generation_metrics": generation_metrics,
            "retrieval_metrics": retrieval_metrics,
            "context_sources": [
                {
                    "doc_id": r.doc_id,
                    "title": r.metadata.get("title", ""),
                    "score": float(r.score)
                }
                for r in filtered_results[:3]  # è¿”å›top3æ¥æº
            ]
        })
        
        # 7.4 è®°å½•æŸ¥è¯¢å®Œæˆæ—¥å¿—
        logger.business("æ£€ç´¢æŸ¥è¯¢å®Œæˆ", context={
            "query_id": query_id,
            "total_time_ms": total_time,
            "results_count": len(filtered_results),
            "response_length": len(full_response),
            "retrieval_mode": retrieval_mode
        })
        
    except Exception as e:
        # å¼‚å¸¸å¤„ç†ï¼šå‘é€é”™è¯¯äº‹ä»¶
        logger.exception(f"æ£€ç´¢æŸ¥è¯¢å¤±è´¥: {query_id}", e)
        
        yield self._create_event("error", {
            "error": str(e),
            "query_id": query_id,
            "code": "QUERY_PROCESSING_ERROR",
            "timestamp": int(time.time() * 1000)
        })
```

## ğŸ§  GraphRAGæ¨ç†å¼•æ“

### çŸ¥è¯†å›¾è°±æ„å»ºä¸æ¨ç†

```mermaid
flowchart TD
    A[åŸå§‹æ–‡æ¡£] --> B[å®ä½“è¯†åˆ«NER]
    A --> C[å…³ç³»æŠ½å–RE]
    
    B --> D[å®ä½“æ¶ˆæ­§]
    C --> E[å…³ç³»éªŒè¯]
    
    D --> F[Neo4jå›¾æ•°æ®åº“]
    E --> F
    
    F --> G[å›¾åµŒå…¥GraphEmbedding]
    F --> H[ç¤¾åŒºå‘ç°Community]
    F --> I[è·¯å¾„åˆ†æPathFinding]
    
    G --> J[å‘é‡åŒ–å®ä½“]
    H --> K[èšç±»å­å›¾]
    I --> L[æ¨ç†è·¯å¾„]
    
    J --> M[æ··åˆæ£€ç´¢HybridRetrieval]
    K --> M
    L --> M
    
    M --> N[å¤šè·³æ¨ç†MultiHop]
    N --> O[ç­”æ¡ˆç”Ÿæˆ]
    
    style A fill:#e3f2fd
    style F fill:#f3e5f5
    style M fill:#e8f5e8
    style O fill:#fff3e0
```

### GraphRAGæ ¸å¿ƒå®ç°

**æ–‡ä»¶ä½ç½®**: `algo/core/graph_rag.py:GraphRAG`

```python
class GraphRAG:
    """
    GraphRAG - åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
    
    æ ¸å¿ƒåŠŸèƒ½:
    - å®ä½“è¯†åˆ«: ä½¿ç”¨NERæ¨¡å‹è¯†åˆ«æ–‡æ¡£ä¸­çš„å®ä½“
    - å…³ç³»æŠ½å–: è¯†åˆ«å®ä½“é—´çš„è¯­ä¹‰å…³ç³»
    - å›¾è°±æ„å»º: æ„å»ºç»“æ„åŒ–çŸ¥è¯†å›¾è°±
    - å¤šè·³æ¨ç†: åŸºäºå›¾ç»“æ„çš„æ¨ç†æŸ¥è¯¢
    - ç¤¾åŒºå‘ç°: è¯†åˆ«ç›¸å…³å®ä½“èšç±»
    - è·¯å¾„åˆ†æ: åˆ†æå®ä½“é—´çš„æ¨ç†è·¯å¾„
    """
    
    def __init__(self, neo4j_client, embedding_service):
        """åˆå§‹åŒ–GraphRAGç³»ç»Ÿ"""
        self.neo4j = neo4j_client
        self.embedding_service = embedding_service
        self.entity_recognizer = self._init_ner_model()
        self.relation_extractor = self._init_re_model() 
        self.graph_embedder = self._init_graph_embedder()
    
    async def build_knowledge_graph(self, documents: List[Document]) -> Dict[str, int]:
        """
        ä»æ–‡æ¡£æ„å»ºçŸ¥è¯†å›¾è°±
        
        Args:
            documents: å¾…å¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            dict: æ„å»ºç»Ÿè®¡ä¿¡æ¯ {"entities_count": 123, "relations_count": 456}
        """
        entities_count = 0
        relations_count = 0
        
        for doc in documents:
            # 1. å®ä½“è¯†åˆ«
            entities = await self._extract_entities(doc.content)
            
            # 2. å…³ç³»æŠ½å–  
            relations = await self._extract_relations(doc.content, entities)
            
            # 3. å­˜å‚¨åˆ°å›¾æ•°æ®åº“
            doc_entities = await self._store_entities(entities, doc.doc_id)
            doc_relations = await self._store_relations(relations, doc.doc_id)
            
            entities_count += len(doc_entities)
            relations_count += len(doc_relations)
        
        return {
            "entities_count": entities_count,
            "relations_count": relations_count
        }
    
    async def reasoning_retrieval(self, query: str, max_depth: int = 2) -> List[ReasoningResult]:
        """
        åŸºäºå›¾æ¨ç†çš„æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            max_depth: æœ€å¤§æ¨ç†æ·±åº¦
            
        Returns:
            List[ReasoningResult]: æ¨ç†ç»“æœåˆ—è¡¨
        """
        # 1. ä»æŸ¥è¯¢ä¸­è¯†åˆ«å…³é”®å®ä½“
        query_entities = await self._extract_entities(query)
        
        if not query_entities:
            return []
        
        # 2. å¤šè·³å›¾éå†
        reasoning_results = []
        
        for entity in query_entities[:3]:  # é™åˆ¶èµ·å§‹å®ä½“æ•°é‡
            # æ‰§è¡Œå¤šè·³æ¨ç†
            paths = await self._multi_hop_reasoning(entity, max_depth)
            
            for path in paths:
                result = ReasoningResult(
                    entities=path['entities'],
                    relations=path['relations'],
                    reasoning_path=path['path'],
                    confidence=path['confidence'],
                    evidence=path['evidence']
                )
                reasoning_results.append(result)
        
        # 3. æŒ‰ç½®ä¿¡åº¦æ’åº
        return sorted(reasoning_results, key=lambda x: x.confidence, reverse=True)
    
    async def _multi_hop_reasoning(self, start_entity: str, max_depth: int) -> List[Dict]:
        """
        å¤šè·³å›¾æ¨ç†å®ç°
        
        Args:
            start_entity: èµ·å§‹å®ä½“
            max_depth: æœ€å¤§æ¨ç†æ·±åº¦
            
        Returns:
            List[Dict]: æ¨ç†è·¯å¾„åˆ—è¡¨
        """
        paths = []
        
        # ä½¿ç”¨CypheræŸ¥è¯¢è¿›è¡Œå›¾éå†
        cypher_query = """
        MATCH path = (start:Entity {name: $entity})-[*1..{max_depth}]-(end:Entity)
        WHERE start <> end
        RETURN path, length(path) as depth,
               [node in nodes(path) | node.name] as entity_path,
               [rel in relationships(path) | type(rel)] as relation_path
        ORDER BY depth ASC
        LIMIT 50
        """.format(max_depth=max_depth)
        
        results = await self.neo4j.run(cypher_query, entity=start_entity)
        
        for record in results:
            path_info = {
                'entities': record['entity_path'],
                'relations': record['relation_path'], 
                'path': record['path'],
                'depth': record['depth'],
                'confidence': self._calculate_path_confidence(record['path'])
            }
            paths.append(path_info)
        
        return paths
```

## ğŸ™ï¸ è¯­éŸ³å¤„ç†æ¨¡å—

### è¯­éŸ³å¤„ç†æ¶æ„å›¾

```mermaid
graph TB
    subgraph "è¯­éŸ³å¤„ç†æµæ°´çº¿"
        AUDIO[éŸ³é¢‘è¾“å…¥<br/>WebSocketæµ]
        
        subgraph "éŸ³é¢‘é¢„å¤„ç†"
            VAD[è¯­éŸ³æ´»åŠ¨æ£€æµ‹<br/>Voice Activity Detection]
            DENOISE[å™ªå£°æŠ‘åˆ¶<br/>Noise Suppression] 
            RESAMPLE[é‡é‡‡æ ·<br/>16kHzå•å£°é“]
        end
        
        subgraph "è¯­éŸ³è¯†åˆ«ASR"
            WHISPER[Whisperæ¨¡å‹<br/>å¤šè¯­è¨€è¯†åˆ«]
            AZURE_ASR[Azure Speech<br/>å®æ—¶è¯†åˆ«]
            LOCAL_ASR[æœ¬åœ°ASR<br/>ç¦»çº¿è¯†åˆ«]
        end
        
        subgraph "è¯­éŸ³åˆæˆTTS"  
            EDGE_TTS[Edge-TTS<br/>å¤šè¯­è¨€åˆæˆ]
            AZURE_TTS[Azure TTS<br/>ç¥ç»ç½‘ç»œè¯­éŸ³]
            LOCAL_TTS[æœ¬åœ°TTS<br/>ç¦»çº¿åˆæˆ]
        end
        
        subgraph "è¯­éŸ³åå¤„ç†"
            SPEED[è¯­é€Ÿè°ƒèŠ‚<br/>Speed Control]
            PITCH[éŸ³è°ƒè°ƒèŠ‚<br/>Pitch Control]
            VOLUME[éŸ³é‡æ ‡å‡†åŒ–<br/>Volume Normalize]
        end
    end
    
    AUDIO --> VAD
    VAD --> DENOISE
    DENOISE --> RESAMPLE
    
    RESAMPLE --> WHISPER
    RESAMPLE --> AZURE_ASR  
    RESAMPLE --> LOCAL_ASR
    
    EDGE_TTS --> SPEED
    AZURE_TTS --> PITCH
    LOCAL_TTS --> VOLUME
    
    style AUDIO fill:#e3f2fd
    style WHISPER fill:#f3e5f5
    style EDGE_TTS fill:#e8f5e8
```

### å¢å¼ºè¯­éŸ³æœåŠ¡å®ç°

**æ–‡ä»¶ä½ç½®**: `algo/core/enhanced_voice_services.py:EnhancedVoiceService`

```python
class EnhancedVoiceService:
    """
    å¢å¼ºè¯­éŸ³æœåŠ¡ - é›†æˆASRã€TTSå’Œæ™ºèƒ½å¯¹è¯çš„å®Œæ•´è¯­éŸ³äº¤äº’ç³»ç»Ÿ
    
    ä¸»è¦ç‰¹æ€§:
    - å¤šProvideræ”¯æŒ: Whisperã€Azureã€Edgeç­‰å¤šç§è¯­éŸ³æœåŠ¡
    - å®æ—¶å¤„ç†: æµå¼ASRè¯†åˆ«å’ŒTTSåˆæˆ
    - æ™ºèƒ½VAD: è¯­éŸ³æ´»åŠ¨æ£€æµ‹å’Œç«¯ç‚¹æ£€æµ‹
    - ä¸Šä¸‹æ–‡ç®¡ç†: å¤šè½®å¯¹è¯å’Œä¼šè¯çŠ¶æ€
    - æ€§èƒ½ä¼˜åŒ–: ç¼“å­˜ã€è¿æ¥æ± ã€å¼‚æ­¥å¤„ç†
    """
    
    def __init__(self, config: VoiceConfig, retrieve_service=None):
        """
        åˆå§‹åŒ–å¢å¼ºè¯­éŸ³æœåŠ¡
        
        Args:
            config (VoiceConfig): è¯­éŸ³æœåŠ¡é…ç½®
            retrieve_service: æ£€ç´¢æœåŠ¡å®ä¾‹ï¼Œç”¨äºRAGå¯¹è¯
        """
        self.config = config
        self.retrieve_service = retrieve_service
        
        # åˆå§‹åŒ–ASRå’ŒTTSæœåŠ¡
        self.asr_service = EnhancedASRService(config)
        self.tts_service = EnhancedTTSService(config)
        
        # ä¼šè¯ç®¡ç†
        self.active_sessions: Dict[str, VoiceSessionState] = {}
        self.session_lock = asyncio.Lock()
        
        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = VoiceServiceMetrics()
        
        # å¯åŠ¨åå°æ¸…ç†ä»»åŠ¡
        self.cleanup_task = asyncio.create_task(self._cleanup_expired_sessions())
    
    async def process_voice_query(self, request: VoiceQueryRequest) -> AsyncGenerator[VoiceQueryResponse, None]:
        """
        å¤„ç†è¯­éŸ³æŸ¥è¯¢çš„å®Œæ•´æµç¨‹
        
        Args:
            request (VoiceQueryRequest): è¯­éŸ³æŸ¥è¯¢è¯·æ±‚
                - session_id: str ä¼šè¯ID
                - audio_chunk: str Base64ç¼–ç çš„éŸ³é¢‘æ•°æ®
                - is_final: bool æ˜¯å¦ä¸ºæœ€ç»ˆéŸ³é¢‘å—
                - language: str è¯­è¨€ä»£ç ï¼Œé»˜è®¤'zh-CN'
                - conversation_id: str å¯¹è¯ID
        
        Yields:
            VoiceQueryResponse: è¯­éŸ³æŸ¥è¯¢å“åº”ï¼ŒåŒ…å«å¤šç§äº‹ä»¶ç±»å‹:
                - asr_partial: éƒ¨åˆ†ASRè¯†åˆ«ç»“æœ
                - asr_final: æœ€ç»ˆASRè¯†åˆ«ç»“æœ  
                - processing_start: å¼€å§‹å¤„ç†æç¤º
                - llm_response_chunk: LLMå›å¤ç‰‡æ®µ
                - llm_response_final: LLMå®Œæ•´å›å¤
                - tts_start: TTSåˆæˆå¼€å§‹
                - tts_audio: TTSéŸ³é¢‘æ•°æ®
                - tts_complete: TTSåˆæˆå®Œæˆ
                - error: é”™è¯¯ä¿¡æ¯
        """
        session_id = request.session_id or self._generate_session_id()
        start_time = time.time()
        
        try:
            # === ç¬¬ä¸€é˜¶æ®µï¼šä¼šè¯åˆå§‹åŒ–å’Œç®¡ç† ===
            
            # 1.1 è·å–æˆ–åˆ›å»ºä¼šè¯çŠ¶æ€
            async with self.session_lock:
                if session_id not in self.active_sessions:
                    self.active_sessions[session_id] = VoiceSessionState(
                        session_id=session_id,
                        conversation_id=request.conversation_id,
                        language=getattr(request, 'language', 'zh-CN'),
                        audio_buffer=b"",
                        transcript_buffer="",
                        last_activity=datetime.now(),
                        context_history=[]
                    )
                
                session = self.active_sessions[session_id]
                session.last_activity = datetime.now()
            
            # === ç¬¬äºŒé˜¶æ®µï¼šéŸ³é¢‘æ•°æ®å¤„ç† ===
            
            # 2.1 è§£ç éŸ³é¢‘æ•°æ®
            if hasattr(request, 'audio_chunk') and request.audio_chunk:
                try:
                    audio_data = base64.b64decode(request.audio_chunk)
                    session.audio_buffer += audio_data
                    
                    # æ›´æ–°éŸ³é¢‘æŒ‡æ ‡
                    self.metrics.total_audio_bytes += len(audio_data)
                    self.metrics.audio_packets_received += 1
                    
                except Exception as e:
                    yield VoiceQueryResponse(
                        type="error",
                        session_id=session_id,
                        error=f"éŸ³é¢‘è§£ç å¤±è´¥: {str(e)}"
                    )
                    return
            
            # === ç¬¬ä¸‰é˜¶æ®µï¼šå®æ—¶ASRå¤„ç† ===
            
            # 3.1 æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤ŸéŸ³é¢‘è¿›è¡Œå¤„ç†
            if len(session.audio_buffer) >= self.config.min_audio_chunk_size:
                
                # 3.2 å®æ—¶ASRè¯†åˆ«
                asr_start_time = time.time()
                partial_result = await self.asr_service.transcribe_partial(
                    audio_data=session.audio_buffer[-self.config.asr_chunk_size:],
                    language=session.language,
                    session_id=session_id
                )
                
                asr_latency = (time.time() - asr_start_time) * 1000
                self.metrics.avg_asr_latency = (self.metrics.avg_asr_latency + asr_latency) / 2
                
                # 3.3 å‘é€éƒ¨åˆ†è¯†åˆ«ç»“æœ
                if partial_result and partial_result.text.strip():
                    yield VoiceQueryResponse(
                        type="asr_partial",
                        session_id=session_id,
                        text=partial_result.text,
                        confidence=partial_result.confidence,
                        timestamp=int(time.time() * 1000)
                    )
                    
                    session.transcript_buffer = partial_result.text
            
            # === ç¬¬å››é˜¶æ®µï¼šå¥å­å®Œæ•´æ€§æ£€æµ‹å’Œæœ€ç»ˆè¯†åˆ« ===
            
            # 4.1 æ£€æµ‹å®Œæ•´å¥å­æˆ–æœ€ç»ˆéŸ³é¢‘
            is_complete_sentence = self._detect_sentence_boundary(
                session.transcript_buffer,
                session.audio_buffer
            )
            
            if is_complete_sentence or getattr(request, 'is_final', False):
                
                # 4.2 æ‰§è¡Œæœ€ç»ˆASRè¯†åˆ«
                final_result = await self.asr_service.transcribe_final(
                    audio_data=session.audio_buffer,
                    language=session.language,
                    session_id=session_id
                )
                
                if final_result and final_result.text.strip():
                    final_text = final_result.text.strip()
                    
                    # 4.3 å‘é€æœ€ç»ˆè¯†åˆ«ç»“æœ
                    yield VoiceQueryResponse(
                        type="asr_final", 
                        session_id=session_id,
                        text=final_text,
                        confidence=final_result.confidence,
                        timestamp=int(time.time() * 1000)
                    )
                    
                    # === ç¬¬äº”é˜¶æ®µï¼šRAGçŸ¥è¯†æ£€ç´¢å’Œå¯¹è¯ç”Ÿæˆ ===
                    
                    if self.retrieve_service and final_text:
                        
                        # 5.1 å‘é€å¤„ç†å¼€å§‹æç¤º
                        yield VoiceQueryResponse(
                            type="processing_start",
                            session_id=session_id,
                            message="æ­£åœ¨æ€è€ƒ...",
                            timestamp=int(time.time() * 1000)
                        )
                        
                        # 5.2 æ„å»ºRAGæŸ¥è¯¢è¯·æ±‚
                        from core.models import QueryRequest, Message
                        
                        # æ„å»ºå¯¹è¯å†å²
                        messages = []
                        for ctx in session.context_history[-5:]:  # ä¿ç•™æœ€è¿‘5è½®å¯¹è¯
                            messages.extend([
                                Message(role="user", content=ctx['user_query']),
                                Message(role="assistant", content=ctx['assistant_response'])
                            ])
                        messages.append(Message(role="user", content=final_text))
                        
                        query_request = QueryRequest(
                            messages=messages,
                            top_k=5,
                            temperature=0.3,
                            collection_name=getattr(request, 'collection_name', 'default')
                        )
                        
                        # 5.3 æµå¼å¤„ç†RAGæŸ¥è¯¢
                        full_response = ""
                        references = []
                        
                        async for response_chunk in self.retrieve_service.stream_query(query_request):
                            try:
                                chunk_data = json.loads(response_chunk)
                                
                                if chunk_data["type"] == "retrieval_result":
                                    references = chunk_data["data"]["results"][:3]  # ä¿ç•™top3å¼•ç”¨
                                    
                                elif chunk_data["type"] == "generation_chunk":
                                    text_chunk = chunk_data["data"]["text"]
                                    full_response += text_chunk
                                    
                                    # è½¬å‘æ–‡æœ¬å›å¤ç‰‡æ®µ
                                    yield VoiceQueryResponse(
                                        type="llm_response_chunk",
                                        session_id=session_id,
                                        text=text_chunk,
                                        timestamp=int(time.time() * 1000)
                                    )
                                    
                                elif chunk_data["type"] == "generation_done":
                                    full_response = chunk_data["data"]["full_text"]
                                    break
                                    
                            except json.JSONDecodeError:
                                continue
                        
                        # 5.4 å‘é€å®Œæ•´æ–‡æœ¬å›å¤
                        if full_response.strip():
                            yield VoiceQueryResponse(
                                type="llm_response_final",
                                session_id=session_id,
                                text=full_response,
                                references=[
                                    {
                                        "title": ref.get("title", ""),
                                        "content": ref.get("content", "")[:200],
                                        "score": ref.get("score", 0.0)
                                    }
                                    for ref in references
                                ],
                                timestamp=int(time.time() * 1000)
                            )
                            
                            # === ç¬¬å…­é˜¶æ®µï¼šTTSè¯­éŸ³åˆæˆ ===
                            
                            await self._synthesize_and_stream_tts(
                                text=full_response,
                                session_id=session_id,
                                language=session.language,
                                voice_config=self.config.tts_config
                            )
                            
                            # 5.5 æ›´æ–°ä¼šè¯ä¸Šä¸‹æ–‡
                            session.context_history.append({
                                'user_query': final_text,
                                'assistant_response': full_response,
                                'timestamp': datetime.now().isoformat(),
                                'references': references
                            })
                            
                            # é™åˆ¶ä¸Šä¸‹æ–‡å†å²é•¿åº¦
                            if len(session.context_history) > 10:
                                session.context_history = session.context_history[-10:]
                    
                    # æ¸…ç©ºéŸ³é¢‘å’Œè½¬å½•ç¼“å†²åŒº
                    session.audio_buffer = b""
                    session.transcript_buffer = ""
                    
        except Exception as e:
            logger.exception(f"è¯­éŸ³æŸ¥è¯¢å¤„ç†å¤±è´¥: {session_id}", e)
            
            yield VoiceQueryResponse(
                type="error",
                session_id=session_id,
                error=f"è¯­éŸ³å¤„ç†å¤±è´¥: {str(e)}",
                timestamp=int(time.time() * 1000)
            )
        
        finally:
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            total_time = (time.time() - start_time) * 1000
            self.metrics.avg_query_time = (self.metrics.avg_query_time + total_time) / 2
            self.metrics.total_queries += 1
    
    async def _synthesize_and_stream_tts(
        self, 
        text: str, 
        session_id: str, 
        language: str,
        voice_config: dict
    ):
        """
        åˆæˆå¹¶æµå¼å‘é€TTSéŸ³é¢‘
        
        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            session_id: ä¼šè¯ID
            language: è¯­è¨€ä»£ç 
            voice_config: è¯­éŸ³é…ç½®
        """
        try:
            # 1. å‘é€TTSå¼€å§‹äº‹ä»¶
            yield VoiceQueryResponse(
                type="tts_start",
                session_id=session_id,
                text=text,
                timestamp=int(time.time() * 1000)
            )
            
            # 2. æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†æ®µ
            text_segments = self._split_text_for_tts(text, max_length=500)
            
            # 3. æµå¼åˆæˆæ¯ä¸ªæ–‡æœ¬æ®µ
            chunk_index = 0
            for segment in text_segments:
                if not segment.strip():
                    continue
                
                # è°ƒç”¨TTSæœåŠ¡åˆæˆéŸ³é¢‘
                async for audio_chunk in self.tts_service.synthesize_streaming(
                    text=segment,
                    voice_id=voice_config.get("voice_id", "zh-CN-XiaoxiaoNeural"),
                    language=language,
                    rate=voice_config.get("rate", "+0%"),
                    pitch=voice_config.get("pitch", "+0Hz")
                ):
                    if audio_chunk:
                        # Base64ç¼–ç éŸ³é¢‘æ•°æ®
                        audio_b64 = base64.b64encode(audio_chunk).decode('utf-8')
                        
                        yield VoiceQueryResponse(
                            type="tts_audio",
                            session_id=session_id,
                            audio_data=audio_b64,
                            chunk_index=chunk_index,
                            audio_format="mp3",
                            sample_rate=16000,
                            timestamp=int(time.time() * 1000)
                        )
                        
                        chunk_index += 1
                        
                        # æµé‡æ§åˆ¶
                        await asyncio.sleep(0.01)
            
            # 4. å‘é€TTSå®Œæˆäº‹ä»¶
            yield VoiceQueryResponse(
                type="tts_complete",
                session_id=session_id,
                total_chunks=chunk_index,
                timestamp=int(time.time() * 1000)
            )
            
        except Exception as e:
            logger.error(f"TTSåˆæˆå¤±è´¥: {session_id}", error=str(e))
            yield VoiceQueryResponse(
                type="error",
                session_id=session_id,
                error=f"è¯­éŸ³åˆæˆå¤±è´¥: {str(e)}"
            )
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

### æ€§èƒ½æŒ‡æ ‡æ”¶é›†

```python
class AlgoServiceMetrics:
    """ç®—æ³•æœåŠ¡æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.request_count = 0
        self.total_response_time = 0
        self.error_count = 0
        
        # æ£€ç´¢æ€§èƒ½æŒ‡æ ‡
        self.retrieval_metrics = {
            "avg_retrieval_time": 0,
            "vector_search_time": 0, 
            "graph_search_time": 0,
            "rerank_time": 0
        }
        
        # è¯­éŸ³æ€§èƒ½æŒ‡æ ‡
        self.voice_metrics = {
            "avg_asr_latency": 0,
            "avg_tts_latency": 0,
            "active_sessions": 0,
            "total_audio_processed": 0
        }
        
        # LLMæ€§èƒ½æŒ‡æ ‡
        self.llm_metrics = {
            "avg_generation_time": 0,
            "tokens_per_second": 0,
            "total_tokens_generated": 0
        }
```

---

## ğŸ› ï¸ æœ€ä½³å®è·µ

### 1. å¼‚æ­¥ç¼–ç¨‹å®è·µ

```python
# é«˜å¹¶å‘å¼‚æ­¥å¤„ç†
async def process_multiple_requests(requests: List[QueryRequest]) -> List[QueryResponse]:
    """å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚"""
    
    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(10)
    
    async def process_single_request(request):
        async with semaphore:
            return await retrieve_service.process_query(request)
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
    tasks = [process_single_request(req) for req in requests]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    return [r for r in results if not isinstance(r, Exception)]
```

### 2. ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

```python
# å¤šå±‚ç¼“å­˜å®ç°
class MultiLevelCache:
    """å¤šå±‚ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self):
        self.memory_cache = {}  # L1: å†…å­˜ç¼“å­˜
        self.redis_client = redis.Redis()  # L2: Redisç¼“å­˜
        
    async def get(self, key: str) -> Optional[Any]:
        # L1ç¼“å­˜æŸ¥æ‰¾
        if key in self.memory_cache:
            return self.memory_cache[key]
        
        # L2ç¼“å­˜æŸ¥æ‰¾ 
        value = await self.redis_client.get(key)
        if value:
            # å›å¡«L1ç¼“å­˜
            self.memory_cache[key] = json.loads(value)
            return self.memory_cache[key]
        
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600):
        # åŒæ—¶æ›´æ–°ä¸¤çº§ç¼“å­˜
        self.memory_cache[key] = value
        await self.redis_client.setex(key, ttl, json.dumps(value))
```

### 3. é”™è¯¯å¤„ç†å’Œæ¢å¤

```python
# è‡ªåŠ¨é‡è¯•å’Œç†”æ–­æœºåˆ¶
class RetryableService:
    """æ”¯æŒé‡è¯•å’Œç†”æ–­çš„æœåŠ¡åŒ…è£…å™¨"""
    
    def __init__(self, service, max_retries=3, circuit_breaker_threshold=5):
        self.service = service
        self.max_retries = max_retries
        self.circuit_breaker_threshold = circuit_breaker_threshold
        self.failure_count = 0
        self.last_failure_time = None
        
    async def call_with_retry(self, method_name: str, *args, **kwargs):
        """å¸¦é‡è¯•çš„æœåŠ¡è°ƒç”¨"""
        
        # ç†”æ–­æ£€æŸ¥
        if self._is_circuit_open():
            raise ServiceUnavailableError("Service circuit breaker is open")
        
        for attempt in range(self.max_retries + 1):
            try:
                method = getattr(self.service, method_name)
                result = await method(*args, **kwargs)
                
                # æˆåŠŸæ—¶é‡ç½®å¤±è´¥è®¡æ•°
                self.failure_count = 0
                return result
                
            except Exception as e:
                if attempt == self.max_retries:
                    self.failure_count += 1
                    self.last_failure_time = time.time()
                    raise
                
                # æŒ‡æ•°é€€é¿
                await asyncio.sleep(2 ** attempt)
    
    def _is_circuit_open(self) -> bool:
        """æ£€æŸ¥ç†”æ–­å™¨æ˜¯å¦å¼€å¯"""
        if self.failure_count < self.circuit_breaker_threshold:
            return False
        
        # ç†”æ–­æ¢å¤æ£€æŸ¥ï¼ˆ30ç§’åå°è¯•æ¢å¤ï¼‰
        if time.time() - self.last_failure_time > 30:
            self.failure_count = 0
            return False
            
        return True
```

è¿™ä»½Pythonç®—æ³•æœåŠ¡çš„è¯¦ç»†åˆ†ææ¶µç›–äº†æ ¸å¿ƒAPIå®ç°ã€GraphRAGæ¨ç†å¼•æ“ã€è¯­éŸ³å¤„ç†æ¨¡å—ã€æ€§èƒ½ä¼˜åŒ–å’Œæœ€ä½³å®è·µï¼Œä¸ºå¼€å‘è€…æä¾›äº†æ·±å…¥ç†è§£ç³»ç»Ÿæ¶æ„å’Œå®ç°ç»†èŠ‚çš„å®Œæ•´æŒ‡å—ã€‚
