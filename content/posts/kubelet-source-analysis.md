# Kubernetes Kubelet è¯¦ç»†æºç å‰–æ

## ğŸ“š æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£æ·±å…¥åˆ†æ Kubernetes Kubelet çš„æ¶æ„è®¾è®¡ã€æºç å®ç°å’Œæ ¸å¿ƒæœºåˆ¶ã€‚Kubelet æ˜¯ Kubernetes é›†ç¾¤ä¸­æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„ä¸»è¦ä»£ç†ï¼Œè´Ÿè´£ç®¡ç† Pod å’Œå®¹å™¨çš„ç”Ÿå‘½å‘¨æœŸï¼Œæ˜¯è¿æ¥ Kubernetes æ§åˆ¶å¹³é¢å’Œå®¹å™¨è¿è¡Œæ—¶çš„å…³é”®æ¡¥æ¢ã€‚

## ğŸ—ï¸ Kubelet æ•´ä½“æ¶æ„

### 1.1 Kubelet æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "kubelet æ¶æ„"
        subgraph "æ ¸å¿ƒç»„ä»¶ (Core Components)"
            KUBELET[Kubelet<br/>æ ¸å¿ƒæ§åˆ¶å™¨]
            POD_MANAGER[Pod Manager<br/>Pod ç®¡ç†å™¨]
            POD_WORKERS[Pod Workers<br/>Pod å·¥ä½œå™¨]
            SYNC_LOOP[Sync Loop<br/>åŒæ­¥å¾ªç¯]
        end
        
        subgraph "å®¹å™¨ç®¡ç† (Container Management)"
            RUNTIME[Container Runtime<br/>å®¹å™¨è¿è¡Œæ—¶]
            CRI[CRI Interface<br/>CRI æ¥å£]
            IMAGE_MANAGER[Image Manager<br/>é•œåƒç®¡ç†å™¨]
            CONTAINER_GC[Container GC<br/>å®¹å™¨åƒåœ¾æ”¶é›†]
        end
        
        subgraph "å­˜å‚¨ç®¡ç† (Volume Management)"
            VOLUME_MANAGER[Volume Manager<br/>å­˜å‚¨å·ç®¡ç†å™¨]
            CSI_DRIVER[CSI Driver<br/>CSI é©±åŠ¨]
            MOUNT_MANAGER[Mount Manager<br/>æŒ‚è½½ç®¡ç†å™¨]
            DEVICE_MANAGER[Device Manager<br/>è®¾å¤‡ç®¡ç†å™¨]
        end
        
        subgraph "ç½‘ç»œç®¡ç† (Network Management)"
            CNI_MANAGER[CNI Manager<br/>CNI ç®¡ç†å™¨]
            DNS_CONFIG[DNS Config<br/>DNS é…ç½®]
            PORT_MANAGER[Port Manager<br/>ç«¯å£ç®¡ç†å™¨]
        end
        
        subgraph "èµ„æºç®¡ç† (Resource Management)"
            CGROUP_MANAGER[CGroup Manager<br/>CGroup ç®¡ç†å™¨]
            CPU_MANAGER[CPU Manager<br/>CPU ç®¡ç†å™¨]
            MEMORY_MANAGER[Memory Manager<br/>å†…å­˜ç®¡ç†å™¨]
            TOPOLOGY_MANAGER[Topology Manager<br/>æ‹“æ‰‘ç®¡ç†å™¨]
        end
        
        subgraph "å¥åº·æ£€æŸ¥ (Health Checks)"
            LIVENESS_PROBER[Liveness Prober<br/>å­˜æ´»æ¢é’ˆ]
            READINESS_PROBER[Readiness Prober<br/>å°±ç»ªæ¢é’ˆ]
            STARTUP_PROBER[Startup Prober<br/>å¯åŠ¨æ¢é’ˆ]
        end
        
        subgraph "ç›‘æ§è§‚æµ‹ (Observability)"
            CADVISOR[cAdvisor<br/>å®¹å™¨ç›‘æ§]
            METRICS_SERVER[Metrics Server<br/>æŒ‡æ ‡æœåŠ¡å™¨]
            EVENT_RECORDER[Event Recorder<br/>äº‹ä»¶è®°å½•å™¨]
            LOG_MANAGER[Log Manager<br/>æ—¥å¿—ç®¡ç†å™¨]
        end
        
        subgraph "å¤–éƒ¨æ¥å£ (External Interfaces)"
            API_CLIENT[API Client<br/>API å®¢æˆ·ç«¯]
            KUBELET_API[Kubelet API<br/>Kubelet API æœåŠ¡å™¨]
            PLUGIN_MANAGER[Plugin Manager<br/>æ’ä»¶ç®¡ç†å™¨]
        end
    end
    
    %% æ ¸å¿ƒæµç¨‹
    KUBELET --> POD_MANAGER
    POD_MANAGER --> POD_WORKERS
    POD_WORKERS --> SYNC_LOOP
    
    %% å®¹å™¨ç®¡ç†
    KUBELET --> RUNTIME
    RUNTIME --> CRI
    KUBELET --> IMAGE_MANAGER
    KUBELET --> CONTAINER_GC
    
    %% å­˜å‚¨ç®¡ç†
    KUBELET --> VOLUME_MANAGER
    VOLUME_MANAGER --> CSI_DRIVER
    VOLUME_MANAGER --> MOUNT_MANAGER
    KUBELET --> DEVICE_MANAGER
    
    %% ç½‘ç»œç®¡ç†
    KUBELET --> CNI_MANAGER
    KUBELET --> DNS_CONFIG
    KUBELET --> PORT_MANAGER
    
    %% èµ„æºç®¡ç†
    KUBELET --> CGROUP_MANAGER
    CGROUP_MANAGER --> CPU_MANAGER
    CGROUP_MANAGER --> MEMORY_MANAGER
    CGROUP_MANAGER --> TOPOLOGY_MANAGER
    
    %% å¥åº·æ£€æŸ¥
    KUBELET --> LIVENESS_PROBER
    KUBELET --> READINESS_PROBER
    KUBELET --> STARTUP_PROBER
    
    %% ç›‘æ§è§‚æµ‹
    KUBELET --> CADVISOR
    KUBELET --> METRICS_SERVER
    KUBELET --> EVENT_RECORDER
    KUBELET --> LOG_MANAGER
    
    %% å¤–éƒ¨æ¥å£
    KUBELET --> API_CLIENT
    KUBELET --> KUBELET_API
    KUBELET --> PLUGIN_MANAGER
    
    %% æ ·å¼å®šä¹‰
    classDef core fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef container fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef storage fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef network fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef resource fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef health fill:#f1f8e9,stroke:#689f38,stroke-width:2px
    classDef observability fill:#fafafa,stroke:#616161,stroke-width:2px
    classDef external fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    
    class KUBELET,POD_MANAGER,POD_WORKERS,SYNC_LOOP core
    class RUNTIME,CRI,IMAGE_MANAGER,CONTAINER_GC container
    class VOLUME_MANAGER,CSI_DRIVER,MOUNT_MANAGER,DEVICE_MANAGER storage
    class CNI_MANAGER,DNS_CONFIG,PORT_MANAGER network
    class CGROUP_MANAGER,CPU_MANAGER,MEMORY_MANAGER,TOPOLOGY_MANAGER resource
    class LIVENESS_PROBER,READINESS_PROBER,STARTUP_PROBER health
    class CADVISOR,METRICS_SERVER,EVENT_RECORDER,LOG_MANAGER observability
    class API_CLIENT,KUBELET_API,PLUGIN_MANAGER external
```

### 1.2 Kubelet å·¥ä½œæµç¨‹

```mermaid
sequenceDiagram
    participant API as API Server
    participant Kubelet as Kubelet
    participant PodManager as Pod Manager
    participant PodWorkers as Pod Workers
    participant Runtime as Container Runtime
    participant VolumeManager as Volume Manager
    participant Prober as Health Prober

    Note over API,Prober: Pod ç”Ÿå‘½å‘¨æœŸç®¡ç†æµç¨‹

    API->>+Kubelet: 1. Pod åˆ›å»º/æ›´æ–°äº‹ä»¶
    Kubelet->>+PodManager: 2. æ›´æ–° Pod é…ç½®
    PodManager->>PodManager: 3. éªŒè¯ Pod è§„æ ¼
    PodManager-->>-Kubelet: Pod é…ç½®å·²æ›´æ–°

    Kubelet->>+PodWorkers: 4. åˆ†å‘ Pod å·¥ä½œä»»åŠ¡
    PodWorkers->>PodWorkers: 5. åˆ›å»º Pod å·¥ä½œåç¨‹
    
    par Pod åŒæ­¥å¤„ç†
        PodWorkers->>+VolumeManager: 6a. å‡†å¤‡å­˜å‚¨å·
        VolumeManager->>VolumeManager: æŒ‚è½½å­˜å‚¨å·
        VolumeManager-->>-PodWorkers: å­˜å‚¨å·å°±ç»ª
        
        PodWorkers->>+Runtime: 6b. åˆ›å»º Pod æ²™ç®±
        Runtime->>Runtime: è®¾ç½®ç½‘ç»œå‘½åç©ºé—´
        Runtime-->>-PodWorkers: æ²™ç®±åˆ›å»ºå®Œæˆ
        
        PodWorkers->>+Runtime: 6c. åˆ›å»ºå®¹å™¨
        Runtime->>Runtime: æ‹‰å–é•œåƒ
        Runtime->>Runtime: åˆ›å»ºå¹¶å¯åŠ¨å®¹å™¨
        Runtime-->>-PodWorkers: å®¹å™¨å¯åŠ¨å®Œæˆ
    end
    
    PodWorkers-->>-Kubelet: 7. Pod åŒæ­¥å®Œæˆ
    
    Kubelet->>+Prober: 8. å¯åŠ¨å¥åº·æ£€æŸ¥
    loop å¥åº·æ£€æŸ¥å¾ªç¯
        Prober->>Runtime: æ‰§è¡Œå­˜æ´»æ¢é’ˆ
        Prober->>Runtime: æ‰§è¡Œå°±ç»ªæ¢é’ˆ
        Prober->>Kubelet: æ›´æ–°æ¢é’ˆç»“æœ
    end
    Prober-->>-Kubelet: å¥åº·æ£€æŸ¥å·²å¯åŠ¨
    
    Kubelet->>+API: 9. æ›´æ–° Pod çŠ¶æ€
    API-->>-Kubelet: çŠ¶æ€æ›´æ–°ç¡®è®¤
    
    Note over API,Prober: Pod è¿è¡Œä¸­ï¼ŒæŒç»­ç›‘æ§å’Œç®¡ç†
    
    loop åŒæ­¥å¾ªç¯
        Kubelet->>PodManager: 10. æ£€æŸ¥ Pod çŠ¶æ€
        Kubelet->>Runtime: 11. åŒæ­¥å®¹å™¨çŠ¶æ€
        Kubelet->>VolumeManager: 12. æ£€æŸ¥å­˜å‚¨å·çŠ¶æ€
        Kubelet->>API: 13. ä¸ŠæŠ¥èŠ‚ç‚¹å’Œ Pod çŠ¶æ€
    end
```

## ğŸš€ å¯åŠ¨æµç¨‹è¯¦ç»†åˆ†æ

### 2.1 Kubelet å¯åŠ¨å…¥å£

```go
// cmd/kubelet/kubelet.go
/*
Kubelet ä¸»å…¥å£æ–‡ä»¶
è´Ÿè´£åˆå§‹åŒ–å’Œå¯åŠ¨ Kubelet æœåŠ¡

ä¸»è¦èŒè´£ï¼š
1. åˆ›å»º Cobra å‘½ä»¤å¯¹è±¡
2. è§£æå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®
3. å¯åŠ¨ Kubelet ä¸»å¾ªç¯
*/
package main

import (
    "context"
    "os"

    "k8s.io/component-base/cli"
    _ "k8s.io/component-base/logs/json/register"          // JSON æ—¥å¿—æ ¼å¼æ³¨å†Œ
    _ "k8s.io/component-base/metrics/prometheus/clientgo" // å®¢æˆ·ç«¯æŒ‡æ ‡æ³¨å†Œ
    _ "k8s.io/component-base/metrics/prometheus/version"  // ç‰ˆæœ¬æŒ‡æ ‡æ³¨å†Œ
    "k8s.io/kubernetes/cmd/kubelet/app"
)

/*
main å‡½æ•°æ˜¯ Kubelet çš„ç¨‹åºå…¥å£ç‚¹

æ‰§è¡Œæµç¨‹ï¼š
1. åˆ›å»º Kubelet å‘½ä»¤å¯¹è±¡
2. é€šè¿‡ CLI æ¡†æ¶æ‰§è¡Œå‘½ä»¤
3. æ ¹æ®æ‰§è¡Œç»“æœé€€å‡ºç¨‹åº

è¿”å›å€¼ï¼š
- ç¨‹åºé€€å‡ºç ï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼Œé 0 è¡¨ç¤ºå¤±è´¥ï¼‰
*/
func main() {
    // åˆ›å»º Kubelet å‘½ä»¤å¯¹è±¡ï¼Œä¼ å…¥ä¸Šä¸‹æ–‡
    command := app.NewKubeletCommand(context.Background())
    
    // æ‰§è¡Œå‘½ä»¤ï¼Œå¯åŠ¨ Kubelet
    code := cli.Run(command)
    
    // æ ¹æ®æ‰§è¡Œç»“æœé€€å‡ºç¨‹åº
    os.Exit(code)
}
```

### 2.2 Kubelet æ ¸å¿ƒç»“æ„

```go
// pkg/kubelet/kubelet.go
/*
Kubelet ç»“æ„ä½“å®šä¹‰äº† Kubelet çš„æ ¸å¿ƒç»„ä»¶å’ŒåŠŸèƒ½

ä¸»è¦åŠŸèƒ½ï¼š
1. ç®¡ç† Pod å’Œå®¹å™¨çš„ç”Ÿå‘½å‘¨æœŸ
2. ä¸å®¹å™¨è¿è¡Œæ—¶äº¤äº’
3. ç›‘æ§èŠ‚ç‚¹å’Œå®¹å™¨çŠ¶æ€
4. å¤„ç†å­˜å‚¨å·å’Œç½‘ç»œé…ç½®
5. æ‰§è¡Œå¥åº·æ£€æŸ¥å’Œèµ„æºç®¡ç†
*/

/*
Kubelet æ ¸å¿ƒç»“æ„ä½“

å­—æ®µè¯´æ˜ï¼š
- hostname: èŠ‚ç‚¹ä¸»æœºå
- nodeName: èŠ‚ç‚¹åç§°
- runtimeService: å®¹å™¨è¿è¡Œæ—¶æœåŠ¡æ¥å£
- imageService: é•œåƒæœåŠ¡æ¥å£
- kubeClient: Kubernetes API å®¢æˆ·ç«¯
- heartbeatClient: å¿ƒè·³å®¢æˆ·ç«¯
- podManager: Pod ç®¡ç†å™¨
- podWorkers: Pod å·¥ä½œå™¨
- containerRuntime: å®¹å™¨è¿è¡Œæ—¶
- volumeManager: å­˜å‚¨å·ç®¡ç†å™¨
- probeManager: æ¢é’ˆç®¡ç†å™¨
- pleg: Pod ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ç”Ÿæˆå™¨
- statusManager: çŠ¶æ€ç®¡ç†å™¨
- resourceAnalyzer: èµ„æºåˆ†æå™¨
*/
type Kubelet struct {
    // èŠ‚ç‚¹ä¿¡æ¯
    hostname                       string
    nodeName                       types.NodeName
    runtimeState                   *runtimeState
    
    // å®¹å™¨è¿è¡Œæ—¶æ¥å£
    runtimeService                 internalapi.RuntimeService
    imageService                   internalapi.ImageManagerService
    
    // Kubernetes API å®¢æˆ·ç«¯
    kubeClient                     clientset.Interface
    heartbeatClient                clientset.Interface
    
    // æ ¸å¿ƒç®¡ç†å™¨
    podManager                     kubepod.Manager
    podWorkers                     PodWorkers
    containerRuntime               kubecontainer.Runtime
    
    // å­˜å‚¨å’Œç½‘ç»œç®¡ç†
    volumeManager                  volumemanager.VolumeManager
    volumePluginMgr                *volume.VolumePluginMgr
    
    // å¥åº·æ£€æŸ¥å’Œç›‘æ§
    probeManager                   prober.Manager
    livenessManager                proberesults.Manager
    readinessManager               proberesults.Manager
    startupManager                 proberesults.Manager
    
    // ç”Ÿå‘½å‘¨æœŸäº‹ä»¶
    pleg                          pleg.PodLifecycleEventGenerator
    
    // çŠ¶æ€ç®¡ç†
    statusManager                  status.Manager
    nodeStatusUpdateFrequency      time.Duration
    
    // èµ„æºç®¡ç†
    resourceAnalyzer               stats.ResourceAnalyzer
    containerManager               cm.ContainerManager
    
    // ç½‘ç»œé…ç½®
    clusterDNS                     []net.IP
    clusterDomain                  string
    resolverConfig                 string
    
    // é…ç½®å’Œç­–ç•¥
    kubeletConfiguration           kubeletconfiginternal.KubeletConfiguration
    
    // äº‹ä»¶å’Œæ—¥å¿—
    recorder                       record.EventRecorder
    
    // åŒæ­¥å’Œæ§åˆ¶
    syncLoopMonitor                atomic.Value
    daemonEndpoints                *v1.NodeDaemonEndpoints
    
    // æœåŠ¡å™¨å’Œ API
    server                         *server.Server
    
    // å…¶ä»–ç»„ä»¶
    cadvisor                       cadvisor.Interface
    cloud                          cloudprovider.Interface
    
    // åŒæ­¥æ§åˆ¶
    mainLock                       sync.RWMutex
    updatePodCIDRMux               sync.Mutex
    
    // åœæ­¢ä¿¡å·
    stopCh                         <-chan struct{}
}
```

### 2.3 Kubelet å¯åŠ¨æµç¨‹

```go
/*
Run å¯åŠ¨ Kubelet çš„ä¸»è¿è¡Œå¾ªç¯

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼Œç”¨äºæ§åˆ¶ç”Ÿå‘½å‘¨æœŸ

å·¥ä½œæµç¨‹ï¼š
1. åˆå§‹åŒ–å„ç§ç®¡ç†å™¨
2. å¯åŠ¨ HTTP æœåŠ¡å™¨
3. å¯åŠ¨åŒæ­¥å¾ªç¯
4. ç­‰å¾…åœæ­¢ä¿¡å·
5. æ¸…ç†èµ„æº
*/
func (kl *Kubelet) Run(ctx context.Context) {
    klog.InfoS("å¯åŠ¨ kubelet")
    
    // 1. åˆå§‹åŒ–äº‘æä¾›å•†
    if kl.cloud != nil {
        kl.cloud.Initialize(kl.kubeClient, ctx.Done())
    }
    
    // 2. åˆå§‹åŒ–æ¨¡å—
    if err := kl.initializeModules(); err != nil {
        klog.ErrorS(err, "åˆå§‹åŒ–æ¨¡å—å¤±è´¥")
        return
    }
    
    // 3. å¯åŠ¨å­˜å‚¨å·ç®¡ç†å™¨
    go kl.volumeManager.Run(kl.sourcesReady, ctx.Done())
    
    // 4. å¯åŠ¨ Pod ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ç”Ÿæˆå™¨
    kl.pleg.Start()
    
    // 5. å¯åŠ¨åŒæ­¥å¾ªç¯
    kl.syncLoop(ctx, kl.configCh, kl.handler)
}

/*
initializeModules åˆå§‹åŒ– Kubelet çš„å„ä¸ªæ¨¡å—

è¿”å›å€¼ï¼š
- error: åˆå§‹åŒ–è¿‡ç¨‹ä¸­çš„é”™è¯¯

åˆå§‹åŒ–æµç¨‹ï¼š
1. è®¾ç½® iptables è§„åˆ™
2. åˆå§‹åŒ–å®¹å™¨ç®¡ç†å™¨
3. å¯åŠ¨å®¹å™¨è¿è¡Œæ—¶
4. åˆå§‹åŒ–æ¢é’ˆç®¡ç†å™¨
5. å¯åŠ¨çŠ¶æ€ç®¡ç†å™¨
6. å¯åŠ¨å…¶ä»–ç®¡ç†å™¨
*/
func (kl *Kubelet) initializeModules() error {
    // 1. åˆå§‹åŒ–å®¹å™¨ç®¡ç†å™¨
    if err := kl.containerManager.Start(kl.node, kl.GetActivePods, kl.sourcesReady, 
        kl.statusManager, kl.runtimeService, kl.supportLocalStorageCapacityIsolation()); err != nil {
        return fmt.Errorf("å¯åŠ¨å®¹å™¨ç®¡ç†å™¨å¤±è´¥: %w", err)
    }
    
    // 2. å¯åŠ¨é•œåƒç®¡ç†å™¨
    kl.imageManager.Start()
    
    // 3. å¯åŠ¨è¯ä¹¦ç®¡ç†å™¨
    if kl.serverCertificateManager != nil {
        kl.serverCertificateManager.Start()
    }
    
    // 4. å¯åŠ¨æ¢é’ˆç®¡ç†å™¨
    kl.probeManager.Start()
    
    // 5. å¯åŠ¨çŠ¶æ€ç®¡ç†å™¨
    kl.statusManager.Start()
    
    // 6. å¯åŠ¨ Pod å·¥ä½œå™¨
    kl.podWorkers.Start()
    
    // 7. å¯åŠ¨è¿è¡Œæ—¶ç±»ç®¡ç†å™¨
    if kl.runtimeClassManager != nil {
        kl.runtimeClassManager.Start(kl.stopCh)
    }
    
    return nil
}

/*
syncLoop æ˜¯ Kubelet çš„ä¸»åŒæ­¥å¾ªç¯

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- configCh: é…ç½®å˜æ›´é€šé“
- handler: åŒæ­¥å¤„ç†å™¨

å·¥ä½œæµç¨‹ï¼š
1. ç›‘å¬å„ç§äº‹ä»¶æº
2. å¤„ç† Pod é…ç½®å˜æ›´
3. æ‰§è¡Œå®šæœŸåŒæ­¥ä»»åŠ¡
4. å¤„ç†æ¢é’ˆç»“æœ
5. æ¸…ç†å­¤å„¿ Pod
*/
func (kl *Kubelet) syncLoop(ctx context.Context, configCh <-chan kubetypes.PodUpdate, handler SyncHandler) {
    klog.InfoS("å¯åŠ¨ kubelet åŒæ­¥å¾ªç¯")
    
    // åŒæ­¥å¾ªç¯ç›‘æ§å™¨
    syncTicker := time.NewTicker(time.Second)
    defer syncTicker.Stop()
    
    housekeepingTicker := time.NewTicker(housekeepingPeriod)
    defer housekeepingTicker.Stop()
    
    plegCh := kl.pleg.Watch()
    
    const (
        base   = 100 * time.Millisecond
        max    = 5 * time.Second
        factor = 2
    )
    duration := base
    
    // è®°å½•åŒæ­¥å¾ªç¯å¼€å§‹æ—¶é—´
    kl.syncLoopMonitor.Store(kl.clock.Now())
    
    for {
        select {
        case <-ctx.Done():
            klog.InfoS("Kubelet åŒæ­¥å¾ªç¯é€€å‡º")
            return
            
        case u, open := <-configCh:
            // å¤„ç†é…ç½®æ›´æ–°
            if !open {
                klog.ErrorS(nil, "é…ç½®é€šé“æ„å¤–å…³é—­")
                return
            }
            
            switch u.Op {
            case kubetypes.ADD:
                klog.V(2).InfoS("SyncLoop ADD", "source", u.Source, "pods", klog.KObjSlice(u.Pods))
            case kubetypes.UPDATE:
                klog.V(2).InfoS("SyncLoop UPDATE", "source", u.Source, "pods", klog.KObjSlice(u.Pods))
            case kubetypes.REMOVE:
                klog.V(2).InfoS("SyncLoop REMOVE", "source", u.Source, "pods", klog.KObjSlice(u.Pods))
            case kubetypes.RECONCILE:
                klog.V(4).InfoS("SyncLoop RECONCILE", "source", u.Source, "pods", klog.KObjSlice(u.Pods))
            case kubetypes.DELETE:
                klog.V(2).InfoS("SyncLoop DELETE", "source", u.Source, "pods", klog.KObjSlice(u.Pods))
            case kubetypes.SET:
                klog.V(2).InfoS("SyncLoop SET", "source", u.Source, "pods", klog.KObjSlice(u.Pods))
            default:
                klog.ErrorS(nil, "æ— æ•ˆçš„æ“ä½œç±»å‹", "operation", u.Op)
            }
            
            kl.podManager.UpdatePods(u)
            
        case e := <-plegCh:
            // å¤„ç† Pod ç”Ÿå‘½å‘¨æœŸäº‹ä»¶
            if e.Type == pleg.ContainerStarted {
                // è®°å½•å®¹å™¨å¯åŠ¨æ—¶é—´
                kl.lastContainerStartedTime.Store(kl.clock.Now())
            }
            
            if isSyncPodWorthy(e) {
                // å¦‚æœäº‹ä»¶å€¼å¾—åŒæ­¥ï¼Œè§¦å‘ Pod åŒæ­¥
                if pod, ok := kl.podManager.GetPodByUID(e.ID); ok {
                    kl.podWorkers.UpdatePod(UpdatePodOptions{
                        Pod:        pod,
                        MirrorPod:  kl.podManager.GetMirrorPodByPod(pod),
                        UpdateType: kubetypes.SyncPodSync,
                        StartTime:  time.Now(),
                    })
                }
            }
            
            if e.Type == pleg.ContainerDied {
                if containerID, ok := e.Data.(string); ok {
                    kl.cleanUpContainersInPod(e.ID, containerID)
                }
            }
            
        case <-syncTicker.C:
            // å®šæœŸåŒæ­¥
            kl.syncLoopMonitor.Store(kl.clock.Now())
            if !kl.syncLoopIteration(ctx, handler, syncTicker.C, housekeepingTicker.C, plegCh) {
                break
            }
            duration = base
            
        case update := <-kl.livenessManager.Updates():
            // å¤„ç†å­˜æ´»æ¢é’ˆç»“æœ
            if update.Result == proberesults.Failure {
                handleProbeSync(kl, update, handler, "liveness", "unhealthy")
            }
            
        case update := <-kl.readinessManager.Updates():
            // å¤„ç†å°±ç»ªæ¢é’ˆç»“æœ
            ready := update.Result == proberesults.Success
            kl.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready)
            
        case update := <-kl.startupManager.Updates():
            // å¤„ç†å¯åŠ¨æ¢é’ˆç»“æœ
            started := update.Result == proberesults.Success
            kl.statusManager.SetContainerStartup(update.PodUID, update.ContainerID, started)
            
        case <-housekeepingTicker.C:
            // æ‰§è¡Œæ¸…ç†ä»»åŠ¡
            if !kl.sourcesReady.AllReady() {
                // å¦‚æœæºæœªå°±ç»ªï¼Œè·³è¿‡æ¸…ç†
                klog.V(4).InfoS("SyncLoop (housekeeping, skipped): sources aren't ready yet.")
            } else {
                start := time.Now()
                klog.V(4).InfoS("SyncLoop (housekeeping)")
                if err := handler.HandlePodCleanups(ctx); err != nil {
                    klog.ErrorS(err, "æ¸…ç† Pod å¤±è´¥")
                }
                duration := time.Since(start)
                if duration > housekeepingWarningDuration {
                    klog.ErrorS(nil, "æ¸…ç†ä»»åŠ¡è€—æ—¶è¿‡é•¿", "duration", duration.String())
                }
            }
        }
    }
}
```

## ğŸ¯ Pod ç”Ÿå‘½å‘¨æœŸç®¡ç†

### 3.1 Pod å·¥ä½œå™¨æ¶æ„

```mermaid
graph TB
    subgraph "Pod Workers æ¶æ„"
        subgraph "å·¥ä½œå™¨ç®¡ç† (Worker Management)"
            POD_WORKERS[Pod Workers<br/>Pod å·¥ä½œå™¨ç®¡ç†å™¨]
            WORKER_POOL[Worker Pool<br/>å·¥ä½œå™¨æ± ]
            UPDATE_QUEUE[Update Queue<br/>æ›´æ–°é˜Ÿåˆ—]
        end
        
        subgraph "Pod åŒæ­¥ (Pod Sync)"
            SYNC_POD[Sync Pod<br/>Pod åŒæ­¥]
            CREATE_POD[Create Pod<br/>åˆ›å»º Pod]
            UPDATE_POD[Update Pod<br/>æ›´æ–° Pod]
            DELETE_POD[Delete Pod<br/>åˆ é™¤ Pod]
        end
        
        subgraph "å®¹å™¨ç®¡ç† (Container Management)"
            SANDBOX[Pod Sandbox<br/>Pod æ²™ç®±]
            INIT_CONTAINERS[Init Containers<br/>åˆå§‹åŒ–å®¹å™¨]
            APP_CONTAINERS[App Containers<br/>åº”ç”¨å®¹å™¨]
            SIDECAR_CONTAINERS[Sidecar Containers<br/>è¾¹è½¦å®¹å™¨]
        end
        
        subgraph "ç”Ÿå‘½å‘¨æœŸé’©å­ (Lifecycle Hooks)"
            POST_START[PostStart Hook<br/>å¯åŠ¨åé’©å­]
            PRE_STOP[PreStop Hook<br/>åœæ­¢å‰é’©å­]
            TERMINATION[Termination<br/>ç»ˆæ­¢å¤„ç†]
        end
        
        subgraph "çŠ¶æ€ç®¡ç† (Status Management)"
            POD_STATUS[Pod Status<br/>Pod çŠ¶æ€]
            CONTAINER_STATUS[Container Status<br/>å®¹å™¨çŠ¶æ€]
            CONDITION_MANAGER[Condition Manager<br/>æ¡ä»¶ç®¡ç†å™¨]
        end
    end
    
    %% å·¥ä½œæµç¨‹
    POD_WORKERS --> WORKER_POOL
    WORKER_POOL --> UPDATE_QUEUE
    UPDATE_QUEUE --> SYNC_POD
    
    %% Pod åŒæ­¥æµç¨‹
    SYNC_POD --> CREATE_POD
    SYNC_POD --> UPDATE_POD
    SYNC_POD --> DELETE_POD
    
    %% å®¹å™¨ç®¡ç†æµç¨‹
    CREATE_POD --> SANDBOX
    SANDBOX --> INIT_CONTAINERS
    INIT_CONTAINERS --> APP_CONTAINERS
    APP_CONTAINERS --> SIDECAR_CONTAINERS
    
    %% ç”Ÿå‘½å‘¨æœŸé’©å­
    APP_CONTAINERS --> POST_START
    DELETE_POD --> PRE_STOP
    PRE_STOP --> TERMINATION
    
    %% çŠ¶æ€ç®¡ç†
    SYNC_POD --> POD_STATUS
    POD_STATUS --> CONTAINER_STATUS
    CONTAINER_STATUS --> CONDITION_MANAGER
    
    %% æ ·å¼å®šä¹‰
    classDef management fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef sync fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef container fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef lifecycle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef status fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    
    class POD_WORKERS,WORKER_POOL,UPDATE_QUEUE management
    class SYNC_POD,CREATE_POD,UPDATE_POD,DELETE_POD sync
    class SANDBOX,INIT_CONTAINERS,APP_CONTAINERS,SIDECAR_CONTAINERS container
    class POST_START,PRE_STOP,TERMINATION lifecycle
    class POD_STATUS,CONTAINER_STATUS,CONDITION_MANAGER status
```

### 3.2 Pod å·¥ä½œå™¨æºç å®ç°

```go
// pkg/kubelet/pod_workers.go
/*
PodWorkers ç®¡ç† Pod çš„å¹¶å‘å¤„ç†

ä¸»è¦åŠŸèƒ½ï¼š
1. ä¸ºæ¯ä¸ª Pod åˆ›å»ºç‹¬ç«‹çš„å·¥ä½œåç¨‹
2. ç®¡ç† Pod çš„æ›´æ–°é˜Ÿåˆ—
3. åè°ƒ Pod çš„ç”Ÿå‘½å‘¨æœŸæ“ä½œ
4. å¤„ç† Pod çš„ç»ˆæ­¢å’Œæ¸…ç†
*/

/*
PodWorkers ç»“æ„ä½“å®šä¹‰

å­—æ®µè¯´æ˜ï¼š
- podLock: Pod é”æ˜ å°„ï¼Œä¿æŠ¤æ¯ä¸ª Pod çš„å¹¶å‘è®¿é—®
- podsSynced: å·²åŒæ­¥çš„ Pod é›†åˆ
- startedStaticPodsByFullname: å·²å¯åŠ¨çš„é™æ€ Pod æ˜ å°„
- waitingToStartStaticPodsByFullname: ç­‰å¾…å¯åŠ¨çš„é™æ€ Pod æ˜ å°„
- workQueue: å·¥ä½œé˜Ÿåˆ—ï¼Œå­˜å‚¨å¾…å¤„ç†çš„ Pod æ›´æ–°
- podUpdates: Pod æ›´æ–°é€šé“æ˜ å°„
- isWorking: å·¥ä½œçŠ¶æ€æ˜ å°„
- lastUndeliveredWorkUpdate: æœ€åæœªäº¤ä»˜çš„å·¥ä½œæ›´æ–°æ˜ å°„
- podSyncStatuses: Pod åŒæ­¥çŠ¶æ€æ˜ å°„
*/
type PodWorkers interface {
    // UpdatePod é€šçŸ¥ Pod å·¥ä½œå™¨æœ‰å…³ Pod çš„æ›´æ–°
    UpdatePod(options UpdatePodOptions)
    
    // ForgetNonExistingPodWorkers æ¸…ç†ä¸å­˜åœ¨çš„ Pod å·¥ä½œå™¨
    ForgetNonExistingPodWorkers(desiredPods map[types.UID]sets.Empty)
    
    // ForgetWorker å¿˜è®°ç‰¹å®šçš„ Pod å·¥ä½œå™¨
    ForgetWorker(uid types.UID)
    
    // IsPodKnownTerminated æ£€æŸ¥ Pod æ˜¯å¦å·²çŸ¥å·²ç»ˆæ­¢
    IsPodKnownTerminated(uid types.UID) bool
    
    // CouldHaveRunningContainers æ£€æŸ¥ Pod æ˜¯å¦å¯èƒ½æœ‰è¿è¡Œä¸­çš„å®¹å™¨
    CouldHaveRunningContainers(uid types.UID) bool
    
    // ShouldPodContentBeRemoved æ£€æŸ¥æ˜¯å¦åº”è¯¥ç§»é™¤ Pod å†…å®¹
    ShouldPodContentBeRemoved(uid types.UID) bool
    
    // IsPodTerminationRequested æ£€æŸ¥æ˜¯å¦è¯·æ±‚äº† Pod ç»ˆæ­¢
    IsPodTerminationRequested(uid types.UID) bool
    
    // Start å¯åŠ¨ Pod å·¥ä½œå™¨
    Start()
}

/*
podWorkers å®ç° PodWorkers æ¥å£

å­—æ®µè¯´æ˜ï¼š
- podLock: ä¿æŠ¤ Pod å·¥ä½œå™¨çŠ¶æ€çš„è¯»å†™é”
- podsSynced: å·²åŒæ­¥ Pod çš„é›†åˆ
- startedStaticPodsByFullname: å·²å¯åŠ¨é™æ€ Pod çš„æ˜ å°„
- waitingToStartStaticPodsByFullname: ç­‰å¾…å¯åŠ¨é™æ€ Pod çš„æ˜ å°„
- workQueue: Pod å·¥ä½œé˜Ÿåˆ—
- podUpdates: Pod æ›´æ–°é€šé“æ˜ å°„
- isWorking: Pod å·¥ä½œçŠ¶æ€æ˜ å°„
- lastUndeliveredWorkUpdate: æœ€åæœªäº¤ä»˜å·¥ä½œæ›´æ–°æ˜ å°„
- podSyncStatuses: Pod åŒæ­¥çŠ¶æ€æ˜ å°„
*/
type podWorkers struct {
    // ç”¨äºä¿æŠ¤ä¸‹é¢å­—æ®µçš„é”
    podLock sync.RWMutex
    
    // è·Ÿè¸ªå·²åŒæ­¥çš„ Pod
    podsSynced map[types.UID]sets.Empty
    
    // è·Ÿè¸ªå·²å¯åŠ¨çš„é™æ€ Pod
    startedStaticPodsByFullname map[string]types.UID
    waitingToStartStaticPodsByFullname map[string][]types.UID
    
    // å·¥ä½œé˜Ÿåˆ—å’Œé€šé“
    workQueue queue.WorkQueue
    
    // æ¯ä¸ª Pod çš„æ›´æ–°é€šé“
    podUpdates map[types.UID]chan UpdatePodOptions
    
    // è·Ÿè¸ªæ¯ä¸ª Pod æ˜¯å¦æ­£åœ¨å·¥ä½œ
    isWorking map[types.UID]bool
    
    // è·Ÿè¸ªæœ€åæœªäº¤ä»˜çš„å·¥ä½œæ›´æ–°
    lastUndeliveredWorkUpdate map[types.UID]UpdatePodOptions
    
    // Pod åŒæ­¥çŠ¶æ€
    podSyncStatuses map[types.UID]*podSyncStatus
    
    // ä¾èµ–é¡¹
    syncPodFn            syncPodFnType
    recorder             record.EventRecorder
    workQueue            queue.WorkQueue
    resyncInterval       time.Duration
    backOffPeriod        time.Duration
    podCache             kubecontainer.Cache
    clock                clock.WithTicker
}

/*
UpdatePod é€šçŸ¥ Pod å·¥ä½œå™¨æœ‰å…³ Pod çš„æ›´æ–°

å‚æ•°ï¼š
- options: æ›´æ–°é€‰é¡¹ï¼ŒåŒ…å« Pod ä¿¡æ¯å’Œæ›´æ–°ç±»å‹

å·¥ä½œæµç¨‹ï¼š
1. éªŒè¯æ›´æ–°é€‰é¡¹
2. è·å–æˆ–åˆ›å»º Pod å·¥ä½œå™¨
3. å°†æ›´æ–°å‘é€åˆ° Pod çš„æ›´æ–°é€šé“
4. å¦‚æœéœ€è¦ï¼Œå¯åŠ¨æ–°çš„å·¥ä½œåç¨‹
*/
func (p *podWorkers) UpdatePod(options UpdatePodOptions) {
    // 1. éªŒè¯æ›´æ–°é€‰é¡¹
    if options.Pod == nil && options.RunningPod == nil {
        klog.ErrorS(nil, "UpdatePod è°ƒç”¨æ—¶ Pod å’Œ RunningPod éƒ½ä¸º nil")
        return
    }
    
    var uid types.UID
    var name, namespace string
    if options.Pod != nil {
        uid = options.Pod.UID
        name = options.Pod.Name
        namespace = options.Pod.Namespace
    } else {
        uid = options.RunningPod.ID
        name = options.RunningPod.Name
        namespace = options.RunningPod.Namespace
    }
    
    // 2. è·å– Pod å·¥ä½œå™¨çŠ¶æ€
    p.podLock.Lock()
    defer p.podLock.Unlock()
    
    // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°çš„å·¥ä½œå™¨
    podUpdates, exists := p.podUpdates[uid]
    if !exists {
        // åˆ›å»ºæ–°çš„æ›´æ–°é€šé“
        podUpdates = make(chan UpdatePodOptions, 1)
        p.podUpdates[uid] = podUpdates
        
        // åˆå§‹åŒ–å·¥ä½œçŠ¶æ€
        p.isWorking[uid] = true
        p.podSyncStatuses[uid] = &podSyncStatus{
            syncedAt: time.Time{},
            fullname: kubecontainer.GetPodFullName(options.Pod),
        }
        
        // å¯åŠ¨æ–°çš„å·¥ä½œåç¨‹
        go func() {
            defer utilruntime.HandleCrash()
            p.managePodLoop(podUpdates)
        }()
    }
    
    // 3. å‘é€æ›´æ–°åˆ° Pod çš„æ›´æ–°é€šé“
    if !p.isWorking[uid] {
        p.isWorking[uid] = true
        p.lastUndeliveredWorkUpdate[uid] = options
    }
    
    select {
    case podUpdates <- options:
        // æ›´æ–°å‘é€æˆåŠŸ
    default:
        // é€šé“å·²æ»¡ï¼Œæ›¿æ¢æœ€åä¸€ä¸ªæ›´æ–°
        select {
        case <-podUpdates:
        default:
        }
        podUpdates <- options
    }
}

/*
managePodLoop ç®¡ç†å•ä¸ª Pod çš„å¤„ç†å¾ªç¯

å‚æ•°ï¼š
- podUpdates: Pod æ›´æ–°é€šé“

å·¥ä½œæµç¨‹ï¼š
1. ç›‘å¬ Pod æ›´æ–°äº‹ä»¶
2. å¤„ç† Pod åŒæ­¥
3. ç®¡ç† Pod ç”Ÿå‘½å‘¨æœŸ
4. å¤„ç†é”™è¯¯å’Œé‡è¯•
*/
func (p *podWorkers) managePodLoop(podUpdates <-chan UpdatePodOptions) {
    var lastSyncTime time.Time
    
    for update := range podUpdates {
        err := func() error {
            podUID := update.Pod.UID
            
            // è®°å½•åŒæ­¥å¼€å§‹æ—¶é—´
            start := time.Now()
            
            // æ‰§è¡Œ Pod åŒæ­¥
            err := p.syncPodFn(context.TODO(), update.UpdateType, update.Pod, update.MirrorPod, update.RunningPod)
            
            // è®°å½•åŒæ­¥å®Œæˆæ—¶é—´
            syncDuration := time.Since(start)
            
            // æ›´æ–°æŒ‡æ ‡
            metrics.PodWorkerDuration.WithLabelValues(string(update.UpdateType)).Observe(syncDuration.Seconds())
            
            if update.StartTime != (time.Time{}) {
                metrics.PodStartupDuration.Observe(time.Since(update.StartTime).Seconds())
            }
            
            // æ›´æ–°åŒæ­¥çŠ¶æ€
            p.podLock.Lock()
            if status, ok := p.podSyncStatuses[podUID]; ok {
                status.syncedAt = time.Now()
                if err == nil {
                    status.fullname = kubecontainer.GetPodFullName(update.Pod)
                }
            }
            p.podLock.Unlock()
            
            lastSyncTime = time.Now()
            return err
        }()
        
        if err != nil {
            // è®°å½•åŒæ­¥é”™è¯¯
            klog.ErrorS(err, "åŒæ­¥ Pod æ—¶å‡ºé”™", "pod", klog.KObj(update.Pod), "podUID", update.Pod.UID)
            
            // å‘é€é”™è¯¯äº‹ä»¶
            p.recorder.Eventf(update.Pod, v1.EventTypeWarning, events.FailedSync, "åŒæ­¥ Pod æ—¶å‡ºé”™: %v", err)
        }
        
        // æ ‡è®°å·¥ä½œå®Œæˆ
        p.podLock.Lock()
        p.isWorking[update.Pod.UID] = false
        p.podLock.Unlock()
    }
}

/*
syncPod åŒæ­¥å•ä¸ª Pod çš„çŠ¶æ€

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- updateType: æ›´æ–°ç±»å‹
- pod: Pod å¯¹è±¡
- mirrorPod: é•œåƒ Podï¼ˆç”¨äºé™æ€ Podï¼‰
- runningPod: è¿è¡Œä¸­çš„ Pod

è¿”å›å€¼ï¼š
- error: åŒæ­¥è¿‡ç¨‹ä¸­çš„é”™è¯¯

åŒæ­¥æµç¨‹ï¼š
1. éªŒè¯ Pod è§„æ ¼
2. åˆ›å»º Pod æ²™ç®±
3. å¯åŠ¨åˆå§‹åŒ–å®¹å™¨
4. å¯åŠ¨åº”ç”¨å®¹å™¨
5. æ›´æ–° Pod çŠ¶æ€
*/
func (kl *Kubelet) syncPod(ctx context.Context, updateType kubetypes.SyncPodType, pod, mirrorPod *v1.Pod, runningPod *kubecontainer.Pod) error {
    klog.V(4).InfoS("åŒæ­¥ Pod", "pod", klog.KObj(pod), "podUID", pod.UID, "updateType", updateType)
    
    // 1. è®°å½•åŒæ­¥å¼€å§‹æ—¶é—´
    start := time.Now()
    defer func() {
        metrics.PodSyncDuration.Observe(metrics.SinceInSeconds(start))
    }()
    
    // 2. ç”Ÿæˆ Pod çŠ¶æ€
    apiPodStatus := kl.generateAPIPodStatus(pod, runningPod, false)
    
    // 3. æ£€æŸ¥ Pod æ˜¯å¦åº”è¯¥è¿è¡Œ
    runnable := kl.canRunPod(pod)
    if !runnable.Admit {
        // Pod ä¸èƒ½è¿è¡Œï¼Œæ›´æ–°çŠ¶æ€å¹¶è¿”å›
        kl.statusManager.SetPodStatus(pod, apiPodStatus)
        return fmt.Errorf("Pod ä¸èƒ½è¿è¡Œ: %s", runnable.Message)
    }
    
    // 4. æ£€æŸ¥ç½‘ç»œæ’ä»¶æ˜¯å¦å°±ç»ª
    if err := kl.runtimeState.networkErrors(); err != nil && !kubecontainer.IsHostNetworkPod(pod) {
        kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, "ç½‘ç»œæœªå°±ç»ª: %v", err)
        return fmt.Errorf("ç½‘ç»œæœªå°±ç»ª: %w", err)
    }
    
    // 5. åˆ›å»º Pod ç›®å½•
    if err := kl.makePodDataDirs(pod); err != nil {
        kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToMakeDataDir, "åˆ›å»º Pod æ•°æ®ç›®å½•å¤±è´¥: %v", err)
        return fmt.Errorf("åˆ›å»º Pod æ•°æ®ç›®å½•å¤±è´¥: %w", err)
    }
    
    // 6. ç­‰å¾…å­˜å‚¨å·æŒ‚è½½
    if !kl.podIsTerminated(pod) {
        if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil {
            kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, "ç­‰å¾…å­˜å‚¨å·æŒ‚è½½å¤±è´¥: %v", err)
            return fmt.Errorf("ç­‰å¾…å­˜å‚¨å·æŒ‚è½½å¤±è´¥: %w", err)
        }
    }
    
    // 7. è·å– Pod çš„æ‹‰å–å¯†é’¥
    pullSecrets := kl.getPullSecretsForPod(pod)
    
    // 8. è°ƒç”¨å®¹å™¨è¿è¡Œæ—¶åŒæ­¥ Pod
    result := kl.containerRuntime.SyncPod(ctx, pod, runningPod, apiPodStatus, pullSecrets, kl.backOff)
    kl.reasonCache.Update(pod.UID, result)
    
    if err := result.Error(); err != nil {
        // åŒæ­¥å¤±è´¥ï¼Œè®°å½•é”™è¯¯
        for _, r := range result.SyncResults {
            if r.Error != kubecontainer.ErrCrashLoopBackOff && r.Error != images.ErrImagePullBackOff {
                kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedSync, "åŒæ­¥å®¹å™¨å¤±è´¥: %v", r.Error)
            }
        }
        return err
    }
    
    return nil
}
```

## ğŸ”Œ å®¹å™¨è¿è¡Œæ—¶æ¥å£ (CRI)

### 4.1 CRI æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "CRI æ¶æ„"
        subgraph "Kubelet å±‚ (Kubelet Layer)"
            KUBELET[Kubelet]
            RUNTIME_SERVICE[Runtime Service<br/>è¿è¡Œæ—¶æœåŠ¡æ¥å£]
            IMAGE_SERVICE[Image Service<br/>é•œåƒæœåŠ¡æ¥å£]
        end
        
        subgraph "CRI æ¥å£å±‚ (CRI Interface Layer)"
            CRI_API[CRI API<br/>CRI æ¥å£å®šä¹‰]
            GRPC_CLIENT[gRPC Client<br/>gRPC å®¢æˆ·ç«¯]
            CRI_SHIM[CRI Shim<br/>CRI é€‚é…å™¨]
        end
        
        subgraph "å®¹å™¨è¿è¡Œæ—¶ (Container Runtime)"
            CONTAINERD[containerd]
            CRIO[CRI-O]
            DOCKER[Docker Engine<br/>(é€šè¿‡ dockershim)]
        end
        
        subgraph "åº•å±‚ç»„ä»¶ (Low-level Components)"
            RUNC[runc<br/>OCI è¿è¡Œæ—¶]
            KATA[Kata Containers<br/>å®‰å…¨å®¹å™¨]
            GVISOR[gVisor<br/>æ²™ç®±è¿è¡Œæ—¶]
        end
        
        subgraph "ç³»ç»Ÿæ¥å£ (System Interfaces)"
            KERNEL[Linux Kernel]
            CGROUPS[cgroups]
            NAMESPACES[namespaces]
            SECCOMP[seccomp]
        end
    end
    
    %% Kubelet åˆ° CRI
    KUBELET --> RUNTIME_SERVICE
    KUBELET --> IMAGE_SERVICE
    
    %% CRI æ¥å£å±‚
    RUNTIME_SERVICE --> CRI_API
    IMAGE_SERVICE --> CRI_API
    CRI_API --> GRPC_CLIENT
    GRPC_CLIENT --> CRI_SHIM
    
    %% å®¹å™¨è¿è¡Œæ—¶
    CRI_SHIM --> CONTAINERD
    CRI_SHIM --> CRIO
    CRI_SHIM --> DOCKER
    
    %% åº•å±‚ç»„ä»¶
    CONTAINERD --> RUNC
    CRIO --> RUNC
    CONTAINERD --> KATA
    CONTAINERD --> GVISOR
    
    %% ç³»ç»Ÿæ¥å£
    RUNC --> KERNEL
    KATA --> KERNEL
    GVISOR --> KERNEL
    KERNEL --> CGROUPS
    KERNEL --> NAMESPACES
    KERNEL --> SECCOMP
    
    %% æ ·å¼å®šä¹‰
    classDef kubelet fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef cri fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef runtime fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef lowlevel fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef system fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    
    class KUBELET,RUNTIME_SERVICE,IMAGE_SERVICE kubelet
    class CRI_API,GRPC_CLIENT,CRI_SHIM cri
    class CONTAINERD,CRIO,DOCKER runtime
    class RUNC,KATA,GVISOR lowlevel
    class KERNEL,CGROUPS,NAMESPACES,SECCOMP system
```

### 4.2 CRI æ¥å£å®ç°

```go
// pkg/kubelet/kuberuntime/kuberuntime_manager.go
/*
kubeGenericRuntimeManager å®ç° CRI æ¥å£çš„é€šç”¨è¿è¡Œæ—¶ç®¡ç†å™¨

ä¸»è¦åŠŸèƒ½ï¼š
1. ç®¡ç† Pod æ²™ç®±çš„ç”Ÿå‘½å‘¨æœŸ
2. ç®¡ç†å®¹å™¨çš„ç”Ÿå‘½å‘¨æœŸ
3. å¤„ç†é•œåƒæ‹‰å–å’Œç®¡ç†
4. æ‰§è¡Œå®¹å™¨æ¢é’ˆæ£€æŸ¥
5. æ”¶é›†å®¹å™¨æ—¥å¿—å’ŒæŒ‡æ ‡
*/

/*
kubeGenericRuntimeManager ç»“æ„ä½“å®šä¹‰

å­—æ®µè¯´æ˜ï¼š
- runtimeName: è¿è¡Œæ—¶åç§°
- runtimeService: CRI è¿è¡Œæ—¶æœåŠ¡æ¥å£
- imageService: CRI é•œåƒæœåŠ¡æ¥å£
- osInterface: æ“ä½œç³»ç»Ÿæ¥å£
- containerRefManager: å®¹å™¨å¼•ç”¨ç®¡ç†å™¨
- machineInfo: æœºå™¨ä¿¡æ¯
- podStateProvider: Pod çŠ¶æ€æä¾›è€…
- runtimeHelper: è¿è¡Œæ—¶åŠ©æ‰‹
*/
type kubeGenericRuntimeManager struct {
    // è¿è¡Œæ—¶ä¿¡æ¯
    runtimeName         string
    runtimeVersion      string
    apiVersion          string
    
    // CRI æœåŠ¡æ¥å£
    runtimeService      internalapi.RuntimeService
    imageService        internalapi.ImageManagerService
    
    // ç³»ç»Ÿæ¥å£
    osInterface         kubecontainer.OSInterface
    
    // ç®¡ç†å™¨å’ŒåŠ©æ‰‹
    containerRefManager *kubecontainer.RefManager
    machineInfo         *cadvisorapi.MachineInfo
    podStateProvider    podStateProvider
    runtimeHelper       kubecontainer.RuntimeHelper
    
    // é…ç½®
    httpClient          types.HTTPGetter
    execProbe           execprobe.Prober
    
    // è®°å½•å™¨
    recorder            record.EventRecorder
    
    // æ—¥å¿—ç®¡ç†
    logManager          logs.ContainerLogManager
    
    // ç”Ÿå‘½å‘¨æœŸç®¡ç†
    internalLifecycle   cm.InternalContainerLifecycle
    
    // å®‰å…¨ä¸Šä¸‹æ–‡
    securityContextProvider securitycontext.SecurityContextProvider
    
    // å…¶ä»–é…ç½®
    memorySwapBehavior          string
    getNodeAllocatableAbsolute  func() v1.ResourceList
    memoryThrottlingFactor      *float64
}

/*
SyncPod åŒæ­¥ Pod çš„çŠ¶æ€ï¼Œè¿™æ˜¯ CRI çš„æ ¸å¿ƒæ–¹æ³•

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- pod: è¦åŒæ­¥çš„ Pod
- runningPod: å½“å‰è¿è¡Œçš„ Pod çŠ¶æ€
- podStatus: Pod çŠ¶æ€
- pullSecrets: æ‹‰å–é•œåƒçš„å¯†é’¥
- backOff: é€€é¿ç®¡ç†å™¨

è¿”å›å€¼ï¼š
- PodSyncResult: åŒæ­¥ç»“æœ

åŒæ­¥æµç¨‹ï¼š
1. è®¡ç®—æ²™ç®±å’Œå®¹å™¨çš„å˜æ›´
2. å¦‚æœéœ€è¦ï¼Œæ€æ­» Pod æ²™ç®±
3. æ€æ­»ä¸éœ€è¦çš„å®¹å™¨
4. åˆ›å»ºæ–°çš„æ²™ç®±ï¼ˆå¦‚æœéœ€è¦ï¼‰
5. åˆ›å»ºä¸´æ—¶å®¹å™¨
6. åˆ›å»ºåˆå§‹åŒ–å®¹å™¨
7. åˆ›å»ºæ™®é€šå®¹å™¨
*/
func (m *kubeGenericRuntimeManager) SyncPod(ctx context.Context, pod *v1.Pod, runningPod kubecontainer.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {
    // 1. è®¡ç®— Pod çš„å˜æ›´
    podContainerChanges := m.computePodActions(ctx, pod, runningPod)
    klog.V(3).InfoS("è®¡ç®— Pod æ“ä½œ", "pod", klog.KObj(pod), "podActions", podContainerChanges)
    
    // 2. å¦‚æœéœ€è¦æ€æ­»æ²™ç®±ï¼Œå…ˆæ€æ­»æ²™ç®±
    if podContainerChanges.KillPod {
        if podContainerChanges.CreateSandbox {
            klog.V(4).InfoS("åœæ­¢ Pod æ²™ç®±", "pod", klog.KObj(pod), "podUID", pod.UID)
            killResult := m.killPodWithSyncResult(ctx, pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil)
            result.AddPodSyncResult(killResult)
            if killResult.Error() != nil {
                klog.ErrorS(killResult.Error(), "æ€æ­» Pod å¤±è´¥", "pod", klog.KObj(pod), "podUID", pod.UID)
                return
            }
        } else {
            // æ€æ­»ä¸éœ€è¦çš„å®¹å™¨
            for containerID, containerInfo := range podContainerChanges.ContainersToKill {
                klog.V(3).InfoS("æ€æ­»å®¹å™¨", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", containerInfo.name, "containerID", containerID)
                killContainerResult := m.killContainer(ctx, pod, containerID, containerInfo.name, containerInfo.message, containerInfo.reason, nil)
                result.AddSyncResult(killContainerResult)
                if killContainerResult.Error() != nil {
                    klog.ErrorS(killContainerResult.Error(), "æ€æ­»å®¹å™¨å¤±è´¥", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", containerInfo.name, "containerID", containerID)
                    return
                }
            }
        }
    }
    
    // 3. å¦‚æœéœ€è¦åˆ›å»ºæ²™ç®±ï¼Œåˆ›å»ºæ²™ç®±
    podSandboxID := podContainerChanges.SandboxID
    if podContainerChanges.CreateSandbox {
        var msg string
        var err error
        
        klog.V(4).InfoS("åˆ›å»º Pod æ²™ç®±", "pod", klog.KObj(pod), "podUID", pod.UID)
        createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, pod.Name)
        result.AddSyncResult(createSandboxResult)
        
        // åˆ›å»ºæ²™ç®±é…ç½®
        podSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)
        if err != nil {
            msg = fmt.Sprintf("ç”Ÿæˆæ²™ç®±é…ç½®å¤±è´¥: %v", err)
            createSandboxResult.Fail(kubecontainer.ErrConfigPodSandbox, msg)
            klog.ErrorS(err, "ç”Ÿæˆæ²™ç®±é…ç½®å¤±è´¥", "pod", klog.KObj(pod), "podUID", pod.UID)
            return
        }
        
        // è°ƒç”¨ CRI åˆ›å»ºæ²™ç®±
        podSandboxID, err = m.runtimeService.RunPodSandbox(ctx, podSandboxConfig, pod.Spec.RuntimeClassName)
        if err != nil {
            msg = fmt.Sprintf("åˆ›å»ºæ²™ç®±å¤±è´¥: %v", err)
            createSandboxResult.Fail(kubecontainer.ErrCreatePodSandbox, msg)
            klog.ErrorS(err, "åˆ›å»ºæ²™ç®±å¤±è´¥", "pod", klog.KObj(pod), "podUID", pod.UID)
            return
        }
        
        klog.V(4).InfoS("åˆ›å»ºæ²™ç®±æˆåŠŸ", "pod", klog.KObj(pod), "podUID", pod.UID, "podSandboxID", podSandboxID)
    }
    
    // 4. è·å–æ²™ç®±çŠ¶æ€
    podSandboxStatus, err := m.runtimeService.PodSandboxStatus(ctx, podSandboxID, false)
    if err != nil {
        msg := fmt.Sprintf("è·å–æ²™ç®±çŠ¶æ€å¤±è´¥: %v", err)
        result.Fail(kubecontainer.ErrPodSandboxStatus, msg)
        klog.ErrorS(err, "è·å–æ²™ç®±çŠ¶æ€å¤±è´¥", "pod", klog.KObj(pod), "podUID", pod.UID, "podSandboxID", podSandboxID)
        return
    }
    
    // 5. åˆ›å»ºä¸´æ—¶å®¹å™¨
    for _, idx := range podContainerChanges.EphemeralContainersToStart {
        start := func() error {
            ephemeralContainer := &pod.Spec.EphemeralContainers[idx]
            return m.startEphemeralContainer(ctx, pod, ephemeralContainer, podSandboxID, podSandboxStatus, pullSecrets, podContainerChanges.Attempt)
        }
        
        ephemeralContainerStartResult := kubecontainer.NewSyncResult(kubecontainer.StartEphemeralContainer, ephemeralContainer.Name)
        result.AddSyncResult(ephemeralContainerStartResult)
        
        if err := start(); err != nil {
            ephemeralContainerStartResult.Fail(err)
            msg := fmt.Sprintf("å¯åŠ¨ä¸´æ—¶å®¹å™¨ %q å¤±è´¥: %v", ephemeralContainer.Name, err)
            klog.ErrorS(err, "å¯åŠ¨ä¸´æ—¶å®¹å™¨å¤±è´¥", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", ephemeralContainer.Name)
            continue
        }
    }
    
    // 6. å¯åŠ¨åˆå§‹åŒ–å®¹å™¨
    if container := podContainerChanges.NextInitContainerToStart; container != nil {
        if err := m.startContainer(ctx, podSandboxID, podSandboxStatus, container, pod, podStatus, pullSecrets, podContainerChanges.Attempt, kubecontainer.StartContainer); err != nil {
            msg := fmt.Sprintf("å¯åŠ¨åˆå§‹åŒ–å®¹å™¨ %q å¤±è´¥: %v", container.Name, err)
            result.Fail(kubecontainer.ErrRunInitContainer, msg)
            klog.ErrorS(err, "å¯åŠ¨åˆå§‹åŒ–å®¹å™¨å¤±è´¥", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", container.Name)
            return
        }
        
        klog.V(4).InfoS("å®Œæˆåˆå§‹åŒ–å®¹å™¨", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", container.Name)
    }
    
    // 7. å¯åŠ¨æ™®é€šå®¹å™¨
    for _, idx := range podContainerChanges.ContainersToStart {
        start := func() error {
            container := &pod.Spec.Containers[idx]
            return m.startContainer(ctx, podSandboxID, podSandboxStatus, container, pod, podStatus, pullSecrets, podContainerChanges.Attempt, kubecontainer.StartContainer)
        }
        
        containerStartResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, pod.Spec.Containers[idx].Name)
        result.AddSyncResult(containerStartResult)
        
        if err := start(); err != nil {
            containerStartResult.Fail(err)
            msg := fmt.Sprintf("å¯åŠ¨å®¹å™¨ %q å¤±è´¥: %v", pod.Spec.Containers[idx].Name, err)
            klog.ErrorS(err, "å¯åŠ¨å®¹å™¨å¤±è´¥", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", pod.Spec.Containers[idx].Name)
            continue
        }
    }
    
    return
}

/*
startContainer å¯åŠ¨å•ä¸ªå®¹å™¨

å‚æ•°ï¼š
- ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
- podSandboxID: Pod æ²™ç®± ID
- podSandboxStatus: Pod æ²™ç®±çŠ¶æ€
- container: å®¹å™¨è§„æ ¼
- pod: Pod å¯¹è±¡
- podStatus: Pod çŠ¶æ€
- pullSecrets: æ‹‰å–å¯†é’¥
- podSandboxAttempt: æ²™ç®±å°è¯•æ¬¡æ•°
- reason: å¯åŠ¨åŸå› 

è¿”å›å€¼ï¼š
- error: å¯åŠ¨è¿‡ç¨‹ä¸­çš„é”™è¯¯

å¯åŠ¨æµç¨‹ï¼š
1. æ‹‰å–å®¹å™¨é•œåƒ
2. åˆ›å»ºå®¹å™¨é…ç½®
3. åˆ›å»ºå®¹å™¨
4. å¯åŠ¨å®¹å™¨
5. æ‰§è¡Œå¯åŠ¨åé’©å­
*/
func (m *kubeGenericRuntimeManager) startContainer(ctx context.Context, podSandboxID string, podSandboxStatus *runtimeapi.PodSandboxStatus, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podSandboxAttempt uint32, reason kubecontainer.SyncAction) error {
    // 1. æ‹‰å–å®¹å™¨é•œåƒ
    imageRef, msg, err := m.imagePuller.EnsureImageExists(ctx, pod, container, pullSecrets, podSandboxStatus.GetConfig())
    if err != nil {
        m.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreateContainer, "æ‹‰å–é•œåƒå¤±è´¥: %v", err)
        return msg.ToError()
    }
    
    // 2. åˆ›å»ºå®¹å™¨é…ç½®
    containerConfig, cleanupAction, err := m.generateContainerConfig(ctx, container, pod, podSandboxAttempt, podSandboxStatus.GetConfig(), imageRef, podStatus.IPs)
    if cleanupAction != nil {
        defer cleanupAction()
    }
    if err != nil {
        m.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreateContainer, "ç”Ÿæˆå®¹å™¨é…ç½®å¤±è´¥: %v", err)
        return fmt.Errorf("ç”Ÿæˆå®¹å™¨ %q çš„é…ç½®å¤±è´¥: %w", container.Name, err)
    }
    
    // 3. åˆ›å»ºå®¹å™¨
    containerID, err := m.runtimeService.CreateContainer(ctx, podSandboxID, containerConfig, podSandboxStatus.GetConfig())
    if err != nil {
        m.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreateContainer, "åˆ›å»ºå®¹å™¨å¤±è´¥: %v", err)
        return fmt.Errorf("åˆ›å»ºå®¹å™¨å¤±è´¥: %w", err)
    }
    
    // 4. å¯åŠ¨å®¹å™¨
    err = m.runtimeService.StartContainer(ctx, containerID)
    if err != nil {
        m.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToStartContainer, "å¯åŠ¨å®¹å™¨å¤±è´¥: %v", err)
        return fmt.Errorf("å¯åŠ¨å®¹å™¨ %q å¤±è´¥: %w", container.Name, err)
    }
    
    // 5. æ‰§è¡Œå¯åŠ¨åé’©å­
    if container.Lifecycle != nil && container.Lifecycle.PostStart != nil {
        kubeContainerID := kubecontainer.ContainerID{
            Type: m.runtimeName,
            ID:   containerID,
        }
        
        msg, handlerErr := m.runner.Run(ctx, kubeContainerID, pod, container, container.Lifecycle.PostStart)
        if handlerErr != nil {
            klog.ErrorS(handlerErr, "æ‰§è¡Œå¯åŠ¨åé’©å­å¤±è´¥", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", container.Name, "containerID", containerID)
            m.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedPostStartHook, "æ‰§è¡Œå¯åŠ¨åé’©å­å¤±è´¥: %v", handlerErr)
            
            // å¦‚æœé’©å­å¤±è´¥ï¼Œæ€æ­»å®¹å™¨
            if err := m.killContainer(ctx, pod, kubeContainerID, container.Name, "å¯åŠ¨åé’©å­å¤±è´¥", reasonFailedPostStartHook, nil); err != nil {
                klog.ErrorS(err, "æ€æ­»å®¹å™¨å¤±è´¥", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", container.Name, "containerID", containerID)
            }
            
            return fmt.Errorf("å¯åŠ¨åé’©å­å¤±è´¥: %w", handlerErr)
        }
        
        if msg != "" {
            klog.V(3).InfoS("å¯åŠ¨åé’©å­æ‰§è¡Œå®Œæˆ", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", container.Name, "containerID", containerID, "message", msg)
        }
    }
    
    return nil
}
```

## ğŸ“Š ç›‘æ§å’Œæ€§èƒ½ä¼˜åŒ–

### 5.1 Kubelet æŒ‡æ ‡ç›‘æ§

```yaml
# Kubelet ç›‘æ§é…ç½®
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: kubelet
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kubelet
  endpoints:
  - port: https-metrics
    scheme: https
    path: /metrics
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      serverName: kubelet
      insecureSkipVerify: false
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    interval: 30s
  - port: https-metrics
    scheme: https
    path: /metrics/cadvisor
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      serverName: kubelet
      insecureSkipVerify: false
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    interval: 30s
    
---
# Kubelet å…³é”®æŒ‡æ ‡å‘Šè­¦è§„åˆ™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubelet-alerts
  namespace: kube-system
spec:
  groups:
  - name: kubelet.rules
    rules:
    # Kubelet å¯ç”¨æ€§å‘Šè­¦
    - alert: KubeletDown
      expr: up{job="kubelet"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubelet ä¸å¯ç”¨"
        description: "èŠ‚ç‚¹ {{ $labels.instance }} ä¸Šçš„ Kubelet å·²ç»å®•æœºè¶…è¿‡ 5 åˆ†é’Ÿ"
    
    # Pod å¯åŠ¨å»¶è¿Ÿå‘Šè­¦
    - alert: KubeletPodStartupLatencyHigh
      expr: |
        histogram_quantile(0.99, sum(rate(kubelet_pod_start_duration_seconds_bucket{job="kubelet"}[5m])) by (le, instance)) > 60
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubelet Pod å¯åŠ¨å»¶è¿Ÿè¿‡é«˜"
        description: "èŠ‚ç‚¹ {{ $labels.instance }} ä¸Š Pod å¯åŠ¨ 99% åˆ†ä½å»¶è¿Ÿä¸º {{ $value }} ç§’"
    
    # å®¹å™¨è¿è¡Œæ—¶æ“ä½œå»¶è¿Ÿå‘Šè­¦
    - alert: KubeletRuntimeOperationsLatencyHigh
      expr: |
        histogram_quantile(0.99, sum(rate(kubelet_runtime_operations_duration_seconds_bucket{job="kubelet"}[5m])) by (le, instance, operation_type)) > 10
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubelet è¿è¡Œæ—¶æ“ä½œå»¶è¿Ÿè¿‡é«˜"
        description: "èŠ‚ç‚¹ {{ $labels.instance }} ä¸Š {{ $labels.operation_type }} æ“ä½œ 99% åˆ†ä½å»¶è¿Ÿä¸º {{ $value }} ç§’"
    
    # å®¹å™¨è¿è¡Œæ—¶æ“ä½œé”™è¯¯å‘Šè­¦
    - alert: KubeletRuntimeOperationsErrors
      expr: |
        sum(rate(kubelet_runtime_operations_errors_total{job="kubelet"}[5m])) by (instance, operation_type) > 0.1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubelet è¿è¡Œæ—¶æ“ä½œé”™è¯¯ç‡è¿‡é«˜"
        description: "èŠ‚ç‚¹ {{ $labels.instance }} ä¸Š {{ $labels.operation_type }} æ“ä½œé”™è¯¯ç‡ä¸º {{ $value }}/s"
    
    # PLEG é‡æ–°åˆ—ä¸¾å»¶è¿Ÿå‘Šè­¦
    - alert: KubeletPLEGDurationHigh
      expr: |
        histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet"}[5m])) by (le, instance)) > 10
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubelet PLEG é‡æ–°åˆ—ä¸¾å»¶è¿Ÿè¿‡é«˜"
        description: "èŠ‚ç‚¹ {{ $labels.instance }} ä¸Š PLEG é‡æ–°åˆ—ä¸¾ 99% åˆ†ä½å»¶è¿Ÿä¸º {{ $value }} ç§’"
    
    # èŠ‚ç‚¹æ–‡ä»¶ç³»ç»Ÿä½¿ç”¨ç‡å‘Šè­¦
    - alert: NodeFilesystemSpaceFillingUp
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 15
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "èŠ‚ç‚¹æ–‡ä»¶ç³»ç»Ÿç©ºé—´å³å°†è€—å°½"
        description: "èŠ‚ç‚¹ {{ $labels.instance }} ä¸ŠæŒ‚è½½ç‚¹ {{ $labels.mountpoint }} çš„æ–‡ä»¶ç³»ç»Ÿé¢„è®¡åœ¨ 24 å°æ—¶å†…è€—å°½ç©ºé—´"
    
    # èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
    - alert: NodeMemoryHighUtilization
      expr: |
        (
          (
            node_memory_MemTotal_bytes{job="node-exporter"}
          -
            node_memory_MemAvailable_bytes{job="node-exporter"}
          )
        /
          node_memory_MemTotal_bytes{job="node-exporter"}
        ) > 0.9
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
        description: "èŠ‚ç‚¹ {{ $labels.instance }} å†…å­˜ä½¿ç”¨ç‡ä¸º {{ $value | humanizePercentage }}"
```

### 5.2 Kubelet æ€§èƒ½ä¼˜åŒ–é…ç½®

```yaml
# Kubelet é«˜æ€§èƒ½é…ç½®
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration

# åŸºç¡€é…ç½®
clusterDomain: cluster.local
clusterDNS:
- 10.96.0.10

# API é…ç½®
port: 10250
readOnlyPort: 0
healthzPort: 10248
healthzBindAddress: 127.0.0.1

# è®¤è¯å’Œæˆæƒ
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
    cacheTTL: 2m0s
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s

# TLS é…ç½®
tlsCertFile: /var/lib/kubelet/pki/kubelet.crt
tlsPrivateKeyFile: /var/lib/kubelet/pki/kubelet.key
tlsCipherSuites:
- TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
- TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
- TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
- TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
- TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
- TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
tlsMinVersion: VersionTLS12

# å®¹å™¨è¿è¡Œæ—¶é…ç½®
containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock
imageServiceEndpoint: unix:///var/run/containerd/containerd.sock

# èµ„æºç®¡ç†
systemReserved:
  cpu: 100m
  memory: 100Mi
  ephemeral-storage: 1Gi
kubeReserved:
  cpu: 100m
  memory: 100Mi
  ephemeral-storage: 1Gi
enforceNodeAllocatable:
- pods
- system-reserved
- kube-reserved

# cgroup é…ç½®
cgroupDriver: systemd
cgroupsPerQOS: true
cgroupRoot: /
runtimeRequestTimeout: 15m0s

# å­˜å‚¨å·é…ç½®
volumeStatsAggPeriod: 1m0s
volumePluginDir: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/

# é•œåƒç®¡ç†
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
imageMaximumGCAge: 0s

# å®¹å™¨æ—¥å¿—é…ç½®
containerLogMaxSize: 50Mi
containerLogMaxFiles: 5

# é©±é€é…ç½®
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionSoft:
  imagefs.available: 20%
  memory.available: 200Mi
  nodefs.available: 15%
  nodefs.inodesFree: 10%
evictionSoftGracePeriod:
  imagefs.available: 2m0s
  memory.available: 1m30s
  nodefs.available: 2m0s
  nodefs.inodesFree: 2m0s
evictionMaxPodGracePeriod: 60
evictionMinimumReclaim:
  imagefs.available: 5%
  memory.available: 0Mi
  nodefs.available: 5%
  nodefs.inodesFree: 5%

# æ€§èƒ½è°ƒä¼˜
maxPods: 110
podPidsLimit: 4096
registryPullQPS: 5
registryBurst: 10
eventRecordQPS: 50
eventBurst: 100
kubeAPIQPS: 50
kubeAPIBurst: 100
serializeImagePulls: false
maxParallelImagePulls: 5

# åŒæ­¥é¢‘ç‡
syncFrequency: 1m0s
fileCheckFrequency: 20s
httpCheckFrequency: 20s
nodeStatusUpdateFrequency: 10s
nodeStatusReportFrequency: 5m0s
nodeLeaseDurationSeconds: 40
nodeLeaseRenewIntervalFraction: 0.25

# æ¢é’ˆé…ç½®
streamingConnectionIdleTimeout: 4h0s
nodeStatusMaxImages: 50

# ç‰¹æ€§é—¨æ§
featureGates:
  RotateKubeletServerCertificate: true
  LocalStorageCapacityIsolation: true
  CSIMigration: true

# æ—¥å¿—é…ç½®
logging:
  format: text
  flushFrequency: 5s
  verbosity: 2
  options:
    text:
      splitStream: false

# å†…å­˜ç®¡ç†
memoryManagerPolicy: Static
reservedMemory:
- numaNode: 0
  limits:
    memory: 1Gi

# CPU ç®¡ç†
cpuManagerPolicy: static
cpuManagerPolicyOptions:
  full-pcpus-only: "true"
cpuManagerReconcilePeriod: 10s

# æ‹“æ‰‘ç®¡ç†
topologyManagerPolicy: single-numa-node
topologyManagerScope: container

# å…³é—­äº¤æ¢
failSwapOn: false

# å…¶ä»–é…ç½®
hairpinMode: promiscuous-bridge
babysitDaemons: false
maxOpenFiles: 1000000
contentType: application/vnd.kubernetes.protobuf
kubeReservedCgroup: /system.slice/kubelet.service
systemReservedCgroup: /system.slice
shutdownGracePeriod: 60s
shutdownGracePeriodCriticalPods: 20s
```

## ğŸ“š æ€»ç»“

### æ ¸å¿ƒç‰¹æ€§æ€»ç»“

1. **Pod ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼šå®Œæ•´çš„ Pod åˆ›å»ºã€æ›´æ–°ã€åˆ é™¤æµç¨‹
2. **å®¹å™¨è¿è¡Œæ—¶é›†æˆ**ï¼šé€šè¿‡ CRI æ¥å£æ”¯æŒå¤šç§å®¹å™¨è¿è¡Œæ—¶
3. **èµ„æºç®¡ç†**ï¼šCPUã€å†…å­˜ã€å­˜å‚¨ç­‰èµ„æºçš„ç²¾ç»†åŒ–ç®¡ç†
4. **å¥åº·æ£€æŸ¥**ï¼šå­˜æ´»ã€å°±ç»ªã€å¯åŠ¨æ¢é’ˆçš„å®Œæ•´å®ç°
5. **å­˜å‚¨å·ç®¡ç†**ï¼šæ”¯æŒå¤šç§å­˜å‚¨ç±»å‹å’ŒåŠ¨æ€æŒ‚è½½

### æœ€ä½³å®è·µå»ºè®®

1. **èµ„æºé…ç½®**ï¼šåˆç†è®¾ç½®ç³»ç»Ÿé¢„ç•™å’Œ Kube é¢„ç•™èµ„æº
2. **æ€§èƒ½è°ƒä¼˜**ï¼šä¼˜åŒ–åŒæ­¥é¢‘ç‡å’Œå¹¶å‘å‚æ•°
3. **ç›‘æ§å®Œå–„**ï¼šå»ºç«‹å…¨é¢çš„ Kubelet å’ŒèŠ‚ç‚¹ç›‘æ§
4. **å®‰å…¨åŠ å›º**ï¼šå¯ç”¨è®¤è¯æˆæƒå’Œ TLS åŠ å¯†
5. **æ•…éšœå¤„ç†**ï¼šé…ç½®åˆé€‚çš„é©±é€ç­–ç•¥å’Œä¼˜é›…å…³é—­

é€šè¿‡æ·±å…¥ç†è§£ Kubelet çš„æ¶æ„å’Œå®ç°ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°ç®¡ç†å’Œä¼˜åŒ– Kubernetes èŠ‚ç‚¹ï¼Œç¡®ä¿ Pod å’Œå®¹å™¨çš„ç¨³å®šè¿è¡Œã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´09æœˆ27æ—¥  
**é€‚ç”¨ç‰ˆæœ¬**: Kubernetes 1.29+
