# LangChain-09-Memory-æ¦‚è§ˆ

## æ¨¡å—åŸºæœ¬ä¿¡æ¯

**æ¨¡å—åç§°**: langchain-memory
**æ¨¡å—è·¯å¾„**: `libs/langchain/langchain/memory/`
**æ ¸å¿ƒèŒè´£**: æä¾›å¯¹è¯è®°å¿†ç®¡ç†ï¼Œè®© LLM åº”ç”¨èƒ½å¤Ÿè®°ä½å†å²å¯¹è¯å¹¶ç»´æŠ¤ä¸Šä¸‹æ–‡

## 1. æ¨¡å—èŒè´£

### 1.1 æ ¸å¿ƒèŒè´£

Memory æ¨¡å—ä¸º LangChain åº”ç”¨æä¾›å¯¹è¯è®°å¿†èƒ½åŠ›ï¼Œä¸»è¦åŠŸèƒ½ï¼š

1. **å¯¹è¯å†å²å­˜å‚¨**: ä¿å­˜ç”¨æˆ·å’Œ AI çš„å†å²æ¶ˆæ¯
2. **ä¸Šä¸‹æ–‡ç®¡ç†**: æ§åˆ¶ä¼ é€’ç»™ LLM çš„å†å²ä¿¡æ¯é‡
3. **å¤šç§è®°å¿†ç­–ç•¥**: ç¼“å†²ã€çª—å£ã€æ‘˜è¦ã€å‘é‡æ£€ç´¢ç­‰
4. **å®ä½“è·Ÿè¸ª**: è®°ä½å¯¹è¯ä¸­çš„å…³é”®å®ä½“
5. **è®°å¿†æŒä¹…åŒ–**: æ”¯æŒä¿å­˜å’Œæ¢å¤è®°å¿†
6. **å¤šä¼šè¯ç®¡ç†**: æ”¯æŒå¤šä¸ªç‹¬ç«‹çš„å¯¹è¯ä¼šè¯

### 1.2 æ ¸å¿ƒæ¦‚å¿µ

```
ç”¨æˆ·æ¶ˆæ¯
  â†“
Memory (ä¿å­˜å†å²)
  â†“
åŠ è½½ç›¸å…³å†å²
  â†“
æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„æç¤ºè¯
  â†“
LLM ç”Ÿæˆå“åº”
  â†“
Memory (ä¿å­˜ AI å“åº”)
```

**å…³é”®æœ¯è¯­**:
- **Memory**: è®°å¿†æŠ½è±¡ï¼Œç®¡ç†å¯¹è¯å†å²
- **ChatMessageHistory**: æ¶ˆæ¯å­˜å‚¨åç«¯
- **ConversationBufferMemory**: ç¼“å†²æ‰€æœ‰å†å²
- **ConversationWindowMemory**: åªä¿ç•™æœ€è¿‘ k è½®
- **ConversationSummaryMemory**: æ‘˜è¦å‹ç¼©å†å²
- **VectorStoreBackedMemory**: åŸºäºå‘é‡æ£€ç´¢çš„è®°å¿†

### 1.3 è®°å¿†ç±»å‹å¯¹æ¯”

| è®°å¿†ç±»å‹ | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | åŠ£åŠ¿ | Tokenæ¶ˆè€— |
|---------|---------|------|------|----------|
| **ConversationBufferMemory** | çŸ­å¯¹è¯ | å®Œæ•´ä¸Šä¸‹æ–‡ | Tokenå¿«é€Ÿå¢é•¿ | é«˜ |
| **ConversationWindowMemory** | ä¸€èˆ¬å¯¹è¯ | Tokenå¯æ§ | ä¸¢å¤±æ—©æœŸä¿¡æ¯ | ä¸­ |
| **ConversationSummaryMemory** | é•¿å¯¹è¯ | å‹ç¼©å†å² | æ‘˜è¦å¯èƒ½ä¸å‡†ç¡® | ä½ |
| **ConversationSummaryBufferMemory** | é•¿å¯¹è¯+ç²¾ç¡®æ€§ | å¹³è¡¡å‹ç¼©å’Œç»†èŠ‚ | éœ€è¦é¢å¤–æ‘˜è¦è°ƒç”¨ | ä¸­ |
| **VectorStoreBackedMemory** | å¤æ‚å¯¹è¯ | æ™ºèƒ½æ£€ç´¢ç›¸å…³å†å² | éœ€è¦å‘é‡å­˜å‚¨ | ä½ |
| **EntityMemory** | å®ä½“å¯†é›†å¯¹è¯ | è·Ÿè¸ªå®ä½“ä¿¡æ¯ | ç»´æŠ¤å¼€é”€å¤§ | ä¸­ |

### 1.4 è¾“å…¥/è¾“å‡º

**è¾“å…¥**:
- **save_context**: `{"input": "user message", "output": "ai response"}`

**è¾“å‡º**:
- **load_memory_variables**: `{"history": "formatted_history"}` æˆ– `{"history": list[BaseMessage]}`

### 1.5 ä¸Šä¸‹æ¸¸ä¾èµ–

**ä¸Šæ¸¸è°ƒç”¨è€…**:
- Chainsï¼ˆå¯¹è¯é“¾ï¼‰
- èŠå¤©åº”ç”¨
- ä»£ç†ï¼ˆå¸¦è®°å¿†çš„ä»£ç†ï¼‰

**ä¸‹æ¸¸ä¾èµ–**:
- `langchain_core.messages`: æ¶ˆæ¯ç±»å‹
- `langchain_core.chat_history`: èŠå¤©å†å²æŠ½è±¡
- å‘é‡å­˜å‚¨ï¼ˆVectorStoreBackedMemoryï¼‰
- LLMï¼ˆç”¨äºæ‘˜è¦ï¼‰

## 2. æ¨¡å—çº§æ¶æ„å›¾

```mermaid
flowchart TB
    subgraph Base["åŸºç¡€æŠ½è±¡å±‚"]
        BM[BaseMemory<br/>è®°å¿†åŸºç±»]
        BCMH[BaseChatMessageHistory<br/>æ¶ˆæ¯å†å²åŸºç±»]
    end

    subgraph Simple["ç®€å•è®°å¿†"]
        CBM[ConversationBufferMemory<br/>ç¼“å†²è®°å¿†]
        CWM[ConversationWindowMemory<br/>çª—å£è®°å¿†]
    end

    subgraph Advanced["é«˜çº§è®°å¿†"]
        CSM[ConversationSummaryMemory<br/>æ‘˜è¦è®°å¿†]
        CSBM[ConversationSummaryBufferMemory<br/>æ‘˜è¦ç¼“å†²è®°å¿†]
        VSBM[VectorStoreBackedMemory<br/>å‘é‡æ£€ç´¢è®°å¿†]
        EM[EntityMemory<br/>å®ä½“è®°å¿†]
    end

    subgraph Storage["å­˜å‚¨åç«¯"]
        CMH[ChatMessageHistory<br/>å†…å­˜å­˜å‚¨]
        REDIS[RedisChatMessageHistory<br/>Rediså­˜å‚¨]
        POSTGRES[PostgresChatMessageHistory<br/>PostgreSQLå­˜å‚¨]
        FILE[FileChatMessageHistory<br/>æ–‡ä»¶å­˜å‚¨]
    end

    subgraph Integration["é›†æˆ"]
        CHAIN[ConversationChain<br/>å¯¹è¯é“¾]
        RWMH[RunnableWithMessageHistory<br/>LCELé›†æˆ]
    end

    BM --> CBM
    BM --> CWM
    BM --> CSM
    BM --> CSBM
    BM --> VSBM
    BM --> EM

    BCMH --> CMH
    BCMH --> REDIS
    BCMH --> POSTGRES
    BCMH --> FILE

    CBM --> CMH
    CWM --> CMH
    CSM --> CMH

    CBM --> CHAIN
    CHAIN --> RWMH

    style Base fill:#e1f5ff
    style Simple fill:#fff4e1
    style Advanced fill:#e8f5e9
    style Storage fill:#f3e5f5
    style Integration fill:#fff3e0
```

### æ¶æ„å›¾è¯¦ç»†è¯´æ˜

**1. åŸºç¡€æŠ½è±¡å±‚**

- **BaseMemory**: æ‰€æœ‰è®°å¿†çš„åŸºç±»
  ```python
  class BaseMemory(ABC):
      @property
      @abstractmethod
      def memory_variables(self) -> list[str]:
          """è®°å¿†æä¾›çš„å˜é‡ååˆ—è¡¨"""

      @abstractmethod
      def load_memory_variables(self, inputs: dict[str, Any]) -> dict[str, Any]:
          """åŠ è½½è®°å¿†å˜é‡"""

      @abstractmethod
      def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -> None:
          """ä¿å­˜å¯¹è¯ä¸Šä¸‹æ–‡"""

      def clear(self) -> None:
          """æ¸…é™¤è®°å¿†"""
  ```

- **BaseChatMessageHistory**: æ¶ˆæ¯å†å²å­˜å‚¨æŠ½è±¡
  ```python
  class BaseChatMessageHistory(ABC):
      messages: list[BaseMessage]  # æ¶ˆæ¯åˆ—è¡¨

      def add_user_message(self, message: str) -> None:
          """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯"""

      def add_ai_message(self, message: str) -> None:
          """æ·»åŠ AIæ¶ˆæ¯"""

      def clear(self) -> None:
          """æ¸…ç©ºå†å²"""
  ```

**2. ç®€å•è®°å¿†å®ç°**

- **ConversationBufferMemory**: ç¼“å†²æ‰€æœ‰å†å²
  - ä¿ç•™å®Œæ•´å¯¹è¯å†å²
  - Token æ¶ˆè€—éšå¯¹è¯å¢é•¿çº¿æ€§å¢åŠ 
  - é€‚åˆçŸ­å¯¹è¯æˆ–éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡çš„åœºæ™¯

  ```python
  class ConversationBufferMemory(BaseMemory):
      chat_memory: BaseChatMessageHistory  # æ¶ˆæ¯å­˜å‚¨
      return_messages: bool = False  # è¿”å›æ¶ˆæ¯å¯¹è±¡è¿˜æ˜¯å­—ç¬¦ä¸²

      def load_memory_variables(self, inputs: dict) -> dict:
          """åŠ è½½æ‰€æœ‰å†å²æ¶ˆæ¯"""
          if self.return_messages:
              return {"history": self.chat_memory.messages}
          else:
              return {"history": self._get_buffer_string()}

      def save_context(self, inputs: dict, outputs: dict) -> None:
          """ä¿å­˜è¾“å…¥å’Œè¾“å‡º"""
          self.chat_memory.add_user_message(inputs["input"])
          self.chat_memory.add_ai_message(outputs["output"])
  ```

- **ConversationWindowMemory**: æ»‘åŠ¨çª—å£è®°å¿†
  - åªä¿ç•™æœ€è¿‘ k è½®å¯¹è¯
  - Token æ¶ˆè€—å›ºå®š
  - é€‚åˆä¸€èˆ¬é•¿åº¦å¯¹è¯

  ```python
  class ConversationWindowMemory(BaseMemory):
      k: int = 5  # ä¿ç•™æœ€è¿‘5è½®å¯¹è¯

      def load_memory_variables(self, inputs: dict) -> dict:
          """åŠ è½½æœ€è¿‘kè½®å¯¹è¯"""
          messages = self.chat_memory.messages[-self.k*2:]  # kè½®=kå¯¹æ¶ˆæ¯
          return {"history": messages}
  ```

**3. é«˜çº§è®°å¿†å®ç°**

- **ConversationSummaryMemory**: æ‘˜è¦è®°å¿†
  - ä½¿ç”¨ LLM å‹ç¼©å†å²ä¸ºæ‘˜è¦
  - èŠ‚çœ Token
  - é€‚åˆé•¿å¯¹è¯

  ```python
  class ConversationSummaryMemory(BaseMemory):
      llm: BaseLanguageModel  # ç”¨äºç”Ÿæˆæ‘˜è¦çš„LLM
      buffer: str = ""  # å½“å‰æ‘˜è¦

      def predict_new_summary(
          self,
          messages: list[BaseMessage],
          existing_summary: str
      ) -> str:
          """ç”Ÿæˆæ–°æ‘˜è¦"""
          # æç¤ºè¯ï¼šæ ¹æ®ç°æœ‰æ‘˜è¦å’Œæ–°æ¶ˆæ¯ï¼Œç”Ÿæˆæ›´æ–°çš„æ‘˜è¦
          prompt = f"""
          Current summary: {existing_summary}
          New messages: {messages}
          Updated summary:
          """
          return self.llm.predict(prompt)

      def save_context(self, inputs: dict, outputs: dict) -> None:
          """ä¿å­˜å¹¶æ›´æ–°æ‘˜è¦"""
          # æ·»åŠ æ–°æ¶ˆæ¯
          self.chat_memory.add_user_message(inputs["input"])
          self.chat_memory.add_ai_message(outputs["output"])

          # æ›´æ–°æ‘˜è¦
          new_messages = self.chat_memory.messages[-2:]
          self.buffer = self.predict_new_summary(new_messages, self.buffer)
  ```

- **ConversationSummaryBufferMemory**: æ··åˆè®°å¿†
  - æœ€è¿‘æ¶ˆæ¯ä¿æŒåŸæ ·
  - è¾ƒæ—©æ¶ˆæ¯å‹ç¼©ä¸ºæ‘˜è¦
  - å¹³è¡¡ç»†èŠ‚å’Œå‹ç¼©

  ```python
  class ConversationSummaryBufferMemory(BaseMemory):
      max_token_limit: int = 2000  # Tokenä¸Šé™

      def load_memory_variables(self, inputs: dict) -> dict:
          """è¿”å›æ‘˜è¦+æœ€è¿‘æ¶ˆæ¯"""
          return {
              "history": self.moving_summary_buffer + recent_messages
          }
  ```

- **VectorStoreBackedMemory**: å‘é‡æ£€ç´¢è®°å¿†
  - åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³å†å²
  - é€‚åˆå¤æ‚ã€éçº¿æ€§å¯¹è¯

  ```python
  class VectorStoreBackedMemory(BaseMemory):
      vectorstore: VectorStore
      k: int = 4  # æ£€ç´¢æ•°é‡

      def load_memory_variables(self, inputs: dict) -> dict:
          """æ£€ç´¢ç›¸å…³å†å²"""
          query = inputs["input"]
          docs = self.vectorstore.similarity_search(query, k=self.k)
          return {"history": docs}

      def save_context(self, inputs: dict, outputs: dict) -> None:
          """ä¿å­˜åˆ°å‘é‡å­˜å‚¨"""
          text = f"Human: {inputs['input']}\nAI: {outputs['output']}"
          self.vectorstore.add_texts([text])
  ```

- **EntityMemory**: å®ä½“è®°å¿†
  - æå–å’Œè·Ÿè¸ªå¯¹è¯ä¸­çš„å®ä½“
  - ç»´æŠ¤å®ä½“çŸ¥è¯†å›¾è°±

  ```python
  class EntityMemory(BaseMemory):
      entity_store: dict[str, str]  # å®ä½“å­˜å‚¨

      def save_context(self, inputs: dict, outputs: dict) -> None:
          """æå–å¹¶ä¿å­˜å®ä½“"""
          entities = self._extract_entities(inputs["input"], outputs["output"])
          for entity, info in entities.items():
              self.entity_store[entity] = info
  ```

**4. å­˜å‚¨åç«¯**

- **ChatMessageHistory**: å†…å­˜å­˜å‚¨ï¼ˆé»˜è®¤ï¼‰
  - å­˜å‚¨åœ¨å†…å­˜ä¸­
  - è¿›ç¨‹é‡å¯åä¸¢å¤±

- **RedisChatMessageHistory**: Redis å­˜å‚¨
  - æŒä¹…åŒ–
  - æ”¯æŒåˆ†å¸ƒå¼

- **PostgresChatMessageHistory**: PostgreSQL å­˜å‚¨
  - å…³ç³»æ•°æ®åº“å­˜å‚¨
  - æ”¯æŒå¤æ‚æŸ¥è¯¢

- **FileChatMessageHistory**: æ–‡ä»¶å­˜å‚¨
  - æœ¬åœ°æ–‡ä»¶æŒä¹…åŒ–
  - ç®€å•æ˜“ç”¨

**5. é›†æˆæ–¹å¼**

- **ConversationChain**: ä¼ ç»Ÿå¯¹è¯é“¾ï¼ˆå·²åºŸå¼ƒï¼‰
  ```python
  chain = ConversationChain(
      llm=llm,
      memory=ConversationBufferMemory()
  )
  ```

- **RunnableWithMessageHistory**: LCEL é›†æˆï¼ˆæ¨èï¼‰
  ```python
  chain_with_history = RunnableWithMessageHistory(
      runnable=chain,
      get_session_history=get_chat_history,
      input_messages_key="input",
      history_messages_key="history"
  )
  ```

## 3. æ ¸å¿ƒ API è¯¦è§£

### 3.1 ConversationBufferMemory - å®Œæ•´å†å²è®°å¿†

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from langchain.memory import ConversationBufferMemory

# åˆ›å»ºè®°å¿†
memory = ConversationBufferMemory()

# ä¿å­˜å¯¹è¯
memory.save_context(
    {"input": "Hi, I'm Alice"},
    {"output": "Hello Alice! Nice to meet you."}
)

memory.save_context(
    {"input": "What's my name?"},
    {"output": "Your name is Alice."}
)

# åŠ è½½è®°å¿†
print(memory.load_memory_variables({}))
# {
#   "history": "Human: Hi, I'm Alice\nAI: Hello Alice! Nice to meet you.\nHuman: What's my name?\nAI: Your name is Alice."
# }

# è¿”å›æ¶ˆæ¯å¯¹è±¡
memory_with_messages = ConversationBufferMemory(return_messages=True)
memory_with_messages.save_context({"input": "Hi"}, {"output": "Hello"})
print(memory_with_messages.load_memory_variables({}))
# {
#   "history": [
#       HumanMessage(content="Hi"),
#       AIMessage(content="Hello")
#   ]
# }

# åœ¨å¯¹è¯é“¾ä¸­ä½¿ç”¨
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI

conversation = ConversationChain(
    llm=ChatOpenAI(),
    memory=ConversationBufferMemory()
)

# å¯¹è¯1
response1 = conversation.predict(input="Hi, I'm Bob")
# "Hello Bob! How can I help you today?"

# å¯¹è¯2ï¼ˆè®°å¾—å‰é¢çš„å¯¹è¯ï¼‰
response2 = conversation.predict(input="What's my name?")
# "Your name is Bob."
```

### 3.2 ConversationWindowMemory - æ»‘åŠ¨çª—å£è®°å¿†

```python
from langchain.memory import ConversationWindowMemory

# åªä¿ç•™æœ€è¿‘2è½®å¯¹è¯
memory = ConversationWindowMemory(k=2)

# æ¨¡æ‹Ÿ5è½®å¯¹è¯
conversations = [
    ("Hi", "Hello"),
    ("My name is Alice", "Nice to meet you, Alice"),
    ("I like pizza", "Pizza is great!"),
    ("What's the weather?", "It's sunny today"),
    ("Thanks", "You're welcome!")
]

for user_msg, ai_msg in conversations:
    memory.save_context({"input": user_msg}, {"output": ai_msg})

# åªä¼šçœ‹åˆ°æœ€å2è½®
print(memory.load_memory_variables({}))
# {
#   "history": "Human: What's the weather?\nAI: It's sunny today\nHuman: Thanks\nAI: You're welcome!"
# }

# åœ¨é“¾ä¸­ä½¿ç”¨
conversation = ConversationChain(
    llm=ChatOpenAI(),
    memory=ConversationWindowMemory(k=3)  # åªè®°ä½æœ€è¿‘3è½®
)
```

### 3.3 ConversationSummaryMemory - æ‘˜è¦è®°å¿†

```python
from langchain.memory import ConversationSummaryMemory
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0)
memory = ConversationSummaryMemory(llm=llm)

# é•¿å¯¹è¯
memory.save_context(
    {"input": "Hi, I'm planning a trip to Japan"},
    {"output": "That sounds exciting! Japan is a wonderful destination. When are you planning to go?"}
)

memory.save_context(
    {"input": "I'm thinking March or April. What's the best time?"},
    {"output": "March and April are great times to visit Japan! You'll be there during cherry blossom season."}
)

memory.save_context(
    {"input": "Where should I visit in Tokyo?"},
    {"output": "In Tokyo, you should visit Shibuya, Shinjuku, Asakusa for the Senso-ji Temple, and Akihabara."}
)

# åŠ è½½æ‘˜è¦ï¼ˆè€Œä¸æ˜¯å®Œæ•´å†å²ï¼‰
print(memory.load_memory_variables({}))
# {
#   "history": "The human is planning a trip to Japan in March or April to see cherry blossoms. They've been given recommendations for places to visit in Tokyo including Shibuya, Shinjuku, Asakusa, and Akihabara."
# }
```

### 3.4 VectorStoreBackedMemory - å‘é‡æ£€ç´¢è®°å¿†

```python
from langchain.memory import VectorStoreRetrieverMemory
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts([], embeddings)

# åˆ›å»ºè®°å¿†
memory = VectorStoreRetrieverMemory(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 2})
)

# ä¿å­˜å¤šä¸ªå¯¹è¯ç‰‡æ®µ
memory.save_context(
    {"input": "My favorite color is blue"},
    {"output": "That's nice! Blue is a calming color."}
)

memory.save_context(
    {"input": "I have a dog named Max"},
    {"output": "Dogs are wonderful pets! Max sounds lovely."}
)

memory.save_context(
    {"input": "I work as a software engineer"},
    {"output": "That's a great profession!"}
)

# åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³è®°å¿†
# é—®å…³äºå® ç‰©çš„é—®é¢˜ï¼Œä¼šæ£€ç´¢åˆ°å…³äºç‹—çš„è®°å¿†
result = memory.load_memory_variables({"input": "Tell me about my pet"})
print(result)
# ä¼šæ£€ç´¢åˆ°ï¼š"I have a dog named Max"

# é—®å…³äºå·¥ä½œçš„é—®é¢˜
result = memory.load_memory_variables({"input": "What do I do for a living?"})
print(result)
# ä¼šæ£€ç´¢åˆ°ï¼š"I work as a software engineer"
```

### 3.5 RunnableWithMessageHistory - LCEL é›†æˆï¼ˆæ¨èï¼‰

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# 1. åˆ›å»ºèŠå¤©é“¾
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

chain = prompt | ChatOpenAI() | StrOutputParser()

# 2. ä¼šè¯å†å²å­˜å‚¨
store = {}  # session_id -> ChatMessageHistory

def get_session_history(session_id: str) -> ChatMessageHistory:
    """è·å–æˆ–åˆ›å»ºä¼šè¯å†å²"""
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# 3. åŒ…è£…é“¾ä»¥æ”¯æŒå†å²
chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# 4. ä½¿ç”¨ï¼ˆä¼ å…¥ session_idï¼‰
config = {"configurable": {"session_id": "user123"}}

response1 = chain_with_history.invoke(
    {"input": "Hi, I'm Alice"},
    config=config
)
print(response1)  # "Hello Alice! How can I help you?"

response2 = chain_with_history.invoke(
    {"input": "What's my name?"},
    config=config
)
print(response2)  # "Your name is Alice."

# 5. ä¸åŒä¼šè¯ç‹¬ç«‹è®°å¿†
config2 = {"configurable": {"session_id": "user456"}}

response3 = chain_with_history.invoke(
    {"input": "What's my name?"},
    config=config2
)
print(response3)  # "I don't know your name. Could you tell me?"
```

### 3.6 æŒä¹…åŒ–è®°å¿†

```python
# Redis æŒä¹…åŒ–
from langchain_community.chat_message_histories import RedisChatMessageHistory

history = RedisChatMessageHistory(
    session_id="user123",
    url="redis://localhost:6379"
)

chain_with_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: RedisChatMessageHistory(
        session_id=session_id,
        url="redis://localhost:6379"
    ),
    input_messages_key="input",
    history_messages_key="history"
)

# æ–‡ä»¶æŒä¹…åŒ–
from langchain_community.chat_message_histories import FileChatMessageHistory

def get_file_history(session_id: str):
    return FileChatMessageHistory(f"./chat_histories/{session_id}.json")

chain_with_history = RunnableWithMessageHistory(
    chain,
    get_file_history,
    input_messages_key="input",
    history_messages_key="history"
)
```

## 4. æ ¸å¿ƒæµç¨‹æ—¶åºå›¾

### 4.1 ConversationBufferMemory å·¥ä½œæµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User as ç”¨æˆ·
    participant Chain as ConversationChain
    participant Memory as ConversationBufferMemory
    participant History as ChatMessageHistory
    participant LLM as ChatModel

    User->>Chain: invoke("What's my name?")
    activate Chain

    Chain->>Memory: load_memory_variables({})
    activate Memory
    Memory->>History: get messages
    History-->>Memory: [msg1, msg2, ...]
    Memory->>Memory: æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    Memory-->>Chain: {"history": "..."}
    deactivate Memory

    Chain->>Chain: æ„å»ºæç¤ºè¯
    Note over Chain: System: ...<br/>History: ...<br/>Human: What's my name?

    Chain->>LLM: invoke(prompt)
    LLM-->>Chain: "Your name is Alice"

    Chain->>Memory: save_context(<br/>  {"input": "What's my name?"},<br/>  {"output": "Your name is Alice"}<br/>)
    activate Memory
    Memory->>History: add_user_message("What's my name?")
    Memory->>History: add_ai_message("Your name is Alice")
    deactivate Memory

    Chain-->>User: "Your name is Alice"
    deactivate Chain
```

### 4.2 ConversationSummaryMemory æ‘˜è¦æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant Chain as Chain
    participant Memory as ConversationSummaryMemory
    participant LLM as LLM (for summary)
    participant History as ChatMessageHistory

    Chain->>Memory: save_context(input, output)
    activate Memory

    Memory->>History: add messages
    History-->>Memory: ok

    Memory->>Memory: æ£€æŸ¥æ˜¯å¦éœ€è¦æ‘˜è¦
    Note over Memory: æ¶ˆæ¯æ•°é‡è¾¾åˆ°é˜ˆå€¼

    Memory->>LLM: ç”Ÿæˆæ‘˜è¦æç¤ºè¯
    Note over LLM: Current summary: ...<br/>New messages: ...<br/>Generate updated summary

    activate LLM
    LLM->>LLM: ç”Ÿæˆå‹ç¼©æ‘˜è¦
    LLM-->>Memory: "User is planning..."
    deactivate LLM

    Memory->>Memory: æ›´æ–°æ‘˜è¦ç¼“å†²
    Memory->>History: æ¸…é™¤å·²æ‘˜è¦çš„æ¶ˆæ¯

    Memory-->>Chain: ok
    deactivate Memory

    Note over Memory: ä¸‹æ¬¡åŠ è½½æ—¶è¿”å›æ‘˜è¦+æœ€è¿‘æ¶ˆæ¯
```

### 4.3 RunnableWithMessageHistory å®Œæ•´æµç¨‹

```mermaid
sequenceDiagram
    autonumber
    participant User as ç”¨æˆ·
    participant RWMH as RunnableWithMessageHistory
    participant Store as Session Store
    participant History as ChatMessageHistory
    participant Chain as Runnable Chain
    participant LLM as ChatModel

    User->>RWMH: invoke(<br/>  {"input": "Hi"},<br/>  config={"session_id": "user123"}<br/>)
    activate RWMH

    RWMH->>Store: get_session_history("user123")
    activate Store

    alt ä¼šè¯å­˜åœ¨
        Store-->>RWMH: existing_history
    else æ–°ä¼šè¯
        Store->>History: create new ChatMessageHistory()
        Store-->>RWMH: new_history
    end
    deactivate Store

    RWMH->>History: get messages
    History-->>RWMH: list[BaseMessage]

    RWMH->>RWMH: æ„å»ºè¾“å…¥
    Note over RWMH: {<br/>  "input": "Hi",<br/>  "history": [...]<br/>}

    RWMH->>Chain: invoke(input_with_history)
    activate Chain
    Chain->>LLM: process
    LLM-->>Chain: response
    Chain-->>RWMH: "Hello!"
    deactivate Chain

    RWMH->>History: add_user_message("Hi")
    RWMH->>History: add_ai_message("Hello!")

    RWMH-->>User: "Hello!"
    deactivate RWMH
```

## 5. æœ€ä½³å®è·µ

### 5.1 é€‰æ‹©åˆé€‚çš„è®°å¿†ç±»å‹

**çŸ­å¯¹è¯ï¼ˆ< 10è½®ï¼‰**: ConversationBufferMemory
```python
memory = ConversationBufferMemory(return_messages=True)
```

**ä¸€èˆ¬å¯¹è¯ï¼ˆ10-50è½®ï¼‰**: ConversationWindowMemory
```python
memory = ConversationWindowMemory(k=5)  # ä¿ç•™æœ€è¿‘5è½®
```

**é•¿å¯¹è¯ï¼ˆ> 50è½®ï¼‰**: ConversationSummaryBufferMemory
```python
memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=2000
)
```

**éçº¿æ€§å¯¹è¯ï¼ˆéœ€è¦æ£€ç´¢å†å²ï¼‰**: VectorStoreRetrieverMemory
```python
memory = VectorStoreRetrieverMemory(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
)
```

### 5.2 å¤šä¼šè¯ç®¡ç†

```python
from typing import Dict
from langchain_community.chat_message_histories import ChatMessageHistory

class SessionManager:
    """ä¼šè¯ç®¡ç†å™¨"""

    def __init__(self):
        self.sessions: Dict[str, ChatMessageHistory] = {}

    def get_history(self, session_id: str) -> ChatMessageHistory:
        """è·å–æˆ–åˆ›å»ºä¼šè¯"""
        if session_id not in self.sessions:
            self.sessions[session_id] = ChatMessageHistory()
        return self.sessions[session_id]

    def clear_session(self, session_id: str):
        """æ¸…é™¤ä¼šè¯"""
        if session_id in self.sessions:
            self.sessions[session_id].clear()

    def delete_session(self, session_id: str):
        """åˆ é™¤ä¼šè¯"""
        if session_id in self.sessions:
            del self.sessions[session_id]

# ä½¿ç”¨
manager = SessionManager()

chain_with_history = RunnableWithMessageHistory(
    chain,
    manager.get_history,
    input_messages_key="input",
    history_messages_key="history"
)
```

### 5.3 é™åˆ¶å†å²é•¿åº¦

```python
from langchain.memory import ConversationBufferMemory

class TruncatedBufferMemory(ConversationBufferMemory):
    """é™åˆ¶æ¶ˆæ¯æ•°é‡çš„ç¼“å†²è®°å¿†"""
    max_messages: int = 20

    def save_context(self, inputs: dict, outputs: dict) -> None:
        """ä¿å­˜å¹¶æˆªæ–­"""
        super().save_context(inputs, outputs)

        # æˆªæ–­åˆ°æœ€å¤§æ¶ˆæ¯æ•°
        messages = self.chat_memory.messages
        if len(messages) > self.max_messages:
            self.chat_memory.messages = messages[-self.max_messages:]

memory = TruncatedBufferMemory(max_messages=10)
```

### 5.4 è‡ªå®šä¹‰è®°å¿†æ ¼å¼

```python
from langchain.memory import ConversationBufferMemory

class CustomFormattedMemory(ConversationBufferMemory):
    """è‡ªå®šä¹‰æ ¼å¼åŒ–è®°å¿†"""

    def _get_buffer_string(self) -> str:
        """è‡ªå®šä¹‰æ ¼å¼"""
        messages = self.chat_memory.messages
        formatted = []

        for msg in messages:
            if msg.type == "human":
                formatted.append(f"ğŸ‘¤ User: {msg.content}")
            elif msg.type == "ai":
                formatted.append(f"ğŸ¤– Assistant: {msg.content}")

        return "\n".join(formatted)

memory = CustomFormattedMemory()
```

### 5.5 æ€§èƒ½ä¼˜åŒ–

**1. å¼‚æ­¥ä¿å­˜**:
```python
import asyncio

async def async_save_conversation(memory, inputs, outputs):
    """å¼‚æ­¥ä¿å­˜å¯¹è¯"""
    await asyncio.to_thread(
        memory.save_context,
        inputs,
        outputs
    )

# ä½¿ç”¨
asyncio.create_task(async_save_conversation(memory, inputs, outputs))
```

**2. æ‰¹é‡ä¿å­˜**:
```python
class BatchMemory:
    """æ‰¹é‡ä¿å­˜è®°å¿†"""

    def __init__(self, memory, batch_size=10):
        self.memory = memory
        self.batch_size = batch_size
        self.buffer = []

    def add(self, inputs, outputs):
        """æ·»åŠ åˆ°ç¼“å†²"""
        self.buffer.append((inputs, outputs))

        if len(self.buffer) >= self.batch_size:
            self.flush()

    def flush(self):
        """æ‰¹é‡ä¿å­˜"""
        for inputs, outputs in self.buffer:
            self.memory.save_context(inputs, outputs)
        self.buffer.clear()
```

**3. Redis è¿æ¥æ± **:
```python
import redis

# ä½¿ç”¨è¿æ¥æ± 
pool = redis.ConnectionPool(
    host='localhost',
    port=6379,
    max_connections=10
)

def get_redis_history(session_id: str):
    return RedisChatMessageHistory(
        session_id=session_id,
        url="redis://localhost:6379",
        ttl=3600  # 1å°æ—¶è¿‡æœŸ
    )
```

## 6. å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 6.1 Token æ¶ˆè€—è¿‡å¿«

**é—®é¢˜**: å¯¹è¯å†å²å¯¼è‡´ Token å¿«é€Ÿå¢é•¿

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ1: ä½¿ç”¨çª—å£è®°å¿†
memory = ConversationWindowMemory(k=3)

# æ–¹æ¡ˆ2: ä½¿ç”¨æ‘˜è¦è®°å¿†
memory = ConversationSummaryMemory(llm=llm)

# æ–¹æ¡ˆ3: è‡ªå®šä¹‰æˆªæ–­
class TokenLimitedMemory(ConversationBufferMemory):
    max_tokens: int = 1000

    def load_memory_variables(self, inputs: dict) -> dict:
        """é™åˆ¶Tokenæ•°é‡"""
        messages = self.chat_memory.messages
        total_tokens = 0
        truncated_messages = []

        # ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹
        for msg in reversed(messages):
            msg_tokens = len(msg.content.split())  # ç®€åŒ–è®¡ç®—
            if total_tokens + msg_tokens > self.max_tokens:
                break
            truncated_messages.insert(0, msg)
            total_tokens += msg_tokens

        self.chat_memory.messages = truncated_messages
        return super().load_memory_variables(inputs)
```

### 6.2 å¤šç”¨æˆ·å¹¶å‘

**é—®é¢˜**: å¤šä¸ªç”¨æˆ·åŒæ—¶è®¿é—®ï¼Œè®°å¿†æ··ä¹±

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„ä¼šè¯ç®¡ç†
from threading import Lock

class ThreadSafeSessionManager:
    def __init__(self):
        self.sessions = {}
        self.locks = {}
        self.global_lock = Lock()

    def get_history(self, session_id: str):
        with self.global_lock:
            if session_id not in self.locks:
                self.locks[session_id] = Lock()

        with self.locks[session_id]:
            if session_id not in self.sessions:
                self.sessions[session_id] = ChatMessageHistory()
            return self.sessions[session_id]
```

### 6.3 è®°å¿†æŒä¹…åŒ–å¤±è´¥

**é—®é¢˜**: Redis/æ•°æ®åº“è¿æ¥å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨å›é€€æœºåˆ¶
class FallbackMemory:
    def __init__(self, primary, fallback):
        self.primary = primary
        self.fallback = fallback

    def save_context(self, inputs, outputs):
        try:
            self.primary.save_context(inputs, outputs)
        except Exception as e:
            logger.warning(f"Primary storage failed: {e}, using fallback")
            self.fallback.save_context(inputs, outputs)

# ä½¿ç”¨
memory = FallbackMemory(
    primary=RedisChatMessageHistory(...),
    fallback=ChatMessageHistory()  # å†…å­˜å›é€€
)
```

## 7. ä¸å…¶ä»–æ¨¡å—çš„åä½œ

- **Prompts**: é€šè¿‡ MessagesPlaceholder æ³¨å…¥å†å²
- **Language Models**: æ¥æ”¶åŒ…å«å†å²çš„æç¤ºè¯
- **Chains**: ConversationChain é›†æˆè®°å¿†
- **Runnables**: RunnableWithMessageHistory æä¾› LCEL æ”¯æŒ
- **VectorStores**: VectorStoreBackedMemory ä½¿ç”¨å‘é‡æ£€ç´¢

## 8. æ€»ç»“

Memory æ¨¡å—ä¸º LangChain æä¾›äº†çµæ´»çš„å¯¹è¯è®°å¿†ç®¡ç†èƒ½åŠ›ã€‚å…³é”®ç‰¹æ€§ï¼š

1. **å¤šç§è®°å¿†ç­–ç•¥**: Bufferã€Windowã€Summaryã€Vector
2. **çµæ´»å­˜å‚¨**: å†…å­˜ã€Redisã€PostgreSQLã€æ–‡ä»¶
3. **LCEL é›†æˆ**: RunnableWithMessageHistory
4. **ä¼šè¯ç®¡ç†**: æ”¯æŒå¤šç”¨æˆ·å¤šä¼šè¯
5. **å¯æ‰©å±•**: æ˜“äºè‡ªå®šä¹‰è®°å¿†é€»è¾‘

**å…³é”®åŸåˆ™**:
- æ ¹æ®å¯¹è¯é•¿åº¦é€‰æ‹©åˆé€‚çš„è®°å¿†ç±»å‹
- ä½¿ç”¨ RunnableWithMessageHistoryï¼ˆLCELï¼‰è€Œéæ—§çš„ ConversationChain
- ç”Ÿäº§ç¯å¢ƒä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨ï¼ˆRedis/PostgreSQLï¼‰
- é™åˆ¶ Token æ¶ˆè€—ï¼ˆçª—å£/æ‘˜è¦ï¼‰
- å¤šç”¨æˆ·åœºæ™¯ä½¿ç”¨ session_id éš”ç¦»

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-03
**ç›¸å…³æ–‡æ¡£**:
- LangChain-00-æ€»è§ˆ.md
- LangChain-04-Prompts-æ¦‚è§ˆ.md
- LangChain-03-LanguageModels-æ¦‚è§ˆ.md

