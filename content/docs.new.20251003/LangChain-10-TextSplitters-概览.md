# LangChain-10-TextSplitters-æ¦‚è§ˆ

## æ¨¡å—åŸºæœ¬ä¿¡æ¯

**æ¨¡å—åç§°**: langchain-text-splitters
**æ¨¡å—è·¯å¾„**: `libs/text-splitters/langchain_text_splitters/`
**æ ¸å¿ƒèŒè´£**: å°†é•¿æ–‡æ¡£åˆ†å‰²æˆé€‚åˆ LLM å¤„ç†çš„å°å—ï¼ˆchunksï¼‰ï¼Œæ˜¯ RAG ç³»ç»Ÿçš„å…³é”®é¢„å¤„ç†æ­¥éª¤

## 1. æ¨¡å—èŒè´£

### 1.1 æ ¸å¿ƒèŒè´£

Text Splitters æ¨¡å—è´Ÿè´£æ™ºèƒ½åˆ†å‰²æ–‡æ¡£ï¼Œæä¾›ä»¥ä¸‹èƒ½åŠ›ï¼š

1. **æ–‡æ¡£åˆ†å—**: å°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå°å—
2. **ä¸Šä¸‹æ–‡ä¿ç•™**: é€šè¿‡é‡å ä¿ç•™å—é—´ä¸Šä¸‹æ–‡
3. **è¯­ä¹‰å®Œæ•´æ€§**: å°½é‡ä¿æŒå¥å­/æ®µè½å®Œæ•´
4. **å¤šç§ç­–ç•¥**: å­—ç¬¦ã€Tokenã€é€’å½’ã€è¯­ä¹‰ç­‰åˆ†å‰²æ–¹å¼
5. **å…ƒæ•°æ®ç®¡ç†**: ä¿ç•™å¹¶ä¼ é€’æ–‡æ¡£å…ƒæ•°æ®
6. **æ ¼å¼æ„ŸçŸ¥**: é’ˆå¯¹ç‰¹å®šæ ¼å¼ï¼ˆä»£ç ã€Markdownï¼‰ä¼˜åŒ–åˆ†å‰²

### 1.2 æ ¸å¿ƒæ¦‚å¿µ

```
é•¿æ–‡æ¡£ (10,000+ å­—ç¬¦)
  â†“
Text Splitter (åˆ†å‰²ç­–ç•¥)
  â†“
æ–‡æ¡£å—åˆ—è¡¨ (æ¯å— 500-1500 å­—ç¬¦)
  â†“
åµŒå…¥ â†’ å‘é‡å­˜å‚¨ â†’ æ£€ç´¢
```

**å…³é”®å‚æ•°**:
- **chunk_size**: æ¯ä¸ªå—çš„ç›®æ ‡å¤§å°ï¼ˆå­—ç¬¦æ•°æˆ– Token æ•°ï¼‰
- **chunk_overlap**: å—ä¹‹é—´çš„é‡å éƒ¨åˆ†ï¼ˆä¿ç•™ä¸Šä¸‹æ–‡ï¼‰
- **separators**: åˆ†éš”ç¬¦åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼‰
- **length_function**: è®¡ç®—é•¿åº¦çš„å‡½æ•°ï¼ˆå­—ç¬¦æ•°æˆ– Token æ•°ï¼‰

### 1.3 Text Splitter ç±»å‹å¯¹æ¯”

| Splitter ç±»å‹ | åˆ†å‰²å•ä½ | é€‚ç”¨åœºæ™¯ | ä¿æŒå®Œæ•´æ€§ | æ€§èƒ½ |
|--------------|---------|---------|----------|------|
| **CharacterTextSplitter** | å­—ç¬¦ | ç®€å•æ–‡æœ¬ | ä½ | âš¡ æœ€å¿« |
| **RecursiveCharacterTextSplitter** | å­—ç¬¦ï¼ˆé€’å½’ï¼‰ | é€šç”¨åœºæ™¯ | é«˜ | âš¡ å¿« |
| **TokenTextSplitter** | Token | Tokené™åˆ¶åœºæ™¯ | ä¸­ | ğŸ¢ è¾ƒæ…¢ |
| **SentenceTextSplitter** | å¥å­ | ä¿æŒå¥å­å®Œæ•´ | é«˜ | ğŸŒ æ…¢ |
| **MarkdownHeaderTextSplitter** | Markdownæ ‡é¢˜ | Markdownæ–‡æ¡£ | é«˜ | âš¡ å¿« |
| **CodeTextSplitter** | ä»£ç è¯­æ³• | æºä»£ç  | é«˜ | ğŸ¢ è¾ƒæ…¢ |
| **SemanticChunker** | è¯­ä¹‰ç›¸ä¼¼åº¦ | é«˜è´¨é‡å— | æœ€é«˜ | ğŸŒ æœ€æ…¢ |

### 1.4 è¾“å…¥/è¾“å‡º

**è¾“å…¥**:
- **texts**: `list[str]` - æ–‡æœ¬åˆ—è¡¨
- **documents**: `list[Document]` - æ–‡æ¡£åˆ—è¡¨

**è¾“å‡º**:
- `list[Document]` - åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨ï¼ˆåŒ…å« `page_content` å’Œ `metadata`ï¼‰

### 1.5 ä¸Šä¸‹æ¸¸ä¾èµ–

**ä¸Šæ¸¸è°ƒç”¨è€…**:
- Document Loadersï¼ˆåŠ è½½ååˆ†å‰²ï¼‰
- RAG åº”ç”¨ï¼ˆæ–‡æ¡£å…¥åº“å‰ï¼‰

**ä¸‹æ¸¸ä¾èµ–**:
- `langchain_core.documents`: Document ç±»
- `tiktoken` æˆ– `transformers`: Token è®¡æ•°

## 2. æ¨¡å—çº§æ¶æ„å›¾

```mermaid
flowchart TB
    subgraph Base["åŸºç¡€æŠ½è±¡å±‚"]
        BTS[TextSplitter<br/>æ–‡æœ¬åˆ†å‰²å™¨åŸºç±»]
    end

    subgraph Character["å­—ç¬¦çº§åˆ†å‰²"]
        CTS[CharacterTextSplitter<br/>ç®€å•å­—ç¬¦åˆ†å‰²]
        RCTS[RecursiveCharacterTextSplitter<br/>é€’å½’å­—ç¬¦åˆ†å‰²]
    end

    subgraph Token["Tokençº§åˆ†å‰²"]
        TTS[TokenTextSplitter<br/>Tokenåˆ†å‰²]
        SPACY[SpacyTextSplitter<br/>Spacyåˆ†å‰²]
    end

    subgraph Semantic["è¯­ä¹‰åˆ†å‰²"]
        SENT[SentenceTextSplitter<br/>å¥å­åˆ†å‰²]
        SEM[SemanticChunker<br/>è¯­ä¹‰å—åˆ†å‰²]
    end

    subgraph Format["æ ¼å¼æ„ŸçŸ¥åˆ†å‰²"]
        MD[MarkdownHeaderTextSplitter<br/>Markdownåˆ†å‰²]
        CODE[CodeTextSplitter<br/>ä»£ç åˆ†å‰²]
        HTML[HTMLHeaderTextSplitter<br/>HTMLåˆ†å‰²]
        LATEX[LatexTextSplitter<br/>LaTeXåˆ†å‰²]
    end

    subgraph Process["åˆ†å‰²æµç¨‹"]
        SPLIT[split_text<br/>åˆ†å‰²æ–‡æœ¬]
        CREATE[create_documents<br/>åˆ›å»ºæ–‡æ¡£]
        MERGE[merge_splits<br/>åˆå¹¶å—]
    end

    BTS --> CTS
    BTS --> RCTS
    BTS --> TTS
    BTS --> SPACY
    BTS --> SENT
    BTS --> SEM
    BTS --> MD
    BTS --> CODE
    BTS --> HTML
    BTS --> LATEX

    BTS --> SPLIT
    BTS --> CREATE
    BTS --> MERGE

    style Base fill:#e1f5ff
    style Character fill:#fff4e1
    style Token fill:#e8f5e9
    style Semantic fill:#f3e5f5
    style Format fill:#fff3e0
    style Process fill:#ffe4e1
```

### æ¶æ„å›¾è¯¦ç»†è¯´æ˜

**1. åŸºç¡€æŠ½è±¡å±‚**

- **TextSplitter**: æ‰€æœ‰åˆ†å‰²å™¨çš„åŸºç±»
  ```python
  class TextSplitter(ABC):
      chunk_size: int = 4000  # å—å¤§å°
      chunk_overlap: int = 200  # é‡å å¤§å°
      length_function: Callable[[str], int] = len  # é•¿åº¦å‡½æ•°
      keep_separator: bool = False  # æ˜¯å¦ä¿ç•™åˆ†éš”ç¬¦
      add_start_index: bool = False  # æ˜¯å¦æ·»åŠ èµ·å§‹ç´¢å¼•

      @abstractmethod
      def split_text(self, text: str) -> list[str]:
          """åˆ†å‰²æ–‡æœ¬ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨"""

      def create_documents(
          self,
          texts: list[str],
          metadatas: Optional[list[dict]] = None
      ) -> list[Document]:
          """åˆ›å»ºæ–‡æ¡£å¯¹è±¡"""

      def split_documents(self, documents: list[Document]) -> list[Document]:
          """åˆ†å‰²æ–‡æ¡£åˆ—è¡¨"""
  ```

**2. å­—ç¬¦çº§åˆ†å‰²**

- **CharacterTextSplitter**: ç®€å•å­—ç¬¦åˆ†å‰²
  - æŒ‰å•ä¸ªåˆ†éš”ç¬¦åˆ†å‰²
  - æœ€ç®€å•ä½†å¯èƒ½ç ´åè¯­ä¹‰

  ```python
  splitter = CharacterTextSplitter(
      separator="\n\n",  # æŒ‰æ®µè½åˆ†å‰²
      chunk_size=1000,
      chunk_overlap=200
  )
  ```

- **RecursiveCharacterTextSplitter**: é€’å½’åˆ†å‰²ï¼ˆæ¨èï¼‰
  - æŒ‰ä¼˜å…ˆçº§å°è¯•å¤šä¸ªåˆ†éš”ç¬¦
  - é»˜è®¤é¡ºåºï¼š`\n\n` â†’ `\n` â†’ ` ` â†’ ``
  - æœ€å¤§åŒ–ä¿æŒæ®µè½å’Œå¥å­å®Œæ•´

  ```python
  splitter = RecursiveCharacterTextSplitter(
      chunk_size=1000,
      chunk_overlap=200,
      separators=["\n\n", "\n", " ", ""]
  )
  ```

**3. Token çº§åˆ†å‰²**

- **TokenTextSplitter**: åŸºäº Token åˆ†å‰²
  - ä½¿ç”¨ `tiktoken` è®¡ç®— Token
  - ç²¾ç¡®æ§åˆ¶ LLM Token é™åˆ¶

  ```python
  from langchain.text_splitter import TokenTextSplitter

  splitter = TokenTextSplitter(
      chunk_size=512,  # Token æ•°é‡
      chunk_overlap=50,
      encoding_name="cl100k_base"  # GPT-4 ç¼–ç 
  )
  ```

- **SpacyTextSplitter**: ä½¿ç”¨ Spacy NLP
  - åŸºäº Spacy çš„å¥å­åˆ†å‰²
  - ä¿æŒå¥å­å®Œæ•´æ€§

**4. è¯­ä¹‰åˆ†å‰²**

- **SentenceTextSplitter**: å¥å­çº§åˆ†å‰²
  - ä¸ä¼šåœ¨å¥å­ä¸­é—´åˆ‡æ–­
  - ä¿æŒè¯­ä¹‰å®Œæ•´

- **SemanticChunker**: è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å‰²
  - ä½¿ç”¨åµŒå…¥æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦
  - æ ¹æ®è¯­ä¹‰è¾¹ç•Œåˆ†å‰²
  - æœ€é«˜è´¨é‡ä½†æœ€æ…¢

**5. æ ¼å¼æ„ŸçŸ¥åˆ†å‰²**

- **MarkdownHeaderTextSplitter**: Markdown åˆ†å‰²
  - æŒ‰æ ‡é¢˜å±‚çº§åˆ†å‰²
  - ä¿ç•™æ ‡é¢˜å±‚çº§ä¿¡æ¯

  ```python
  from langchain.text_splitter import MarkdownHeaderTextSplitter

  headers_to_split_on = [
      ("#", "Header 1"),
      ("##", "Header 2"),
      ("###", "Header 3"),
  ]

  splitter = MarkdownHeaderTextSplitter(
      headers_to_split_on=headers_to_split_on
  )
  ```

- **CodeTextSplitter**: ä»£ç åˆ†å‰²
  - æŒ‰ç¼–ç¨‹è¯­è¨€è¯­æ³•åˆ†å‰²
  - æ”¯æŒ Pythonã€JavaScriptã€Java ç­‰
  - ä¿æŒå‡½æ•°/ç±»å®Œæ•´æ€§

  ```python
  from langchain.text_splitter import RecursiveCharacterTextSplitter

  python_splitter = RecursiveCharacterTextSplitter.from_language(
      language=Language.PYTHON,
      chunk_size=500,
      chunk_overlap=50
  )
  ```

- **HTMLHeaderTextSplitter**: HTML åˆ†å‰²
  - æŒ‰ HTML æ ‡ç­¾åˆ†å‰²
  - ä¿ç•™ç»“æ„ä¿¡æ¯

- **LatexTextSplitter**: LaTeX åˆ†å‰²
  - è¯†åˆ« LaTeX ç»“æ„
  - ä¿æŒå…¬å¼å®Œæ•´

**6. åˆ†å‰²æµç¨‹**

- **split_text**: æ ¸å¿ƒåˆ†å‰²é€»è¾‘
  - é€’å½’å°è¯•åˆ†éš”ç¬¦
  - åˆå¹¶å°å—
  - æ§åˆ¶å—å¤§å°

- **create_documents**: åˆ›å»º Document å¯¹è±¡
  - æ·»åŠ å…ƒæ•°æ®
  - æ·»åŠ èµ·å§‹ç´¢å¼•ï¼ˆå¯é€‰ï¼‰

- **merge_splits**: åˆå¹¶å—
  - åˆå¹¶è¿‡å°çš„å—
  - ä¿æŒé‡å 

## 3. æ ¸å¿ƒ API è¯¦è§£

### 3.1 RecursiveCharacterTextSplitter - æ¨èä½¿ç”¨

**æ ¸å¿ƒä»£ç **:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

class RecursiveCharacterTextSplitter(TextSplitter):
    separators: list[str] = ["\n\n", "\n", " ", ""]

    def split_text(self, text: str) -> list[str]:
        """
        é€’å½’åˆ†å‰²æ–‡æœ¬

        1. å°è¯•ç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦åˆ†å‰²
        2. æ£€æŸ¥æ¯ä¸ªå—å¤§å°
        3. å¦‚æœå—å¤ªå¤§ï¼Œé€’å½’ä½¿ç”¨ä¸‹ä¸€ä¸ªåˆ†éš”ç¬¦
        4. åˆå¹¶å°å—
        """
        final_chunks = []
        separator = self.separators[-1]
        new_separators = []

        # æ‰¾åˆ°æœ‰æ•ˆçš„åˆ†éš”ç¬¦
        for i, _s in enumerate(self.separators):
            if _s == "":
                separator = _s
                break
            if _s in text:
                separator = _s
                new_separators = self.separators[i + 1:]
                break

        # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²
        splits = text.split(separator)

        # å¤„ç†æ¯ä¸ªåˆ†å‰²
        good_splits = []
        for s in splits:
            if self._length_function(s) < self.chunk_size:
                good_splits.append(s)
            else:
                # é€’å½’åˆ†å‰²å¤§å—
                if good_splits:
                    merged = self._merge_splits(good_splits, separator)
                    final_chunks.extend(merged)
                    good_splits = []

                # ä½¿ç”¨ä¸‹ä¸€ä¸ªåˆ†éš”ç¬¦
                if new_separators:
                    other_splits = self._split_text(s, new_separators)
                    final_chunks.extend(other_splits)
                else:
                    # å¼ºåˆ¶åˆ†å‰²
                    final_chunks.append(s)

        # åˆå¹¶å‰©ä½™çš„å°å—
        if good_splits:
            merged = self._merge_splits(good_splits, separator)
            final_chunks.extend(merged)

        return final_chunks

    def _merge_splits(
        self,
        splits: list[str],
        separator: str
    ) -> list[str]:
        """
        åˆå¹¶å°å—å¹¶ä¿æŒé‡å 
        """
        docs = []
        current_doc = []
        total = 0

        for d in splits:
            _len = self._length_function(d)
            if total + _len >= self.chunk_size:
                if total > self.chunk_size:
                    # è­¦å‘Šï¼šå—è¿‡å¤§
                    pass
                if len(current_doc) > 0:
                    doc = separator.join(current_doc)
                    docs.append(doc)

                    # ä¿æŒé‡å 
                    while total > self.chunk_overlap or (
                        total + _len > self.chunk_size and total > 0
                    ):
                        total -= self._length_function(current_doc[0])
                        current_doc = current_doc[1:]

            current_doc.append(d)
            total += _len

        # æ·»åŠ æœ€åä¸€ä¸ªæ–‡æ¡£
        if current_doc:
            doc = separator.join(current_doc)
            docs.append(doc)

        return docs
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åŸºç¡€ç”¨æ³•
text = """
# Introduction

LangChain is a framework for building applications with large language models.

## Features

- LCEL (LangChain Expression Language)
- Agents and Tools
- Memory Management
- RAG (Retrieval Augmented Generation)

## Getting Started

First, install LangChain:
```bash
pip install langchain
```

Then import and use:
```python
from langchain import OpenAI
llm = OpenAI()
```
"""

splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50,
    length_function=len,
    is_separator_regex=False
)

chunks = splitter.split_text(text)
print(f"åˆ†å‰²æˆ {len(chunks)} ä¸ªå—")

for i, chunk in enumerate(chunks):
    print(f"\nå— {i+1} ({len(chunk)} å­—ç¬¦):")
    print(chunk[:100] + "...")

# ä»æ–‡æ¡£åˆ†å‰²
from langchain_core.documents import Document

documents = [
    Document(
        page_content=text,
        metadata={"source": "langchain_intro.md"}
    )
]

split_docs = splitter.split_documents(documents)
print(f"\nåˆ†å‰²æˆ {len(split_docs)} ä¸ªæ–‡æ¡£")

for doc in split_docs:
    print(f"Metadata: {doc.metadata}")
    print(f"Content: {doc.page_content[:100]}...")
```

### 3.2 TokenTextSplitter - Token ç²¾ç¡®æ§åˆ¶

```python
from langchain.text_splitter import TokenTextSplitter

# åŸºäº Token åˆ†å‰²
splitter = TokenTextSplitter(
    chunk_size=512,  # Token æ•°é‡
    chunk_overlap=50,
    encoding_name="cl100k_base"  # GPT-4 ç¼–ç 
)

text = "..." * 10000  # é•¿æ–‡æœ¬

chunks = splitter.split_text(text)

# éªŒè¯ Token æ•°é‡
import tiktoken

enc = tiktoken.get_encoding("cl100k_base")
for i, chunk in enumerate(chunks):
    token_count = len(enc.encode(chunk))
    print(f"å— {i+1}: {token_count} tokens")
    assert token_count <= 512  # ç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
```

### 3.3 MarkdownHeaderTextSplitter - Markdown ç»“æ„åŒ–åˆ†å‰²

```python
from langchain.text_splitter import MarkdownHeaderTextSplitter

markdown_text = """
# Main Title

This is the introduction.

## Section 1

Content of section 1.

### Subsection 1.1

Details of subsection 1.1.

### Subsection 1.2

Details of subsection 1.2.

## Section 2

Content of section 2.
"""

# å®šä¹‰è¦åˆ†å‰²çš„æ ‡é¢˜å±‚çº§
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)

docs = splitter.split_text(markdown_text)

for doc in docs:
    print(f"Content: {doc.page_content}")
    print(f"Metadata: {doc.metadata}\n")
    # Metadata åŒ…å«æ ‡é¢˜å±‚çº§:
    # {"Header 1": "Main Title", "Header 2": "Section 1", "Header 3": "Subsection 1.1"}
```

### 3.4 CodeTextSplitter - ä»£ç åˆ†å‰²

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter, Language

# Python ä»£ç åˆ†å‰²
python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=500,
    chunk_overlap=50
)

python_code = """
def hello_world():
    \"\"\"æ‰“å° Hello World\"\"\"
    print("Hello World")

class Calculator:
    \"\"\"ç®€å•è®¡ç®—å™¨\"\"\"

    def add(self, a, b):
        return a + b

    def subtract(self, a, b):
        return a - b

if __name__ == "__main__":
    calc = Calculator()
    print(calc.add(1, 2))
"""

chunks = python_splitter.split_text(python_code)

# ä»£ç åˆ†å‰²å™¨ä¼šå°è¯•ä¿æŒå‡½æ•°/ç±»å®Œæ•´

# JavaScript ä»£ç åˆ†å‰²
js_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.JS,
    chunk_size=500,
    chunk_overlap=50
)

# æ”¯æŒçš„è¯­è¨€ï¼š
# Language.PYTHON, Language.JS, Language.JAVA, Language.CPP,
# Language.GO, Language.RUST, Language.MARKDOWN, Language.HTML, etc.
```

### 3.5 SemanticChunker - è¯­ä¹‰åˆ†å‰²ï¼ˆé«˜è´¨é‡ï¼‰

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å‰²
embeddings = OpenAIEmbeddings()

semantic_chunker = SemanticChunker(
    embeddings=embeddings,
    breakpoint_threshold_type="percentile",  # æˆ– "standard_deviation", "interquartile"
    breakpoint_threshold_amount=95  # ç™¾åˆ†ä½æ•°
)

text = """
LangChain is a framework for building LLM applications.
It provides tools for prompts, chains, and agents.

RAG is a technique that combines retrieval and generation.
It retrieves relevant documents and uses them to generate answers.

Vector stores are used to store document embeddings.
Popular options include FAISS, Chroma, and Pinecone.
"""

chunks = semantic_chunker.create_documents([text])

# SemanticChunker ä¼šåœ¨è¯­ä¹‰è¾¹ç•Œå¤„åˆ†å‰²
# ä¾‹å¦‚ï¼Œå°† LangChain ç›¸å…³å†…å®¹åˆ†ä¸ºä¸€å—ï¼ŒRAG ç›¸å…³å†…å®¹åˆ†ä¸ºå¦ä¸€å—
```

### 3.6 è‡ªå®šä¹‰ Text Splitter

```python
from langchain.text_splitter import TextSplitter

class CustomSentenceSplitter(TextSplitter):
    """è‡ªå®šä¹‰å¥å­åˆ†å‰²å™¨"""

    def split_text(self, text: str) -> list[str]:
        """
        æŒ‰å¥å­åˆ†å‰²ï¼Œä¿æŒ chunk_size é™åˆ¶
        """
        import re

        # ç®€å•çš„å¥å­åˆ†å‰²ï¼ˆå®é™…åº”ç”¨å¯ä½¿ç”¨ nltk æˆ– spacyï¼‰
        sentences = re.split(r'(?<=[.!?])\s+', text)

        chunks = []
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            sentence_length = len(sentence)

            if current_length + sentence_length > self.chunk_size:
                # å¼€å§‹æ–°å—
                if current_chunk:
                    chunks.append(" ".join(current_chunk))

                    # ä¿æŒé‡å 
                    overlap_sentences = []
                    overlap_length = 0
                    for s in reversed(current_chunk):
                        overlap_length += len(s)
                        if overlap_length > self.chunk_overlap:
                            break
                        overlap_sentences.insert(0, s)

                    current_chunk = overlap_sentences
                    current_length = overlap_length

            current_chunk.append(sentence)
            current_length += sentence_length

        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(" ".join(current_chunk))

        return chunks

# ä½¿ç”¨
splitter = CustomSentenceSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_text(long_text)
```

## 4. æœ€ä½³å®è·µ

### 4.1 é€‰æ‹©åˆé€‚çš„ chunk_size

```python
# ä¸€èˆ¬å»ºè®®
# å°å— (200-500): ç²¾ç¡®æ£€ç´¢ï¼Œä½†å¯èƒ½ç¼ºä¹ä¸Šä¸‹æ–‡
# ä¸­å— (500-1500): å¹³è¡¡ç²¾åº¦å’Œä¸Šä¸‹æ–‡ï¼ˆæ¨èï¼‰
# å¤§å— (1500-3000): æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†æ£€ç´¢ç²¾åº¦é™ä½

# æ ¹æ®ç”¨ä¾‹è°ƒæ•´
qa_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,  # é—®ç­”ï¼šä¸­ç­‰å—
    chunk_overlap=200
)

summarization_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,  # æ‘˜è¦ï¼šå¤§å—
    chunk_overlap=500
)

search_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,  # æœç´¢ï¼šå°å—
    chunk_overlap=100
)
```

### 4.2 åˆç†è®¾ç½® chunk_overlap

```python
# chunk_overlap = 10-20% of chunk_sizeï¼ˆæ¨èï¼‰

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200  # 20%
)

# ä¸ºä»€ä¹ˆéœ€è¦é‡å ï¼Ÿ
# 1. é¿å…åœ¨å…³é”®ä¿¡æ¯å¤„åˆ‡æ–­
# 2. æä¾›è·¨å—çš„ä¸Šä¸‹æ–‡è¿ç»­æ€§
# 3. æé«˜æ£€ç´¢å¬å›ç‡
```

### 4.3 æ·»åŠ å…ƒæ•°æ®

```python
from langchain_core.documents import Document

documents = [
    Document(
        page_content=chunk,
        metadata={
            "source": "langchain_docs.pdf",
            "page": 5,
            "chunk_id": i,
            "total_chunks": len(chunks),
            "language": "en"
        }
    )
    for i, chunk in enumerate(chunks)
]

# å…ƒæ•°æ®å¯ç”¨äºè¿‡æ»¤å’Œè¿½æº¯
```

### 4.4 æ·»åŠ èµ·å§‹ç´¢å¼•

```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    add_start_index=True  # æ·»åŠ èµ·å§‹ç´¢å¼•
)

docs = splitter.create_documents([text])

for doc in docs:
    print(doc.metadata)
    # {"start_index": 0}  # å—åœ¨åŸå§‹æ–‡æ¡£ä¸­çš„èµ·å§‹ä½ç½®
```

### 4.5 ç»„åˆå¤šä¸ªåˆ†å‰²å™¨

```python
# å…ˆæŒ‰ Markdown æ ‡é¢˜åˆ†å‰²ï¼Œå†æŒ‰å­—ç¬¦åˆ†å‰²
md_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[("#", "Header 1"), ("##", "Header 2")]
)

char_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

# ä¸¤é˜¶æ®µåˆ†å‰²
md_docs = md_splitter.split_text(markdown_text)
final_docs = char_splitter.split_documents(md_docs)
```

### 4.6 æ€§èƒ½ä¼˜åŒ–

```python
# æ‰¹é‡å¤„ç†
texts = [doc.page_content for doc in documents]
chunks = splitter.create_documents(
    texts,
    metadatas=[doc.metadata for doc in documents]
)

# å¹¶è¡Œå¤„ç†ï¼ˆå¤§é‡æ–‡æ¡£ï¼‰
from multiprocessing import Pool

def split_batch(args):
    splitter, texts, metadatas = args
    return splitter.create_documents(texts, metadatas)

with Pool(4) as pool:
    results = pool.map(split_batch, batches)
```

## 5. å¸¸è§åœºæ™¯å’Œè§£å†³æ–¹æ¡ˆ

### 5.1 ä»£ç æ–‡æ¡£åˆ†å‰²

```python
# ç»“åˆä»£ç å’Œ Markdown
splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.MARKDOWN,
    chunk_size=1000,
    chunk_overlap=200
)

# Markdown ä¸­çš„ä»£ç å—ä¼šè¢«è¯†åˆ«å¹¶ä¿æŒå®Œæ•´
```

### 5.2 å¤šè¯­è¨€æ–‡æ¡£

```python
# ä¸­æ–‡æ–‡æ¡£
chinese_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # ä¸­æ–‡å­—ç¬¦æ•°è¾ƒå°‘
    chunk_overlap=100,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
)

# å¤šè¯­è¨€æ··åˆ
multilingual_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150,
    separators=["\n\n", "\n", ". ", "ã€‚", " ", ""]
)
```

### 5.3 PDF æ–‡æ¡£åˆ†å‰²

```python
from langchain_community.document_loaders import PyPDFLoader

# åŠ è½½ PDF
loader = PyPDFLoader("document.pdf")
pages = loader.load()

# åˆ†å‰²ï¼ˆä¿ç•™é¡µç ä¿¡æ¯ï¼‰
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

chunks = splitter.split_documents(pages)

# æ¯ä¸ªå—éƒ½åŒ…å«åŸå§‹é¡µç 
for chunk in chunks:
    print(f"Page {chunk.metadata['page']}: {chunk.page_content[:100]}...")
```

### 5.4 é•¿ä»£ç æ–‡ä»¶

```python
# æŒ‰å‡½æ•°/ç±»åˆ†å‰²
python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=1500,  # ä»£ç éœ€è¦æ›´å¤§çš„å—
    chunk_overlap=200
)

# ä»£ç å—ä¼šå°½é‡ä¿æŒå‡½æ•°/ç±»å®Œæ•´
```

## 6. ä¸å…¶ä»–æ¨¡å—çš„åä½œ

- **Document Loaders**: åŠ è½½æ–‡æ¡£ååˆ†å‰²
- **VectorStores**: åˆ†å‰²åçš„å—å­˜å…¥å‘é‡å­˜å‚¨
- **Embeddings**: æ¯ä¸ªå—ç”ŸæˆåµŒå…¥
- **Retrievers**: æ£€ç´¢åˆ†å‰²åçš„å—

## 7. å®Œæ•´å·¥ä½œæµç¤ºä¾‹

```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# 1. åŠ è½½æ–‡æ¡£
loader = TextLoader("article.txt")
documents = loader.load()

# 2. åˆ†å‰²æ–‡æ¡£
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    add_start_index=True
)

texts = text_splitter.split_documents(documents)
print(f"åˆ†å‰²æˆ {len(texts)} ä¸ªå—")

# 3. åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(texts, embeddings)

# 4. æ£€ç´¢
query = "What is the main topic?"
results = vectorstore.similarity_search(query, k=3)

for result in results:
    print(f"Source: {result.metadata['source']}")
    print(f"Start index: {result.metadata['start_index']}")
    print(f"Content: {result.page_content}\n")
```

## 8. æ€»ç»“

Text Splitters æ˜¯ RAG ç³»ç»Ÿçš„å…³é”®é¢„å¤„ç†ç»„ä»¶ï¼Œæä¾›æ™ºèƒ½æ–‡æ¡£åˆ†å‰²èƒ½åŠ›ã€‚å…³é”®ç‰¹æ€§ï¼š

1. **å¤šç§åˆ†å‰²ç­–ç•¥**: å­—ç¬¦ã€Tokenã€è¯­ä¹‰ã€æ ¼å¼æ„ŸçŸ¥
2. **ä¸Šä¸‹æ–‡ä¿ç•™**: é€šè¿‡é‡å ä¿æŒè¿ç»­æ€§
3. **è¯­ä¹‰å®Œæ•´æ€§**: å°½é‡ä¿æŒå¥å­/æ®µè½å®Œæ•´
4. **å…ƒæ•°æ®ç®¡ç†**: ä¿ç•™æ¥æºå’Œä½ç½®ä¿¡æ¯
5. **æ ¼å¼æ„ŸçŸ¥**: é’ˆå¯¹ Markdownã€ä»£ç ç­‰ä¼˜åŒ–

**å…³é”®åŸåˆ™**:
- ä¼˜å…ˆä½¿ç”¨ **RecursiveCharacterTextSplitter**ï¼ˆé€šç”¨åœºæ™¯ï¼‰
- **chunk_size**: 500-1500 å­—ç¬¦ï¼ˆæ¨èï¼‰
- **chunk_overlap**: chunk_size çš„ 10-20%
- ç‰¹æ®Šæ ¼å¼ä½¿ç”¨ä¸“ç”¨åˆ†å‰²å™¨ï¼ˆMarkdownã€Codeï¼‰
- æ·»åŠ èµ·å§‹ç´¢å¼•å’Œä¸°å¯Œå…ƒæ•°æ®
- å…ˆæŒ‰ç»“æ„åˆ†å‰²ï¼Œå†æŒ‰å¤§å°åˆ†å‰²

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-03
**ç›¸å…³æ–‡æ¡£**:
- LangChain-00-æ€»è§ˆ.md
- LangChain-08-VectorStores-Retrievers-æ¦‚è§ˆ.md
- LangChain-11-DocumentLoaders-æ¦‚è§ˆ.mdï¼ˆå¾…ç”Ÿæˆï¼‰

